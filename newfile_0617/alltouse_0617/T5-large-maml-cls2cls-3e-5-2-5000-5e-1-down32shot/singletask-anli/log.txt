05/21/2022 21:20:41 - INFO - __main__ - Namespace(task_dir='data_32/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:20:41 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli
05/21/2022 21:20:41 - INFO - __main__ - Namespace(task_dir='data_32/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:20:41 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli
05/21/2022 21:20:43 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:20:43 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:20:43 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:20:43 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:20:43 - INFO - __main__ - Using 2 gpus
05/21/2022 21:20:43 - INFO - __main__ - Using 2 gpus
05/21/2022 21:20:43 - INFO - __main__ - Fine-tuning the following samples: ['anli_32_100', 'anli_32_13', 'anli_32_21', 'anli_32_42', 'anli_32_87']
05/21/2022 21:20:43 - INFO - __main__ - Fine-tuning the following samples: ['anli_32_100', 'anli_32_13', 'anli_32_21', 'anli_32_42', 'anli_32_87']
05/21/2022 21:20:48 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.5, bsz=8 ...
06/12/2022 09:03:02 - INFO - __main__ - Namespace(task_dir='data_32/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/12/2022 09:03:02 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli
06/12/2022 09:03:02 - INFO - __main__ - Namespace(task_dir='data_32/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/12/2022 09:03:02 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli
06/12/2022 09:03:03 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/12/2022 09:03:03 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/12/2022 09:03:03 - INFO - __main__ - args.device: cuda:0
06/12/2022 09:03:03 - INFO - __main__ - args.device: cuda:1
06/12/2022 09:03:03 - INFO - __main__ - Using 2 gpus
06/12/2022 09:03:03 - INFO - __main__ - Using 2 gpus
06/12/2022 09:03:03 - INFO - __main__ - Fine-tuning the following samples: ['anli_32_100', 'anli_32_13', 'anli_32_21', 'anli_32_42', 'anli_32_87']
06/12/2022 09:03:03 - INFO - __main__ - Fine-tuning the following samples: ['anli_32_100', 'anli_32_13', 'anli_32_21', 'anli_32_42', 'anli_32_87']
06/12/2022 09:03:08 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.5, bsz=8 ...
06/12/2022 09:03:08 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:03:08 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:03:08 - INFO - __main__ - Printing 3 examples
06/12/2022 09:03:08 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/12/2022 09:03:08 - INFO - __main__ - Printing 3 examples
06/12/2022 09:03:08 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/12/2022 09:03:09 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/12/2022 09:03:09 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/12/2022 09:03:09 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/12/2022 09:03:09 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:03:09 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/12/2022 09:03:09 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:03:09 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:03:09 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:03:09 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 09:03:09 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:03:09 - INFO - __main__ - Printing 3 examples
06/12/2022 09:03:09 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
06/12/2022 09:03:09 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 09:03:09 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
06/12/2022 09:03:09 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:03:09 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
06/12/2022 09:03:09 - INFO - __main__ - Printing 3 examples
06/12/2022 09:03:09 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
06/12/2022 09:03:09 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:03:09 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
06/12/2022 09:03:09 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
06/12/2022 09:03:09 - INFO - __main__ - ['neutral']
06/12/2022 09:03:09 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:03:09 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:03:09 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:03:09 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 09:03:09 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 09:03:27 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 09:03:27 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 09:03:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 09:03:28 - INFO - __main__ - Starting training!
06/12/2022 09:03:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 09:03:33 - INFO - __main__ - Starting training!
06/12/2022 09:03:37 - INFO - __main__ - Step 10 Global step 10 Train loss 0.86 on epoch=1
06/12/2022 09:03:40 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=3
06/12/2022 09:03:43 - INFO - __main__ - Step 30 Global step 30 Train loss 0.59 on epoch=4
06/12/2022 09:03:46 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=6
06/12/2022 09:03:48 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=8
06/12/2022 09:03:50 - INFO - __main__ - Global step 50 Train loss 0.62 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 09:03:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 09:03:53 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=9
06/12/2022 09:03:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=11
06/12/2022 09:03:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=13
06/12/2022 09:04:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=14
06/12/2022 09:04:04 - INFO - __main__ - Step 100 Global step 100 Train loss 0.89 on epoch=16
06/12/2022 09:04:07 - INFO - __main__ - Global step 100 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 09:04:10 - INFO - __main__ - Step 110 Global step 110 Train loss 1.30 on epoch=18
06/12/2022 09:04:12 - INFO - __main__ - Step 120 Global step 120 Train loss 2.18 on epoch=19
06/12/2022 09:04:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.76 on epoch=21
06/12/2022 09:04:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=23
06/12/2022 09:04:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=24
06/12/2022 09:04:24 - INFO - __main__ - Global step 150 Train loss 1.03 Classification-F1 0.2467307692307692 on epoch=24
06/12/2022 09:04:24 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2467307692307692 on epoch=24, global_step=150
06/12/2022 09:04:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=26
06/12/2022 09:04:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=28
06/12/2022 09:04:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=29
06/12/2022 09:04:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=31
06/12/2022 09:04:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=33
06/12/2022 09:04:40 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16402116402116398 on epoch=33
06/12/2022 09:04:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=34
06/12/2022 09:04:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=36
06/12/2022 09:04:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=38
06/12/2022 09:04:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=39
06/12/2022 09:04:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=41
06/12/2022 09:04:56 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 09:04:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=43
06/12/2022 09:05:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=44
06/12/2022 09:05:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=46
06/12/2022 09:05:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.64 on epoch=48
06/12/2022 09:05:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.68 on epoch=49
06/12/2022 09:05:12 - INFO - __main__ - Global step 300 Train loss 0.56 Classification-F1 0.2333333333333333 on epoch=49
06/12/2022 09:05:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.76 on epoch=51
06/12/2022 09:05:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=53
06/12/2022 09:05:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=54
06/12/2022 09:05:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=56
06/12/2022 09:05:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.60 on epoch=58
06/12/2022 09:05:29 - INFO - __main__ - Global step 350 Train loss 0.62 Classification-F1 0.16666666666666666 on epoch=58
06/12/2022 09:05:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=59
06/12/2022 09:05:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.60 on epoch=61
06/12/2022 09:05:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.59 on epoch=63
06/12/2022 09:05:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.57 on epoch=64
06/12/2022 09:05:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.58 on epoch=66
06/12/2022 09:05:46 - INFO - __main__ - Global step 400 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 09:05:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.56 on epoch=68
06/12/2022 09:05:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=69
06/12/2022 09:05:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=71
06/12/2022 09:05:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=73
06/12/2022 09:06:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=74
06/12/2022 09:06:03 - INFO - __main__ - Global step 450 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=74
06/12/2022 09:06:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=76
06/12/2022 09:06:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.56 on epoch=78
06/12/2022 09:06:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.59 on epoch=79
06/12/2022 09:06:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=81
06/12/2022 09:06:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=83
06/12/2022 09:06:19 - INFO - __main__ - Global step 500 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=83
06/12/2022 09:06:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=84
06/12/2022 09:06:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=86
06/12/2022 09:06:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=88
06/12/2022 09:06:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=89
06/12/2022 09:06:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=91
06/12/2022 09:06:35 - INFO - __main__ - Global step 550 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 09:06:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=93
06/12/2022 09:06:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.53 on epoch=94
06/12/2022 09:06:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.59 on epoch=96
06/12/2022 09:06:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=98
06/12/2022 09:06:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=99
06/12/2022 09:06:51 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=99
06/12/2022 09:06:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=101
06/12/2022 09:06:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.50 on epoch=103
06/12/2022 09:06:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=104
06/12/2022 09:07:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=106
06/12/2022 09:07:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=108
06/12/2022 09:07:07 - INFO - __main__ - Global step 650 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=108
06/12/2022 09:07:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=109
06/12/2022 09:07:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.56 on epoch=111
06/12/2022 09:07:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=113
06/12/2022 09:07:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=114
06/12/2022 09:07:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=116
06/12/2022 09:07:24 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 09:07:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=118
06/12/2022 09:07:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=119
06/12/2022 09:07:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=121
06/12/2022 09:07:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=123
06/12/2022 09:07:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=124
06/12/2022 09:07:39 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=124
06/12/2022 09:07:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=126
06/12/2022 09:07:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=128
06/12/2022 09:07:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=129
06/12/2022 09:07:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=131
06/12/2022 09:07:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=133
06/12/2022 09:07:55 - INFO - __main__ - Global step 800 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=133
06/12/2022 09:07:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=134
06/12/2022 09:08:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=136
06/12/2022 09:08:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.51 on epoch=138
06/12/2022 09:08:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=139
06/12/2022 09:08:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=141
06/12/2022 09:08:11 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=141
06/12/2022 09:08:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=143
06/12/2022 09:08:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=144
06/12/2022 09:08:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=146
06/12/2022 09:08:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.50 on epoch=148
06/12/2022 09:08:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=149
06/12/2022 09:08:28 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=149
06/12/2022 09:08:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.49 on epoch=151
06/12/2022 09:08:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=153
06/12/2022 09:08:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=154
06/12/2022 09:08:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=156
06/12/2022 09:08:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=158
06/12/2022 09:08:44 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=158
06/12/2022 09:08:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=159
06/12/2022 09:08:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=161
06/12/2022 09:08:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=163
06/12/2022 09:08:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=164
06/12/2022 09:08:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=166
06/12/2022 09:09:00 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=166
06/12/2022 09:09:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=168
06/12/2022 09:09:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=169
06/12/2022 09:09:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.53 on epoch=171
06/12/2022 09:09:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=173
06/12/2022 09:09:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=174
06/12/2022 09:09:16 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=174
06/12/2022 09:09:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=176
06/12/2022 09:09:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=178
06/12/2022 09:09:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.45 on epoch=179
06/12/2022 09:09:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=181
06/12/2022 09:09:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=183
06/12/2022 09:09:32 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=183
06/12/2022 09:09:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=184
06/12/2022 09:09:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=186
06/12/2022 09:09:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=188
06/12/2022 09:09:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=189
06/12/2022 09:09:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=191
06/12/2022 09:09:49 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=191
06/12/2022 09:09:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=193
06/12/2022 09:09:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=194
06/12/2022 09:09:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=196
06/12/2022 09:09:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=198
06/12/2022 09:10:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=199
06/12/2022 09:10:05 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=199
06/12/2022 09:10:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=201
06/12/2022 09:10:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.47 on epoch=203
06/12/2022 09:10:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=204
06/12/2022 09:10:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=206
06/12/2022 09:10:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=208
06/12/2022 09:10:21 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=208
06/12/2022 09:10:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=209
06/12/2022 09:10:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=211
06/12/2022 09:10:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=213
06/12/2022 09:10:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.46 on epoch=214
06/12/2022 09:10:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=216
06/12/2022 09:10:37 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=216
06/12/2022 09:10:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=218
06/12/2022 09:10:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=219
06/12/2022 09:10:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=221
06/12/2022 09:10:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=223
06/12/2022 09:10:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=224
06/12/2022 09:10:53 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=224
06/12/2022 09:10:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=226
06/12/2022 09:10:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=228
06/12/2022 09:11:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=229
06/12/2022 09:11:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.45 on epoch=231
06/12/2022 09:11:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=233
06/12/2022 09:11:10 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=233
06/12/2022 09:11:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=234
06/12/2022 09:11:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=236
06/12/2022 09:11:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=238
06/12/2022 09:11:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.46 on epoch=239
06/12/2022 09:11:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=241
06/12/2022 09:11:26 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=241
06/12/2022 09:11:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=243
06/12/2022 09:11:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=244
06/12/2022 09:11:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=246
06/12/2022 09:11:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=248
06/12/2022 09:11:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.47 on epoch=249
06/12/2022 09:11:42 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.16402116402116398 on epoch=249
06/12/2022 09:11:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=251
06/12/2022 09:11:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=253
06/12/2022 09:11:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=254
06/12/2022 09:11:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=256
06/12/2022 09:11:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=258
06/12/2022 09:11:58 - INFO - __main__ - Global step 1550 Train loss 0.44 Classification-F1 0.18333333333333335 on epoch=258
06/12/2022 09:12:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=259
06/12/2022 09:12:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=261
06/12/2022 09:12:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=263
06/12/2022 09:12:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=264
06/12/2022 09:12:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.46 on epoch=266
06/12/2022 09:12:15 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=266
06/12/2022 09:12:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=268
06/12/2022 09:12:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=269
06/12/2022 09:12:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=271
06/12/2022 09:12:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=273
06/12/2022 09:12:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=274
06/12/2022 09:12:31 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.1722879904698087 on epoch=274
06/12/2022 09:12:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=276
06/12/2022 09:12:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=278
06/12/2022 09:12:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=279
06/12/2022 09:12:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=281
06/12/2022 09:12:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=283
06/12/2022 09:12:48 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.18892001244942422 on epoch=283
06/12/2022 09:12:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=284
06/12/2022 09:12:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=286
06/12/2022 09:12:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=288
06/12/2022 09:12:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=289
06/12/2022 09:13:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=291
06/12/2022 09:13:05 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=291
06/12/2022 09:13:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=293
06/12/2022 09:13:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=294
06/12/2022 09:13:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=296
06/12/2022 09:13:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=298
06/12/2022 09:13:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=299
06/12/2022 09:13:21 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=299
06/12/2022 09:13:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=301
06/12/2022 09:13:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=303
06/12/2022 09:13:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=304
06/12/2022 09:13:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=306
06/12/2022 09:13:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=308
06/12/2022 09:13:38 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.1679790026246719 on epoch=308
06/12/2022 09:13:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.48 on epoch=309
06/12/2022 09:13:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=311
06/12/2022 09:13:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.44 on epoch=313
06/12/2022 09:13:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=314
06/12/2022 09:13:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=316
06/12/2022 09:13:54 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=316
06/12/2022 09:13:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=318
06/12/2022 09:14:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=319
06/12/2022 09:14:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.43 on epoch=321
06/12/2022 09:14:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=323
06/12/2022 09:14:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=324
06/12/2022 09:14:11 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=324
06/12/2022 09:14:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=326
06/12/2022 09:14:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=328
06/12/2022 09:14:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=329
06/12/2022 09:14:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=331
06/12/2022 09:14:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.43 on epoch=333
06/12/2022 09:14:27 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=333
06/12/2022 09:14:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.47 on epoch=334
06/12/2022 09:14:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.44 on epoch=336
06/12/2022 09:14:34 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.43 on epoch=338
06/12/2022 09:14:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=339
06/12/2022 09:14:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.43 on epoch=341
06/12/2022 09:14:43 - INFO - __main__ - Global step 2050 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=341
06/12/2022 09:14:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.41 on epoch=343
06/12/2022 09:14:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.44 on epoch=344
06/12/2022 09:14:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.43 on epoch=346
06/12/2022 09:14:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.45 on epoch=348
06/12/2022 09:14:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.46 on epoch=349
06/12/2022 09:14:59 - INFO - __main__ - Global step 2100 Train loss 0.44 Classification-F1 0.25599813650128117 on epoch=349
06/12/2022 09:14:59 - INFO - __main__ - Saving model with best Classification-F1: 0.2467307692307692 -> 0.25599813650128117 on epoch=349, global_step=2100
06/12/2022 09:15:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.41 on epoch=351
06/12/2022 09:15:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=353
06/12/2022 09:15:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.40 on epoch=354
06/12/2022 09:15:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.42 on epoch=356
06/12/2022 09:15:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.43 on epoch=358
06/12/2022 09:15:15 - INFO - __main__ - Global step 2150 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=358
06/12/2022 09:15:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=359
06/12/2022 09:15:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=361
06/12/2022 09:15:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.41 on epoch=363
06/12/2022 09:15:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=364
06/12/2022 09:15:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.62 on epoch=366
06/12/2022 09:15:32 - INFO - __main__ - Global step 2200 Train loss 0.46 Classification-F1 0.3357268389146628 on epoch=366
06/12/2022 09:15:32 - INFO - __main__ - Saving model with best Classification-F1: 0.25599813650128117 -> 0.3357268389146628 on epoch=366, global_step=2200
06/12/2022 09:15:34 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.62 on epoch=368
06/12/2022 09:15:37 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.43 on epoch=369
06/12/2022 09:15:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.42 on epoch=371
06/12/2022 09:15:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=373
06/12/2022 09:15:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.47 on epoch=374
06/12/2022 09:15:48 - INFO - __main__ - Global step 2250 Train loss 0.47 Classification-F1 0.20004099200655875 on epoch=374
06/12/2022 09:15:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.44 on epoch=376
06/12/2022 09:15:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.42 on epoch=378
06/12/2022 09:15:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.42 on epoch=379
06/12/2022 09:15:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.43 on epoch=381
06/12/2022 09:16:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.43 on epoch=383
06/12/2022 09:16:05 - INFO - __main__ - Global step 2300 Train loss 0.43 Classification-F1 0.24160933917031477 on epoch=383
06/12/2022 09:16:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.42 on epoch=384
06/12/2022 09:16:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.44 on epoch=386
06/12/2022 09:16:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.40 on epoch=388
06/12/2022 09:16:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.41 on epoch=389
06/12/2022 09:16:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.47 on epoch=391
06/12/2022 09:16:22 - INFO - __main__ - Global step 2350 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=391
06/12/2022 09:16:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.42 on epoch=393
06/12/2022 09:16:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.36 on epoch=394
06/12/2022 09:16:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.43 on epoch=396
06/12/2022 09:16:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.43 on epoch=398
06/12/2022 09:16:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.40 on epoch=399
06/12/2022 09:16:38 - INFO - __main__ - Global step 2400 Train loss 0.41 Classification-F1 0.1744324970131422 on epoch=399
06/12/2022 09:16:41 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.44 on epoch=401
06/12/2022 09:16:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.39 on epoch=403
06/12/2022 09:16:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.41 on epoch=404
06/12/2022 09:16:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.44 on epoch=406
06/12/2022 09:16:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.40 on epoch=408
06/12/2022 09:16:55 - INFO - __main__ - Global step 2450 Train loss 0.41 Classification-F1 0.2221131293296242 on epoch=408
06/12/2022 09:16:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.46 on epoch=409
06/12/2022 09:17:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.42 on epoch=411
06/12/2022 09:17:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.41 on epoch=413
06/12/2022 09:17:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=414
06/12/2022 09:17:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.41 on epoch=416
06/12/2022 09:17:12 - INFO - __main__ - Global step 2500 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=416
06/12/2022 09:17:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.41 on epoch=418
06/12/2022 09:17:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.44 on epoch=419
06/12/2022 09:17:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.42 on epoch=421
06/12/2022 09:17:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.39 on epoch=423
06/12/2022 09:17:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.43 on epoch=424
06/12/2022 09:17:29 - INFO - __main__ - Global step 2550 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=424
06/12/2022 09:17:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.43 on epoch=426
06/12/2022 09:17:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.43 on epoch=428
06/12/2022 09:17:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.44 on epoch=429
06/12/2022 09:17:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.46 on epoch=431
06/12/2022 09:17:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.42 on epoch=433
06/12/2022 09:17:45 - INFO - __main__ - Global step 2600 Train loss 0.44 Classification-F1 0.23078656750056345 on epoch=433
06/12/2022 09:17:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.42 on epoch=434
06/12/2022 09:17:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.39 on epoch=436
06/12/2022 09:17:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.41 on epoch=438
06/12/2022 09:17:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=439
06/12/2022 09:17:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.45 on epoch=441
06/12/2022 09:18:02 - INFO - __main__ - Global step 2650 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=441
06/12/2022 09:18:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.40 on epoch=443
06/12/2022 09:18:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.39 on epoch=444
06/12/2022 09:18:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.44 on epoch=446
06/12/2022 09:18:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.42 on epoch=448
06/12/2022 09:18:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.42 on epoch=449
06/12/2022 09:18:18 - INFO - __main__ - Global step 2700 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=449
06/12/2022 09:18:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.40 on epoch=451
06/12/2022 09:18:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.41 on epoch=453
06/12/2022 09:18:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.42 on epoch=454
06/12/2022 09:18:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.40 on epoch=456
06/12/2022 09:18:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.43 on epoch=458
06/12/2022 09:18:35 - INFO - __main__ - Global step 2750 Train loss 0.41 Classification-F1 0.24468460832097194 on epoch=458
06/12/2022 09:18:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.43 on epoch=459
06/12/2022 09:18:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.43 on epoch=461
06/12/2022 09:18:43 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.42 on epoch=463
06/12/2022 09:18:46 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.43 on epoch=464
06/12/2022 09:18:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.41 on epoch=466
06/12/2022 09:18:51 - INFO - __main__ - Global step 2800 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=466
06/12/2022 09:18:54 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=468
06/12/2022 09:18:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.43 on epoch=469
06/12/2022 09:19:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.42 on epoch=471
06/12/2022 09:19:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.43 on epoch=473
06/12/2022 09:19:05 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.42 on epoch=474
06/12/2022 09:19:08 - INFO - __main__ - Global step 2850 Train loss 0.42 Classification-F1 0.15873015873015875 on epoch=474
06/12/2022 09:19:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.41 on epoch=476
06/12/2022 09:19:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.42 on epoch=478
06/12/2022 09:19:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.43 on epoch=479
06/12/2022 09:19:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.40 on epoch=481
06/12/2022 09:19:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.40 on epoch=483
06/12/2022 09:19:25 - INFO - __main__ - Global step 2900 Train loss 0.41 Classification-F1 0.2887587698475497 on epoch=483
06/12/2022 09:19:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.40 on epoch=484
06/12/2022 09:19:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.45 on epoch=486
06/12/2022 09:19:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.42 on epoch=488
06/12/2022 09:19:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.42 on epoch=489
06/12/2022 09:19:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.39 on epoch=491
06/12/2022 09:19:42 - INFO - __main__ - Global step 2950 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=491
06/12/2022 09:19:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.40 on epoch=493
06/12/2022 09:19:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.41 on epoch=494
06/12/2022 09:19:50 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.49 on epoch=496
06/12/2022 09:19:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.44 on epoch=498
06/12/2022 09:19:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.45 on epoch=499
06/12/2022 09:19:57 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:19:57 - INFO - __main__ - Printing 3 examples
06/12/2022 09:19:57 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/12/2022 09:19:57 - INFO - __main__ - ['neutral']
06/12/2022 09:19:57 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/12/2022 09:19:57 - INFO - __main__ - ['neutral']
06/12/2022 09:19:57 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/12/2022 09:19:57 - INFO - __main__ - ['neutral']
06/12/2022 09:19:57 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:19:57 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:19:57 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 09:19:57 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:19:57 - INFO - __main__ - Printing 3 examples
06/12/2022 09:19:57 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
06/12/2022 09:19:57 - INFO - __main__ - ['neutral']
06/12/2022 09:19:57 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
06/12/2022 09:19:57 - INFO - __main__ - ['neutral']
06/12/2022 09:19:57 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
06/12/2022 09:19:57 - INFO - __main__ - ['neutral']
06/12/2022 09:19:57 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:19:57 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:19:57 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 09:19:58 - INFO - __main__ - Global step 3000 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=499
06/12/2022 09:19:58 - INFO - __main__ - save last model!
06/12/2022 09:19:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 09:19:58 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 09:19:58 - INFO - __main__ - Printing 3 examples
06/12/2022 09:19:58 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 09:19:58 - INFO - __main__ - ['contradiction']
06/12/2022 09:19:58 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 09:19:58 - INFO - __main__ - ['entailment']
06/12/2022 09:19:58 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 09:19:58 - INFO - __main__ - ['contradiction']
06/12/2022 09:19:58 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:19:59 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:20:00 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 09:20:13 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 09:20:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 09:20:14 - INFO - __main__ - Starting training!
06/12/2022 09:20:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_100_0.5_8_predictions.txt
06/12/2022 09:20:29 - INFO - __main__ - Classification-F1 on test data: 0.1665
06/12/2022 09:20:29 - INFO - __main__ - prefix=anli_32_100, lr=0.5, bsz=8, dev_performance=0.3357268389146628, test_performance=0.16654163540885222
06/12/2022 09:20:29 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.4, bsz=8 ...
06/12/2022 09:20:30 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:20:30 - INFO - __main__ - Printing 3 examples
06/12/2022 09:20:30 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/12/2022 09:20:30 - INFO - __main__ - ['neutral']
06/12/2022 09:20:30 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/12/2022 09:20:30 - INFO - __main__ - ['neutral']
06/12/2022 09:20:30 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/12/2022 09:20:30 - INFO - __main__ - ['neutral']
06/12/2022 09:20:30 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:20:30 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:20:30 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 09:20:30 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:20:30 - INFO - __main__ - Printing 3 examples
06/12/2022 09:20:30 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
06/12/2022 09:20:30 - INFO - __main__ - ['neutral']
06/12/2022 09:20:30 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
06/12/2022 09:20:30 - INFO - __main__ - ['neutral']
06/12/2022 09:20:30 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
06/12/2022 09:20:30 - INFO - __main__ - ['neutral']
06/12/2022 09:20:30 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:20:30 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:20:30 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 09:20:49 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 09:20:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 09:20:49 - INFO - __main__ - Starting training!
06/12/2022 09:20:53 - INFO - __main__ - Step 10 Global step 10 Train loss 0.88 on epoch=1
06/12/2022 09:20:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.58 on epoch=3
06/12/2022 09:20:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.65 on epoch=4
06/12/2022 09:21:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=6
06/12/2022 09:21:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=8
06/12/2022 09:21:06 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 09:21:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 09:21:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=9
06/12/2022 09:21:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=11
06/12/2022 09:21:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.53 on epoch=13
06/12/2022 09:21:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=14
06/12/2022 09:21:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=16
06/12/2022 09:21:22 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 09:21:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=18
06/12/2022 09:21:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=19
06/12/2022 09:21:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=21
06/12/2022 09:21:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=23
06/12/2022 09:21:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=24
06/12/2022 09:21:39 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 09:21:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=26
06/12/2022 09:21:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=28
06/12/2022 09:21:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=29
06/12/2022 09:21:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=31
06/12/2022 09:21:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=33
06/12/2022 09:21:56 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 09:21:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=34
06/12/2022 09:22:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=36
06/12/2022 09:22:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=38
06/12/2022 09:22:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=39
06/12/2022 09:22:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=41
06/12/2022 09:22:12 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 09:22:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=43
06/12/2022 09:22:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=44
06/12/2022 09:22:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=46
06/12/2022 09:22:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=48
06/12/2022 09:22:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=49
06/12/2022 09:22:29 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=49
06/12/2022 09:22:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=51
06/12/2022 09:22:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=53
06/12/2022 09:22:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=54
06/12/2022 09:22:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=56
06/12/2022 09:22:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=58
06/12/2022 09:22:46 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=58
06/12/2022 09:22:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=59
06/12/2022 09:22:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=61
06/12/2022 09:22:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=63
06/12/2022 09:22:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=64
06/12/2022 09:22:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=66
06/12/2022 09:23:02 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 09:23:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=68
06/12/2022 09:23:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=69
06/12/2022 09:23:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=71
06/12/2022 09:23:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=73
06/12/2022 09:23:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=74
06/12/2022 09:23:19 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.2629940865234983 on epoch=74
06/12/2022 09:23:19 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2629940865234983 on epoch=74, global_step=450
06/12/2022 09:23:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=76
06/12/2022 09:23:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=78
06/12/2022 09:23:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=79
06/12/2022 09:23:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=81
06/12/2022 09:23:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=83
06/12/2022 09:23:36 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=83
06/12/2022 09:23:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=84
06/12/2022 09:23:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=86
06/12/2022 09:23:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=88
06/12/2022 09:23:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=89
06/12/2022 09:23:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=91
06/12/2022 09:23:53 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 09:23:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=93
06/12/2022 09:23:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=94
06/12/2022 09:24:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=96
06/12/2022 09:24:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=98
06/12/2022 09:24:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=99
06/12/2022 09:24:10 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.16533333333333333 on epoch=99
06/12/2022 09:24:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=101
06/12/2022 09:24:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=103
06/12/2022 09:24:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=104
06/12/2022 09:24:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=106
06/12/2022 09:24:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=108
06/12/2022 09:24:27 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=108
06/12/2022 09:24:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=109
06/12/2022 09:24:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=111
06/12/2022 09:24:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=113
06/12/2022 09:24:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=114
06/12/2022 09:24:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=116
06/12/2022 09:24:43 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 09:24:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=118
06/12/2022 09:24:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=119
06/12/2022 09:24:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=121
06/12/2022 09:24:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=123
06/12/2022 09:24:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=124
06/12/2022 09:25:00 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.2489725286610426 on epoch=124
06/12/2022 09:25:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=126
06/12/2022 09:25:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=128
06/12/2022 09:25:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=129
06/12/2022 09:25:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=131
06/12/2022 09:25:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=133
06/12/2022 09:25:17 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=133
06/12/2022 09:25:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=134
06/12/2022 09:25:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=136
06/12/2022 09:25:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=138
06/12/2022 09:25:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=139
06/12/2022 09:25:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=141
06/12/2022 09:25:34 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=141
06/12/2022 09:25:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=143
06/12/2022 09:25:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=144
06/12/2022 09:25:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=146
06/12/2022 09:25:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=148
06/12/2022 09:25:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=149
06/12/2022 09:25:51 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.1843809523809524 on epoch=149
06/12/2022 09:25:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=151
06/12/2022 09:25:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=153
06/12/2022 09:25:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=154
06/12/2022 09:26:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=156
06/12/2022 09:26:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=158
06/12/2022 09:26:07 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.34310672893962985 on epoch=158
06/12/2022 09:26:08 - INFO - __main__ - Saving model with best Classification-F1: 0.2629940865234983 -> 0.34310672893962985 on epoch=158, global_step=950
06/12/2022 09:26:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=159
06/12/2022 09:26:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=161
06/12/2022 09:26:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=163
06/12/2022 09:26:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=164
06/12/2022 09:26:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=166
06/12/2022 09:26:24 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=166
06/12/2022 09:26:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=168
06/12/2022 09:26:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=169
06/12/2022 09:26:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=171
06/12/2022 09:26:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=173
06/12/2022 09:26:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=174
06/12/2022 09:26:42 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.3917301917301918 on epoch=174
06/12/2022 09:26:42 - INFO - __main__ - Saving model with best Classification-F1: 0.34310672893962985 -> 0.3917301917301918 on epoch=174, global_step=1050
06/12/2022 09:26:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=176
06/12/2022 09:26:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=178
06/12/2022 09:26:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.34 on epoch=179
06/12/2022 09:26:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=181
06/12/2022 09:26:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=183
06/12/2022 09:26:59 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.36289855072463767 on epoch=183
06/12/2022 09:27:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=184
06/12/2022 09:27:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=186
06/12/2022 09:27:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=188
06/12/2022 09:27:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=189
06/12/2022 09:27:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=191
06/12/2022 09:27:15 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.27847439916405436 on epoch=191
06/12/2022 09:27:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=193
06/12/2022 09:27:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=194
06/12/2022 09:27:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=196
06/12/2022 09:27:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=198
06/12/2022 09:27:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=199
06/12/2022 09:27:32 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.35443338074917025 on epoch=199
06/12/2022 09:27:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=201
06/12/2022 09:27:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=203
06/12/2022 09:27:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=204
06/12/2022 09:27:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=206
06/12/2022 09:27:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=208
06/12/2022 09:27:49 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.3564495530012772 on epoch=208
06/12/2022 09:27:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=209
06/12/2022 09:27:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=211
06/12/2022 09:27:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=213
06/12/2022 09:28:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.32 on epoch=214
06/12/2022 09:28:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.30 on epoch=216
06/12/2022 09:28:06 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.3011429450884142 on epoch=216
06/12/2022 09:28:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=218
06/12/2022 09:28:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=219
06/12/2022 09:28:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=221
06/12/2022 09:28:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=223
06/12/2022 09:28:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=224
06/12/2022 09:28:23 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.28772378516624036 on epoch=224
06/12/2022 09:28:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=226
06/12/2022 09:28:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=228
06/12/2022 09:28:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=229
06/12/2022 09:28:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.32 on epoch=231
06/12/2022 09:28:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.33 on epoch=233
06/12/2022 09:28:40 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.2579262007833436 on epoch=233
06/12/2022 09:28:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=234
06/12/2022 09:28:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=236
06/12/2022 09:28:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=238
06/12/2022 09:28:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=239
06/12/2022 09:28:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.30 on epoch=241
06/12/2022 09:28:56 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.2642558777897123 on epoch=241
06/12/2022 09:28:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=243
06/12/2022 09:29:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=244
06/12/2022 09:29:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=246
06/12/2022 09:29:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=248
06/12/2022 09:29:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=249
06/12/2022 09:29:13 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.25402763734411865 on epoch=249
06/12/2022 09:29:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=251
06/12/2022 09:29:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.26 on epoch=253
06/12/2022 09:29:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=254
06/12/2022 09:29:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.29 on epoch=256
06/12/2022 09:29:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=258
06/12/2022 09:29:30 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.297470820252757 on epoch=258
06/12/2022 09:29:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.29 on epoch=259
06/12/2022 09:29:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=261
06/12/2022 09:29:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=263
06/12/2022 09:29:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=264
06/12/2022 09:29:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=266
06/12/2022 09:29:47 - INFO - __main__ - Global step 1600 Train loss 0.25 Classification-F1 0.26693512654103785 on epoch=266
06/12/2022 09:29:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.30 on epoch=268
06/12/2022 09:29:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.27 on epoch=269
06/12/2022 09:29:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=271
06/12/2022 09:29:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=273
06/12/2022 09:30:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=274
06/12/2022 09:30:03 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.27197973970565825 on epoch=274
06/12/2022 09:30:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.30 on epoch=276
06/12/2022 09:30:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=278
06/12/2022 09:30:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=279
06/12/2022 09:30:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=281
06/12/2022 09:30:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.24 on epoch=283
06/12/2022 09:30:20 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.3092668226558984 on epoch=283
06/12/2022 09:30:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=284
06/12/2022 09:30:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=286
06/12/2022 09:30:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=288
06/12/2022 09:30:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=289
06/12/2022 09:30:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.28 on epoch=291
06/12/2022 09:30:37 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.3383802884840667 on epoch=291
06/12/2022 09:30:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.27 on epoch=293
06/12/2022 09:30:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=294
06/12/2022 09:30:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=296
06/12/2022 09:30:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=298
06/12/2022 09:30:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=299
06/12/2022 09:30:54 - INFO - __main__ - Global step 1800 Train loss 0.27 Classification-F1 0.21268985252036102 on epoch=299
06/12/2022 09:30:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=301
06/12/2022 09:31:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=303
06/12/2022 09:31:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=304
06/12/2022 09:31:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=306
06/12/2022 09:31:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=308
06/12/2022 09:31:11 - INFO - __main__ - Global step 1850 Train loss 0.22 Classification-F1 0.28658515731874146 on epoch=308
06/12/2022 09:31:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=309
06/12/2022 09:31:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=311
06/12/2022 09:31:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=313
06/12/2022 09:31:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=314
06/12/2022 09:31:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=316
06/12/2022 09:31:28 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.24955015744489428 on epoch=316
06/12/2022 09:31:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=318
06/12/2022 09:31:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=319
06/12/2022 09:31:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=321
06/12/2022 09:31:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=323
06/12/2022 09:31:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=324
06/12/2022 09:31:45 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.24630838436808586 on epoch=324
06/12/2022 09:31:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=326
06/12/2022 09:31:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.15 on epoch=328
06/12/2022 09:31:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=329
06/12/2022 09:31:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=331
06/12/2022 09:31:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=333
06/12/2022 09:32:02 - INFO - __main__ - Global step 2000 Train loss 0.17 Classification-F1 0.10238595438175269 on epoch=333
06/12/2022 09:32:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.18 on epoch=334
06/12/2022 09:32:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=336
06/12/2022 09:32:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=338
06/12/2022 09:32:14 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=339
06/12/2022 09:32:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=341
06/12/2022 09:32:20 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.09738319271068221 on epoch=341
06/12/2022 09:32:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.16 on epoch=343
06/12/2022 09:32:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.16 on epoch=344
06/12/2022 09:32:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.21 on epoch=346
06/12/2022 09:32:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=348
06/12/2022 09:32:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.14 on epoch=349
06/12/2022 09:32:37 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.1385951168559864 on epoch=349
06/12/2022 09:32:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.10 on epoch=351
06/12/2022 09:32:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=353
06/12/2022 09:32:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=354
06/12/2022 09:32:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.14 on epoch=356
06/12/2022 09:32:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=358
06/12/2022 09:32:54 - INFO - __main__ - Global step 2150 Train loss 0.12 Classification-F1 0.2141471048513302 on epoch=358
06/12/2022 09:32:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=359
06/12/2022 09:32:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=361
06/12/2022 09:33:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=363
06/12/2022 09:33:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=364
06/12/2022 09:33:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=366
06/12/2022 09:33:11 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.3241088350816487 on epoch=366
06/12/2022 09:33:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.15 on epoch=368
06/12/2022 09:33:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=369
06/12/2022 09:33:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=371
06/12/2022 09:33:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=373
06/12/2022 09:33:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=374
06/12/2022 09:33:28 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.20045751633986927 on epoch=374
06/12/2022 09:33:30 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=376
06/12/2022 09:33:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=378
06/12/2022 09:33:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=379
06/12/2022 09:33:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=381
06/12/2022 09:33:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=383
06/12/2022 09:33:45 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.12284878863826232 on epoch=383
06/12/2022 09:33:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=384
06/12/2022 09:33:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=386
06/12/2022 09:33:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.13 on epoch=388
06/12/2022 09:33:56 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=389
06/12/2022 09:33:59 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=391
06/12/2022 09:34:02 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.14427521406380237 on epoch=391
06/12/2022 09:34:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=393
06/12/2022 09:34:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=394
06/12/2022 09:34:11 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=396
06/12/2022 09:34:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=398
06/12/2022 09:34:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=399
06/12/2022 09:34:19 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.11818691259199857 on epoch=399
06/12/2022 09:34:22 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=401
06/12/2022 09:34:25 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=403
06/12/2022 09:34:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=404
06/12/2022 09:34:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=406
06/12/2022 09:34:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=408
06/12/2022 09:34:36 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.09241109468741848 on epoch=408
06/12/2022 09:34:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=409
06/12/2022 09:34:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=411
06/12/2022 09:34:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=413
06/12/2022 09:34:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=414
06/12/2022 09:34:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=416
06/12/2022 09:34:54 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.09806757474042763 on epoch=416
06/12/2022 09:34:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=418
06/12/2022 09:34:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=419
06/12/2022 09:35:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=421
06/12/2022 09:35:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=423
06/12/2022 09:35:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=424
06/12/2022 09:35:11 - INFO - __main__ - Global step 2550 Train loss 0.08 Classification-F1 0.14109977324263037 on epoch=424
06/12/2022 09:35:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=426
06/12/2022 09:35:16 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=428
06/12/2022 09:35:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=429
06/12/2022 09:35:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=431
06/12/2022 09:35:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=433
06/12/2022 09:35:28 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.06697062173735602 on epoch=433
06/12/2022 09:35:30 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=434
06/12/2022 09:35:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=436
06/12/2022 09:35:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=438
06/12/2022 09:35:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=439
06/12/2022 09:35:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=441
06/12/2022 09:35:45 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.15347037825100673 on epoch=441
06/12/2022 09:35:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=443
06/12/2022 09:35:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.07 on epoch=444
06/12/2022 09:35:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=446
06/12/2022 09:35:56 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=448
06/12/2022 09:35:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=449
06/12/2022 09:36:02 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.12922504756962463 on epoch=449
06/12/2022 09:36:05 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=451
06/12/2022 09:36:08 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=453
06/12/2022 09:36:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=454
06/12/2022 09:36:13 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=456
06/12/2022 09:36:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=458
06/12/2022 09:36:19 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.23148634453781514 on epoch=458
06/12/2022 09:36:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=459
06/12/2022 09:36:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=461
06/12/2022 09:36:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=463
06/12/2022 09:36:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=464
06/12/2022 09:36:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=466
06/12/2022 09:36:36 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.14107936507936508 on epoch=466
06/12/2022 09:36:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=468
06/12/2022 09:36:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=469
06/12/2022 09:36:45 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=471
06/12/2022 09:36:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=473
06/12/2022 09:36:50 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=474
06/12/2022 09:36:53 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.18805889795149264 on epoch=474
06/12/2022 09:36:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=476
06/12/2022 09:36:59 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=478
06/12/2022 09:37:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.09 on epoch=479
06/12/2022 09:37:04 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=481
06/12/2022 09:37:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=483
06/12/2022 09:37:10 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.12782429925287067 on epoch=483
06/12/2022 09:37:13 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=484
06/12/2022 09:37:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=486
06/12/2022 09:37:18 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=488
06/12/2022 09:37:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=489
06/12/2022 09:37:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=491
06/12/2022 09:37:27 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.19395646606914213 on epoch=491
06/12/2022 09:37:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=493
06/12/2022 09:37:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=494
06/12/2022 09:37:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=496
06/12/2022 09:37:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=498
06/12/2022 09:37:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=499
06/12/2022 09:37:42 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:37:42 - INFO - __main__ - Printing 3 examples
06/12/2022 09:37:42 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/12/2022 09:37:42 - INFO - __main__ - ['neutral']
06/12/2022 09:37:42 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/12/2022 09:37:42 - INFO - __main__ - ['neutral']
06/12/2022 09:37:42 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/12/2022 09:37:42 - INFO - __main__ - ['neutral']
06/12/2022 09:37:42 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:37:43 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:37:43 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 09:37:43 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:37:43 - INFO - __main__ - Printing 3 examples
06/12/2022 09:37:43 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
06/12/2022 09:37:43 - INFO - __main__ - ['neutral']
06/12/2022 09:37:43 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
06/12/2022 09:37:43 - INFO - __main__ - ['neutral']
06/12/2022 09:37:43 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
06/12/2022 09:37:43 - INFO - __main__ - ['neutral']
06/12/2022 09:37:43 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:37:43 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:37:43 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 09:37:44 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.07816269764536138 on epoch=499
06/12/2022 09:37:44 - INFO - __main__ - save last model!
06/12/2022 09:37:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 09:37:44 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 09:37:44 - INFO - __main__ - Printing 3 examples
06/12/2022 09:37:44 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 09:37:44 - INFO - __main__ - ['contradiction']
06/12/2022 09:37:44 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 09:37:44 - INFO - __main__ - ['entailment']
06/12/2022 09:37:44 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 09:37:44 - INFO - __main__ - ['contradiction']
06/12/2022 09:37:44 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:37:45 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:37:46 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 09:37:59 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 09:38:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 09:38:00 - INFO - __main__ - Starting training!
06/12/2022 09:38:16 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_100_0.4_8_predictions.txt
06/12/2022 09:38:16 - INFO - __main__ - Classification-F1 on test data: 0.0227
06/12/2022 09:38:17 - INFO - __main__ - prefix=anli_32_100, lr=0.4, bsz=8, dev_performance=0.3917301917301918, test_performance=0.02267971831793202
06/12/2022 09:38:17 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.3, bsz=8 ...
06/12/2022 09:38:18 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:38:18 - INFO - __main__ - Printing 3 examples
06/12/2022 09:38:18 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/12/2022 09:38:18 - INFO - __main__ - ['neutral']
06/12/2022 09:38:18 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/12/2022 09:38:18 - INFO - __main__ - ['neutral']
06/12/2022 09:38:18 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/12/2022 09:38:18 - INFO - __main__ - ['neutral']
06/12/2022 09:38:18 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:38:18 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:38:18 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 09:38:18 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:38:18 - INFO - __main__ - Printing 3 examples
06/12/2022 09:38:18 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
06/12/2022 09:38:18 - INFO - __main__ - ['neutral']
06/12/2022 09:38:18 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
06/12/2022 09:38:18 - INFO - __main__ - ['neutral']
06/12/2022 09:38:18 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
06/12/2022 09:38:18 - INFO - __main__ - ['neutral']
06/12/2022 09:38:18 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:38:18 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:38:18 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 09:38:37 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 09:38:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 09:38:37 - INFO - __main__ - Starting training!
06/12/2022 09:38:41 - INFO - __main__ - Step 10 Global step 10 Train loss 1.01 on epoch=1
06/12/2022 09:38:44 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=3
06/12/2022 09:38:47 - INFO - __main__ - Step 30 Global step 30 Train loss 0.62 on epoch=4
06/12/2022 09:38:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.59 on epoch=6
06/12/2022 09:38:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=8
06/12/2022 09:38:54 - INFO - __main__ - Global step 50 Train loss 0.67 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 09:38:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 09:38:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=9
06/12/2022 09:39:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=11
06/12/2022 09:39:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=13
06/12/2022 09:39:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=14
06/12/2022 09:39:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=16
06/12/2022 09:39:11 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 09:39:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=18
06/12/2022 09:39:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=19
06/12/2022 09:39:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=21
06/12/2022 09:39:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=23
06/12/2022 09:39:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=24
06/12/2022 09:39:28 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 09:39:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=26
06/12/2022 09:39:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=28
06/12/2022 09:39:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=29
06/12/2022 09:39:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=31
06/12/2022 09:39:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=33
06/12/2022 09:39:45 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 09:39:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=34
06/12/2022 09:39:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=36
06/12/2022 09:39:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=38
06/12/2022 09:39:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=39
06/12/2022 09:39:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=41
06/12/2022 09:40:02 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 09:40:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=43
06/12/2022 09:40:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=44
06/12/2022 09:40:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=46
06/12/2022 09:40:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=48
06/12/2022 09:40:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=49
06/12/2022 09:40:19 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.28235294117647064 on epoch=49
06/12/2022 09:40:19 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.28235294117647064 on epoch=49, global_step=300
06/12/2022 09:40:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=51
06/12/2022 09:40:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=53
06/12/2022 09:40:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.64 on epoch=54
06/12/2022 09:40:30 - INFO - __main__ - Step 340 Global step 340 Train loss 2.22 on epoch=56
06/12/2022 09:40:33 - INFO - __main__ - Step 350 Global step 350 Train loss 4.13 on epoch=58
06/12/2022 09:40:35 - INFO - __main__ - Global step 350 Train loss 1.58 Classification-F1 0.18971428571428572 on epoch=58
06/12/2022 09:40:38 - INFO - __main__ - Step 360 Global step 360 Train loss 1.06 on epoch=59
06/12/2022 09:40:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=61
06/12/2022 09:40:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.56 on epoch=63
06/12/2022 09:40:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=64
06/12/2022 09:40:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=66
06/12/2022 09:40:52 - INFO - __main__ - Global step 400 Train loss 0.66 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 09:40:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=68
06/12/2022 09:40:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=69
06/12/2022 09:41:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=71
06/12/2022 09:41:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=73
06/12/2022 09:41:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=74
06/12/2022 09:41:09 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=74
06/12/2022 09:41:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=76
06/12/2022 09:41:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=78
06/12/2022 09:41:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=79
06/12/2022 09:41:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=81
06/12/2022 09:41:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=83
06/12/2022 09:41:26 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=83
06/12/2022 09:41:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=84
06/12/2022 09:41:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=86
06/12/2022 09:41:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=88
06/12/2022 09:41:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=89
06/12/2022 09:41:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=91
06/12/2022 09:41:43 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 09:41:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=93
06/12/2022 09:41:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=94
06/12/2022 09:41:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=96
06/12/2022 09:41:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=98
06/12/2022 09:41:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=99
06/12/2022 09:42:00 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=99
06/12/2022 09:42:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=101
06/12/2022 09:42:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=103
06/12/2022 09:42:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=104
06/12/2022 09:42:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=106
06/12/2022 09:42:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=108
06/12/2022 09:42:17 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=108
06/12/2022 09:42:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=109
06/12/2022 09:42:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=111
06/12/2022 09:42:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=113
06/12/2022 09:42:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=114
06/12/2022 09:42:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=116
06/12/2022 09:42:34 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 09:42:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=118
06/12/2022 09:42:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=119
06/12/2022 09:42:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=121
06/12/2022 09:42:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=123
06/12/2022 09:42:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=124
06/12/2022 09:42:51 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=124
06/12/2022 09:42:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=126
06/12/2022 09:42:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=128
06/12/2022 09:43:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=129
06/12/2022 09:43:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=131
06/12/2022 09:43:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=133
06/12/2022 09:43:08 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=133
06/12/2022 09:43:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=134
06/12/2022 09:43:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=136
06/12/2022 09:43:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=138
06/12/2022 09:43:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=139
06/12/2022 09:43:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=141
06/12/2022 09:43:26 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=141
06/12/2022 09:43:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=143
06/12/2022 09:43:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=144
06/12/2022 09:43:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=146
06/12/2022 09:43:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.52 on epoch=148
06/12/2022 09:43:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=149
06/12/2022 09:43:43 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=149
06/12/2022 09:43:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=151
06/12/2022 09:43:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=153
06/12/2022 09:43:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=154
06/12/2022 09:43:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=156
06/12/2022 09:43:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=158
06/12/2022 09:44:00 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=158
06/12/2022 09:44:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=159
06/12/2022 09:44:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=161
06/12/2022 09:44:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=163
06/12/2022 09:44:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=164
06/12/2022 09:44:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=166
06/12/2022 09:44:17 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=166
06/12/2022 09:44:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=168
06/12/2022 09:44:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=169
06/12/2022 09:44:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=171
06/12/2022 09:44:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=173
06/12/2022 09:44:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=174
06/12/2022 09:44:34 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=174
06/12/2022 09:44:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=176
06/12/2022 09:44:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=178
06/12/2022 09:44:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=179
06/12/2022 09:44:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=181
06/12/2022 09:44:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=183
06/12/2022 09:44:51 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.2087619047619048 on epoch=183
06/12/2022 09:44:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=184
06/12/2022 09:44:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=186
06/12/2022 09:45:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=188
06/12/2022 09:45:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=189
06/12/2022 09:45:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=191
06/12/2022 09:45:08 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=191
06/12/2022 09:45:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=193
06/12/2022 09:45:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=194
06/12/2022 09:45:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=196
06/12/2022 09:45:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=198
06/12/2022 09:45:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=199
06/12/2022 09:45:25 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=199
06/12/2022 09:45:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=201
06/12/2022 09:45:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=203
06/12/2022 09:45:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=204
06/12/2022 09:45:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=206
06/12/2022 09:45:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=208
06/12/2022 09:45:42 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.2087619047619048 on epoch=208
06/12/2022 09:45:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=209
06/12/2022 09:45:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=211
06/12/2022 09:45:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=213
06/12/2022 09:45:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.43 on epoch=214
06/12/2022 09:45:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=216
06/12/2022 09:45:59 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=216
06/12/2022 09:46:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=218
06/12/2022 09:46:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=219
06/12/2022 09:46:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=221
06/12/2022 09:46:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=223
06/12/2022 09:46:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=224
06/12/2022 09:46:16 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=224
06/12/2022 09:46:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=226
06/12/2022 09:46:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=228
06/12/2022 09:46:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.52 on epoch=229
06/12/2022 09:46:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=231
06/12/2022 09:46:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=233
06/12/2022 09:46:33 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.18892001244942422 on epoch=233
06/12/2022 09:46:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=234
06/12/2022 09:46:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=236
06/12/2022 09:46:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=238
06/12/2022 09:46:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=239
06/12/2022 09:46:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=241
06/12/2022 09:46:50 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=241
06/12/2022 09:46:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=243
06/12/2022 09:46:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=244
06/12/2022 09:46:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.45 on epoch=246
06/12/2022 09:47:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=248
06/12/2022 09:47:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.48 on epoch=249
06/12/2022 09:47:07 - INFO - __main__ - Global step 1500 Train loss 0.45 Classification-F1 0.20908004778972522 on epoch=249
06/12/2022 09:47:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=251
06/12/2022 09:47:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=253
06/12/2022 09:47:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=254
06/12/2022 09:47:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.49 on epoch=256
06/12/2022 09:47:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=258
06/12/2022 09:47:23 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=258
06/12/2022 09:47:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.47 on epoch=259
06/12/2022 09:47:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=261
06/12/2022 09:47:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=263
06/12/2022 09:47:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=264
06/12/2022 09:47:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=266
06/12/2022 09:47:40 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=266
06/12/2022 09:47:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=268
06/12/2022 09:47:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.49 on epoch=269
06/12/2022 09:47:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=271
06/12/2022 09:47:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=273
06/12/2022 09:47:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=274
06/12/2022 09:47:57 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=274
06/12/2022 09:48:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=276
06/12/2022 09:48:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=278
06/12/2022 09:48:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=279
06/12/2022 09:48:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=281
06/12/2022 09:48:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=283
06/12/2022 09:48:14 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=283
06/12/2022 09:48:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=284
06/12/2022 09:48:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=286
06/12/2022 09:48:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=288
06/12/2022 09:48:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=289
06/12/2022 09:48:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=291
06/12/2022 09:48:31 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=291
06/12/2022 09:48:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=293
06/12/2022 09:48:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=294
06/12/2022 09:48:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=296
06/12/2022 09:48:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=298
06/12/2022 09:48:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=299
06/12/2022 09:48:49 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=299
06/12/2022 09:48:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=301
06/12/2022 09:48:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.46 on epoch=303
06/12/2022 09:48:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=304
06/12/2022 09:49:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=306
06/12/2022 09:49:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=308
06/12/2022 09:49:06 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=308
06/12/2022 09:49:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=309
06/12/2022 09:49:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=311
06/12/2022 09:49:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.47 on epoch=313
06/12/2022 09:49:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=314
06/12/2022 09:49:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=316
06/12/2022 09:49:22 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=316
06/12/2022 09:49:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=318
06/12/2022 09:49:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=319
06/12/2022 09:49:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=321
06/12/2022 09:49:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=323
06/12/2022 09:49:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.45 on epoch=324
06/12/2022 09:49:39 - INFO - __main__ - Global step 1950 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=324
06/12/2022 09:49:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=326
06/12/2022 09:49:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=328
06/12/2022 09:49:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=329
06/12/2022 09:49:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=331
06/12/2022 09:49:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=333
06/12/2022 09:49:57 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=333
06/12/2022 09:49:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=334
06/12/2022 09:50:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=336
06/12/2022 09:50:05 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.41 on epoch=338
06/12/2022 09:50:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.40 on epoch=339
06/12/2022 09:50:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.47 on epoch=341
06/12/2022 09:50:14 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=341
06/12/2022 09:50:17 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.42 on epoch=343
06/12/2022 09:50:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.44 on epoch=344
06/12/2022 09:50:22 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.41 on epoch=346
06/12/2022 09:50:25 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.46 on epoch=348
06/12/2022 09:50:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.46 on epoch=349
06/12/2022 09:50:31 - INFO - __main__ - Global step 2100 Train loss 0.44 Classification-F1 0.20947777045338023 on epoch=349
06/12/2022 09:50:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.45 on epoch=351
06/12/2022 09:50:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.41 on epoch=353
06/12/2022 09:50:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=354
06/12/2022 09:50:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.44 on epoch=356
06/12/2022 09:50:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.41 on epoch=358
06/12/2022 09:50:48 - INFO - __main__ - Global step 2150 Train loss 0.43 Classification-F1 0.263030303030303 on epoch=358
06/12/2022 09:50:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.40 on epoch=359
06/12/2022 09:50:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.53 on epoch=361
06/12/2022 09:50:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.42 on epoch=363
06/12/2022 09:50:59 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.41 on epoch=364
06/12/2022 09:51:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=366
06/12/2022 09:51:05 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=366
06/12/2022 09:51:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.38 on epoch=368
06/12/2022 09:51:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.44 on epoch=369
06/12/2022 09:51:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.43 on epoch=371
06/12/2022 09:51:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.45 on epoch=373
06/12/2022 09:51:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.44 on epoch=374
06/12/2022 09:51:22 - INFO - __main__ - Global step 2250 Train loss 0.43 Classification-F1 0.18892001244942422 on epoch=374
06/12/2022 09:51:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.39 on epoch=376
06/12/2022 09:51:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.41 on epoch=378
06/12/2022 09:51:31 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.44 on epoch=379
06/12/2022 09:51:34 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.41 on epoch=381
06/12/2022 09:51:36 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.46 on epoch=383
06/12/2022 09:51:39 - INFO - __main__ - Global step 2300 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=383
06/12/2022 09:51:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.43 on epoch=384
06/12/2022 09:51:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.41 on epoch=386
06/12/2022 09:51:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.46 on epoch=388
06/12/2022 09:51:51 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.44 on epoch=389
06/12/2022 09:51:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.40 on epoch=391
06/12/2022 09:51:56 - INFO - __main__ - Global step 2350 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=391
06/12/2022 09:51:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.39 on epoch=393
06/12/2022 09:52:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.40 on epoch=394
06/12/2022 09:52:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.41 on epoch=396
06/12/2022 09:52:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.40 on epoch=398
06/12/2022 09:52:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.44 on epoch=399
06/12/2022 09:52:13 - INFO - __main__ - Global step 2400 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=399
06/12/2022 09:52:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.46 on epoch=401
06/12/2022 09:52:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.42 on epoch=403
06/12/2022 09:52:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.39 on epoch=404
06/12/2022 09:52:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.41 on epoch=406
06/12/2022 09:52:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.41 on epoch=408
06/12/2022 09:52:30 - INFO - __main__ - Global step 2450 Train loss 0.42 Classification-F1 0.292250893523428 on epoch=408
06/12/2022 09:52:30 - INFO - __main__ - Saving model with best Classification-F1: 0.28235294117647064 -> 0.292250893523428 on epoch=408, global_step=2450
06/12/2022 09:52:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.44 on epoch=409
06/12/2022 09:52:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.39 on epoch=411
06/12/2022 09:52:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.46 on epoch=413
06/12/2022 09:52:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.42 on epoch=414
06/12/2022 09:52:44 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.41 on epoch=416
06/12/2022 09:52:47 - INFO - __main__ - Global step 2500 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=416
06/12/2022 09:52:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.43 on epoch=418
06/12/2022 09:52:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.39 on epoch=419
06/12/2022 09:52:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.41 on epoch=421
06/12/2022 09:52:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.47 on epoch=423
06/12/2022 09:53:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.41 on epoch=424
06/12/2022 09:53:04 - INFO - __main__ - Global step 2550 Train loss 0.42 Classification-F1 0.27991273703641556 on epoch=424
06/12/2022 09:53:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.38 on epoch=426
06/12/2022 09:53:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.46 on epoch=428
06/12/2022 09:53:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.45 on epoch=429
06/12/2022 09:53:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.44 on epoch=431
06/12/2022 09:53:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.41 on epoch=433
06/12/2022 09:53:21 - INFO - __main__ - Global step 2600 Train loss 0.43 Classification-F1 0.1851851851851852 on epoch=433
06/12/2022 09:53:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.45 on epoch=434
06/12/2022 09:53:26 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.44 on epoch=436
06/12/2022 09:53:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.39 on epoch=438
06/12/2022 09:53:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.41 on epoch=439
06/12/2022 09:53:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.37 on epoch=441
06/12/2022 09:53:37 - INFO - __main__ - Global step 2650 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=441
06/12/2022 09:53:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=443
06/12/2022 09:53:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.44 on epoch=444
06/12/2022 09:53:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.42 on epoch=446
06/12/2022 09:53:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.41 on epoch=448
06/12/2022 09:53:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.44 on epoch=449
06/12/2022 09:53:54 - INFO - __main__ - Global step 2700 Train loss 0.42 Classification-F1 0.2735024477080552 on epoch=449
06/12/2022 09:53:57 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.44 on epoch=451
06/12/2022 09:54:00 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.45 on epoch=453
06/12/2022 09:54:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.42 on epoch=454
06/12/2022 09:54:05 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.40 on epoch=456
06/12/2022 09:54:08 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.41 on epoch=458
06/12/2022 09:54:11 - INFO - __main__ - Global step 2750 Train loss 0.43 Classification-F1 0.24099784344582387 on epoch=458
06/12/2022 09:54:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.41 on epoch=459
06/12/2022 09:54:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.43 on epoch=461
06/12/2022 09:54:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.42 on epoch=463
06/12/2022 09:54:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.42 on epoch=464
06/12/2022 09:54:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.36 on epoch=466
06/12/2022 09:54:28 - INFO - __main__ - Global step 2800 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=466
06/12/2022 09:54:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.44 on epoch=468
06/12/2022 09:54:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.39 on epoch=469
06/12/2022 09:54:36 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.40 on epoch=471
06/12/2022 09:54:39 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.39 on epoch=473
06/12/2022 09:54:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.39 on epoch=474
06/12/2022 09:54:45 - INFO - __main__ - Global step 2850 Train loss 0.40 Classification-F1 0.23473389355742294 on epoch=474
06/12/2022 09:54:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.39 on epoch=476
06/12/2022 09:54:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.41 on epoch=478
06/12/2022 09:54:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.39 on epoch=479
06/12/2022 09:54:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.37 on epoch=481
06/12/2022 09:54:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.42 on epoch=483
06/12/2022 09:55:02 - INFO - __main__ - Global step 2900 Train loss 0.40 Classification-F1 0.1836290071584189 on epoch=483
06/12/2022 09:55:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.41 on epoch=484
06/12/2022 09:55:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.39 on epoch=486
06/12/2022 09:55:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.40 on epoch=488
06/12/2022 09:55:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.43 on epoch=489
06/12/2022 09:55:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.39 on epoch=491
06/12/2022 09:55:19 - INFO - __main__ - Global step 2950 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=491
06/12/2022 09:55:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.41 on epoch=493
06/12/2022 09:55:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.43 on epoch=494
06/12/2022 09:55:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.41 on epoch=496
06/12/2022 09:55:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.42 on epoch=498
06/12/2022 09:55:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.43 on epoch=499
06/12/2022 09:55:34 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:55:34 - INFO - __main__ - Printing 3 examples
06/12/2022 09:55:34 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/12/2022 09:55:34 - INFO - __main__ - ['neutral']
06/12/2022 09:55:34 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/12/2022 09:55:34 - INFO - __main__ - ['neutral']
06/12/2022 09:55:34 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/12/2022 09:55:34 - INFO - __main__ - ['neutral']
06/12/2022 09:55:34 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:55:34 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:55:35 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 09:55:35 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:55:35 - INFO - __main__ - Printing 3 examples
06/12/2022 09:55:35 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
06/12/2022 09:55:35 - INFO - __main__ - ['neutral']
06/12/2022 09:55:35 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
06/12/2022 09:55:35 - INFO - __main__ - ['neutral']
06/12/2022 09:55:35 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
06/12/2022 09:55:35 - INFO - __main__ - ['neutral']
06/12/2022 09:55:35 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:55:35 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:55:35 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 09:55:36 - INFO - __main__ - Global step 3000 Train loss 0.42 Classification-F1 0.2936259143155695 on epoch=499
06/12/2022 09:55:36 - INFO - __main__ - Saving model with best Classification-F1: 0.292250893523428 -> 0.2936259143155695 on epoch=499, global_step=3000
06/12/2022 09:55:36 - INFO - __main__ - save last model!
06/12/2022 09:55:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 09:55:36 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 09:55:36 - INFO - __main__ - Printing 3 examples
06/12/2022 09:55:36 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 09:55:36 - INFO - __main__ - ['contradiction']
06/12/2022 09:55:36 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 09:55:36 - INFO - __main__ - ['entailment']
06/12/2022 09:55:36 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 09:55:36 - INFO - __main__ - ['contradiction']
06/12/2022 09:55:36 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:55:37 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:55:38 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 09:55:53 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 09:55:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 09:55:54 - INFO - __main__ - Starting training!
06/12/2022 09:56:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_100_0.3_8_predictions.txt
06/12/2022 09:56:10 - INFO - __main__ - Classification-F1 on test data: 0.2068
06/12/2022 09:56:10 - INFO - __main__ - prefix=anli_32_100, lr=0.3, bsz=8, dev_performance=0.2936259143155695, test_performance=0.20675858423398677
06/12/2022 09:56:10 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.2, bsz=8 ...
06/12/2022 09:56:11 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:56:11 - INFO - __main__ - Printing 3 examples
06/12/2022 09:56:11 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/12/2022 09:56:11 - INFO - __main__ - ['neutral']
06/12/2022 09:56:11 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/12/2022 09:56:11 - INFO - __main__ - ['neutral']
06/12/2022 09:56:11 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/12/2022 09:56:11 - INFO - __main__ - ['neutral']
06/12/2022 09:56:11 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:56:11 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:56:11 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 09:56:11 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 09:56:11 - INFO - __main__ - Printing 3 examples
06/12/2022 09:56:11 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
06/12/2022 09:56:11 - INFO - __main__ - ['neutral']
06/12/2022 09:56:11 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
06/12/2022 09:56:11 - INFO - __main__ - ['neutral']
06/12/2022 09:56:11 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
06/12/2022 09:56:11 - INFO - __main__ - ['neutral']
06/12/2022 09:56:11 - INFO - __main__ - Tokenizing Input ...
06/12/2022 09:56:11 - INFO - __main__ - Tokenizing Output ...
06/12/2022 09:56:11 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 09:56:27 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 09:56:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 09:56:28 - INFO - __main__ - Starting training!
06/12/2022 09:56:31 - INFO - __main__ - Step 10 Global step 10 Train loss 0.97 on epoch=1
06/12/2022 09:56:34 - INFO - __main__ - Step 20 Global step 20 Train loss 0.72 on epoch=3
06/12/2022 09:56:37 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=4
06/12/2022 09:56:40 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=6
06/12/2022 09:56:42 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=8
06/12/2022 09:56:44 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 09:56:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 09:56:47 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=9
06/12/2022 09:56:50 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=11
06/12/2022 09:56:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=13
06/12/2022 09:56:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=14
06/12/2022 09:56:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=16
06/12/2022 09:57:00 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 09:57:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=18
06/12/2022 09:57:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=19
06/12/2022 09:57:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=21
06/12/2022 09:57:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=23
06/12/2022 09:57:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=24
06/12/2022 09:57:17 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 09:57:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=26
06/12/2022 09:57:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=28
06/12/2022 09:57:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=29
06/12/2022 09:57:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=31
06/12/2022 09:57:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=33
06/12/2022 09:57:33 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.21580667354661162 on epoch=33
06/12/2022 09:57:33 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.21580667354661162 on epoch=33, global_step=200
06/12/2022 09:57:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=34
06/12/2022 09:57:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=36
06/12/2022 09:57:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=38
06/12/2022 09:57:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=39
06/12/2022 09:57:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=41
06/12/2022 09:57:49 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 09:57:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=43
06/12/2022 09:57:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=44
06/12/2022 09:57:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=46
06/12/2022 09:58:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=48
06/12/2022 09:58:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=49
06/12/2022 09:58:05 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
06/12/2022 09:58:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=51
06/12/2022 09:58:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=53
06/12/2022 09:58:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=54
06/12/2022 09:58:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=56
06/12/2022 09:58:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=58
06/12/2022 09:58:22 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=58
06/12/2022 09:58:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=59
06/12/2022 09:58:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=61
06/12/2022 09:58:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=63
06/12/2022 09:58:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=64
06/12/2022 09:58:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=66
06/12/2022 09:58:38 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 09:58:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
06/12/2022 09:58:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=69
06/12/2022 09:58:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=71
06/12/2022 09:58:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=73
06/12/2022 09:58:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=74
06/12/2022 09:58:55 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=74
06/12/2022 09:58:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=76
06/12/2022 09:59:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=78
06/12/2022 09:59:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=79
06/12/2022 09:59:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=81
06/12/2022 09:59:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=83
06/12/2022 09:59:11 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=83
06/12/2022 09:59:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.42 on epoch=84
06/12/2022 09:59:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=86
06/12/2022 09:59:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=88
06/12/2022 09:59:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=89
06/12/2022 09:59:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=91
06/12/2022 09:59:28 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 09:59:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=93
06/12/2022 09:59:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=94
06/12/2022 09:59:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=96
06/12/2022 09:59:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=98
06/12/2022 09:59:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=99
06/12/2022 09:59:44 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.2554945054945055 on epoch=99
06/12/2022 09:59:44 - INFO - __main__ - Saving model with best Classification-F1: 0.21580667354661162 -> 0.2554945054945055 on epoch=99, global_step=600
06/12/2022 09:59:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=101
06/12/2022 09:59:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=103
06/12/2022 09:59:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=104
06/12/2022 09:59:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=106
06/12/2022 09:59:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=108
06/12/2022 10:00:01 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=108
06/12/2022 10:00:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=109
06/12/2022 10:00:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=111
06/12/2022 10:00:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=113
06/12/2022 10:00:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=114
06/12/2022 10:00:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=116
06/12/2022 10:00:18 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 10:00:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=118
06/12/2022 10:00:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=119
06/12/2022 10:00:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=121
06/12/2022 10:00:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=123
06/12/2022 10:00:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=124
06/12/2022 10:00:34 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=124
06/12/2022 10:00:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=126
06/12/2022 10:00:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=128
06/12/2022 10:00:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=129
06/12/2022 10:00:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=131
06/12/2022 10:00:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=133
06/12/2022 10:00:51 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=133
06/12/2022 10:00:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=134
06/12/2022 10:00:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=136
06/12/2022 10:00:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=138
06/12/2022 10:01:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=139
06/12/2022 10:01:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=141
06/12/2022 10:01:08 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=141
06/12/2022 10:01:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=143
06/12/2022 10:01:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=144
06/12/2022 10:01:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=146
06/12/2022 10:01:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=148
06/12/2022 10:01:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=149
06/12/2022 10:01:25 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.2574719966024314 on epoch=149
06/12/2022 10:01:25 - INFO - __main__ - Saving model with best Classification-F1: 0.2554945054945055 -> 0.2574719966024314 on epoch=149, global_step=900
06/12/2022 10:01:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=151
06/12/2022 10:01:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=153
06/12/2022 10:01:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=154
06/12/2022 10:01:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=156
06/12/2022 10:01:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=158
06/12/2022 10:01:43 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.28237259816207183 on epoch=158
06/12/2022 10:01:43 - INFO - __main__ - Saving model with best Classification-F1: 0.2574719966024314 -> 0.28237259816207183 on epoch=158, global_step=950
06/12/2022 10:01:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=159
06/12/2022 10:01:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=161
06/12/2022 10:01:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=163
06/12/2022 10:01:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=164
06/12/2022 10:01:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=166
06/12/2022 10:01:59 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=166
06/12/2022 10:02:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=168
06/12/2022 10:02:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=169
06/12/2022 10:02:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=171
06/12/2022 10:02:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=173
06/12/2022 10:02:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=174
06/12/2022 10:02:17 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.2622874446773818 on epoch=174
06/12/2022 10:02:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=176
06/12/2022 10:02:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=178
06/12/2022 10:02:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=179
06/12/2022 10:02:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=181
06/12/2022 10:02:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=183
06/12/2022 10:02:34 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=183
06/12/2022 10:02:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.43 on epoch=184
06/12/2022 10:02:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=186
06/12/2022 10:02:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=188
06/12/2022 10:02:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=189
06/12/2022 10:02:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=191
06/12/2022 10:02:51 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=191
06/12/2022 10:02:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=193
06/12/2022 10:02:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=194
06/12/2022 10:02:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=196
06/12/2022 10:03:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=198
06/12/2022 10:03:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=199
06/12/2022 10:03:07 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.2916666666666667 on epoch=199
06/12/2022 10:03:07 - INFO - __main__ - Saving model with best Classification-F1: 0.28237259816207183 -> 0.2916666666666667 on epoch=199, global_step=1200
06/12/2022 10:03:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=201
06/12/2022 10:03:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=203
06/12/2022 10:03:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=204
06/12/2022 10:03:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=206
06/12/2022 10:03:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=208
06/12/2022 10:03:24 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.20140056022408961 on epoch=208
06/12/2022 10:03:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=209
06/12/2022 10:03:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=211
06/12/2022 10:03:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=213
06/12/2022 10:03:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=214
06/12/2022 10:03:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=216
06/12/2022 10:03:41 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=216
06/12/2022 10:03:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=218
06/12/2022 10:03:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=219
06/12/2022 10:03:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=221
06/12/2022 10:03:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=223
06/12/2022 10:03:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=224
06/12/2022 10:03:59 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3599723947550035 on epoch=224
06/12/2022 10:03:59 - INFO - __main__ - Saving model with best Classification-F1: 0.2916666666666667 -> 0.3599723947550035 on epoch=224, global_step=1350
06/12/2022 10:04:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=226
06/12/2022 10:04:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=228
06/12/2022 10:04:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=229
06/12/2022 10:04:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=231
06/12/2022 10:04:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=233
06/12/2022 10:04:15 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.31353086160171945 on epoch=233
06/12/2022 10:04:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=234
06/12/2022 10:04:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=236
06/12/2022 10:04:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=238
06/12/2022 10:04:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=239
06/12/2022 10:04:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=241
06/12/2022 10:04:32 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.24936803989888637 on epoch=241
06/12/2022 10:04:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=243
06/12/2022 10:04:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=244
06/12/2022 10:04:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=246
06/12/2022 10:04:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=248
06/12/2022 10:04:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=249
06/12/2022 10:04:50 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.38510192665443804 on epoch=249
06/12/2022 10:04:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3599723947550035 -> 0.38510192665443804 on epoch=249, global_step=1500
06/12/2022 10:04:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=251
06/12/2022 10:04:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=253
06/12/2022 10:04:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=254
06/12/2022 10:05:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=256
06/12/2022 10:05:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=258
06/12/2022 10:05:07 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.3867064050815107 on epoch=258
06/12/2022 10:05:07 - INFO - __main__ - Saving model with best Classification-F1: 0.38510192665443804 -> 0.3867064050815107 on epoch=258, global_step=1550
06/12/2022 10:05:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=259
06/12/2022 10:05:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=261
06/12/2022 10:05:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=263
06/12/2022 10:05:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=264
06/12/2022 10:05:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=266
06/12/2022 10:05:24 - INFO - __main__ - Global step 1600 Train loss 0.38 Classification-F1 0.3815030096360019 on epoch=266
06/12/2022 10:05:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=268
06/12/2022 10:05:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=269
06/12/2022 10:05:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=271
06/12/2022 10:05:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.38 on epoch=273
06/12/2022 10:05:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=274
06/12/2022 10:05:41 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.36437246963562747 on epoch=274
06/12/2022 10:05:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=276
06/12/2022 10:05:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=278
06/12/2022 10:05:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=279
06/12/2022 10:05:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.34 on epoch=281
06/12/2022 10:05:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=283
06/12/2022 10:05:58 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.30878010878010875 on epoch=283
06/12/2022 10:06:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=284
06/12/2022 10:06:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.31 on epoch=286
06/12/2022 10:06:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=288
06/12/2022 10:06:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=289
06/12/2022 10:06:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=291
06/12/2022 10:06:15 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.330446423374658 on epoch=291
06/12/2022 10:06:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=293
06/12/2022 10:06:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=294
06/12/2022 10:06:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=296
06/12/2022 10:06:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=298
06/12/2022 10:06:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=299
06/12/2022 10:06:32 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.3428904428904429 on epoch=299
06/12/2022 10:06:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=301
06/12/2022 10:06:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=303
06/12/2022 10:06:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=304
06/12/2022 10:06:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=306
06/12/2022 10:06:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=308
06/12/2022 10:06:49 - INFO - __main__ - Global step 1850 Train loss 0.30 Classification-F1 0.29887516410072806 on epoch=308
06/12/2022 10:06:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=309
06/12/2022 10:06:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=311
06/12/2022 10:06:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=313
06/12/2022 10:07:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.29 on epoch=314
06/12/2022 10:07:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=316
06/12/2022 10:07:06 - INFO - __main__ - Global step 1900 Train loss 0.31 Classification-F1 0.3126586746868867 on epoch=316
06/12/2022 10:07:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=318
06/12/2022 10:07:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=319
06/12/2022 10:07:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=321
06/12/2022 10:07:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.25 on epoch=323
06/12/2022 10:07:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=324
06/12/2022 10:07:23 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.3413632119514473 on epoch=324
06/12/2022 10:07:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=326
06/12/2022 10:07:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=328
06/12/2022 10:07:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=329
06/12/2022 10:07:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=331
06/12/2022 10:07:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=333
06/12/2022 10:07:40 - INFO - __main__ - Global step 2000 Train loss 0.29 Classification-F1 0.37542087542087543 on epoch=333
06/12/2022 10:07:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=334
06/12/2022 10:07:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.23 on epoch=336
06/12/2022 10:07:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.25 on epoch=338
06/12/2022 10:07:52 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.25 on epoch=339
06/12/2022 10:07:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=341
06/12/2022 10:07:58 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.3303960304532169 on epoch=341
06/12/2022 10:08:00 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.27 on epoch=343
06/12/2022 10:08:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.25 on epoch=344
06/12/2022 10:08:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.24 on epoch=346
06/12/2022 10:08:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=348
06/12/2022 10:08:12 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.27 on epoch=349
06/12/2022 10:08:15 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.3643863382993818 on epoch=349
06/12/2022 10:08:18 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=351
06/12/2022 10:08:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.24 on epoch=353
06/12/2022 10:08:23 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.30 on epoch=354
06/12/2022 10:08:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=356
06/12/2022 10:08:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.28 on epoch=358
06/12/2022 10:08:32 - INFO - __main__ - Global step 2150 Train loss 0.25 Classification-F1 0.3492592592592592 on epoch=358
06/12/2022 10:08:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=359
06/12/2022 10:08:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=361
06/12/2022 10:08:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.24 on epoch=363
06/12/2022 10:08:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=364
06/12/2022 10:08:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=366
06/12/2022 10:08:49 - INFO - __main__ - Global step 2200 Train loss 0.24 Classification-F1 0.37871025727446356 on epoch=366
06/12/2022 10:08:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=368
06/12/2022 10:08:55 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=369
06/12/2022 10:08:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=371
06/12/2022 10:09:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=373
06/12/2022 10:09:03 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.20 on epoch=374
06/12/2022 10:09:06 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.29587932649782006 on epoch=374
06/12/2022 10:09:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=376
06/12/2022 10:09:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=378
06/12/2022 10:09:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=379
06/12/2022 10:09:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=381
06/12/2022 10:09:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=383
06/12/2022 10:09:23 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.34187192118226606 on epoch=383
06/12/2022 10:09:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=384
06/12/2022 10:09:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.17 on epoch=386
06/12/2022 10:09:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.22 on epoch=388
06/12/2022 10:09:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=389
06/12/2022 10:09:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=391
06/12/2022 10:09:40 - INFO - __main__ - Global step 2350 Train loss 0.19 Classification-F1 0.4063714063714064 on epoch=391
06/12/2022 10:09:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3867064050815107 -> 0.4063714063714064 on epoch=391, global_step=2350
06/12/2022 10:09:42 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.23 on epoch=393
06/12/2022 10:09:45 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=394
06/12/2022 10:09:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=396
06/12/2022 10:09:51 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=398
06/12/2022 10:09:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=399
06/12/2022 10:09:56 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.33240740740740743 on epoch=399
06/12/2022 10:09:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.23 on epoch=401
06/12/2022 10:10:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=403
06/12/2022 10:10:04 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=404
06/12/2022 10:10:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=406
06/12/2022 10:10:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=408
06/12/2022 10:10:13 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.3180131350863058 on epoch=408
06/12/2022 10:10:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=409
06/12/2022 10:10:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=411
06/12/2022 10:10:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=413
06/12/2022 10:10:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.17 on epoch=414
06/12/2022 10:10:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.15 on epoch=416
06/12/2022 10:10:29 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.4050985959430129 on epoch=416
06/12/2022 10:10:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=418
06/12/2022 10:10:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=419
06/12/2022 10:10:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=421
06/12/2022 10:10:40 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=423
06/12/2022 10:10:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=424
06/12/2022 10:10:46 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.35634821304408293 on epoch=424
06/12/2022 10:10:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.22 on epoch=426
06/12/2022 10:10:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=428
06/12/2022 10:10:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.14 on epoch=429
06/12/2022 10:10:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=431
06/12/2022 10:10:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=433
06/12/2022 10:11:02 - INFO - __main__ - Global step 2600 Train loss 0.17 Classification-F1 0.3905218539364881 on epoch=433
06/12/2022 10:11:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=434
06/12/2022 10:11:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=436
06/12/2022 10:11:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=438
06/12/2022 10:11:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=439
06/12/2022 10:11:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=441
06/12/2022 10:11:19 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.3132850241545893 on epoch=441
06/12/2022 10:11:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=443
06/12/2022 10:11:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=444
06/12/2022 10:11:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=446
06/12/2022 10:11:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=448
06/12/2022 10:11:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=449
06/12/2022 10:11:35 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.21985405898449378 on epoch=449
06/12/2022 10:11:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=451
06/12/2022 10:11:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=453
06/12/2022 10:11:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.13 on epoch=454
06/12/2022 10:11:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=456
06/12/2022 10:11:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=458
06/12/2022 10:11:52 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.20424483306836247 on epoch=458
06/12/2022 10:11:55 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=459
06/12/2022 10:11:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=461
06/12/2022 10:12:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.16 on epoch=463
06/12/2022 10:12:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=464
06/12/2022 10:12:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.10 on epoch=466
06/12/2022 10:12:08 - INFO - __main__ - Global step 2800 Train loss 0.13 Classification-F1 0.18890657305627037 on epoch=466
06/12/2022 10:12:11 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=468
06/12/2022 10:12:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=469
06/12/2022 10:12:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=471
06/12/2022 10:12:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=473
06/12/2022 10:12:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=474
06/12/2022 10:12:25 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.2214489139521374 on epoch=474
06/12/2022 10:12:27 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=476
06/12/2022 10:12:30 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.16 on epoch=478
06/12/2022 10:12:33 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=479
06/12/2022 10:12:36 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=481
06/12/2022 10:12:38 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=483
06/12/2022 10:12:41 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.2894238856181665 on epoch=483
06/12/2022 10:12:44 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=484
06/12/2022 10:12:47 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=486
06/12/2022 10:12:50 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=488
06/12/2022 10:12:52 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=489
06/12/2022 10:12:55 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=491
06/12/2022 10:12:58 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.17716599190283402 on epoch=491
06/12/2022 10:13:01 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=493
06/12/2022 10:13:03 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=494
06/12/2022 10:13:06 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=496
06/12/2022 10:13:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=498
06/12/2022 10:13:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.12 on epoch=499
06/12/2022 10:13:13 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:13:13 - INFO - __main__ - Printing 3 examples
06/12/2022 10:13:13 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/12/2022 10:13:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:13 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/12/2022 10:13:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:13 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/12/2022 10:13:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:13 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:13:13 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:13:13 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 10:13:13 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:13:13 - INFO - __main__ - Printing 3 examples
06/12/2022 10:13:13 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
06/12/2022 10:13:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:13 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
06/12/2022 10:13:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:13 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
06/12/2022 10:13:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:13 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:13:13 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:13:14 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 10:13:14 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.19489664082687339 on epoch=499
06/12/2022 10:13:14 - INFO - __main__ - save last model!
06/12/2022 10:13:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 10:13:15 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 10:13:15 - INFO - __main__ - Printing 3 examples
06/12/2022 10:13:15 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 10:13:15 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:15 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 10:13:15 - INFO - __main__ - ['entailment']
06/12/2022 10:13:15 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 10:13:15 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:15 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:13:15 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:13:16 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 10:13:29 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 10:13:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 10:13:30 - INFO - __main__ - Starting training!
06/12/2022 10:13:46 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_100_0.2_8_predictions.txt
06/12/2022 10:13:46 - INFO - __main__ - Classification-F1 on test data: 0.1622
06/12/2022 10:13:46 - INFO - __main__ - prefix=anli_32_100, lr=0.2, bsz=8, dev_performance=0.4063714063714064, test_performance=0.1622282224139809
06/12/2022 10:13:46 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.5, bsz=8 ...
06/12/2022 10:13:47 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:13:47 - INFO - __main__ - Printing 3 examples
06/12/2022 10:13:47 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/12/2022 10:13:47 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:47 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/12/2022 10:13:47 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:47 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/12/2022 10:13:47 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:47 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:13:47 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:13:47 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 10:13:47 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:13:47 - INFO - __main__ - Printing 3 examples
06/12/2022 10:13:47 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
06/12/2022 10:13:47 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:47 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
06/12/2022 10:13:47 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:47 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
06/12/2022 10:13:47 - INFO - __main__ - ['contradiction']
06/12/2022 10:13:47 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:13:47 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:13:47 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 10:14:02 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 10:14:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 10:14:03 - INFO - __main__ - Starting training!
06/12/2022 10:14:07 - INFO - __main__ - Step 10 Global step 10 Train loss 0.96 on epoch=1
06/12/2022 10:14:09 - INFO - __main__ - Step 20 Global step 20 Train loss 0.74 on epoch=3
06/12/2022 10:14:12 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=4
06/12/2022 10:14:15 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=6
06/12/2022 10:14:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=8
06/12/2022 10:14:20 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.1836290071584189 on epoch=8
06/12/2022 10:14:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1836290071584189 on epoch=8, global_step=50
06/12/2022 10:14:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=9
06/12/2022 10:14:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=11
06/12/2022 10:14:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=13
06/12/2022 10:14:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=14
06/12/2022 10:14:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=16
06/12/2022 10:14:37 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 10:14:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=18
06/12/2022 10:14:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=19
06/12/2022 10:14:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=21
06/12/2022 10:14:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=23
06/12/2022 10:14:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=24
06/12/2022 10:14:53 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 10:14:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=26
06/12/2022 10:14:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=28
06/12/2022 10:15:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.55 on epoch=29
06/12/2022 10:15:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=31
06/12/2022 10:15:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=33
06/12/2022 10:15:09 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 10:15:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=34
06/12/2022 10:15:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=36
06/12/2022 10:15:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=38
06/12/2022 10:15:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=39
06/12/2022 10:15:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=41
06/12/2022 10:15:26 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.1881810228266921 on epoch=41
06/12/2022 10:15:26 - INFO - __main__ - Saving model with best Classification-F1: 0.1836290071584189 -> 0.1881810228266921 on epoch=41, global_step=250
06/12/2022 10:15:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=43
06/12/2022 10:15:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=44
06/12/2022 10:15:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=46
06/12/2022 10:15:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=48
06/12/2022 10:15:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=49
06/12/2022 10:15:42 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=49
06/12/2022 10:15:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=51
06/12/2022 10:15:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=53
06/12/2022 10:15:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=54
06/12/2022 10:15:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=56
06/12/2022 10:15:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=58
06/12/2022 10:15:59 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.2087619047619048 on epoch=58
06/12/2022 10:15:59 - INFO - __main__ - Saving model with best Classification-F1: 0.1881810228266921 -> 0.2087619047619048 on epoch=58, global_step=350
06/12/2022 10:16:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=59
06/12/2022 10:16:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=61
06/12/2022 10:16:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=63
06/12/2022 10:16:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=64
06/12/2022 10:16:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=66
06/12/2022 10:16:16 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 10:16:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=68
06/12/2022 10:16:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=69
06/12/2022 10:16:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=71
06/12/2022 10:16:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=73
06/12/2022 10:16:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=74
06/12/2022 10:16:33 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.27336300063572794 on epoch=74
06/12/2022 10:16:33 - INFO - __main__ - Saving model with best Classification-F1: 0.2087619047619048 -> 0.27336300063572794 on epoch=74, global_step=450
06/12/2022 10:16:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=76
06/12/2022 10:16:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=78
06/12/2022 10:16:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=79
06/12/2022 10:16:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=81
06/12/2022 10:16:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=83
06/12/2022 10:16:49 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=83
06/12/2022 10:16:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=84
06/12/2022 10:16:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=86
06/12/2022 10:16:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=88
06/12/2022 10:17:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=89
06/12/2022 10:17:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=91
06/12/2022 10:17:06 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 10:17:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=93
06/12/2022 10:17:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=94
06/12/2022 10:17:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=96
06/12/2022 10:17:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=98
06/12/2022 10:17:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=99
06/12/2022 10:17:22 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.28012654587287894 on epoch=99
06/12/2022 10:17:22 - INFO - __main__ - Saving model with best Classification-F1: 0.27336300063572794 -> 0.28012654587287894 on epoch=99, global_step=600
06/12/2022 10:17:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=101
06/12/2022 10:17:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=103
06/12/2022 10:17:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=104
06/12/2022 10:17:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=106
06/12/2022 10:17:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=108
06/12/2022 10:17:39 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.1990221455277538 on epoch=108
06/12/2022 10:17:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=109
06/12/2022 10:17:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=111
06/12/2022 10:17:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=113
06/12/2022 10:17:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=114
06/12/2022 10:17:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=116
06/12/2022 10:17:56 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 10:17:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=118
06/12/2022 10:18:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=119
06/12/2022 10:18:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=121
06/12/2022 10:18:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=123
06/12/2022 10:18:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=124
06/12/2022 10:18:12 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.25120772946859904 on epoch=124
06/12/2022 10:18:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=126
06/12/2022 10:18:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=128
06/12/2022 10:18:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=129
06/12/2022 10:18:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=131
06/12/2022 10:18:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=133
06/12/2022 10:18:29 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.2222222222222222 on epoch=133
06/12/2022 10:18:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=134
06/12/2022 10:18:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=136
06/12/2022 10:18:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=138
06/12/2022 10:18:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=139
06/12/2022 10:18:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=141
06/12/2022 10:18:45 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.16666666666666666 on epoch=141
06/12/2022 10:18:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=143
06/12/2022 10:18:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=144
06/12/2022 10:18:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=146
06/12/2022 10:18:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=148
06/12/2022 10:18:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=149
06/12/2022 10:19:02 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.2663276836158192 on epoch=149
06/12/2022 10:19:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=151
06/12/2022 10:19:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=153
06/12/2022 10:19:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=154
06/12/2022 10:19:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=156
06/12/2022 10:19:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=158
06/12/2022 10:19:19 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.20543494313986113 on epoch=158
06/12/2022 10:19:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=159
06/12/2022 10:19:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=161
06/12/2022 10:19:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=163
06/12/2022 10:19:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=164
06/12/2022 10:19:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=166
06/12/2022 10:19:36 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.16666666666666666 on epoch=166
06/12/2022 10:19:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=168
06/12/2022 10:19:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=169
06/12/2022 10:19:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=171
06/12/2022 10:19:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=173
06/12/2022 10:19:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=174
06/12/2022 10:19:52 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.3361236424394319 on epoch=174
06/12/2022 10:19:53 - INFO - __main__ - Saving model with best Classification-F1: 0.28012654587287894 -> 0.3361236424394319 on epoch=174, global_step=1050
06/12/2022 10:19:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=176
06/12/2022 10:19:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=178
06/12/2022 10:20:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=179
06/12/2022 10:20:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=181
06/12/2022 10:20:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=183
06/12/2022 10:20:09 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.4084692465724708 on epoch=183
06/12/2022 10:20:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3361236424394319 -> 0.4084692465724708 on epoch=183, global_step=1100
06/12/2022 10:20:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=184
06/12/2022 10:20:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=186
06/12/2022 10:20:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=188
06/12/2022 10:20:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=189
06/12/2022 10:20:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=191
06/12/2022 10:20:25 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.2593538863339579 on epoch=191
06/12/2022 10:20:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=193
06/12/2022 10:20:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=194
06/12/2022 10:20:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=196
06/12/2022 10:20:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=198
06/12/2022 10:20:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=199
06/12/2022 10:20:42 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.2281464309861876 on epoch=199
06/12/2022 10:20:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=201
06/12/2022 10:20:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=203
06/12/2022 10:20:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=204
06/12/2022 10:20:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=206
06/12/2022 10:20:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=208
06/12/2022 10:20:58 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.45445499773857984 on epoch=208
06/12/2022 10:20:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4084692465724708 -> 0.45445499773857984 on epoch=208, global_step=1250
06/12/2022 10:21:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=209
06/12/2022 10:21:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=211
06/12/2022 10:21:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=213
06/12/2022 10:21:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=214
06/12/2022 10:21:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=216
06/12/2022 10:21:15 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.1833575229801645 on epoch=216
06/12/2022 10:21:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=218
06/12/2022 10:21:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=219
06/12/2022 10:21:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=221
06/12/2022 10:21:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=223
06/12/2022 10:21:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=224
06/12/2022 10:21:31 - INFO - __main__ - Global step 1350 Train loss 0.34 Classification-F1 0.3549479143486994 on epoch=224
06/12/2022 10:21:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=226
06/12/2022 10:21:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=228
06/12/2022 10:21:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=229
06/12/2022 10:21:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=231
06/12/2022 10:21:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.33 on epoch=233
06/12/2022 10:21:48 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.31454042551910744 on epoch=233
06/12/2022 10:21:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=234
06/12/2022 10:21:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=236
06/12/2022 10:21:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=238
06/12/2022 10:21:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.30 on epoch=239
06/12/2022 10:22:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=241
06/12/2022 10:22:05 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.3063890161347788 on epoch=241
06/12/2022 10:22:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=243
06/12/2022 10:22:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=244
06/12/2022 10:22:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=246
06/12/2022 10:22:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=248
06/12/2022 10:22:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=249
06/12/2022 10:22:22 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.3586859052743132 on epoch=249
06/12/2022 10:22:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.30 on epoch=251
06/12/2022 10:22:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=253
06/12/2022 10:22:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=254
06/12/2022 10:22:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.29 on epoch=256
06/12/2022 10:22:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=258
06/12/2022 10:22:39 - INFO - __main__ - Global step 1550 Train loss 0.30 Classification-F1 0.36586299272866435 on epoch=258
06/12/2022 10:22:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=259
06/12/2022 10:22:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.30 on epoch=261
06/12/2022 10:22:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=263
06/12/2022 10:22:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=264
06/12/2022 10:22:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=266
06/12/2022 10:22:56 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.32376763021924315 on epoch=266
06/12/2022 10:22:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.28 on epoch=268
06/12/2022 10:23:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=269
06/12/2022 10:23:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=271
06/12/2022 10:23:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=273
06/12/2022 10:23:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.27 on epoch=274
06/12/2022 10:23:13 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.41504747606442516 on epoch=274
06/12/2022 10:23:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=276
06/12/2022 10:23:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=278
06/12/2022 10:23:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=279
06/12/2022 10:23:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=281
06/12/2022 10:23:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=283
06/12/2022 10:23:30 - INFO - __main__ - Global step 1700 Train loss 0.27 Classification-F1 0.3653637517399492 on epoch=283
06/12/2022 10:23:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=284
06/12/2022 10:23:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=286
06/12/2022 10:23:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=288
06/12/2022 10:23:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.29 on epoch=289
06/12/2022 10:23:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=291
06/12/2022 10:23:47 - INFO - __main__ - Global step 1750 Train loss 0.26 Classification-F1 0.28811475878471865 on epoch=291
06/12/2022 10:23:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.23 on epoch=293
06/12/2022 10:23:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=294
06/12/2022 10:23:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=296
06/12/2022 10:23:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=298
06/12/2022 10:24:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=299
06/12/2022 10:24:04 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.35966777408637873 on epoch=299
06/12/2022 10:24:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=301
06/12/2022 10:24:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=303
06/12/2022 10:24:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=304
06/12/2022 10:24:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.17 on epoch=306
06/12/2022 10:24:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=308
06/12/2022 10:24:21 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.35244252873563214 on epoch=308
06/12/2022 10:24:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=309
06/12/2022 10:24:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=311
06/12/2022 10:24:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=313
06/12/2022 10:24:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=314
06/12/2022 10:24:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=316
06/12/2022 10:24:39 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.313812451416861 on epoch=316
06/12/2022 10:24:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.15 on epoch=318
06/12/2022 10:24:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=319
06/12/2022 10:24:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=321
06/12/2022 10:24:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=323
06/12/2022 10:24:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=324
06/12/2022 10:24:56 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.33454177701214444 on epoch=324
06/12/2022 10:24:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=326
06/12/2022 10:25:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=328
06/12/2022 10:25:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=329
06/12/2022 10:25:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=331
06/12/2022 10:25:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=333
06/12/2022 10:25:13 - INFO - __main__ - Global step 2000 Train loss 0.16 Classification-F1 0.1894305372566242 on epoch=333
06/12/2022 10:25:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=334
06/12/2022 10:25:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=336
06/12/2022 10:25:21 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=338
06/12/2022 10:25:24 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=339
06/12/2022 10:25:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=341
06/12/2022 10:25:30 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.2479957374570958 on epoch=341
06/12/2022 10:25:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=343
06/12/2022 10:25:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.13 on epoch=344
06/12/2022 10:25:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=346
06/12/2022 10:25:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=348
06/12/2022 10:25:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.12 on epoch=349
06/12/2022 10:25:47 - INFO - __main__ - Global step 2100 Train loss 0.15 Classification-F1 0.38892175176421756 on epoch=349
06/12/2022 10:25:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.12 on epoch=351
06/12/2022 10:25:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=353
06/12/2022 10:25:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=354
06/12/2022 10:25:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=356
06/12/2022 10:26:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=358
06/12/2022 10:26:04 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.31934731934731936 on epoch=358
06/12/2022 10:26:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=359
06/12/2022 10:26:10 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=361
06/12/2022 10:26:13 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=363
06/12/2022 10:26:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.18 on epoch=364
06/12/2022 10:26:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=366
06/12/2022 10:26:22 - INFO - __main__ - Global step 2200 Train loss 0.14 Classification-F1 0.26786645800730313 on epoch=366
06/12/2022 10:26:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=368
06/12/2022 10:26:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=369
06/12/2022 10:26:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.12 on epoch=371
06/12/2022 10:26:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=373
06/12/2022 10:26:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=374
06/12/2022 10:26:39 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.2804399057344855 on epoch=374
06/12/2022 10:26:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.10 on epoch=376
06/12/2022 10:26:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=378
06/12/2022 10:26:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=379
06/12/2022 10:26:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=381
06/12/2022 10:26:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.16 on epoch=383
06/12/2022 10:26:56 - INFO - __main__ - Global step 2300 Train loss 0.12 Classification-F1 0.20816007154035324 on epoch=383
06/12/2022 10:26:59 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=384
06/12/2022 10:27:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=386
06/12/2022 10:27:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=388
06/12/2022 10:27:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=389
06/12/2022 10:27:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=391
06/12/2022 10:27:13 - INFO - __main__ - Global step 2350 Train loss 0.14 Classification-F1 0.3704337492572787 on epoch=391
06/12/2022 10:27:16 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=393
06/12/2022 10:27:19 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=394
06/12/2022 10:27:22 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=396
06/12/2022 10:27:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=398
06/12/2022 10:27:27 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=399
06/12/2022 10:27:30 - INFO - __main__ - Global step 2400 Train loss 0.12 Classification-F1 0.3422542460113227 on epoch=399
06/12/2022 10:27:33 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=401
06/12/2022 10:27:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=403
06/12/2022 10:27:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=404
06/12/2022 10:27:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=406
06/12/2022 10:27:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=408
06/12/2022 10:27:48 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.3619152990349745 on epoch=408
06/12/2022 10:27:50 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=409
06/12/2022 10:27:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.13 on epoch=411
06/12/2022 10:27:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=413
06/12/2022 10:27:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=414
06/12/2022 10:28:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.11 on epoch=416
06/12/2022 10:28:05 - INFO - __main__ - Global step 2500 Train loss 0.12 Classification-F1 0.2638663573537523 on epoch=416
06/12/2022 10:28:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=418
06/12/2022 10:28:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=419
06/12/2022 10:28:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=421
06/12/2022 10:28:16 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=423
06/12/2022 10:28:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=424
06/12/2022 10:28:21 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.28326904532304725 on epoch=424
06/12/2022 10:28:24 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=426
06/12/2022 10:28:27 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=428
06/12/2022 10:28:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=429
06/12/2022 10:28:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=431
06/12/2022 10:28:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=433
06/12/2022 10:28:39 - INFO - __main__ - Global step 2600 Train loss 0.10 Classification-F1 0.16716853408029878 on epoch=433
06/12/2022 10:28:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=434
06/12/2022 10:28:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=436
06/12/2022 10:28:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=438
06/12/2022 10:28:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=439
06/12/2022 10:28:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=441
06/12/2022 10:28:56 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.33343869089376615 on epoch=441
06/12/2022 10:28:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.10 on epoch=443
06/12/2022 10:29:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=444
06/12/2022 10:29:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=446
06/12/2022 10:29:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=448
06/12/2022 10:29:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=449
06/12/2022 10:29:13 - INFO - __main__ - Global step 2700 Train loss 0.08 Classification-F1 0.2587014411027569 on epoch=449
06/12/2022 10:29:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=451
06/12/2022 10:29:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=453
06/12/2022 10:29:21 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=454
06/12/2022 10:29:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=456
06/12/2022 10:29:26 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=458
06/12/2022 10:29:29 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.372615039281706 on epoch=458
06/12/2022 10:29:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=459
06/12/2022 10:29:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=461
06/12/2022 10:29:37 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=463
06/12/2022 10:29:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=464
06/12/2022 10:29:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.10 on epoch=466
06/12/2022 10:29:46 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.35610504862884707 on epoch=466
06/12/2022 10:29:48 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=468
06/12/2022 10:29:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.11 on epoch=469
06/12/2022 10:29:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=471
06/12/2022 10:29:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=473
06/12/2022 10:30:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=474
06/12/2022 10:30:02 - INFO - __main__ - Global step 2850 Train loss 0.09 Classification-F1 0.20749288100886276 on epoch=474
06/12/2022 10:30:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=476
06/12/2022 10:30:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=478
06/12/2022 10:30:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=479
06/12/2022 10:30:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=481
06/12/2022 10:30:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=483
06/12/2022 10:30:20 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.31519904931669634 on epoch=483
06/12/2022 10:30:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=484
06/12/2022 10:30:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=486
06/12/2022 10:30:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=488
06/12/2022 10:30:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=489
06/12/2022 10:30:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=491
06/12/2022 10:30:36 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.29039099374920274 on epoch=491
06/12/2022 10:30:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.12 on epoch=493
06/12/2022 10:30:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=494
06/12/2022 10:30:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=496
06/12/2022 10:30:47 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=498
06/12/2022 10:30:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=499
06/12/2022 10:30:51 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:30:51 - INFO - __main__ - Printing 3 examples
06/12/2022 10:30:51 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/12/2022 10:30:51 - INFO - __main__ - ['contradiction']
06/12/2022 10:30:51 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/12/2022 10:30:51 - INFO - __main__ - ['contradiction']
06/12/2022 10:30:51 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/12/2022 10:30:51 - INFO - __main__ - ['contradiction']
06/12/2022 10:30:51 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:30:52 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:30:52 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 10:30:52 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:30:52 - INFO - __main__ - Printing 3 examples
06/12/2022 10:30:52 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
06/12/2022 10:30:52 - INFO - __main__ - ['contradiction']
06/12/2022 10:30:52 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
06/12/2022 10:30:52 - INFO - __main__ - ['contradiction']
06/12/2022 10:30:52 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
06/12/2022 10:30:52 - INFO - __main__ - ['contradiction']
06/12/2022 10:30:52 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:30:52 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:30:52 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 10:30:53 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.19622415343122565 on epoch=499
06/12/2022 10:30:53 - INFO - __main__ - save last model!
06/12/2022 10:30:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 10:30:53 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 10:30:53 - INFO - __main__ - Printing 3 examples
06/12/2022 10:30:53 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 10:30:53 - INFO - __main__ - ['contradiction']
06/12/2022 10:30:53 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 10:30:53 - INFO - __main__ - ['entailment']
06/12/2022 10:30:53 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 10:30:53 - INFO - __main__ - ['contradiction']
06/12/2022 10:30:53 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:30:54 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:30:55 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 10:31:08 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 10:31:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 10:31:09 - INFO - __main__ - Starting training!
06/12/2022 10:31:26 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_13_0.5_8_predictions.txt
06/12/2022 10:31:26 - INFO - __main__ - Classification-F1 on test data: 0.1033
06/12/2022 10:31:26 - INFO - __main__ - prefix=anli_32_13, lr=0.5, bsz=8, dev_performance=0.45445499773857984, test_performance=0.10329595722848348
06/12/2022 10:31:26 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.4, bsz=8 ...
06/12/2022 10:31:27 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:31:27 - INFO - __main__ - Printing 3 examples
06/12/2022 10:31:27 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/12/2022 10:31:27 - INFO - __main__ - ['contradiction']
06/12/2022 10:31:27 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/12/2022 10:31:27 - INFO - __main__ - ['contradiction']
06/12/2022 10:31:27 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/12/2022 10:31:27 - INFO - __main__ - ['contradiction']
06/12/2022 10:31:27 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:31:27 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:31:27 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 10:31:27 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:31:27 - INFO - __main__ - Printing 3 examples
06/12/2022 10:31:27 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
06/12/2022 10:31:27 - INFO - __main__ - ['contradiction']
06/12/2022 10:31:27 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
06/12/2022 10:31:27 - INFO - __main__ - ['contradiction']
06/12/2022 10:31:27 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
06/12/2022 10:31:27 - INFO - __main__ - ['contradiction']
06/12/2022 10:31:27 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:31:27 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:31:27 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 10:31:43 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 10:31:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 10:31:44 - INFO - __main__ - Starting training!
06/12/2022 10:31:48 - INFO - __main__ - Step 10 Global step 10 Train loss 0.98 on epoch=1
06/12/2022 10:31:51 - INFO - __main__ - Step 20 Global step 20 Train loss 0.71 on epoch=3
06/12/2022 10:31:54 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=4
06/12/2022 10:31:56 - INFO - __main__ - Step 40 Global step 40 Train loss 1.73 on epoch=6
06/12/2022 10:31:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.76 on epoch=8
06/12/2022 10:32:01 - INFO - __main__ - Global step 50 Train loss 0.94 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 10:32:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 10:32:04 - INFO - __main__ - Step 60 Global step 60 Train loss 1.02 on epoch=9
06/12/2022 10:32:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=11
06/12/2022 10:32:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=13
06/12/2022 10:32:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.57 on epoch=14
06/12/2022 10:32:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.69 on epoch=16
06/12/2022 10:32:17 - INFO - __main__ - Global step 100 Train loss 0.67 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 10:32:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=18
06/12/2022 10:32:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.64 on epoch=19
06/12/2022 10:32:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=21
06/12/2022 10:32:29 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=23
06/12/2022 10:32:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=24
06/12/2022 10:32:33 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 10:32:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=26
06/12/2022 10:32:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=28
06/12/2022 10:32:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=29
06/12/2022 10:32:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=31
06/12/2022 10:32:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=33
06/12/2022 10:32:50 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 10:32:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=34
06/12/2022 10:32:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=36
06/12/2022 10:32:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=38
06/12/2022 10:33:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.54 on epoch=39
06/12/2022 10:33:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=41
06/12/2022 10:33:08 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.2661360711784532 on epoch=41
06/12/2022 10:33:08 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2661360711784532 on epoch=41, global_step=250
06/12/2022 10:33:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=43
06/12/2022 10:33:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=44
06/12/2022 10:33:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=46
06/12/2022 10:33:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=48
06/12/2022 10:33:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.52 on epoch=49
06/12/2022 10:33:25 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=49
06/12/2022 10:33:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=51
06/12/2022 10:33:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=53
06/12/2022 10:33:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=54
06/12/2022 10:33:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=56
06/12/2022 10:33:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=58
06/12/2022 10:33:41 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=58
06/12/2022 10:33:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=59
06/12/2022 10:33:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=61
06/12/2022 10:33:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=63
06/12/2022 10:33:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=64
06/12/2022 10:33:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=66
06/12/2022 10:33:57 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 10:34:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=68
06/12/2022 10:34:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=69
06/12/2022 10:34:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=71
06/12/2022 10:34:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=73
06/12/2022 10:34:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=74
06/12/2022 10:34:14 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=74
06/12/2022 10:34:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=76
06/12/2022 10:34:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=78
06/12/2022 10:34:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=79
06/12/2022 10:34:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=81
06/12/2022 10:34:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=83
06/12/2022 10:34:32 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.30029121863799285 on epoch=83
06/12/2022 10:34:32 - INFO - __main__ - Saving model with best Classification-F1: 0.2661360711784532 -> 0.30029121863799285 on epoch=83, global_step=500
06/12/2022 10:34:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=84
06/12/2022 10:34:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=86
06/12/2022 10:34:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=88
06/12/2022 10:34:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=89
06/12/2022 10:34:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=91
06/12/2022 10:34:48 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 10:34:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=93
06/12/2022 10:34:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=94
06/12/2022 10:34:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=96
06/12/2022 10:34:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=98
06/12/2022 10:35:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=99
06/12/2022 10:35:05 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=99
06/12/2022 10:35:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=101
06/12/2022 10:35:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=103
06/12/2022 10:35:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=104
06/12/2022 10:35:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=106
06/12/2022 10:35:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=108
06/12/2022 10:35:22 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.20035778175313057 on epoch=108
06/12/2022 10:35:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=109
06/12/2022 10:35:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=111
06/12/2022 10:35:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=113
06/12/2022 10:35:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=114
06/12/2022 10:35:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=116
06/12/2022 10:35:38 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 10:35:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=118
06/12/2022 10:35:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=119
06/12/2022 10:35:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=121
06/12/2022 10:35:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=123
06/12/2022 10:35:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=124
06/12/2022 10:35:55 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.3069281045751634 on epoch=124
06/12/2022 10:35:55 - INFO - __main__ - Saving model with best Classification-F1: 0.30029121863799285 -> 0.3069281045751634 on epoch=124, global_step=750
06/12/2022 10:35:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=126
06/12/2022 10:36:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=128
06/12/2022 10:36:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=129
06/12/2022 10:36:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=131
06/12/2022 10:36:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=133
06/12/2022 10:36:12 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=133
06/12/2022 10:36:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=134
06/12/2022 10:36:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=136
06/12/2022 10:36:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=138
06/12/2022 10:36:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=139
06/12/2022 10:36:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=141
06/12/2022 10:36:29 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=141
06/12/2022 10:36:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=143
06/12/2022 10:36:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=144
06/12/2022 10:36:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=146
06/12/2022 10:36:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=148
06/12/2022 10:36:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=149
06/12/2022 10:36:46 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.1881810228266921 on epoch=149
06/12/2022 10:36:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=151
06/12/2022 10:36:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=153
06/12/2022 10:36:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=154
06/12/2022 10:36:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=156
06/12/2022 10:37:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=158
06/12/2022 10:37:02 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=158
06/12/2022 10:37:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=159
06/12/2022 10:37:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=161
06/12/2022 10:37:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=163
06/12/2022 10:37:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=164
06/12/2022 10:37:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=166
06/12/2022 10:37:19 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=166
06/12/2022 10:37:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=168
06/12/2022 10:37:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=169
06/12/2022 10:37:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=171
06/12/2022 10:37:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=173
06/12/2022 10:37:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=174
06/12/2022 10:37:36 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=174
06/12/2022 10:37:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=176
06/12/2022 10:37:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=178
06/12/2022 10:37:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=179
06/12/2022 10:37:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=181
06/12/2022 10:37:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=183
06/12/2022 10:37:53 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.2409718454211129 on epoch=183
06/12/2022 10:37:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=184
06/12/2022 10:37:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=186
06/12/2022 10:38:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=188
06/12/2022 10:38:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=189
06/12/2022 10:38:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=191
06/12/2022 10:38:10 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=191
06/12/2022 10:38:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=193
06/12/2022 10:38:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=194
06/12/2022 10:38:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=196
06/12/2022 10:38:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=198
06/12/2022 10:38:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=199
06/12/2022 10:38:26 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=199
06/12/2022 10:38:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=201
06/12/2022 10:38:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=203
06/12/2022 10:38:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=204
06/12/2022 10:38:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=206
06/12/2022 10:38:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=208
06/12/2022 10:38:43 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.22240868154612073 on epoch=208
06/12/2022 10:38:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=209
06/12/2022 10:38:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=211
06/12/2022 10:38:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=213
06/12/2022 10:38:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=214
06/12/2022 10:38:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=216
06/12/2022 10:38:59 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=216
06/12/2022 10:39:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=218
06/12/2022 10:39:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=219
06/12/2022 10:39:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=221
06/12/2022 10:39:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=223
06/12/2022 10:39:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=224
06/12/2022 10:39:16 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.21098468886079505 on epoch=224
06/12/2022 10:39:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=226
06/12/2022 10:39:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=228
06/12/2022 10:39:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=229
06/12/2022 10:39:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=231
06/12/2022 10:39:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=233
06/12/2022 10:39:32 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.33703772686823535 on epoch=233
06/12/2022 10:39:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3069281045751634 -> 0.33703772686823535 on epoch=233, global_step=1400
06/12/2022 10:39:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=234
06/12/2022 10:39:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=236
06/12/2022 10:39:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=238
06/12/2022 10:39:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=239
06/12/2022 10:39:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=241
06/12/2022 10:39:49 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.2177099065838787 on epoch=241
06/12/2022 10:39:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=243
06/12/2022 10:39:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=244
06/12/2022 10:39:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=246
06/12/2022 10:40:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=248
06/12/2022 10:40:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=249
06/12/2022 10:40:06 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.3382059800664452 on epoch=249
06/12/2022 10:40:06 - INFO - __main__ - Saving model with best Classification-F1: 0.33703772686823535 -> 0.3382059800664452 on epoch=249, global_step=1500
06/12/2022 10:40:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=251
06/12/2022 10:40:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=253
06/12/2022 10:40:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=254
06/12/2022 10:40:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=256
06/12/2022 10:40:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=258
06/12/2022 10:40:23 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.31054534024831054 on epoch=258
06/12/2022 10:40:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=259
06/12/2022 10:40:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=261
06/12/2022 10:40:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=263
06/12/2022 10:40:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=264
06/12/2022 10:40:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=266
06/12/2022 10:40:40 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.2465451662817376 on epoch=266
06/12/2022 10:40:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=268
06/12/2022 10:40:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=269
06/12/2022 10:40:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.34 on epoch=271
06/12/2022 10:40:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=273
06/12/2022 10:40:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=274
06/12/2022 10:40:56 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.21277298708164002 on epoch=274
06/12/2022 10:40:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=276
06/12/2022 10:41:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=278
06/12/2022 10:41:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.36 on epoch=279
06/12/2022 10:41:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=281
06/12/2022 10:41:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=283
06/12/2022 10:41:13 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.3179395296752519 on epoch=283
06/12/2022 10:41:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=284
06/12/2022 10:41:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=286
06/12/2022 10:41:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=288
06/12/2022 10:41:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=289
06/12/2022 10:41:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=291
06/12/2022 10:41:30 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.31315510501557015 on epoch=291
06/12/2022 10:41:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.31 on epoch=293
06/12/2022 10:41:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=294
06/12/2022 10:41:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=296
06/12/2022 10:41:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=298
06/12/2022 10:41:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=299
06/12/2022 10:41:47 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.18971428571428572 on epoch=299
06/12/2022 10:41:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=301
06/12/2022 10:41:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=303
06/12/2022 10:41:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=304
06/12/2022 10:41:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=306
06/12/2022 10:42:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=308
06/12/2022 10:42:03 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.27820717294401504 on epoch=308
06/12/2022 10:42:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=309
06/12/2022 10:42:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=311
06/12/2022 10:42:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=313
06/12/2022 10:42:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=314
06/12/2022 10:42:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=316
06/12/2022 10:42:20 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.26352657004830915 on epoch=316
06/12/2022 10:42:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=318
06/12/2022 10:42:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.34 on epoch=319
06/12/2022 10:42:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.34 on epoch=321
06/12/2022 10:42:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.30 on epoch=323
06/12/2022 10:42:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=324
06/12/2022 10:42:37 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.17204301075268816 on epoch=324
06/12/2022 10:42:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=326
06/12/2022 10:42:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=328
06/12/2022 10:42:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=329
06/12/2022 10:42:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=331
06/12/2022 10:42:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=333
06/12/2022 10:42:54 - INFO - __main__ - Global step 2000 Train loss 0.31 Classification-F1 0.36594997806055285 on epoch=333
06/12/2022 10:42:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3382059800664452 -> 0.36594997806055285 on epoch=333, global_step=2000
06/12/2022 10:42:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=334
06/12/2022 10:43:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.31 on epoch=336
06/12/2022 10:43:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.27 on epoch=338
06/12/2022 10:43:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.33 on epoch=339
06/12/2022 10:43:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=341
06/12/2022 10:43:11 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.25132107376875923 on epoch=341
06/12/2022 10:43:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.31 on epoch=343
06/12/2022 10:43:17 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.31 on epoch=344
06/12/2022 10:43:20 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.33 on epoch=346
06/12/2022 10:43:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.32 on epoch=348
06/12/2022 10:43:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.31 on epoch=349
06/12/2022 10:43:29 - INFO - __main__ - Global step 2100 Train loss 0.31 Classification-F1 0.40594699418228836 on epoch=349
06/12/2022 10:43:29 - INFO - __main__ - Saving model with best Classification-F1: 0.36594997806055285 -> 0.40594699418228836 on epoch=349, global_step=2100
06/12/2022 10:43:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.30 on epoch=351
06/12/2022 10:43:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.26 on epoch=353
06/12/2022 10:43:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=354
06/12/2022 10:43:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=356
06/12/2022 10:43:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.27 on epoch=358
06/12/2022 10:43:46 - INFO - __main__ - Global step 2150 Train loss 0.27 Classification-F1 0.36501547987616095 on epoch=358
06/12/2022 10:43:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.30 on epoch=359
06/12/2022 10:43:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=361
06/12/2022 10:43:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.24 on epoch=363
06/12/2022 10:43:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=364
06/12/2022 10:44:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.29 on epoch=366
06/12/2022 10:44:03 - INFO - __main__ - Global step 2200 Train loss 0.26 Classification-F1 0.2912091288128154 on epoch=366
06/12/2022 10:44:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=368
06/12/2022 10:44:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=369
06/12/2022 10:44:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=371
06/12/2022 10:44:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=373
06/12/2022 10:44:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.27 on epoch=374
06/12/2022 10:44:20 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.34871362768228287 on epoch=374
06/12/2022 10:44:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.23 on epoch=376
06/12/2022 10:44:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.20 on epoch=378
06/12/2022 10:44:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=379
06/12/2022 10:44:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=381
06/12/2022 10:44:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.25 on epoch=383
06/12/2022 10:44:37 - INFO - __main__ - Global step 2300 Train loss 0.22 Classification-F1 0.40824915824915825 on epoch=383
06/12/2022 10:44:37 - INFO - __main__ - Saving model with best Classification-F1: 0.40594699418228836 -> 0.40824915824915825 on epoch=383, global_step=2300
06/12/2022 10:44:40 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.19 on epoch=384
06/12/2022 10:44:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=386
06/12/2022 10:44:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=388
06/12/2022 10:44:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=389
06/12/2022 10:44:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=391
06/12/2022 10:44:54 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.2747112704101951 on epoch=391
06/12/2022 10:44:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.23 on epoch=393
06/12/2022 10:45:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.24 on epoch=394
06/12/2022 10:45:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=396
06/12/2022 10:45:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=398
06/12/2022 10:45:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.16 on epoch=399
06/12/2022 10:45:11 - INFO - __main__ - Global step 2400 Train loss 0.21 Classification-F1 0.2904685223525803 on epoch=399
06/12/2022 10:45:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=401
06/12/2022 10:45:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.23 on epoch=403
06/12/2022 10:45:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=404
06/12/2022 10:45:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=406
06/12/2022 10:45:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.24 on epoch=408
06/12/2022 10:45:28 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.2654589371980676 on epoch=408
06/12/2022 10:45:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.19 on epoch=409
06/12/2022 10:45:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.20 on epoch=411
06/12/2022 10:45:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.21 on epoch=413
06/12/2022 10:45:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=414
06/12/2022 10:45:42 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=416
06/12/2022 10:45:45 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.3051327150623601 on epoch=416
06/12/2022 10:45:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=418
06/12/2022 10:45:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=419
06/12/2022 10:45:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=421
06/12/2022 10:45:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=423
06/12/2022 10:46:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=424
06/12/2022 10:46:03 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.354109719578833 on epoch=424
06/12/2022 10:46:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=426
06/12/2022 10:46:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.26 on epoch=428
06/12/2022 10:46:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=429
06/12/2022 10:46:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=431
06/12/2022 10:46:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=433
06/12/2022 10:46:20 - INFO - __main__ - Global step 2600 Train loss 0.18 Classification-F1 0.3184005643881176 on epoch=433
06/12/2022 10:46:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=434
06/12/2022 10:46:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=436
06/12/2022 10:46:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=438
06/12/2022 10:46:31 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=439
06/12/2022 10:46:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=441
06/12/2022 10:46:37 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.3171682719637467 on epoch=441
06/12/2022 10:46:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=443
06/12/2022 10:46:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=444
06/12/2022 10:46:45 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=446
06/12/2022 10:46:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=448
06/12/2022 10:46:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=449
06/12/2022 10:46:54 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.3649799645885752 on epoch=449
06/12/2022 10:46:57 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=451
06/12/2022 10:46:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.15 on epoch=453
06/12/2022 10:47:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=454
06/12/2022 10:47:05 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=456
06/12/2022 10:47:08 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.21 on epoch=458
06/12/2022 10:47:11 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.3549382716049383 on epoch=458
06/12/2022 10:47:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.12 on epoch=459
06/12/2022 10:47:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=461
06/12/2022 10:47:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=463
06/12/2022 10:47:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=464
06/12/2022 10:47:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=466
06/12/2022 10:47:28 - INFO - __main__ - Global step 2800 Train loss 0.14 Classification-F1 0.38541224331451285 on epoch=466
06/12/2022 10:47:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=468
06/12/2022 10:47:34 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.13 on epoch=469
06/12/2022 10:47:36 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=471
06/12/2022 10:47:39 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=473
06/12/2022 10:47:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.12 on epoch=474
06/12/2022 10:47:45 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.32114081996434934 on epoch=474
06/12/2022 10:47:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=476
06/12/2022 10:47:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.21 on epoch=478
06/12/2022 10:47:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.18 on epoch=479
06/12/2022 10:47:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=481
06/12/2022 10:47:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.17 on epoch=483
06/12/2022 10:48:02 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.33323996265172734 on epoch=483
06/12/2022 10:48:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=484
06/12/2022 10:48:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=486
06/12/2022 10:48:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=488
06/12/2022 10:48:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=489
06/12/2022 10:48:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.14 on epoch=491
06/12/2022 10:48:19 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.3544457201173619 on epoch=491
06/12/2022 10:48:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=493
06/12/2022 10:48:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=494
06/12/2022 10:48:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=496
06/12/2022 10:48:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=498
06/12/2022 10:48:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.14 on epoch=499
06/12/2022 10:48:35 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:48:35 - INFO - __main__ - Printing 3 examples
06/12/2022 10:48:35 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/12/2022 10:48:35 - INFO - __main__ - ['contradiction']
06/12/2022 10:48:35 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/12/2022 10:48:35 - INFO - __main__ - ['contradiction']
06/12/2022 10:48:35 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/12/2022 10:48:35 - INFO - __main__ - ['contradiction']
06/12/2022 10:48:35 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:48:35 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:48:35 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 10:48:35 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:48:35 - INFO - __main__ - Printing 3 examples
06/12/2022 10:48:35 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
06/12/2022 10:48:35 - INFO - __main__ - ['contradiction']
06/12/2022 10:48:35 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
06/12/2022 10:48:35 - INFO - __main__ - ['contradiction']
06/12/2022 10:48:35 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
06/12/2022 10:48:35 - INFO - __main__ - ['contradiction']
06/12/2022 10:48:35 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:48:35 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:48:35 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 10:48:37 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.25902272970503243 on epoch=499
06/12/2022 10:48:37 - INFO - __main__ - save last model!
06/12/2022 10:48:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 10:48:37 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 10:48:37 - INFO - __main__ - Printing 3 examples
06/12/2022 10:48:37 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 10:48:37 - INFO - __main__ - ['contradiction']
06/12/2022 10:48:37 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 10:48:37 - INFO - __main__ - ['entailment']
06/12/2022 10:48:37 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 10:48:37 - INFO - __main__ - ['contradiction']
06/12/2022 10:48:37 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:48:37 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:48:38 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 10:48:51 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 10:48:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 10:48:52 - INFO - __main__ - Starting training!
06/12/2022 10:49:11 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_13_0.4_8_predictions.txt
06/12/2022 10:49:11 - INFO - __main__ - Classification-F1 on test data: 0.1557
06/12/2022 10:49:12 - INFO - __main__ - prefix=anli_32_13, lr=0.4, bsz=8, dev_performance=0.40824915824915825, test_performance=0.15574778811356246
06/12/2022 10:49:12 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.3, bsz=8 ...
06/12/2022 10:49:13 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:49:13 - INFO - __main__ - Printing 3 examples
06/12/2022 10:49:13 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/12/2022 10:49:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:49:13 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/12/2022 10:49:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:49:13 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/12/2022 10:49:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:49:13 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:49:13 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:49:13 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 10:49:13 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 10:49:13 - INFO - __main__ - Printing 3 examples
06/12/2022 10:49:13 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
06/12/2022 10:49:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:49:13 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
06/12/2022 10:49:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:49:13 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
06/12/2022 10:49:13 - INFO - __main__ - ['contradiction']
06/12/2022 10:49:13 - INFO - __main__ - Tokenizing Input ...
06/12/2022 10:49:13 - INFO - __main__ - Tokenizing Output ...
06/12/2022 10:49:13 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 10:49:33 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 10:49:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 10:49:33 - INFO - __main__ - Starting training!
06/12/2022 10:49:37 - INFO - __main__ - Step 10 Global step 10 Train loss 1.04 on epoch=1
06/12/2022 10:49:40 - INFO - __main__ - Step 20 Global step 20 Train loss 1.81 on epoch=3
06/12/2022 10:49:43 - INFO - __main__ - Step 30 Global step 30 Train loss 0.70 on epoch=4
06/12/2022 10:49:45 - INFO - __main__ - Step 40 Global step 40 Train loss 0.68 on epoch=6
06/12/2022 10:49:48 - INFO - __main__ - Step 50 Global step 50 Train loss 0.64 on epoch=8
06/12/2022 10:49:50 - INFO - __main__ - Global step 50 Train loss 0.97 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 10:49:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 10:49:53 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=9
06/12/2022 10:49:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=11
06/12/2022 10:49:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=13
06/12/2022 10:50:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=14
06/12/2022 10:50:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=16
06/12/2022 10:50:07 - INFO - __main__ - Global step 100 Train loss 0.59 Classification-F1 0.1679790026246719 on epoch=16
06/12/2022 10:50:07 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1679790026246719 on epoch=16, global_step=100
06/12/2022 10:50:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=18
06/12/2022 10:50:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=19
06/12/2022 10:50:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=21
06/12/2022 10:50:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=23
06/12/2022 10:50:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=24
06/12/2022 10:50:23 - INFO - __main__ - Global step 150 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 10:50:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=26
06/12/2022 10:50:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=28
06/12/2022 10:50:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=29
06/12/2022 10:50:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=31
06/12/2022 10:50:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=33
06/12/2022 10:50:40 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.18892001244942422 on epoch=33
06/12/2022 10:50:40 - INFO - __main__ - Saving model with best Classification-F1: 0.1679790026246719 -> 0.18892001244942422 on epoch=33, global_step=200
06/12/2022 10:50:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=34
06/12/2022 10:50:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=36
06/12/2022 10:50:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=38
06/12/2022 10:50:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=39
06/12/2022 10:50:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=41
06/12/2022 10:50:57 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 10:51:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=43
06/12/2022 10:51:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=44
06/12/2022 10:51:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=46
06/12/2022 10:51:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=48
06/12/2022 10:51:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=49
06/12/2022 10:51:14 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=49
06/12/2022 10:51:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=51
06/12/2022 10:51:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=53
06/12/2022 10:51:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=54
06/12/2022 10:51:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=56
06/12/2022 10:51:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=58
06/12/2022 10:51:30 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.21684771469563435 on epoch=58
06/12/2022 10:51:30 - INFO - __main__ - Saving model with best Classification-F1: 0.18892001244942422 -> 0.21684771469563435 on epoch=58, global_step=350
06/12/2022 10:51:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=59
06/12/2022 10:51:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=61
06/12/2022 10:51:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=63
06/12/2022 10:51:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=64
06/12/2022 10:51:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=66
06/12/2022 10:51:46 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 10:51:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=68
06/12/2022 10:51:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=69
06/12/2022 10:51:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=71
06/12/2022 10:51:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=73
06/12/2022 10:52:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=74
06/12/2022 10:52:03 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=74
06/12/2022 10:52:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=76
06/12/2022 10:52:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=78
06/12/2022 10:52:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=79
06/12/2022 10:52:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=81
06/12/2022 10:52:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=83
06/12/2022 10:52:20 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.2087619047619048 on epoch=83
06/12/2022 10:52:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=84
06/12/2022 10:52:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=86
06/12/2022 10:52:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=88
06/12/2022 10:52:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=89
06/12/2022 10:52:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=91
06/12/2022 10:52:37 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 10:52:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=93
06/12/2022 10:52:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=94
06/12/2022 10:52:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=96
06/12/2022 10:52:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=98
06/12/2022 10:52:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=99
06/12/2022 10:52:54 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=99
06/12/2022 10:52:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=101
06/12/2022 10:53:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=103
06/12/2022 10:53:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=104
06/12/2022 10:53:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=106
06/12/2022 10:53:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=108
06/12/2022 10:53:12 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=108
06/12/2022 10:53:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=109
06/12/2022 10:53:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=111
06/12/2022 10:53:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=113
06/12/2022 10:53:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=114
06/12/2022 10:53:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=116
06/12/2022 10:53:29 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 10:53:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=118
06/12/2022 10:53:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=119
06/12/2022 10:53:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=121
06/12/2022 10:53:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=123
06/12/2022 10:53:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=124
06/12/2022 10:53:46 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=124
06/12/2022 10:53:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=126
06/12/2022 10:53:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=128
06/12/2022 10:53:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=129
06/12/2022 10:53:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=131
06/12/2022 10:54:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=133
06/12/2022 10:54:03 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=133
06/12/2022 10:54:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=134
06/12/2022 10:54:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=136
06/12/2022 10:54:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=138
06/12/2022 10:54:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=139
06/12/2022 10:54:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=141
06/12/2022 10:54:20 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.1983273596176822 on epoch=141
06/12/2022 10:54:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=143
06/12/2022 10:54:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=144
06/12/2022 10:54:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=146
06/12/2022 10:54:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=148
06/12/2022 10:54:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=149
06/12/2022 10:54:37 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=149
06/12/2022 10:54:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=151
06/12/2022 10:54:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=153
06/12/2022 10:54:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=154
06/12/2022 10:54:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=156
06/12/2022 10:54:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=158
06/12/2022 10:54:54 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=158
06/12/2022 10:54:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=159
06/12/2022 10:55:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=161
06/12/2022 10:55:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=163
06/12/2022 10:55:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=164
06/12/2022 10:55:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=166
06/12/2022 10:55:12 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.2085278555866791 on epoch=166
06/12/2022 10:55:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=168
06/12/2022 10:55:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=169
06/12/2022 10:55:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=171
06/12/2022 10:55:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=173
06/12/2022 10:55:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=174
06/12/2022 10:55:29 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=174
06/12/2022 10:55:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=176
06/12/2022 10:55:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=178
06/12/2022 10:55:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=179
06/12/2022 10:55:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=181
06/12/2022 10:55:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=183
06/12/2022 10:55:46 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.2333409698358152 on epoch=183
06/12/2022 10:55:46 - INFO - __main__ - Saving model with best Classification-F1: 0.21684771469563435 -> 0.2333409698358152 on epoch=183, global_step=1100
06/12/2022 10:55:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.43 on epoch=184
06/12/2022 10:55:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=186
06/12/2022 10:55:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=188
06/12/2022 10:55:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=189
06/12/2022 10:56:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=191
06/12/2022 10:56:03 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.1684153887543718 on epoch=191
06/12/2022 10:56:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=193
06/12/2022 10:56:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=194
06/12/2022 10:56:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=196
06/12/2022 10:56:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=198
06/12/2022 10:56:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=199
06/12/2022 10:56:20 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.23310787729726035 on epoch=199
06/12/2022 10:56:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=201
06/12/2022 10:56:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=203
06/12/2022 10:56:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=204
06/12/2022 10:56:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=206
06/12/2022 10:56:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=208
06/12/2022 10:56:38 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.1626016260162602 on epoch=208
06/12/2022 10:56:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=209
06/12/2022 10:56:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=211
06/12/2022 10:56:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=213
06/12/2022 10:56:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=214
06/12/2022 10:56:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=216
06/12/2022 10:56:54 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.3187017602309956 on epoch=216
06/12/2022 10:56:54 - INFO - __main__ - Saving model with best Classification-F1: 0.2333409698358152 -> 0.3187017602309956 on epoch=216, global_step=1300
06/12/2022 10:56:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=218
06/12/2022 10:57:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=219
06/12/2022 10:57:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=221
06/12/2022 10:57:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=223
06/12/2022 10:57:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=224
06/12/2022 10:57:11 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.30342758773499684 on epoch=224
06/12/2022 10:57:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=226
06/12/2022 10:57:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.35 on epoch=228
06/12/2022 10:57:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=229
06/12/2022 10:57:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=231
06/12/2022 10:57:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=233
06/12/2022 10:57:28 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.21021663172606572 on epoch=233
06/12/2022 10:57:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=234
06/12/2022 10:57:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=236
06/12/2022 10:57:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=238
06/12/2022 10:57:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=239
06/12/2022 10:57:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=241
06/12/2022 10:57:46 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.27199878627386076 on epoch=241
06/12/2022 10:57:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=243
06/12/2022 10:57:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=244
06/12/2022 10:57:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=246
06/12/2022 10:57:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=248
06/12/2022 10:58:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=249
06/12/2022 10:58:03 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.25887365762430836 on epoch=249
06/12/2022 10:58:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=251
06/12/2022 10:58:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=253
06/12/2022 10:58:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.34 on epoch=254
06/12/2022 10:58:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=256
06/12/2022 10:58:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=258
06/12/2022 10:58:20 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.2948529411764706 on epoch=258
06/12/2022 10:58:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=259
06/12/2022 10:58:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=261
06/12/2022 10:58:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.30 on epoch=263
06/12/2022 10:58:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=264
06/12/2022 10:58:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=266
06/12/2022 10:58:37 - INFO - __main__ - Global step 1600 Train loss 0.32 Classification-F1 0.33610327035751064 on epoch=266
06/12/2022 10:58:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3187017602309956 -> 0.33610327035751064 on epoch=266, global_step=1600
06/12/2022 10:58:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.30 on epoch=268
06/12/2022 10:58:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=269
06/12/2022 10:58:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=271
06/12/2022 10:58:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=273
06/12/2022 10:58:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.28 on epoch=274
06/12/2022 10:58:54 - INFO - __main__ - Global step 1650 Train loss 0.31 Classification-F1 0.25320745323945665 on epoch=274
06/12/2022 10:58:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=276
06/12/2022 10:59:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=278
06/12/2022 10:59:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=279
06/12/2022 10:59:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.31 on epoch=281
06/12/2022 10:59:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=283
06/12/2022 10:59:12 - INFO - __main__ - Global step 1700 Train loss 0.32 Classification-F1 0.2388809475188891 on epoch=283
06/12/2022 10:59:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=284
06/12/2022 10:59:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.26 on epoch=286
06/12/2022 10:59:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=288
06/12/2022 10:59:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=289
06/12/2022 10:59:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.28 on epoch=291
06/12/2022 10:59:29 - INFO - __main__ - Global step 1750 Train loss 0.27 Classification-F1 0.33122605363984675 on epoch=291
06/12/2022 10:59:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=293
06/12/2022 10:59:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=294
06/12/2022 10:59:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=296
06/12/2022 10:59:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=298
06/12/2022 10:59:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.24 on epoch=299
06/12/2022 10:59:46 - INFO - __main__ - Global step 1800 Train loss 0.26 Classification-F1 0.3273085224981617 on epoch=299
06/12/2022 10:59:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=301
06/12/2022 10:59:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.27 on epoch=303
06/12/2022 10:59:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=304
06/12/2022 10:59:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=306
06/12/2022 11:00:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=308
06/12/2022 11:00:03 - INFO - __main__ - Global step 1850 Train loss 0.26 Classification-F1 0.3159854851031321 on epoch=308
06/12/2022 11:00:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.29 on epoch=309
06/12/2022 11:00:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=311
06/12/2022 11:00:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=313
06/12/2022 11:00:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=314
06/12/2022 11:00:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=316
06/12/2022 11:00:20 - INFO - __main__ - Global step 1900 Train loss 0.25 Classification-F1 0.36465626861107087 on epoch=316
06/12/2022 11:00:20 - INFO - __main__ - Saving model with best Classification-F1: 0.33610327035751064 -> 0.36465626861107087 on epoch=316, global_step=1900
06/12/2022 11:00:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=318
06/12/2022 11:00:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.24 on epoch=319
06/12/2022 11:00:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=321
06/12/2022 11:00:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=323
06/12/2022 11:00:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=324
06/12/2022 11:00:37 - INFO - __main__ - Global step 1950 Train loss 0.25 Classification-F1 0.3619047619047619 on epoch=324
06/12/2022 11:00:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=326
06/12/2022 11:00:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=328
06/12/2022 11:00:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=329
06/12/2022 11:00:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.23 on epoch=331
06/12/2022 11:00:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=333
06/12/2022 11:00:54 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.20487014408969473 on epoch=333
06/12/2022 11:00:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=334
06/12/2022 11:00:59 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.22 on epoch=336
06/12/2022 11:01:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=338
06/12/2022 11:01:05 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.23 on epoch=339
06/12/2022 11:01:08 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=341
06/12/2022 11:01:11 - INFO - __main__ - Global step 2050 Train loss 0.22 Classification-F1 0.17974957930464047 on epoch=341
06/12/2022 11:01:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.22 on epoch=343
06/12/2022 11:01:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=344
06/12/2022 11:01:19 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=346
06/12/2022 11:01:22 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=348
06/12/2022 11:01:25 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=349
06/12/2022 11:01:28 - INFO - __main__ - Global step 2100 Train loss 0.22 Classification-F1 0.2620107962213225 on epoch=349
06/12/2022 11:01:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=351
06/12/2022 11:01:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=353
06/12/2022 11:01:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=354
06/12/2022 11:01:39 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=356
06/12/2022 11:01:42 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=358
06/12/2022 11:01:45 - INFO - __main__ - Global step 2150 Train loss 0.19 Classification-F1 0.3277847309136421 on epoch=358
06/12/2022 11:01:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=359
06/12/2022 11:01:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=361
06/12/2022 11:01:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=363
06/12/2022 11:01:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=364
06/12/2022 11:02:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=366
06/12/2022 11:02:03 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.3246845746845746 on epoch=366
06/12/2022 11:02:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.17 on epoch=368
06/12/2022 11:02:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=369
06/12/2022 11:02:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=371
06/12/2022 11:02:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=373
06/12/2022 11:02:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=374
06/12/2022 11:02:20 - INFO - __main__ - Global step 2250 Train loss 0.16 Classification-F1 0.20657127715951246 on epoch=374
06/12/2022 11:02:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=376
06/12/2022 11:02:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=378
06/12/2022 11:02:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=379
06/12/2022 11:02:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=381
06/12/2022 11:02:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=383
06/12/2022 11:02:36 - INFO - __main__ - Global step 2300 Train loss 0.16 Classification-F1 0.12229929568639246 on epoch=383
06/12/2022 11:02:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.17 on epoch=384
06/12/2022 11:02:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=386
06/12/2022 11:02:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=388
06/12/2022 11:02:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=389
06/12/2022 11:02:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=391
06/12/2022 11:02:54 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.14213564213564214 on epoch=391
06/12/2022 11:02:56 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=393
06/12/2022 11:02:59 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=394
06/12/2022 11:03:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=396
06/12/2022 11:03:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.18 on epoch=398
06/12/2022 11:03:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=399
06/12/2022 11:03:11 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.15516941357794514 on epoch=399
06/12/2022 11:03:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.28 on epoch=401
06/12/2022 11:03:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=403
06/12/2022 11:03:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=404
06/12/2022 11:03:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=406
06/12/2022 11:03:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=408
06/12/2022 11:03:28 - INFO - __main__ - Global step 2450 Train loss 0.22 Classification-F1 0.23842737722048069 on epoch=408
06/12/2022 11:03:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=409
06/12/2022 11:03:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=411
06/12/2022 11:03:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=413
06/12/2022 11:03:40 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=414
06/12/2022 11:03:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=416
06/12/2022 11:03:46 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.19998157870498295 on epoch=416
06/12/2022 11:03:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=418
06/12/2022 11:03:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.21 on epoch=419
06/12/2022 11:03:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=421
06/12/2022 11:03:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=423
06/12/2022 11:04:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=424
06/12/2022 11:04:03 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.18124185844550586 on epoch=424
06/12/2022 11:04:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=426
06/12/2022 11:04:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=428
06/12/2022 11:04:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.16 on epoch=429
06/12/2022 11:04:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=431
06/12/2022 11:04:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=433
06/12/2022 11:04:20 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.21704545454545454 on epoch=433
06/12/2022 11:04:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.14 on epoch=434
06/12/2022 11:04:26 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.15 on epoch=436
06/12/2022 11:04:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.15 on epoch=438
06/12/2022 11:04:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=439
06/12/2022 11:04:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.22 on epoch=441
06/12/2022 11:04:38 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.38573779865441854 on epoch=441
06/12/2022 11:04:38 - INFO - __main__ - Saving model with best Classification-F1: 0.36465626861107087 -> 0.38573779865441854 on epoch=441, global_step=2650
06/12/2022 11:04:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=443
06/12/2022 11:04:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=444
06/12/2022 11:04:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=446
06/12/2022 11:04:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.18 on epoch=448
06/12/2022 11:04:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.12 on epoch=449
06/12/2022 11:04:55 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.3229173156281799 on epoch=449
06/12/2022 11:04:58 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=451
06/12/2022 11:05:00 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=453
06/12/2022 11:05:03 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.16 on epoch=454
06/12/2022 11:05:06 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=456
06/12/2022 11:05:09 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.25 on epoch=458
06/12/2022 11:05:12 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.21359324236517221 on epoch=458
06/12/2022 11:05:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=459
06/12/2022 11:05:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.20 on epoch=461
06/12/2022 11:05:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=463
06/12/2022 11:05:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=464
06/12/2022 11:05:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.10 on epoch=466
06/12/2022 11:05:29 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.09354621848739494 on epoch=466
06/12/2022 11:05:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=468
06/12/2022 11:05:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=469
06/12/2022 11:05:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=471
06/12/2022 11:05:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.12 on epoch=473
06/12/2022 11:05:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=474
06/12/2022 11:05:47 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.153638517781358 on epoch=474
06/12/2022 11:05:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=476
06/12/2022 11:05:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=478
06/12/2022 11:05:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=479
06/12/2022 11:05:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.11 on epoch=481
06/12/2022 11:06:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=483
06/12/2022 11:06:04 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.24930201314104583 on epoch=483
06/12/2022 11:06:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=484
06/12/2022 11:06:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=486
06/12/2022 11:06:12 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=488
06/12/2022 11:06:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=489
06/12/2022 11:06:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=491
06/12/2022 11:06:21 - INFO - __main__ - Global step 2950 Train loss 0.13 Classification-F1 0.15058547387404247 on epoch=491
06/12/2022 11:06:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.12 on epoch=493
06/12/2022 11:06:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=494
06/12/2022 11:06:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=496
06/12/2022 11:06:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=498
06/12/2022 11:06:35 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=499
06/12/2022 11:06:36 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:06:36 - INFO - __main__ - Printing 3 examples
06/12/2022 11:06:36 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/12/2022 11:06:36 - INFO - __main__ - ['contradiction']
06/12/2022 11:06:36 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/12/2022 11:06:36 - INFO - __main__ - ['contradiction']
06/12/2022 11:06:36 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/12/2022 11:06:36 - INFO - __main__ - ['contradiction']
06/12/2022 11:06:36 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:06:36 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:06:36 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 11:06:36 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:06:36 - INFO - __main__ - Printing 3 examples
06/12/2022 11:06:36 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
06/12/2022 11:06:36 - INFO - __main__ - ['contradiction']
06/12/2022 11:06:36 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
06/12/2022 11:06:36 - INFO - __main__ - ['contradiction']
06/12/2022 11:06:36 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
06/12/2022 11:06:36 - INFO - __main__ - ['contradiction']
06/12/2022 11:06:36 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:06:36 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:06:37 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 11:06:38 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.13651918743483674 on epoch=499
06/12/2022 11:06:38 - INFO - __main__ - save last model!
06/12/2022 11:06:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 11:06:38 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 11:06:38 - INFO - __main__ - Printing 3 examples
06/12/2022 11:06:38 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 11:06:38 - INFO - __main__ - ['contradiction']
06/12/2022 11:06:38 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 11:06:38 - INFO - __main__ - ['entailment']
06/12/2022 11:06:38 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 11:06:38 - INFO - __main__ - ['contradiction']
06/12/2022 11:06:38 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:06:39 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:06:40 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 11:06:53 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 11:06:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 11:06:54 - INFO - __main__ - Starting training!
06/12/2022 11:07:11 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_13_0.3_8_predictions.txt
06/12/2022 11:07:11 - INFO - __main__ - Classification-F1 on test data: 0.0157
06/12/2022 11:07:11 - INFO - __main__ - prefix=anli_32_13, lr=0.3, bsz=8, dev_performance=0.38573779865441854, test_performance=0.015723108646700873
06/12/2022 11:07:11 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.2, bsz=8 ...
06/12/2022 11:07:13 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:07:13 - INFO - __main__ - Printing 3 examples
06/12/2022 11:07:13 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/12/2022 11:07:13 - INFO - __main__ - ['contradiction']
06/12/2022 11:07:13 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/12/2022 11:07:13 - INFO - __main__ - ['contradiction']
06/12/2022 11:07:13 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/12/2022 11:07:13 - INFO - __main__ - ['contradiction']
06/12/2022 11:07:13 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:07:13 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:07:13 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 11:07:13 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:07:13 - INFO - __main__ - Printing 3 examples
06/12/2022 11:07:13 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
06/12/2022 11:07:13 - INFO - __main__ - ['contradiction']
06/12/2022 11:07:13 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
06/12/2022 11:07:13 - INFO - __main__ - ['contradiction']
06/12/2022 11:07:13 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
06/12/2022 11:07:13 - INFO - __main__ - ['contradiction']
06/12/2022 11:07:13 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:07:13 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:07:14 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 11:07:29 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 11:07:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 11:07:30 - INFO - __main__ - Starting training!
06/12/2022 11:07:33 - INFO - __main__ - Step 10 Global step 10 Train loss 1.09 on epoch=1
06/12/2022 11:07:36 - INFO - __main__ - Step 20 Global step 20 Train loss 0.69 on epoch=3
06/12/2022 11:07:39 - INFO - __main__ - Step 30 Global step 30 Train loss 0.68 on epoch=4
06/12/2022 11:07:41 - INFO - __main__ - Step 40 Global step 40 Train loss 0.62 on epoch=6
06/12/2022 11:07:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.56 on epoch=8
06/12/2022 11:07:47 - INFO - __main__ - Global step 50 Train loss 0.73 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 11:07:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 11:07:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=9
06/12/2022 11:07:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=11
06/12/2022 11:07:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=13
06/12/2022 11:07:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=14
06/12/2022 11:08:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=16
06/12/2022 11:08:04 - INFO - __main__ - Global step 100 Train loss 0.56 Classification-F1 0.16402116402116398 on epoch=16
06/12/2022 11:08:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=18
06/12/2022 11:08:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=19
06/12/2022 11:08:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=21
06/12/2022 11:08:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=23
06/12/2022 11:08:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=24
06/12/2022 11:08:20 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 11:08:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=26
06/12/2022 11:08:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=28
06/12/2022 11:08:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=29
06/12/2022 11:08:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=31
06/12/2022 11:08:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=33
06/12/2022 11:08:37 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 11:08:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=34
06/12/2022 11:08:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=36
06/12/2022 11:08:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=38
06/12/2022 11:08:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=39
06/12/2022 11:08:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=41
06/12/2022 11:08:54 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.17980884109916365 on epoch=41
06/12/2022 11:08:54 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17980884109916365 on epoch=41, global_step=250
06/12/2022 11:08:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=43
06/12/2022 11:09:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=44
06/12/2022 11:09:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=46
06/12/2022 11:09:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=48
06/12/2022 11:09:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=49
06/12/2022 11:09:11 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=49
06/12/2022 11:09:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=51
06/12/2022 11:09:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=53
06/12/2022 11:09:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=54
06/12/2022 11:09:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=56
06/12/2022 11:09:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=58
06/12/2022 11:09:27 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.22431431619908362 on epoch=58
06/12/2022 11:09:28 - INFO - __main__ - Saving model with best Classification-F1: 0.17980884109916365 -> 0.22431431619908362 on epoch=58, global_step=350
06/12/2022 11:09:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=59
06/12/2022 11:09:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=61
06/12/2022 11:09:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=63
06/12/2022 11:09:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=64
06/12/2022 11:09:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=66
06/12/2022 11:09:44 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16272965879265092 on epoch=66
06/12/2022 11:09:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
06/12/2022 11:09:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=69
06/12/2022 11:09:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=71
06/12/2022 11:09:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=73
06/12/2022 11:09:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=74
06/12/2022 11:10:00 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=74
06/12/2022 11:10:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=76
06/12/2022 11:10:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=78
06/12/2022 11:10:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=79
06/12/2022 11:10:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=81
06/12/2022 11:10:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=83
06/12/2022 11:10:16 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=83
06/12/2022 11:10:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=84
06/12/2022 11:10:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=86
06/12/2022 11:10:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=88
06/12/2022 11:10:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=89
06/12/2022 11:10:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=91
06/12/2022 11:10:32 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.1796078431372549 on epoch=91
06/12/2022 11:10:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=93
06/12/2022 11:10:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=94
06/12/2022 11:10:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=96
06/12/2022 11:10:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=98
06/12/2022 11:10:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=99
06/12/2022 11:10:48 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=99
06/12/2022 11:10:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=101
06/12/2022 11:10:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=103
06/12/2022 11:10:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=104
06/12/2022 11:10:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=106
06/12/2022 11:11:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=108
06/12/2022 11:11:05 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=108
06/12/2022 11:11:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=109
06/12/2022 11:11:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=111
06/12/2022 11:11:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=113
06/12/2022 11:11:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=114
06/12/2022 11:11:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=116
06/12/2022 11:11:21 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.16402116402116398 on epoch=116
06/12/2022 11:11:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=118
06/12/2022 11:11:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=119
06/12/2022 11:11:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=121
06/12/2022 11:11:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=123
06/12/2022 11:11:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=124
06/12/2022 11:11:37 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.20370370370370372 on epoch=124
06/12/2022 11:11:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=126
06/12/2022 11:11:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=128
06/12/2022 11:11:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=129
06/12/2022 11:11:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=131
06/12/2022 11:11:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=133
06/12/2022 11:11:54 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.1851851851851852 on epoch=133
06/12/2022 11:11:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=134
06/12/2022 11:11:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=136
06/12/2022 11:12:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=138
06/12/2022 11:12:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=139
06/12/2022 11:12:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=141
06/12/2022 11:12:10 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.16272965879265092 on epoch=141
06/12/2022 11:12:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=143
06/12/2022 11:12:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=144
06/12/2022 11:12:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=146
06/12/2022 11:12:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=148
06/12/2022 11:12:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=149
06/12/2022 11:12:26 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.1679790026246719 on epoch=149
06/12/2022 11:12:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=151
06/12/2022 11:12:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.42 on epoch=153
06/12/2022 11:12:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=154
06/12/2022 11:12:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=156
06/12/2022 11:12:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=158
06/12/2022 11:12:42 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.24357864357864356 on epoch=158
06/12/2022 11:12:42 - INFO - __main__ - Saving model with best Classification-F1: 0.22431431619908362 -> 0.24357864357864356 on epoch=158, global_step=950
06/12/2022 11:12:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=159
06/12/2022 11:12:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=161
06/12/2022 11:12:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=163
06/12/2022 11:12:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=164
06/12/2022 11:12:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=166
06/12/2022 11:12:59 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.16666666666666666 on epoch=166
06/12/2022 11:13:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=168
06/12/2022 11:13:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=169
06/12/2022 11:13:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=171
06/12/2022 11:13:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=173
06/12/2022 11:13:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=174
06/12/2022 11:13:15 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.31684981684981683 on epoch=174
06/12/2022 11:13:15 - INFO - __main__ - Saving model with best Classification-F1: 0.24357864357864356 -> 0.31684981684981683 on epoch=174, global_step=1050
06/12/2022 11:13:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=176
06/12/2022 11:13:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=178
06/12/2022 11:13:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=179
06/12/2022 11:13:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=181
06/12/2022 11:13:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=183
06/12/2022 11:13:32 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.2697013852460609 on epoch=183
06/12/2022 11:13:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=184
06/12/2022 11:13:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=186
06/12/2022 11:13:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=188
06/12/2022 11:13:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=189
06/12/2022 11:13:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=191
06/12/2022 11:13:49 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.2042302140212138 on epoch=191
06/12/2022 11:13:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=193
06/12/2022 11:13:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=194
06/12/2022 11:13:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=196
06/12/2022 11:13:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=198
06/12/2022 11:14:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=199
06/12/2022 11:14:05 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.1851851851851852 on epoch=199
06/12/2022 11:14:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=201
06/12/2022 11:14:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=203
06/12/2022 11:14:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=204
06/12/2022 11:14:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=206
06/12/2022 11:14:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=208
06/12/2022 11:14:22 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.31084656084656087 on epoch=208
06/12/2022 11:14:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=209
06/12/2022 11:14:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=211
06/12/2022 11:14:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=213
06/12/2022 11:14:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.43 on epoch=214
06/12/2022 11:14:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.43 on epoch=216
06/12/2022 11:14:39 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.16272965879265092 on epoch=216
06/12/2022 11:14:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=218
06/12/2022 11:14:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=219
06/12/2022 11:14:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=221
06/12/2022 11:14:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=223
06/12/2022 11:14:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=224
06/12/2022 11:14:56 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.2288050786502799 on epoch=224
06/12/2022 11:14:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=226
06/12/2022 11:15:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=228
06/12/2022 11:15:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=229
06/12/2022 11:15:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=231
06/12/2022 11:15:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=233
06/12/2022 11:15:13 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.2716931216931217 on epoch=233
06/12/2022 11:15:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=234
06/12/2022 11:15:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=236
06/12/2022 11:15:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=238
06/12/2022 11:15:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=239
06/12/2022 11:15:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=241
06/12/2022 11:15:29 - INFO - __main__ - Global step 1450 Train loss 0.40 Classification-F1 0.2548043971357273 on epoch=241
06/12/2022 11:15:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=243
06/12/2022 11:15:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=244
06/12/2022 11:15:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=246
06/12/2022 11:15:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=248
06/12/2022 11:15:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=249
06/12/2022 11:15:46 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.2087619047619048 on epoch=249
06/12/2022 11:15:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=251
06/12/2022 11:15:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=253
06/12/2022 11:15:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=254
06/12/2022 11:15:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=256
06/12/2022 11:16:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=258
06/12/2022 11:16:03 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.3193052273763628 on epoch=258
06/12/2022 11:16:03 - INFO - __main__ - Saving model with best Classification-F1: 0.31684981684981683 -> 0.3193052273763628 on epoch=258, global_step=1550
06/12/2022 11:16:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=259
06/12/2022 11:16:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=261
06/12/2022 11:16:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=263
06/12/2022 11:16:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=264
06/12/2022 11:16:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=266
06/12/2022 11:16:19 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.25334168755221387 on epoch=266
06/12/2022 11:16:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=268
06/12/2022 11:16:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=269
06/12/2022 11:16:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.36 on epoch=271
06/12/2022 11:16:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=273
06/12/2022 11:16:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=274
06/12/2022 11:16:36 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.2723037417461482 on epoch=274
06/12/2022 11:16:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=276
06/12/2022 11:16:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.34 on epoch=278
06/12/2022 11:16:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=279
06/12/2022 11:16:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=281
06/12/2022 11:16:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=283
06/12/2022 11:16:53 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.2401360544217687 on epoch=283
06/12/2022 11:16:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=284
06/12/2022 11:16:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=286
06/12/2022 11:17:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=288
06/12/2022 11:17:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=289
06/12/2022 11:17:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=291
06/12/2022 11:17:10 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.1693121693121693 on epoch=291
06/12/2022 11:17:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=293
06/12/2022 11:17:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=294
06/12/2022 11:17:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.36 on epoch=296
06/12/2022 11:17:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=298
06/12/2022 11:17:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=299
06/12/2022 11:17:27 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.1895141895141895 on epoch=299
06/12/2022 11:17:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.31 on epoch=301
06/12/2022 11:17:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=303
06/12/2022 11:17:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=304
06/12/2022 11:17:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=306
06/12/2022 11:17:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=308
06/12/2022 11:17:43 - INFO - __main__ - Global step 1850 Train loss 0.33 Classification-F1 0.23809523809523805 on epoch=308
06/12/2022 11:17:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=309
06/12/2022 11:17:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=311
06/12/2022 11:17:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=313
06/12/2022 11:17:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=314
06/12/2022 11:17:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=316
06/12/2022 11:18:00 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.23716538046635138 on epoch=316
06/12/2022 11:18:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=318
06/12/2022 11:18:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=319
06/12/2022 11:18:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.34 on epoch=321
06/12/2022 11:18:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=323
06/12/2022 11:18:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=324
06/12/2022 11:18:17 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.3286641342196898 on epoch=324
06/12/2022 11:18:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3193052273763628 -> 0.3286641342196898 on epoch=324, global_step=1950
06/12/2022 11:18:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=326
06/12/2022 11:18:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=328
06/12/2022 11:18:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.35 on epoch=329
06/12/2022 11:18:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=331
06/12/2022 11:18:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=333
06/12/2022 11:18:34 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.228539108772465 on epoch=333
06/12/2022 11:18:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=334
06/12/2022 11:18:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.29 on epoch=336
06/12/2022 11:18:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=338
06/12/2022 11:18:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.31 on epoch=339
06/12/2022 11:18:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.34 on epoch=341
06/12/2022 11:18:51 - INFO - __main__ - Global step 2050 Train loss 0.31 Classification-F1 0.28620268620268624 on epoch=341
06/12/2022 11:18:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.29 on epoch=343
06/12/2022 11:18:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.31 on epoch=344
06/12/2022 11:18:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.33 on epoch=346
06/12/2022 11:19:02 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.31 on epoch=348
06/12/2022 11:19:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.29 on epoch=349
06/12/2022 11:19:07 - INFO - __main__ - Global step 2100 Train loss 0.30 Classification-F1 0.26110419261104195 on epoch=349
06/12/2022 11:19:10 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.28 on epoch=351
06/12/2022 11:19:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.33 on epoch=353
06/12/2022 11:19:15 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=354
06/12/2022 11:19:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.35 on epoch=356
06/12/2022 11:19:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=358
06/12/2022 11:19:24 - INFO - __main__ - Global step 2150 Train loss 0.31 Classification-F1 0.3381041403996928 on epoch=358
06/12/2022 11:19:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3286641342196898 -> 0.3381041403996928 on epoch=358, global_step=2150
06/12/2022 11:19:27 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.26 on epoch=359
06/12/2022 11:19:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.30 on epoch=361
06/12/2022 11:19:32 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.30 on epoch=363
06/12/2022 11:19:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.29 on epoch=364
06/12/2022 11:19:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.33 on epoch=366
06/12/2022 11:19:41 - INFO - __main__ - Global step 2200 Train loss 0.30 Classification-F1 0.2217741935483871 on epoch=366
06/12/2022 11:19:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.32 on epoch=368
06/12/2022 11:19:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=369
06/12/2022 11:19:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=371
06/12/2022 11:19:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=373
06/12/2022 11:19:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.35 on epoch=374
06/12/2022 11:19:58 - INFO - __main__ - Global step 2250 Train loss 0.30 Classification-F1 0.2564699530991666 on epoch=374
06/12/2022 11:20:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.25 on epoch=376
06/12/2022 11:20:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=378
06/12/2022 11:20:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.27 on epoch=379
06/12/2022 11:20:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.29 on epoch=381
06/12/2022 11:20:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.30 on epoch=383
06/12/2022 11:20:15 - INFO - __main__ - Global step 2300 Train loss 0.27 Classification-F1 0.3286528286528287 on epoch=383
06/12/2022 11:20:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.27 on epoch=384
06/12/2022 11:20:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.24 on epoch=386
06/12/2022 11:20:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=388
06/12/2022 11:20:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.25 on epoch=389
06/12/2022 11:20:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=391
06/12/2022 11:20:32 - INFO - __main__ - Global step 2350 Train loss 0.26 Classification-F1 0.28201026278582225 on epoch=391
06/12/2022 11:20:35 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=393
06/12/2022 11:20:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.26 on epoch=394
06/12/2022 11:20:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.30 on epoch=396
06/12/2022 11:20:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.25 on epoch=398
06/12/2022 11:20:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=399
06/12/2022 11:20:49 - INFO - __main__ - Global step 2400 Train loss 0.26 Classification-F1 0.30342564589139936 on epoch=399
06/12/2022 11:20:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.26 on epoch=401
06/12/2022 11:20:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.29 on epoch=403
06/12/2022 11:20:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.28 on epoch=404
06/12/2022 11:21:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.27 on epoch=406
06/12/2022 11:21:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=408
06/12/2022 11:21:05 - INFO - __main__ - Global step 2450 Train loss 0.26 Classification-F1 0.2717936117936118 on epoch=408
06/12/2022 11:21:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=409
06/12/2022 11:21:11 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.28 on epoch=411
06/12/2022 11:21:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.27 on epoch=413
06/12/2022 11:21:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.25 on epoch=414
06/12/2022 11:21:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=416
06/12/2022 11:21:22 - INFO - __main__ - Global step 2500 Train loss 0.25 Classification-F1 0.3260770975056689 on epoch=416
06/12/2022 11:21:25 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.24 on epoch=418
06/12/2022 11:21:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=419
06/12/2022 11:21:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.26 on epoch=421
06/12/2022 11:21:33 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.23 on epoch=423
06/12/2022 11:21:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.23 on epoch=424
06/12/2022 11:21:39 - INFO - __main__ - Global step 2550 Train loss 0.23 Classification-F1 0.2073509015256588 on epoch=424
06/12/2022 11:21:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.22 on epoch=426
06/12/2022 11:21:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.24 on epoch=428
06/12/2022 11:21:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.22 on epoch=429
06/12/2022 11:21:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=431
06/12/2022 11:21:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=433
06/12/2022 11:21:56 - INFO - __main__ - Global step 2600 Train loss 0.22 Classification-F1 0.290311986863711 on epoch=433
06/12/2022 11:21:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.22 on epoch=434
06/12/2022 11:22:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.24 on epoch=436
06/12/2022 11:22:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.28 on epoch=438
06/12/2022 11:22:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.22 on epoch=439
06/12/2022 11:22:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.25 on epoch=441
06/12/2022 11:22:14 - INFO - __main__ - Global step 2650 Train loss 0.24 Classification-F1 0.27513168804189364 on epoch=441
06/12/2022 11:22:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.19 on epoch=443
06/12/2022 11:22:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.17 on epoch=444
06/12/2022 11:22:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.22 on epoch=446
06/12/2022 11:22:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.23 on epoch=448
06/12/2022 11:22:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.22 on epoch=449
06/12/2022 11:22:30 - INFO - __main__ - Global step 2700 Train loss 0.21 Classification-F1 0.3117411215345926 on epoch=449
06/12/2022 11:22:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.20 on epoch=451
06/12/2022 11:22:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.23 on epoch=453
06/12/2022 11:22:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.21 on epoch=454
06/12/2022 11:22:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=456
06/12/2022 11:22:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.21 on epoch=458
06/12/2022 11:22:47 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.24295228653499076 on epoch=458
06/12/2022 11:22:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.22 on epoch=459
06/12/2022 11:22:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=461
06/12/2022 11:22:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.19 on epoch=463
06/12/2022 11:22:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=464
06/12/2022 11:23:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=466
06/12/2022 11:23:04 - INFO - __main__ - Global step 2800 Train loss 0.19 Classification-F1 0.24613899613899615 on epoch=466
06/12/2022 11:23:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=468
06/12/2022 11:23:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=469
06/12/2022 11:23:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=471
06/12/2022 11:23:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.21 on epoch=473
06/12/2022 11:23:18 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=474
06/12/2022 11:23:21 - INFO - __main__ - Global step 2850 Train loss 0.18 Classification-F1 0.2710920391136333 on epoch=474
06/12/2022 11:23:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.21 on epoch=476
06/12/2022 11:23:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.24 on epoch=478
06/12/2022 11:23:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=479
06/12/2022 11:23:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=481
06/12/2022 11:23:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.23 on epoch=483
06/12/2022 11:23:37 - INFO - __main__ - Global step 2900 Train loss 0.21 Classification-F1 0.26983670136762605 on epoch=483
06/12/2022 11:23:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=484
06/12/2022 11:23:43 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=486
06/12/2022 11:23:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.20 on epoch=488
06/12/2022 11:23:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=489
06/12/2022 11:23:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.17 on epoch=491
06/12/2022 11:23:53 - INFO - __main__ - Global step 2950 Train loss 0.17 Classification-F1 0.21028138528138532 on epoch=491
06/12/2022 11:23:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.16 on epoch=493
06/12/2022 11:23:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=494
06/12/2022 11:24:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.17 on epoch=496
06/12/2022 11:24:05 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.21 on epoch=498
06/12/2022 11:24:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=499
06/12/2022 11:24:09 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:24:09 - INFO - __main__ - Printing 3 examples
06/12/2022 11:24:09 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/12/2022 11:24:09 - INFO - __main__ - ['entailment']
06/12/2022 11:24:09 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/12/2022 11:24:09 - INFO - __main__ - ['entailment']
06/12/2022 11:24:09 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/12/2022 11:24:09 - INFO - __main__ - ['entailment']
06/12/2022 11:24:09 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:24:09 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:24:09 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 11:24:09 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:24:09 - INFO - __main__ - Printing 3 examples
06/12/2022 11:24:09 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
06/12/2022 11:24:09 - INFO - __main__ - ['entailment']
06/12/2022 11:24:09 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
06/12/2022 11:24:09 - INFO - __main__ - ['entailment']
06/12/2022 11:24:09 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
06/12/2022 11:24:09 - INFO - __main__ - ['entailment']
06/12/2022 11:24:09 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:24:09 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:24:09 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 11:24:10 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.32841458058849365 on epoch=499
06/12/2022 11:24:10 - INFO - __main__ - save last model!
06/12/2022 11:24:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 11:24:10 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 11:24:10 - INFO - __main__ - Printing 3 examples
06/12/2022 11:24:10 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 11:24:10 - INFO - __main__ - ['contradiction']
06/12/2022 11:24:10 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 11:24:10 - INFO - __main__ - ['entailment']
06/12/2022 11:24:10 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 11:24:10 - INFO - __main__ - ['contradiction']
06/12/2022 11:24:10 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:24:11 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:24:12 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 11:24:25 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 11:24:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 11:24:26 - INFO - __main__ - Starting training!
06/12/2022 11:24:43 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_13_0.2_8_predictions.txt
06/12/2022 11:24:43 - INFO - __main__ - Classification-F1 on test data: 0.1839
06/12/2022 11:24:44 - INFO - __main__ - prefix=anli_32_13, lr=0.2, bsz=8, dev_performance=0.3381041403996928, test_performance=0.18393320964749538
06/12/2022 11:24:44 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.5, bsz=8 ...
06/12/2022 11:24:44 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:24:44 - INFO - __main__ - Printing 3 examples
06/12/2022 11:24:44 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/12/2022 11:24:44 - INFO - __main__ - ['entailment']
06/12/2022 11:24:44 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/12/2022 11:24:44 - INFO - __main__ - ['entailment']
06/12/2022 11:24:44 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/12/2022 11:24:44 - INFO - __main__ - ['entailment']
06/12/2022 11:24:44 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:24:45 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:24:45 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 11:24:45 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:24:45 - INFO - __main__ - Printing 3 examples
06/12/2022 11:24:45 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
06/12/2022 11:24:45 - INFO - __main__ - ['entailment']
06/12/2022 11:24:45 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
06/12/2022 11:24:45 - INFO - __main__ - ['entailment']
06/12/2022 11:24:45 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
06/12/2022 11:24:45 - INFO - __main__ - ['entailment']
06/12/2022 11:24:45 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:24:45 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:24:45 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 11:25:05 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 11:25:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 11:25:05 - INFO - __main__ - Starting training!
06/12/2022 11:25:09 - INFO - __main__ - Step 10 Global step 10 Train loss 0.82 on epoch=1
06/12/2022 11:25:12 - INFO - __main__ - Step 20 Global step 20 Train loss 0.65 on epoch=3
06/12/2022 11:25:15 - INFO - __main__ - Step 30 Global step 30 Train loss 0.67 on epoch=4
06/12/2022 11:25:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=6
06/12/2022 11:25:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=8
06/12/2022 11:25:23 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 11:25:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 11:25:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=9
06/12/2022 11:25:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=11
06/12/2022 11:25:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=13
06/12/2022 11:25:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=14
06/12/2022 11:25:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=16
06/12/2022 11:25:39 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 11:25:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=18
06/12/2022 11:25:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=19
06/12/2022 11:25:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=21
06/12/2022 11:25:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=23
06/12/2022 11:25:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=24
06/12/2022 11:25:57 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.1881810228266921 on epoch=24
06/12/2022 11:25:57 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1881810228266921 on epoch=24, global_step=150
06/12/2022 11:25:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=26
06/12/2022 11:26:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=28
06/12/2022 11:26:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=29
06/12/2022 11:26:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=31
06/12/2022 11:26:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=33
06/12/2022 11:26:14 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 11:26:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=34
06/12/2022 11:26:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=36
06/12/2022 11:26:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=38
06/12/2022 11:26:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=39
06/12/2022 11:26:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=41
06/12/2022 11:26:31 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 11:26:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=43
06/12/2022 11:26:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=44
06/12/2022 11:26:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=46
06/12/2022 11:26:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=48
06/12/2022 11:26:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=49
06/12/2022 11:26:48 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.24673202614379086 on epoch=49
06/12/2022 11:26:48 - INFO - __main__ - Saving model with best Classification-F1: 0.1881810228266921 -> 0.24673202614379086 on epoch=49, global_step=300
06/12/2022 11:26:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=51
06/12/2022 11:26:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=53
06/12/2022 11:26:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=54
06/12/2022 11:26:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=56
06/12/2022 11:27:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=58
06/12/2022 11:27:05 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.29101510255131186 on epoch=58
06/12/2022 11:27:05 - INFO - __main__ - Saving model with best Classification-F1: 0.24673202614379086 -> 0.29101510255131186 on epoch=58, global_step=350
06/12/2022 11:27:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=59
06/12/2022 11:27:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=61
06/12/2022 11:27:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=63
06/12/2022 11:27:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=64
06/12/2022 11:27:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=66
06/12/2022 11:27:22 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 11:27:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=68
06/12/2022 11:27:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=69
06/12/2022 11:27:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=71
06/12/2022 11:27:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=73
06/12/2022 11:27:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=74
06/12/2022 11:27:39 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.23815073815073814 on epoch=74
06/12/2022 11:27:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=76
06/12/2022 11:27:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=78
06/12/2022 11:27:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=79
06/12/2022 11:27:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=81
06/12/2022 11:27:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=83
06/12/2022 11:27:56 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.2493336728630846 on epoch=83
06/12/2022 11:27:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=84
06/12/2022 11:28:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=86
06/12/2022 11:28:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=88
06/12/2022 11:28:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=89
06/12/2022 11:28:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=91
06/12/2022 11:28:13 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 11:28:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=93
06/12/2022 11:28:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=94
06/12/2022 11:28:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=96
06/12/2022 11:28:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=98
06/12/2022 11:28:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=99
06/12/2022 11:28:29 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.26766807489699057 on epoch=99
06/12/2022 11:28:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=101
06/12/2022 11:28:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=103
06/12/2022 11:28:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=104
06/12/2022 11:28:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=106
06/12/2022 11:28:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=108
06/12/2022 11:28:46 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.2746700188560654 on epoch=108
06/12/2022 11:28:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=109
06/12/2022 11:28:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=111
06/12/2022 11:28:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=113
06/12/2022 11:28:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=114
06/12/2022 11:29:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=116
06/12/2022 11:29:03 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 11:29:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=118
06/12/2022 11:29:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=119
06/12/2022 11:29:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=121
06/12/2022 11:29:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=123
06/12/2022 11:29:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=124
06/12/2022 11:29:19 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.27106227106227104 on epoch=124
06/12/2022 11:29:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=126
06/12/2022 11:29:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=128
06/12/2022 11:29:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=129
06/12/2022 11:29:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=131
06/12/2022 11:29:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=133
06/12/2022 11:29:36 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.23443223443223446 on epoch=133
06/12/2022 11:29:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=134
06/12/2022 11:29:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=136
06/12/2022 11:29:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=138
06/12/2022 11:29:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=139
06/12/2022 11:29:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=141
06/12/2022 11:29:53 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.16272965879265092 on epoch=141
06/12/2022 11:29:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=143
06/12/2022 11:29:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=144
06/12/2022 11:30:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=146
06/12/2022 11:30:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=148
06/12/2022 11:30:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=149
06/12/2022 11:30:10 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.3086284086284086 on epoch=149
06/12/2022 11:30:10 - INFO - __main__ - Saving model with best Classification-F1: 0.29101510255131186 -> 0.3086284086284086 on epoch=149, global_step=900
06/12/2022 11:30:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=151
06/12/2022 11:30:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=153
06/12/2022 11:30:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=154
06/12/2022 11:30:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=156
06/12/2022 11:30:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=158
06/12/2022 11:30:27 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.2872305140961857 on epoch=158
06/12/2022 11:30:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=159
06/12/2022 11:30:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=161
06/12/2022 11:30:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=163
06/12/2022 11:30:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=164
06/12/2022 11:30:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=166
06/12/2022 11:30:43 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.25438596491228066 on epoch=166
06/12/2022 11:30:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=168
06/12/2022 11:30:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=169
06/12/2022 11:30:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=171
06/12/2022 11:30:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=173
06/12/2022 11:30:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=174
06/12/2022 11:31:00 - INFO - __main__ - Global step 1050 Train loss 0.30 Classification-F1 0.25326797385620914 on epoch=174
06/12/2022 11:31:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=176
06/12/2022 11:31:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=178
06/12/2022 11:31:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=179
06/12/2022 11:31:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=181
06/12/2022 11:31:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=183
06/12/2022 11:31:17 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.28952380952380957 on epoch=183
06/12/2022 11:31:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=184
06/12/2022 11:31:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.28 on epoch=186
06/12/2022 11:31:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=188
06/12/2022 11:31:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=189
06/12/2022 11:31:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=191
06/12/2022 11:31:34 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.25817663817663816 on epoch=191
06/12/2022 11:31:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=193
06/12/2022 11:31:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=194
06/12/2022 11:31:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=196
06/12/2022 11:31:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=198
06/12/2022 11:31:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.24 on epoch=199
06/12/2022 11:31:50 - INFO - __main__ - Global step 1200 Train loss 0.26 Classification-F1 0.10051981806367771 on epoch=199
06/12/2022 11:31:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.29 on epoch=201
06/12/2022 11:31:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=203
06/12/2022 11:31:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=204
06/12/2022 11:32:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=206
06/12/2022 11:32:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=208
06/12/2022 11:32:07 - INFO - __main__ - Global step 1250 Train loss 0.24 Classification-F1 0.3004486012835057 on epoch=208
06/12/2022 11:32:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=209
06/12/2022 11:32:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=211
06/12/2022 11:32:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=213
06/12/2022 11:32:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=214
06/12/2022 11:32:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=216
06/12/2022 11:32:24 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.27875061939916185 on epoch=216
06/12/2022 11:32:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=218
06/12/2022 11:32:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=219
06/12/2022 11:32:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=221
06/12/2022 11:32:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=223
06/12/2022 11:32:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=224
06/12/2022 11:32:40 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.06311421911421913 on epoch=224
06/12/2022 11:32:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=226
06/12/2022 11:32:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=228
06/12/2022 11:32:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=229
06/12/2022 11:32:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.14 on epoch=231
06/12/2022 11:32:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=233
06/12/2022 11:32:57 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.2134107600525511 on epoch=233
06/12/2022 11:33:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=234
06/12/2022 11:33:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=236
06/12/2022 11:33:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=238
06/12/2022 11:33:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=239
06/12/2022 11:33:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=241
06/12/2022 11:33:13 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.21938265923539738 on epoch=241
06/12/2022 11:33:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=243
06/12/2022 11:33:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=244
06/12/2022 11:33:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=246
06/12/2022 11:33:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=248
06/12/2022 11:33:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=249
06/12/2022 11:33:30 - INFO - __main__ - Global step 1500 Train loss 0.14 Classification-F1 0.17575757575757578 on epoch=249
06/12/2022 11:33:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=251
06/12/2022 11:33:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=253
06/12/2022 11:33:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=254
06/12/2022 11:33:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=256
06/12/2022 11:33:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=258
06/12/2022 11:33:47 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.24745873430629023 on epoch=258
06/12/2022 11:33:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=259
06/12/2022 11:33:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=261
06/12/2022 11:33:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=263
06/12/2022 11:33:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=264
06/12/2022 11:34:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=266
06/12/2022 11:34:03 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.33631772821646244 on epoch=266
06/12/2022 11:34:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3086284086284086 -> 0.33631772821646244 on epoch=266, global_step=1600
06/12/2022 11:34:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=268
06/12/2022 11:34:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=269
06/12/2022 11:34:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=271
06/12/2022 11:34:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=273
06/12/2022 11:34:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=274
06/12/2022 11:34:20 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.18156502555097814 on epoch=274
06/12/2022 11:34:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=276
06/12/2022 11:34:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=278
06/12/2022 11:34:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=279
06/12/2022 11:34:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=281
06/12/2022 11:34:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=283
06/12/2022 11:34:37 - INFO - __main__ - Global step 1700 Train loss 0.07 Classification-F1 0.240047182946885 on epoch=283
06/12/2022 11:34:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=284
06/12/2022 11:34:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.10 on epoch=286
06/12/2022 11:34:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=288
06/12/2022 11:34:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=289
06/12/2022 11:34:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=291
06/12/2022 11:34:53 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.26970175043197353 on epoch=291
06/12/2022 11:34:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=293
06/12/2022 11:34:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=294
06/12/2022 11:35:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=296
06/12/2022 11:35:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=298
06/12/2022 11:35:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=299
06/12/2022 11:35:10 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.16582375478927203 on epoch=299
06/12/2022 11:35:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=301
06/12/2022 11:35:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=303
06/12/2022 11:35:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=304
06/12/2022 11:35:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=306
06/12/2022 11:35:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=308
06/12/2022 11:35:26 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.13088023088023085 on epoch=308
06/12/2022 11:35:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=309
06/12/2022 11:35:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=311
06/12/2022 11:35:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=313
06/12/2022 11:35:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=314
06/12/2022 11:35:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=316
06/12/2022 11:35:43 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.16502695810868243 on epoch=316
06/12/2022 11:35:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=318
06/12/2022 11:35:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=319
06/12/2022 11:35:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=321
06/12/2022 11:35:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=323
06/12/2022 11:35:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=324
06/12/2022 11:35:59 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.15476190476190477 on epoch=324
06/12/2022 11:36:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=326
06/12/2022 11:36:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=328
06/12/2022 11:36:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=329
06/12/2022 11:36:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=331
06/12/2022 11:36:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=333
06/12/2022 11:36:16 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.12556497175141243 on epoch=333
06/12/2022 11:36:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=334
06/12/2022 11:36:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=336
06/12/2022 11:36:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=338
06/12/2022 11:36:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=339
06/12/2022 11:36:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=341
06/12/2022 11:36:33 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.2849924585218703 on epoch=341
06/12/2022 11:36:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=343
06/12/2022 11:36:38 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=344
06/12/2022 11:36:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=346
06/12/2022 11:36:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=348
06/12/2022 11:36:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=349
06/12/2022 11:36:49 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.2589869281045752 on epoch=349
06/12/2022 11:36:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=351
06/12/2022 11:36:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=353
06/12/2022 11:36:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=354
06/12/2022 11:37:00 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=356
06/12/2022 11:37:03 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=358
06/12/2022 11:37:06 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.20548053383874282 on epoch=358
06/12/2022 11:37:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=359
06/12/2022 11:37:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=361
06/12/2022 11:37:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=363
06/12/2022 11:37:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=364
06/12/2022 11:37:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=366
06/12/2022 11:37:23 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.1990824555628703 on epoch=366
06/12/2022 11:37:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=368
06/12/2022 11:37:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=369
06/12/2022 11:37:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=371
06/12/2022 11:37:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=373
06/12/2022 11:37:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=374
06/12/2022 11:37:39 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.1897635814889336 on epoch=374
06/12/2022 11:37:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=376
06/12/2022 11:37:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=378
06/12/2022 11:37:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=379
06/12/2022 11:37:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=381
06/12/2022 11:37:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=383
06/12/2022 11:37:56 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.15745967741935485 on epoch=383
06/12/2022 11:37:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=384
06/12/2022 11:38:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=386
06/12/2022 11:38:04 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=388
06/12/2022 11:38:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=389
06/12/2022 11:38:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=391
06/12/2022 11:38:12 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.21149732620320857 on epoch=391
06/12/2022 11:38:15 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=393
06/12/2022 11:38:18 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=394
06/12/2022 11:38:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=396
06/12/2022 11:38:23 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=398
06/12/2022 11:38:26 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=399
06/12/2022 11:38:29 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.17379593031766943 on epoch=399
06/12/2022 11:38:31 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=401
06/12/2022 11:38:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=403
06/12/2022 11:38:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=404
06/12/2022 11:38:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=406
06/12/2022 11:38:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=408
06/12/2022 11:38:45 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.1960468103325246 on epoch=408
06/12/2022 11:38:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=409
06/12/2022 11:38:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=411
06/12/2022 11:38:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=413
06/12/2022 11:38:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=414
06/12/2022 11:38:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=416
06/12/2022 11:39:02 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.3140827631150212 on epoch=416
06/12/2022 11:39:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=418
06/12/2022 11:39:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=419
06/12/2022 11:39:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=421
06/12/2022 11:39:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=423
06/12/2022 11:39:15 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=424
06/12/2022 11:39:18 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.3121517847907704 on epoch=424
06/12/2022 11:39:21 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=426
06/12/2022 11:39:23 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=428
06/12/2022 11:39:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=429
06/12/2022 11:39:29 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=431
06/12/2022 11:39:32 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=433
06/12/2022 11:39:35 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.19906382907330822 on epoch=433
06/12/2022 11:39:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=434
06/12/2022 11:39:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=436
06/12/2022 11:39:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=438
06/12/2022 11:39:45 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=439
06/12/2022 11:39:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=441
06/12/2022 11:39:51 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.14589061462572148 on epoch=441
06/12/2022 11:39:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=443
06/12/2022 11:39:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=444
06/12/2022 11:39:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=446
06/12/2022 11:40:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=448
06/12/2022 11:40:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=449
06/12/2022 11:40:07 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.1949425287356322 on epoch=449
06/12/2022 11:40:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=451
06/12/2022 11:40:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=453
06/12/2022 11:40:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=454
06/12/2022 11:40:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=456
06/12/2022 11:40:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=458
06/12/2022 11:40:24 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.17756023096834142 on epoch=458
06/12/2022 11:40:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=459
06/12/2022 11:40:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=461
06/12/2022 11:40:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=463
06/12/2022 11:40:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=464
06/12/2022 11:40:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=466
06/12/2022 11:40:40 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.23458725182863116 on epoch=466
06/12/2022 11:40:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=468
06/12/2022 11:40:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=469
06/12/2022 11:40:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=471
06/12/2022 11:40:51 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=473
06/12/2022 11:40:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=474
06/12/2022 11:40:57 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.2247854599233299 on epoch=474
06/12/2022 11:41:00 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=476
06/12/2022 11:41:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=478
06/12/2022 11:41:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=479
06/12/2022 11:41:08 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=481
06/12/2022 11:41:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=483
06/12/2022 11:41:13 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.17078155205744266 on epoch=483
06/12/2022 11:41:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=484
06/12/2022 11:41:19 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=486
06/12/2022 11:41:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=488
06/12/2022 11:41:24 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=489
06/12/2022 11:41:27 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=491
06/12/2022 11:41:30 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.2413852432778489 on epoch=491
06/12/2022 11:41:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=493
06/12/2022 11:41:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=494
06/12/2022 11:41:38 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=496
06/12/2022 11:41:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=498
06/12/2022 11:41:43 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=499
06/12/2022 11:41:45 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:41:45 - INFO - __main__ - Printing 3 examples
06/12/2022 11:41:45 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/12/2022 11:41:45 - INFO - __main__ - ['entailment']
06/12/2022 11:41:45 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/12/2022 11:41:45 - INFO - __main__ - ['entailment']
06/12/2022 11:41:45 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/12/2022 11:41:45 - INFO - __main__ - ['entailment']
06/12/2022 11:41:45 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:41:45 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:41:45 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 11:41:45 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:41:45 - INFO - __main__ - Printing 3 examples
06/12/2022 11:41:45 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
06/12/2022 11:41:45 - INFO - __main__ - ['entailment']
06/12/2022 11:41:45 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
06/12/2022 11:41:45 - INFO - __main__ - ['entailment']
06/12/2022 11:41:45 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
06/12/2022 11:41:45 - INFO - __main__ - ['entailment']
06/12/2022 11:41:45 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:41:45 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:41:45 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 11:41:46 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.3221611721611722 on epoch=499
06/12/2022 11:41:46 - INFO - __main__ - save last model!
06/12/2022 11:41:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 11:41:46 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 11:41:46 - INFO - __main__ - Printing 3 examples
06/12/2022 11:41:46 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 11:41:46 - INFO - __main__ - ['contradiction']
06/12/2022 11:41:46 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 11:41:46 - INFO - __main__ - ['entailment']
06/12/2022 11:41:46 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 11:41:46 - INFO - __main__ - ['contradiction']
06/12/2022 11:41:46 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:41:47 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:41:48 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 11:42:04 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 11:42:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 11:42:04 - INFO - __main__ - Starting training!
06/12/2022 11:42:18 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_21_0.5_8_predictions.txt
06/12/2022 11:42:19 - INFO - __main__ - Classification-F1 on test data: 0.1646
06/12/2022 11:42:19 - INFO - __main__ - prefix=anli_32_21, lr=0.5, bsz=8, dev_performance=0.33631772821646244, test_performance=0.16464241353542353
06/12/2022 11:42:19 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.4, bsz=8 ...
06/12/2022 11:42:20 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:42:20 - INFO - __main__ - Printing 3 examples
06/12/2022 11:42:20 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/12/2022 11:42:20 - INFO - __main__ - ['entailment']
06/12/2022 11:42:20 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/12/2022 11:42:20 - INFO - __main__ - ['entailment']
06/12/2022 11:42:20 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/12/2022 11:42:20 - INFO - __main__ - ['entailment']
06/12/2022 11:42:20 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:42:20 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:42:20 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 11:42:20 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:42:20 - INFO - __main__ - Printing 3 examples
06/12/2022 11:42:20 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
06/12/2022 11:42:20 - INFO - __main__ - ['entailment']
06/12/2022 11:42:20 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
06/12/2022 11:42:20 - INFO - __main__ - ['entailment']
06/12/2022 11:42:20 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
06/12/2022 11:42:20 - INFO - __main__ - ['entailment']
06/12/2022 11:42:20 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:42:20 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:42:20 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 11:42:39 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 11:42:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 11:42:40 - INFO - __main__ - Starting training!
06/12/2022 11:42:43 - INFO - __main__ - Step 10 Global step 10 Train loss 0.96 on epoch=1
06/12/2022 11:42:46 - INFO - __main__ - Step 20 Global step 20 Train loss 0.59 on epoch=3
06/12/2022 11:42:49 - INFO - __main__ - Step 30 Global step 30 Train loss 0.62 on epoch=4
06/12/2022 11:42:51 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=6
06/12/2022 11:42:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=8
06/12/2022 11:42:56 - INFO - __main__ - Global step 50 Train loss 0.66 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 11:42:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 11:42:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=9
06/12/2022 11:43:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.57 on epoch=11
06/12/2022 11:43:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=13
06/12/2022 11:43:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=14
06/12/2022 11:43:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=16
06/12/2022 11:43:12 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 11:43:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=18
06/12/2022 11:43:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=19
06/12/2022 11:43:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=21
06/12/2022 11:43:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=23
06/12/2022 11:43:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=24
06/12/2022 11:43:28 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.29643625675746604 on epoch=24
06/12/2022 11:43:28 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.29643625675746604 on epoch=24, global_step=150
06/12/2022 11:43:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=26
06/12/2022 11:43:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=28
06/12/2022 11:43:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=29
06/12/2022 11:43:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=31
06/12/2022 11:43:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=33
06/12/2022 11:43:45 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 11:43:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=34
06/12/2022 11:43:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=36
06/12/2022 11:43:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=38
06/12/2022 11:43:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=39
06/12/2022 11:43:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=41
06/12/2022 11:44:01 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 11:44:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=43
06/12/2022 11:44:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=44
06/12/2022 11:44:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=46
06/12/2022 11:44:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=48
06/12/2022 11:44:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.54 on epoch=49
06/12/2022 11:44:17 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.21537816886654096 on epoch=49
06/12/2022 11:44:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=51
06/12/2022 11:44:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=53
06/12/2022 11:44:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=54
06/12/2022 11:44:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=56
06/12/2022 11:44:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=58
06/12/2022 11:44:34 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=58
06/12/2022 11:44:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=59
06/12/2022 11:44:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=61
06/12/2022 11:44:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=63
06/12/2022 11:44:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=64
06/12/2022 11:44:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=66
06/12/2022 11:44:50 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 11:44:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
06/12/2022 11:44:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=69
06/12/2022 11:44:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=71
06/12/2022 11:45:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=73
06/12/2022 11:45:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=74
06/12/2022 11:45:07 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.23178280321137465 on epoch=74
06/12/2022 11:45:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=76
06/12/2022 11:45:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=78
06/12/2022 11:45:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=79
06/12/2022 11:45:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=81
06/12/2022 11:45:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=83
06/12/2022 11:45:24 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.32172791747259827 on epoch=83
06/12/2022 11:45:24 - INFO - __main__ - Saving model with best Classification-F1: 0.29643625675746604 -> 0.32172791747259827 on epoch=83, global_step=500
06/12/2022 11:45:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=84
06/12/2022 11:45:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=86
06/12/2022 11:45:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=88
06/12/2022 11:45:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=89
06/12/2022 11:45:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=91
06/12/2022 11:45:40 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 11:45:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=93
06/12/2022 11:45:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=94
06/12/2022 11:45:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=96
06/12/2022 11:45:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=98
06/12/2022 11:45:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=99
06/12/2022 11:45:57 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.23473389355742294 on epoch=99
06/12/2022 11:45:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=101
06/12/2022 11:46:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=103
06/12/2022 11:46:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=104
06/12/2022 11:46:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=106
06/12/2022 11:46:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=108
06/12/2022 11:46:13 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.2904040404040404 on epoch=108
06/12/2022 11:46:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=109
06/12/2022 11:46:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=111
06/12/2022 11:46:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=113
06/12/2022 11:46:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=114
06/12/2022 11:46:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=116
06/12/2022 11:46:30 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 11:46:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=118
06/12/2022 11:46:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=119
06/12/2022 11:46:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=121
06/12/2022 11:46:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=123
06/12/2022 11:46:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=124
06/12/2022 11:46:46 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.2728835978835979 on epoch=124
06/12/2022 11:46:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=126
06/12/2022 11:46:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=128
06/12/2022 11:46:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=129
06/12/2022 11:46:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=131
06/12/2022 11:47:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=133
06/12/2022 11:47:03 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=133
06/12/2022 11:47:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=134
06/12/2022 11:47:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=136
06/12/2022 11:47:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=138
06/12/2022 11:47:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=139
06/12/2022 11:47:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=141
06/12/2022 11:47:20 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.1881810228266921 on epoch=141
06/12/2022 11:47:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=143
06/12/2022 11:47:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=144
06/12/2022 11:47:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=146
06/12/2022 11:47:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=148
06/12/2022 11:47:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=149
06/12/2022 11:47:37 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.2905865376453612 on epoch=149
06/12/2022 11:47:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=151
06/12/2022 11:47:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=153
06/12/2022 11:47:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=154
06/12/2022 11:47:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=156
06/12/2022 11:47:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=158
06/12/2022 11:47:53 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.25777777777777783 on epoch=158
06/12/2022 11:47:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=159
06/12/2022 11:47:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=161
06/12/2022 11:48:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=163
06/12/2022 11:48:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=164
06/12/2022 11:48:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=166
06/12/2022 11:48:10 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=166
06/12/2022 11:48:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=168
06/12/2022 11:48:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=169
06/12/2022 11:48:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=171
06/12/2022 11:48:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=173
06/12/2022 11:48:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=174
06/12/2022 11:48:26 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.3753885003885004 on epoch=174
06/12/2022 11:48:26 - INFO - __main__ - Saving model with best Classification-F1: 0.32172791747259827 -> 0.3753885003885004 on epoch=174, global_step=1050
06/12/2022 11:48:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=176
06/12/2022 11:48:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=178
06/12/2022 11:48:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=179
06/12/2022 11:48:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=181
06/12/2022 11:48:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=183
06/12/2022 11:48:42 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.263770537454748 on epoch=183
06/12/2022 11:48:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=184
06/12/2022 11:48:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=186
06/12/2022 11:48:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=188
06/12/2022 11:48:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=189
06/12/2022 11:48:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=191
06/12/2022 11:48:59 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.21934331025240114 on epoch=191
06/12/2022 11:49:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=193
06/12/2022 11:49:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=194
06/12/2022 11:49:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=196
06/12/2022 11:49:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=198
06/12/2022 11:49:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=199
06/12/2022 11:49:16 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.27574680544977576 on epoch=199
06/12/2022 11:49:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=201
06/12/2022 11:49:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=203
06/12/2022 11:49:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=204
06/12/2022 11:49:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=206
06/12/2022 11:49:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=208
06/12/2022 11:49:32 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.263770537454748 on epoch=208
06/12/2022 11:49:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=209
06/12/2022 11:49:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=211
06/12/2022 11:49:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=213
06/12/2022 11:49:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=214
06/12/2022 11:49:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=216
06/12/2022 11:49:49 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.277599633531837 on epoch=216
06/12/2022 11:49:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=218
06/12/2022 11:49:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=219
06/12/2022 11:49:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=221
06/12/2022 11:49:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=223
06/12/2022 11:50:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=224
06/12/2022 11:50:05 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.2937023996161985 on epoch=224
06/12/2022 11:50:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=226
06/12/2022 11:50:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=228
06/12/2022 11:50:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=229
06/12/2022 11:50:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=231
06/12/2022 11:50:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=233
06/12/2022 11:50:22 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.29530063432502457 on epoch=233
06/12/2022 11:50:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=234
06/12/2022 11:50:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=236
06/12/2022 11:50:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=238
06/12/2022 11:50:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.33 on epoch=239
06/12/2022 11:50:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=241
06/12/2022 11:50:39 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.2002164502164502 on epoch=241
06/12/2022 11:50:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=243
06/12/2022 11:50:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=244
06/12/2022 11:50:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=246
06/12/2022 11:50:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=248
06/12/2022 11:50:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=249
06/12/2022 11:50:55 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.3397872340425532 on epoch=249
06/12/2022 11:50:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=251
06/12/2022 11:51:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=253
06/12/2022 11:51:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=254
06/12/2022 11:51:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=256
06/12/2022 11:51:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=258
06/12/2022 11:51:12 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.34713101160862353 on epoch=258
06/12/2022 11:51:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=259
06/12/2022 11:51:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=261
06/12/2022 11:51:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.33 on epoch=263
06/12/2022 11:51:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=264
06/12/2022 11:51:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=266
06/12/2022 11:51:28 - INFO - __main__ - Global step 1600 Train loss 0.30 Classification-F1 0.28894812558178895 on epoch=266
06/12/2022 11:51:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=268
06/12/2022 11:51:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=269
06/12/2022 11:51:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.27 on epoch=271
06/12/2022 11:51:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.27 on epoch=273
06/12/2022 11:51:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=274
06/12/2022 11:51:45 - INFO - __main__ - Global step 1650 Train loss 0.26 Classification-F1 0.33233194760373364 on epoch=274
06/12/2022 11:51:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.30 on epoch=276
06/12/2022 11:51:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=278
06/12/2022 11:51:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=279
06/12/2022 11:51:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=281
06/12/2022 11:51:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=283
06/12/2022 11:52:01 - INFO - __main__ - Global step 1700 Train loss 0.28 Classification-F1 0.3275384691851813 on epoch=283
06/12/2022 11:52:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=284
06/12/2022 11:52:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=286
06/12/2022 11:52:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=288
06/12/2022 11:52:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.29 on epoch=289
06/12/2022 11:52:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=291
06/12/2022 11:52:17 - INFO - __main__ - Global step 1750 Train loss 0.26 Classification-F1 0.26255209870899954 on epoch=291
06/12/2022 11:52:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=293
06/12/2022 11:52:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=294
06/12/2022 11:52:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=296
06/12/2022 11:52:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=298
06/12/2022 11:52:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=299
06/12/2022 11:52:34 - INFO - __main__ - Global step 1800 Train loss 0.26 Classification-F1 0.2626438528326995 on epoch=299
06/12/2022 11:52:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.24 on epoch=301
06/12/2022 11:52:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=303
06/12/2022 11:52:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=304
06/12/2022 11:52:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=306
06/12/2022 11:52:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.23 on epoch=308
06/12/2022 11:52:50 - INFO - __main__ - Global step 1850 Train loss 0.24 Classification-F1 0.31962087699792624 on epoch=308
06/12/2022 11:52:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=309
06/12/2022 11:52:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.25 on epoch=311
06/12/2022 11:52:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=313
06/12/2022 11:53:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=314
06/12/2022 11:53:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=316
06/12/2022 11:53:07 - INFO - __main__ - Global step 1900 Train loss 0.23 Classification-F1 0.3125651967757231 on epoch=316
06/12/2022 11:53:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.21 on epoch=318
06/12/2022 11:53:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=319
06/12/2022 11:53:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=321
06/12/2022 11:53:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=323
06/12/2022 11:53:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=324
06/12/2022 11:53:23 - INFO - __main__ - Global step 1950 Train loss 0.22 Classification-F1 0.2872041913315643 on epoch=324
06/12/2022 11:53:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=326
06/12/2022 11:53:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=328
06/12/2022 11:53:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=329
06/12/2022 11:53:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=331
06/12/2022 11:53:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=333
06/12/2022 11:53:40 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.27738883461775027 on epoch=333
06/12/2022 11:53:42 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.18 on epoch=334
06/12/2022 11:53:45 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=336
06/12/2022 11:53:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=338
06/12/2022 11:53:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=339
06/12/2022 11:53:53 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.15 on epoch=341
06/12/2022 11:53:56 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.25484979492295756 on epoch=341
06/12/2022 11:53:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=343
06/12/2022 11:54:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=344
06/12/2022 11:54:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=346
06/12/2022 11:54:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=348
06/12/2022 11:54:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=349
06/12/2022 11:54:12 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.32372482967669114 on epoch=349
06/12/2022 11:54:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=351
06/12/2022 11:54:18 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=353
06/12/2022 11:54:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=354
06/12/2022 11:54:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=356
06/12/2022 11:54:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.13 on epoch=358
06/12/2022 11:54:29 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.30574033013057406 on epoch=358
06/12/2022 11:54:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=359
06/12/2022 11:54:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=361
06/12/2022 11:54:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=363
06/12/2022 11:54:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=364
06/12/2022 11:54:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.13 on epoch=366
06/12/2022 11:54:46 - INFO - __main__ - Global step 2200 Train loss 0.14 Classification-F1 0.319460765112939 on epoch=366
06/12/2022 11:54:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=368
06/12/2022 11:54:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=369
06/12/2022 11:54:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.12 on epoch=371
06/12/2022 11:54:56 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=373
06/12/2022 11:54:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=374
06/12/2022 11:55:02 - INFO - __main__ - Global step 2250 Train loss 0.11 Classification-F1 0.21303827751196172 on epoch=374
06/12/2022 11:55:05 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=376
06/12/2022 11:55:07 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=378
06/12/2022 11:55:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=379
06/12/2022 11:55:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=381
06/12/2022 11:55:15 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=383
06/12/2022 11:55:18 - INFO - __main__ - Global step 2300 Train loss 0.10 Classification-F1 0.22783436076119 on epoch=383
06/12/2022 11:55:21 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.19 on epoch=384
06/12/2022 11:55:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=386
06/12/2022 11:55:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.13 on epoch=388
06/12/2022 11:55:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.13 on epoch=389
06/12/2022 11:55:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=391
06/12/2022 11:55:35 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.22740424132829196 on epoch=391
06/12/2022 11:55:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=393
06/12/2022 11:55:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=394
06/12/2022 11:55:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=396
06/12/2022 11:55:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.15 on epoch=398
06/12/2022 11:55:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=399
06/12/2022 11:55:51 - INFO - __main__ - Global step 2400 Train loss 0.13 Classification-F1 0.30527446251153295 on epoch=399
06/12/2022 11:55:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=401
06/12/2022 11:55:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.09 on epoch=403
06/12/2022 11:55:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=404
06/12/2022 11:56:02 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=406
06/12/2022 11:56:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.13 on epoch=408
06/12/2022 11:56:07 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.23974331242339028 on epoch=408
06/12/2022 11:56:10 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=409
06/12/2022 11:56:13 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=411
06/12/2022 11:56:15 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.14 on epoch=413
06/12/2022 11:56:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.09 on epoch=414
06/12/2022 11:56:20 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=416
06/12/2022 11:56:23 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.30499239591552035 on epoch=416
06/12/2022 11:56:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=418
06/12/2022 11:56:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=419
06/12/2022 11:56:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=421
06/12/2022 11:56:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=423
06/12/2022 11:56:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.11 on epoch=424
06/12/2022 11:56:39 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.268265503875969 on epoch=424
06/12/2022 11:56:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=426
06/12/2022 11:56:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=428
06/12/2022 11:56:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=429
06/12/2022 11:56:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=431
06/12/2022 11:56:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=433
06/12/2022 11:56:55 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.23049645390070922 on epoch=433
06/12/2022 11:56:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=434
06/12/2022 11:57:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=436
06/12/2022 11:57:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=438
06/12/2022 11:57:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=439
06/12/2022 11:57:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=441
06/12/2022 11:57:11 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.22171409214092141 on epoch=441
06/12/2022 11:57:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=443
06/12/2022 11:57:17 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=444
06/12/2022 11:57:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=446
06/12/2022 11:57:22 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=448
06/12/2022 11:57:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=449
06/12/2022 11:57:28 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.2295608566007048 on epoch=449
06/12/2022 11:57:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=451
06/12/2022 11:57:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=453
06/12/2022 11:57:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=454
06/12/2022 11:57:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=456
06/12/2022 11:57:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=458
06/12/2022 11:57:44 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.2236929736929737 on epoch=458
06/12/2022 11:57:46 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=459
06/12/2022 11:57:49 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=461
06/12/2022 11:57:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=463
06/12/2022 11:57:54 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=464
06/12/2022 11:57:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=466
06/12/2022 11:58:00 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.20859213250517597 on epoch=466
06/12/2022 11:58:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=468
06/12/2022 11:58:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=469
06/12/2022 11:58:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=471
06/12/2022 11:58:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=473
06/12/2022 11:58:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=474
06/12/2022 11:58:17 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.1975 on epoch=474
06/12/2022 11:58:19 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=476
06/12/2022 11:58:22 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=478
06/12/2022 11:58:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=479
06/12/2022 11:58:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=481
06/12/2022 11:58:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=483
06/12/2022 11:58:33 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.22036541889483063 on epoch=483
06/12/2022 11:58:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=484
06/12/2022 11:58:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=486
06/12/2022 11:58:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=488
06/12/2022 11:58:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=489
06/12/2022 11:58:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=491
06/12/2022 11:58:50 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.14922654204798683 on epoch=491
06/12/2022 11:58:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=493
06/12/2022 11:58:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=494
06/12/2022 11:58:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=496
06/12/2022 11:59:01 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=498
06/12/2022 11:59:03 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=499
06/12/2022 11:59:05 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:59:05 - INFO - __main__ - Printing 3 examples
06/12/2022 11:59:05 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/12/2022 11:59:05 - INFO - __main__ - ['entailment']
06/12/2022 11:59:05 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/12/2022 11:59:05 - INFO - __main__ - ['entailment']
06/12/2022 11:59:05 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/12/2022 11:59:05 - INFO - __main__ - ['entailment']
06/12/2022 11:59:05 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:59:05 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:59:05 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 11:59:05 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:59:05 - INFO - __main__ - Printing 3 examples
06/12/2022 11:59:05 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
06/12/2022 11:59:05 - INFO - __main__ - ['entailment']
06/12/2022 11:59:05 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
06/12/2022 11:59:05 - INFO - __main__ - ['entailment']
06/12/2022 11:59:05 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
06/12/2022 11:59:05 - INFO - __main__ - ['entailment']
06/12/2022 11:59:05 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:59:05 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:59:05 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 11:59:06 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.21222663453597168 on epoch=499
06/12/2022 11:59:06 - INFO - __main__ - save last model!
06/12/2022 11:59:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 11:59:07 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 11:59:07 - INFO - __main__ - Printing 3 examples
06/12/2022 11:59:07 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 11:59:07 - INFO - __main__ - ['contradiction']
06/12/2022 11:59:07 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 11:59:07 - INFO - __main__ - ['entailment']
06/12/2022 11:59:07 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 11:59:07 - INFO - __main__ - ['contradiction']
06/12/2022 11:59:07 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:59:07 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:59:09 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 11:59:21 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 11:59:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 11:59:22 - INFO - __main__ - Starting training!
06/12/2022 11:59:39 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_21_0.4_8_predictions.txt
06/12/2022 11:59:39 - INFO - __main__ - Classification-F1 on test data: 0.1494
06/12/2022 11:59:40 - INFO - __main__ - prefix=anli_32_21, lr=0.4, bsz=8, dev_performance=0.3753885003885004, test_performance=0.14940158364099396
06/12/2022 11:59:40 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.3, bsz=8 ...
06/12/2022 11:59:41 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:59:41 - INFO - __main__ - Printing 3 examples
06/12/2022 11:59:41 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/12/2022 11:59:41 - INFO - __main__ - ['entailment']
06/12/2022 11:59:41 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/12/2022 11:59:41 - INFO - __main__ - ['entailment']
06/12/2022 11:59:41 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/12/2022 11:59:41 - INFO - __main__ - ['entailment']
06/12/2022 11:59:41 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:59:41 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:59:41 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 11:59:41 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 11:59:41 - INFO - __main__ - Printing 3 examples
06/12/2022 11:59:41 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
06/12/2022 11:59:41 - INFO - __main__ - ['entailment']
06/12/2022 11:59:41 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
06/12/2022 11:59:41 - INFO - __main__ - ['entailment']
06/12/2022 11:59:41 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
06/12/2022 11:59:41 - INFO - __main__ - ['entailment']
06/12/2022 11:59:41 - INFO - __main__ - Tokenizing Input ...
06/12/2022 11:59:41 - INFO - __main__ - Tokenizing Output ...
06/12/2022 11:59:41 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 11:59:58 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 11:59:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 11:59:59 - INFO - __main__ - Starting training!
06/12/2022 12:00:02 - INFO - __main__ - Step 10 Global step 10 Train loss 1.06 on epoch=1
06/12/2022 12:00:05 - INFO - __main__ - Step 20 Global step 20 Train loss 0.85 on epoch=3
06/12/2022 12:00:07 - INFO - __main__ - Step 30 Global step 30 Train loss 0.67 on epoch=4
06/12/2022 12:00:10 - INFO - __main__ - Step 40 Global step 40 Train loss 0.63 on epoch=6
06/12/2022 12:00:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.56 on epoch=8
06/12/2022 12:00:14 - INFO - __main__ - Global step 50 Train loss 0.76 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 12:00:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 12:00:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.64 on epoch=9
06/12/2022 12:00:20 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=11
06/12/2022 12:00:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=13
06/12/2022 12:00:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.57 on epoch=14
06/12/2022 12:00:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=16
06/12/2022 12:00:29 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 12:00:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=18
06/12/2022 12:00:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=19
06/12/2022 12:00:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=21
06/12/2022 12:00:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=23
06/12/2022 12:00:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=24
06/12/2022 12:00:45 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.17980884109916365 on epoch=24
06/12/2022 12:00:45 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17980884109916365 on epoch=24, global_step=150
06/12/2022 12:00:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=26
06/12/2022 12:00:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=28
06/12/2022 12:00:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=29
06/12/2022 12:00:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=31
06/12/2022 12:00:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=33
06/12/2022 12:01:00 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 12:01:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=34
06/12/2022 12:01:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=36
06/12/2022 12:01:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=38
06/12/2022 12:01:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=39
06/12/2022 12:01:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=41
06/12/2022 12:01:15 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 12:01:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=43
06/12/2022 12:01:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=44
06/12/2022 12:01:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=46
06/12/2022 12:01:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=48
06/12/2022 12:01:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=49
06/12/2022 12:01:30 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=49
06/12/2022 12:01:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=51
06/12/2022 12:01:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=53
06/12/2022 12:01:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=54
06/12/2022 12:01:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=56
06/12/2022 12:01:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=58
06/12/2022 12:01:46 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=58
06/12/2022 12:01:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=59
06/12/2022 12:01:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=61
06/12/2022 12:01:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=63
06/12/2022 12:01:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=64
06/12/2022 12:01:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=66
06/12/2022 12:02:02 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 12:02:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=68
06/12/2022 12:02:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=69
06/12/2022 12:02:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=71
06/12/2022 12:02:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=73
06/12/2022 12:02:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=74
06/12/2022 12:02:18 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.2235772357723577 on epoch=74
06/12/2022 12:02:18 - INFO - __main__ - Saving model with best Classification-F1: 0.17980884109916365 -> 0.2235772357723577 on epoch=74, global_step=450
06/12/2022 12:02:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=76
06/12/2022 12:02:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=78
06/12/2022 12:02:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=79
06/12/2022 12:02:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=81
06/12/2022 12:02:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=83
06/12/2022 12:02:34 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.26499969357112213 on epoch=83
06/12/2022 12:02:34 - INFO - __main__ - Saving model with best Classification-F1: 0.2235772357723577 -> 0.26499969357112213 on epoch=83, global_step=500
06/12/2022 12:02:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=84
06/12/2022 12:02:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=86
06/12/2022 12:02:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=88
06/12/2022 12:02:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=89
06/12/2022 12:02:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=91
06/12/2022 12:02:50 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 12:02:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=93
06/12/2022 12:02:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=94
06/12/2022 12:02:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=96
06/12/2022 12:03:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=98
06/12/2022 12:03:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=99
06/12/2022 12:03:06 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=99
06/12/2022 12:03:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=101
06/12/2022 12:03:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=103
06/12/2022 12:03:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=104
06/12/2022 12:03:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=106
06/12/2022 12:03:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=108
06/12/2022 12:03:22 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.22207573427085622 on epoch=108
06/12/2022 12:03:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=109
06/12/2022 12:03:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=111
06/12/2022 12:03:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=113
06/12/2022 12:03:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=114
06/12/2022 12:03:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=116
06/12/2022 12:03:37 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.1881810228266921 on epoch=116
06/12/2022 12:03:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=118
06/12/2022 12:03:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=119
06/12/2022 12:03:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=121
06/12/2022 12:03:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=123
06/12/2022 12:03:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=124
06/12/2022 12:03:53 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.19465977605512488 on epoch=124
06/12/2022 12:03:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=126
06/12/2022 12:03:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=128
06/12/2022 12:04:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=129
06/12/2022 12:04:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=131
06/12/2022 12:04:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=133
06/12/2022 12:04:09 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.2532279314888011 on epoch=133
06/12/2022 12:04:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=134
06/12/2022 12:04:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.73 on epoch=136
06/12/2022 12:04:17 - INFO - __main__ - Step 830 Global step 830 Train loss 2.18 on epoch=138
06/12/2022 12:04:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.64 on epoch=139
06/12/2022 12:04:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=141
06/12/2022 12:04:25 - INFO - __main__ - Global step 850 Train loss 0.89 Classification-F1 0.29367616611862135 on epoch=141
06/12/2022 12:04:25 - INFO - __main__ - Saving model with best Classification-F1: 0.26499969357112213 -> 0.29367616611862135 on epoch=141, global_step=850
06/12/2022 12:04:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=143
06/12/2022 12:04:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=144
06/12/2022 12:04:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=146
06/12/2022 12:04:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=148
06/12/2022 12:04:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=149
06/12/2022 12:04:41 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.2649350649350649 on epoch=149
06/12/2022 12:04:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=151
06/12/2022 12:04:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=153
06/12/2022 12:04:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=154
06/12/2022 12:04:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=156
06/12/2022 12:04:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=158
06/12/2022 12:04:57 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.24528301886792456 on epoch=158
06/12/2022 12:05:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=159
06/12/2022 12:05:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=161
06/12/2022 12:05:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=163
06/12/2022 12:05:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=164
06/12/2022 12:05:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=166
06/12/2022 12:05:13 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.272873445459461 on epoch=166
06/12/2022 12:05:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=168
06/12/2022 12:05:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=169
06/12/2022 12:05:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=171
06/12/2022 12:05:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=173
06/12/2022 12:05:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=174
06/12/2022 12:05:29 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.2734350036310821 on epoch=174
06/12/2022 12:05:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=176
06/12/2022 12:05:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=178
06/12/2022 12:05:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=179
06/12/2022 12:05:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=181
06/12/2022 12:05:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=183
06/12/2022 12:05:45 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.25199709513435004 on epoch=183
06/12/2022 12:05:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.43 on epoch=184
06/12/2022 12:05:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=186
06/12/2022 12:05:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=188
06/12/2022 12:05:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=189
06/12/2022 12:05:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=191
06/12/2022 12:06:01 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.264990276117395 on epoch=191
06/12/2022 12:06:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=193
06/12/2022 12:06:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=194
06/12/2022 12:06:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=196
06/12/2022 12:06:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=198
06/12/2022 12:06:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=199
06/12/2022 12:06:17 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.2523547400611621 on epoch=199
06/12/2022 12:06:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=201
06/12/2022 12:06:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=203
06/12/2022 12:06:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=204
06/12/2022 12:06:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=206
06/12/2022 12:06:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=208
06/12/2022 12:06:33 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.2583896160513884 on epoch=208
06/12/2022 12:06:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=209
06/12/2022 12:06:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=211
06/12/2022 12:06:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=213
06/12/2022 12:06:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=214
06/12/2022 12:06:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=216
06/12/2022 12:06:49 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.2751937984496124 on epoch=216
06/12/2022 12:06:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=218
06/12/2022 12:06:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=219
06/12/2022 12:06:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=221
06/12/2022 12:06:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=223
06/12/2022 12:07:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=224
06/12/2022 12:07:04 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.2256742915670877 on epoch=224
06/12/2022 12:07:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=226
06/12/2022 12:07:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=228
06/12/2022 12:07:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=229
06/12/2022 12:07:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=231
06/12/2022 12:07:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=233
06/12/2022 12:07:20 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.2370621468926554 on epoch=233
06/12/2022 12:07:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=234
06/12/2022 12:07:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=236
06/12/2022 12:07:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=238
06/12/2022 12:07:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=239
06/12/2022 12:07:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.46 on epoch=241
06/12/2022 12:07:36 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.2736694677871148 on epoch=241
06/12/2022 12:07:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=243
06/12/2022 12:07:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=244
06/12/2022 12:07:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=246
06/12/2022 12:07:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=248
06/12/2022 12:07:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=249
06/12/2022 12:07:52 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.2726177263358173 on epoch=249
06/12/2022 12:07:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=251
06/12/2022 12:07:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=253
06/12/2022 12:08:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=254
06/12/2022 12:08:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=256
06/12/2022 12:08:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=258
06/12/2022 12:08:08 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.2600050658561297 on epoch=258
06/12/2022 12:08:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=259
06/12/2022 12:08:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=261
06/12/2022 12:08:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=263
06/12/2022 12:08:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=264
06/12/2022 12:08:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=266
06/12/2022 12:08:24 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.27997364953886694 on epoch=266
06/12/2022 12:08:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=268
06/12/2022 12:08:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=269
06/12/2022 12:08:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=271
06/12/2022 12:08:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.38 on epoch=273
06/12/2022 12:08:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=274
06/12/2022 12:08:40 - INFO - __main__ - Global step 1650 Train loss 0.39 Classification-F1 0.2347883597883598 on epoch=274
06/12/2022 12:08:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=276
06/12/2022 12:08:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=278
06/12/2022 12:08:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=279
06/12/2022 12:08:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=281
06/12/2022 12:08:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=283
06/12/2022 12:08:56 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.2400916380297823 on epoch=283
06/12/2022 12:08:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=284
06/12/2022 12:09:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=286
06/12/2022 12:09:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=288
06/12/2022 12:09:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=289
06/12/2022 12:09:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=291
06/12/2022 12:09:12 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.23160800552104902 on epoch=291
06/12/2022 12:09:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=293
06/12/2022 12:09:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=294
06/12/2022 12:09:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=296
06/12/2022 12:09:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=298
06/12/2022 12:09:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=299
06/12/2022 12:09:28 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.23745444350995237 on epoch=299
06/12/2022 12:09:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=301
06/12/2022 12:09:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=303
06/12/2022 12:09:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=304
06/12/2022 12:09:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=306
06/12/2022 12:09:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=308
06/12/2022 12:09:44 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.234320987654321 on epoch=308
06/12/2022 12:09:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=309
06/12/2022 12:09:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=311
06/12/2022 12:09:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=313
06/12/2022 12:09:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=314
06/12/2022 12:09:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=316
06/12/2022 12:10:01 - INFO - __main__ - Global step 1900 Train loss 0.41 Classification-F1 0.23317805383022774 on epoch=316
06/12/2022 12:10:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.42 on epoch=318
06/12/2022 12:10:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=319
06/12/2022 12:10:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=321
06/12/2022 12:10:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=323
06/12/2022 12:10:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=324
06/12/2022 12:10:17 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.2593590895477688 on epoch=324
06/12/2022 12:10:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.39 on epoch=326
06/12/2022 12:10:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=328
06/12/2022 12:10:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.40 on epoch=329
06/12/2022 12:10:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=331
06/12/2022 12:10:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=333
06/12/2022 12:10:34 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.23783371472158663 on epoch=333
06/12/2022 12:10:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.41 on epoch=334
06/12/2022 12:10:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=336
06/12/2022 12:10:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.43 on epoch=338
06/12/2022 12:10:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=339
06/12/2022 12:10:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.42 on epoch=341
06/12/2022 12:10:50 - INFO - __main__ - Global step 2050 Train loss 0.40 Classification-F1 0.2345583494519665 on epoch=341
06/12/2022 12:10:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.46 on epoch=343
06/12/2022 12:10:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=344
06/12/2022 12:10:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.37 on epoch=346
06/12/2022 12:11:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.37 on epoch=348
06/12/2022 12:11:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.42 on epoch=349
06/12/2022 12:11:07 - INFO - __main__ - Global step 2100 Train loss 0.41 Classification-F1 0.25314228255404725 on epoch=349
06/12/2022 12:11:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.39 on epoch=351
06/12/2022 12:11:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.37 on epoch=353
06/12/2022 12:11:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.40 on epoch=354
06/12/2022 12:11:17 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.42 on epoch=356
06/12/2022 12:11:20 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.36 on epoch=358
06/12/2022 12:11:23 - INFO - __main__ - Global step 2150 Train loss 0.39 Classification-F1 0.3239940895658104 on epoch=358
06/12/2022 12:11:23 - INFO - __main__ - Saving model with best Classification-F1: 0.29367616611862135 -> 0.3239940895658104 on epoch=358, global_step=2150
06/12/2022 12:11:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.42 on epoch=359
06/12/2022 12:11:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.37 on epoch=361
06/12/2022 12:11:31 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.35 on epoch=363
06/12/2022 12:11:34 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.38 on epoch=364
06/12/2022 12:11:36 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.39 on epoch=366
06/12/2022 12:11:39 - INFO - __main__ - Global step 2200 Train loss 0.38 Classification-F1 0.203679559611763 on epoch=366
06/12/2022 12:11:42 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.38 on epoch=368
06/12/2022 12:11:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.42 on epoch=369
06/12/2022 12:11:47 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.36 on epoch=371
06/12/2022 12:11:50 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.37 on epoch=373
06/12/2022 12:11:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.38 on epoch=374
06/12/2022 12:11:56 - INFO - __main__ - Global step 2250 Train loss 0.38 Classification-F1 0.310549815027427 on epoch=374
06/12/2022 12:11:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.34 on epoch=376
06/12/2022 12:12:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.38 on epoch=378
06/12/2022 12:12:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.35 on epoch=379
06/12/2022 12:12:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.36 on epoch=381
06/12/2022 12:12:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.41 on epoch=383
06/12/2022 12:12:12 - INFO - __main__ - Global step 2300 Train loss 0.37 Classification-F1 0.29213947990543737 on epoch=383
06/12/2022 12:12:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.36 on epoch=384
06/12/2022 12:12:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.37 on epoch=386
06/12/2022 12:12:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.38 on epoch=388
06/12/2022 12:12:23 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.38 on epoch=389
06/12/2022 12:12:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=391
06/12/2022 12:12:29 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.2558814912473449 on epoch=391
06/12/2022 12:12:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.33 on epoch=393
06/12/2022 12:12:34 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.38 on epoch=394
06/12/2022 12:12:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.34 on epoch=396
06/12/2022 12:12:39 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.37 on epoch=398
06/12/2022 12:12:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.38 on epoch=399
06/12/2022 12:12:45 - INFO - __main__ - Global step 2400 Train loss 0.36 Classification-F1 0.27794484540675396 on epoch=399
06/12/2022 12:12:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.36 on epoch=401
06/12/2022 12:12:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.36 on epoch=403
06/12/2022 12:12:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.39 on epoch=404
06/12/2022 12:12:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.38 on epoch=406
06/12/2022 12:12:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=408
06/12/2022 12:13:01 - INFO - __main__ - Global step 2450 Train loss 0.37 Classification-F1 0.3046869205534161 on epoch=408
06/12/2022 12:13:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.32 on epoch=409
06/12/2022 12:13:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=411
06/12/2022 12:13:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.33 on epoch=413
06/12/2022 12:13:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.34 on epoch=414
06/12/2022 12:13:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.35 on epoch=416
06/12/2022 12:13:17 - INFO - __main__ - Global step 2500 Train loss 0.34 Classification-F1 0.20974005567028828 on epoch=416
06/12/2022 12:13:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.31 on epoch=418
06/12/2022 12:13:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.39 on epoch=419
06/12/2022 12:13:26 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.36 on epoch=421
06/12/2022 12:13:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.33 on epoch=423
06/12/2022 12:13:31 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.33 on epoch=424
06/12/2022 12:13:34 - INFO - __main__ - Global step 2550 Train loss 0.34 Classification-F1 0.25937081659973227 on epoch=424
06/12/2022 12:13:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.32 on epoch=426
06/12/2022 12:13:39 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.32 on epoch=428
06/12/2022 12:13:42 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.34 on epoch=429
06/12/2022 12:13:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.33 on epoch=431
06/12/2022 12:13:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.39 on epoch=433
06/12/2022 12:13:50 - INFO - __main__ - Global step 2600 Train loss 0.34 Classification-F1 0.31286903341697864 on epoch=433
06/12/2022 12:13:53 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.33 on epoch=434
06/12/2022 12:13:55 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.32 on epoch=436
06/12/2022 12:13:58 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.30 on epoch=438
06/12/2022 12:14:01 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.39 on epoch=439
06/12/2022 12:14:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.33 on epoch=441
06/12/2022 12:14:06 - INFO - __main__ - Global step 2650 Train loss 0.33 Classification-F1 0.2678485409754873 on epoch=441
06/12/2022 12:14:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.33 on epoch=443
06/12/2022 12:14:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.35 on epoch=444
06/12/2022 12:14:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.33 on epoch=446
06/12/2022 12:14:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.29 on epoch=448
06/12/2022 12:14:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.27 on epoch=449
06/12/2022 12:14:23 - INFO - __main__ - Global step 2700 Train loss 0.32 Classification-F1 0.2937969894550431 on epoch=449
06/12/2022 12:14:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.31 on epoch=451
06/12/2022 12:14:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.34 on epoch=453
06/12/2022 12:14:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.30 on epoch=454
06/12/2022 12:14:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.25 on epoch=456
06/12/2022 12:14:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.31 on epoch=458
06/12/2022 12:14:39 - INFO - __main__ - Global step 2750 Train loss 0.30 Classification-F1 0.28643249991564596 on epoch=458
06/12/2022 12:14:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.29 on epoch=459
06/12/2022 12:14:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.27 on epoch=461
06/12/2022 12:14:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.27 on epoch=463
06/12/2022 12:14:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.36 on epoch=464
06/12/2022 12:14:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.33 on epoch=466
06/12/2022 12:14:56 - INFO - __main__ - Global step 2800 Train loss 0.30 Classification-F1 0.2833333333333333 on epoch=466
06/12/2022 12:14:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.26 on epoch=468
06/12/2022 12:15:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.31 on epoch=469
06/12/2022 12:15:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.31 on epoch=471
06/12/2022 12:15:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.32 on epoch=473
06/12/2022 12:15:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.30 on epoch=474
06/12/2022 12:15:12 - INFO - __main__ - Global step 2850 Train loss 0.30 Classification-F1 0.2619047619047619 on epoch=474
06/12/2022 12:15:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.31 on epoch=476
06/12/2022 12:15:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.30 on epoch=478
06/12/2022 12:15:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.30 on epoch=479
06/12/2022 12:15:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.25 on epoch=481
06/12/2022 12:15:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.31 on epoch=483
06/12/2022 12:15:28 - INFO - __main__ - Global step 2900 Train loss 0.29 Classification-F1 0.3269854692046379 on epoch=483
06/12/2022 12:15:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3239940895658104 -> 0.3269854692046379 on epoch=483, global_step=2900
06/12/2022 12:15:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.36 on epoch=484
06/12/2022 12:15:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.33 on epoch=486
06/12/2022 12:15:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.30 on epoch=488
06/12/2022 12:15:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.32 on epoch=489
06/12/2022 12:15:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.32 on epoch=491
06/12/2022 12:15:45 - INFO - __main__ - Global step 2950 Train loss 0.32 Classification-F1 0.24681598594642074 on epoch=491
06/12/2022 12:15:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.24 on epoch=493
06/12/2022 12:15:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.29 on epoch=494
06/12/2022 12:15:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.30 on epoch=496
06/12/2022 12:15:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.27 on epoch=498
06/12/2022 12:15:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.24 on epoch=499
06/12/2022 12:15:59 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:15:59 - INFO - __main__ - Printing 3 examples
06/12/2022 12:15:59 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/12/2022 12:15:59 - INFO - __main__ - ['entailment']
06/12/2022 12:15:59 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/12/2022 12:15:59 - INFO - __main__ - ['entailment']
06/12/2022 12:15:59 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/12/2022 12:15:59 - INFO - __main__ - ['entailment']
06/12/2022 12:15:59 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:15:59 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:16:00 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 12:16:00 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:16:00 - INFO - __main__ - Printing 3 examples
06/12/2022 12:16:00 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
06/12/2022 12:16:00 - INFO - __main__ - ['entailment']
06/12/2022 12:16:00 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
06/12/2022 12:16:00 - INFO - __main__ - ['entailment']
06/12/2022 12:16:00 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
06/12/2022 12:16:00 - INFO - __main__ - ['entailment']
06/12/2022 12:16:00 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:16:00 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:16:00 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 12:16:01 - INFO - __main__ - Global step 3000 Train loss 0.27 Classification-F1 0.2983390200188874 on epoch=499
06/12/2022 12:16:01 - INFO - __main__ - save last model!
06/12/2022 12:16:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 12:16:01 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 12:16:01 - INFO - __main__ - Printing 3 examples
06/12/2022 12:16:01 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 12:16:01 - INFO - __main__ - ['contradiction']
06/12/2022 12:16:01 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 12:16:01 - INFO - __main__ - ['entailment']
06/12/2022 12:16:01 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 12:16:01 - INFO - __main__ - ['contradiction']
06/12/2022 12:16:01 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:16:02 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:16:03 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 12:16:19 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 12:16:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 12:16:20 - INFO - __main__ - Starting training!
06/12/2022 12:16:34 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_21_0.3_8_predictions.txt
06/12/2022 12:16:34 - INFO - __main__ - Classification-F1 on test data: 0.2773
06/12/2022 12:16:34 - INFO - __main__ - prefix=anli_32_21, lr=0.3, bsz=8, dev_performance=0.3269854692046379, test_performance=0.27732425745294215
06/12/2022 12:16:34 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.2, bsz=8 ...
06/12/2022 12:16:35 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:16:35 - INFO - __main__ - Printing 3 examples
06/12/2022 12:16:35 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/12/2022 12:16:35 - INFO - __main__ - ['entailment']
06/12/2022 12:16:35 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/12/2022 12:16:35 - INFO - __main__ - ['entailment']
06/12/2022 12:16:35 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/12/2022 12:16:35 - INFO - __main__ - ['entailment']
06/12/2022 12:16:35 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:16:35 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:16:35 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 12:16:35 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:16:35 - INFO - __main__ - Printing 3 examples
06/12/2022 12:16:35 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
06/12/2022 12:16:35 - INFO - __main__ - ['entailment']
06/12/2022 12:16:35 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
06/12/2022 12:16:35 - INFO - __main__ - ['entailment']
06/12/2022 12:16:35 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
06/12/2022 12:16:35 - INFO - __main__ - ['entailment']
06/12/2022 12:16:35 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:16:35 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:16:36 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 12:16:54 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 12:16:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 12:16:55 - INFO - __main__ - Starting training!
06/12/2022 12:16:59 - INFO - __main__ - Step 10 Global step 10 Train loss 1.08 on epoch=1
06/12/2022 12:17:02 - INFO - __main__ - Step 20 Global step 20 Train loss 0.70 on epoch=3
06/12/2022 12:17:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=4
06/12/2022 12:17:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=6
06/12/2022 12:17:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=8
06/12/2022 12:17:12 - INFO - __main__ - Global step 50 Train loss 0.71 Classification-F1 0.24845955078513218 on epoch=8
06/12/2022 12:17:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.24845955078513218 on epoch=8, global_step=50
06/12/2022 12:17:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.56 on epoch=9
06/12/2022 12:17:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.60 on epoch=11
06/12/2022 12:17:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=13
06/12/2022 12:17:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=14
06/12/2022 12:17:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=16
06/12/2022 12:17:28 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 12:17:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=18
06/12/2022 12:17:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=19
06/12/2022 12:17:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=21
06/12/2022 12:17:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=23
06/12/2022 12:17:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=24
06/12/2022 12:17:44 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 12:17:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=26
06/12/2022 12:17:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=28
06/12/2022 12:17:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=29
06/12/2022 12:17:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=31
06/12/2022 12:17:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=33
06/12/2022 12:18:01 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 12:18:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=34
06/12/2022 12:18:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=36
06/12/2022 12:18:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=38
06/12/2022 12:18:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=39
06/12/2022 12:18:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=41
06/12/2022 12:18:18 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 12:18:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.56 on epoch=43
06/12/2022 12:18:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=44
06/12/2022 12:18:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=46
06/12/2022 12:18:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=48
06/12/2022 12:18:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=49
06/12/2022 12:18:34 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=49
06/12/2022 12:18:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=51
06/12/2022 12:18:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=53
06/12/2022 12:18:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=54
06/12/2022 12:18:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=56
06/12/2022 12:18:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=58
06/12/2022 12:18:51 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=58
06/12/2022 12:18:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=59
06/12/2022 12:18:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=61
06/12/2022 12:18:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=63
06/12/2022 12:19:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=64
06/12/2022 12:19:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=66
06/12/2022 12:19:08 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 12:19:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=68
06/12/2022 12:19:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=69
06/12/2022 12:19:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=71
06/12/2022 12:19:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=73
06/12/2022 12:19:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=74
06/12/2022 12:19:25 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.2063740856844305 on epoch=74
06/12/2022 12:19:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=76
06/12/2022 12:19:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=78
06/12/2022 12:19:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=79
06/12/2022 12:19:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=81
06/12/2022 12:19:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=83
06/12/2022 12:19:41 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.1679790026246719 on epoch=83
06/12/2022 12:19:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=84
06/12/2022 12:19:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=86
06/12/2022 12:19:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=88
06/12/2022 12:19:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=89
06/12/2022 12:19:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=91
06/12/2022 12:19:58 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 12:20:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=93
06/12/2022 12:20:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=94
06/12/2022 12:20:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=96
06/12/2022 12:20:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=98
06/12/2022 12:20:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=99
06/12/2022 12:20:15 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.2851370851370851 on epoch=99
06/12/2022 12:20:15 - INFO - __main__ - Saving model with best Classification-F1: 0.24845955078513218 -> 0.2851370851370851 on epoch=99, global_step=600
06/12/2022 12:20:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=101
06/12/2022 12:20:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=103
06/12/2022 12:20:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=104
06/12/2022 12:20:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=106
06/12/2022 12:20:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=108
06/12/2022 12:20:32 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.2947871789177585 on epoch=108
06/12/2022 12:20:32 - INFO - __main__ - Saving model with best Classification-F1: 0.2851370851370851 -> 0.2947871789177585 on epoch=108, global_step=650
06/12/2022 12:20:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=109
06/12/2022 12:20:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=111
06/12/2022 12:20:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=113
06/12/2022 12:20:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=114
06/12/2022 12:20:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=116
06/12/2022 12:20:49 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 12:20:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=118
06/12/2022 12:20:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=119
06/12/2022 12:20:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=121
06/12/2022 12:21:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=123
06/12/2022 12:21:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=124
06/12/2022 12:21:06 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.1881810228266921 on epoch=124
06/12/2022 12:21:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=126
06/12/2022 12:21:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=128
06/12/2022 12:21:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=129
06/12/2022 12:21:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=131
06/12/2022 12:21:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=133
06/12/2022 12:21:23 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.2275985663082437 on epoch=133
06/12/2022 12:21:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=134
06/12/2022 12:21:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=136
06/12/2022 12:21:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=138
06/12/2022 12:21:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=139
06/12/2022 12:21:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=141
06/12/2022 12:21:39 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.1679790026246719 on epoch=141
06/12/2022 12:21:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=143
06/12/2022 12:21:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=144
06/12/2022 12:21:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=146
06/12/2022 12:21:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=148
06/12/2022 12:21:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=149
06/12/2022 12:21:56 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.25879404393336286 on epoch=149
06/12/2022 12:21:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=151
06/12/2022 12:22:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=153
06/12/2022 12:22:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=154
06/12/2022 12:22:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=156
06/12/2022 12:22:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=158
06/12/2022 12:22:12 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.1679790026246719 on epoch=158
06/12/2022 12:22:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.47 on epoch=159
06/12/2022 12:22:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=161
06/12/2022 12:22:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=163
06/12/2022 12:22:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=164
06/12/2022 12:22:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=166
06/12/2022 12:22:29 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.2085278555866791 on epoch=166
06/12/2022 12:22:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=168
06/12/2022 12:22:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=169
06/12/2022 12:22:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=171
06/12/2022 12:22:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=173
06/12/2022 12:22:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=174
06/12/2022 12:22:46 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.23453217583023292 on epoch=174
06/12/2022 12:22:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=176
06/12/2022 12:22:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=178
06/12/2022 12:22:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=179
06/12/2022 12:22:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=181
06/12/2022 12:23:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=183
06/12/2022 12:23:03 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.28706679144726704 on epoch=183
06/12/2022 12:23:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=184
06/12/2022 12:23:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=186
06/12/2022 12:23:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=188
06/12/2022 12:23:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=189
06/12/2022 12:23:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=191
06/12/2022 12:23:19 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.21047653000594177 on epoch=191
06/12/2022 12:23:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=193
06/12/2022 12:23:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=194
06/12/2022 12:23:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=196
06/12/2022 12:23:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=198
06/12/2022 12:23:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=199
06/12/2022 12:23:36 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.25779714841821955 on epoch=199
06/12/2022 12:23:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=201
06/12/2022 12:23:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=203
06/12/2022 12:23:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=204
06/12/2022 12:23:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=206
06/12/2022 12:23:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=208
06/12/2022 12:23:53 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.2592029719689294 on epoch=208
06/12/2022 12:23:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=209
06/12/2022 12:23:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=211
06/12/2022 12:24:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=213
06/12/2022 12:24:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.46 on epoch=214
06/12/2022 12:24:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=216
06/12/2022 12:24:10 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.25105386416861825 on epoch=216
06/12/2022 12:24:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=218
06/12/2022 12:24:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=219
06/12/2022 12:24:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=221
06/12/2022 12:24:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=223
06/12/2022 12:24:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=224
06/12/2022 12:24:27 - INFO - __main__ - Global step 1350 Train loss 0.40 Classification-F1 0.27391204986366385 on epoch=224
06/12/2022 12:24:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=226
06/12/2022 12:24:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=228
06/12/2022 12:24:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=229
06/12/2022 12:24:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=231
06/12/2022 12:24:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=233
06/12/2022 12:24:43 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.31575594217103653 on epoch=233
06/12/2022 12:24:43 - INFO - __main__ - Saving model with best Classification-F1: 0.2947871789177585 -> 0.31575594217103653 on epoch=233, global_step=1400
06/12/2022 12:24:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=234
06/12/2022 12:24:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=236
06/12/2022 12:24:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=238
06/12/2022 12:24:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=239
06/12/2022 12:24:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.35 on epoch=241
06/12/2022 12:25:00 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.26029482910725976 on epoch=241
06/12/2022 12:25:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=243
06/12/2022 12:25:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=244
06/12/2022 12:25:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=246
06/12/2022 12:25:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=248
06/12/2022 12:25:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=249
06/12/2022 12:25:17 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.29545259545259545 on epoch=249
06/12/2022 12:25:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=251
06/12/2022 12:25:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.32 on epoch=253
06/12/2022 12:25:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=254
06/12/2022 12:25:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=256
06/12/2022 12:25:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=258
06/12/2022 12:25:34 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.30433225921947726 on epoch=258
06/12/2022 12:25:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=259
06/12/2022 12:25:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=261
06/12/2022 12:25:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.33 on epoch=263
06/12/2022 12:25:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=264
06/12/2022 12:25:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=266
06/12/2022 12:25:51 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.2551851851851852 on epoch=266
06/12/2022 12:25:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=268
06/12/2022 12:25:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=269
06/12/2022 12:25:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=271
06/12/2022 12:26:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=273
06/12/2022 12:26:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=274
06/12/2022 12:26:07 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.35141700404858306 on epoch=274
06/12/2022 12:26:07 - INFO - __main__ - Saving model with best Classification-F1: 0.31575594217103653 -> 0.35141700404858306 on epoch=274, global_step=1650
06/12/2022 12:26:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=276
06/12/2022 12:26:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.33 on epoch=278
06/12/2022 12:26:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=279
06/12/2022 12:26:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=281
06/12/2022 12:26:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=283
06/12/2022 12:26:24 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.2643355067214039 on epoch=283
06/12/2022 12:26:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=284
06/12/2022 12:26:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=286
06/12/2022 12:26:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=288
06/12/2022 12:26:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=289
06/12/2022 12:26:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=291
06/12/2022 12:26:41 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.26238640576312283 on epoch=291
06/12/2022 12:26:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=293
06/12/2022 12:26:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=294
06/12/2022 12:26:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=296
06/12/2022 12:26:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.29 on epoch=298
06/12/2022 12:26:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=299
06/12/2022 12:26:58 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.2300315472170326 on epoch=299
06/12/2022 12:27:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=301
06/12/2022 12:27:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=303
06/12/2022 12:27:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.28 on epoch=304
06/12/2022 12:27:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=306
06/12/2022 12:27:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=308
06/12/2022 12:27:15 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.31060074064737625 on epoch=308
06/12/2022 12:27:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=309
06/12/2022 12:27:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=311
06/12/2022 12:27:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.32 on epoch=313
06/12/2022 12:27:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=314
06/12/2022 12:27:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.29 on epoch=316
06/12/2022 12:27:31 - INFO - __main__ - Global step 1900 Train loss 0.29 Classification-F1 0.2610366260851698 on epoch=316
06/12/2022 12:27:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=318
06/12/2022 12:27:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=319
06/12/2022 12:27:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=321
06/12/2022 12:27:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=323
06/12/2022 12:27:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=324
06/12/2022 12:27:48 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.31865253411306044 on epoch=324
06/12/2022 12:27:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=326
06/12/2022 12:27:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=328
06/12/2022 12:27:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.26 on epoch=329
06/12/2022 12:27:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=331
06/12/2022 12:28:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=333
06/12/2022 12:28:04 - INFO - __main__ - Global step 2000 Train loss 0.27 Classification-F1 0.2908730158730159 on epoch=333
06/12/2022 12:28:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=334
06/12/2022 12:28:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.27 on epoch=336
06/12/2022 12:28:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.25 on epoch=338
06/12/2022 12:28:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.25 on epoch=339
06/12/2022 12:28:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.31 on epoch=341
06/12/2022 12:28:21 - INFO - __main__ - Global step 2050 Train loss 0.27 Classification-F1 0.31440185830429734 on epoch=341
06/12/2022 12:28:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.27 on epoch=343
06/12/2022 12:28:27 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.24 on epoch=344
06/12/2022 12:28:29 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.22 on epoch=346
06/12/2022 12:28:32 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.24 on epoch=348
06/12/2022 12:28:35 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.30 on epoch=349
06/12/2022 12:28:38 - INFO - __main__ - Global step 2100 Train loss 0.26 Classification-F1 0.30550775832506366 on epoch=349
06/12/2022 12:28:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.27 on epoch=351
06/12/2022 12:28:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=353
06/12/2022 12:28:46 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.26 on epoch=354
06/12/2022 12:28:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.27 on epoch=356
06/12/2022 12:28:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.20 on epoch=358
06/12/2022 12:28:54 - INFO - __main__ - Global step 2150 Train loss 0.25 Classification-F1 0.2971902937420179 on epoch=358
06/12/2022 12:28:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.24 on epoch=359
06/12/2022 12:29:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=361
06/12/2022 12:29:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=363
06/12/2022 12:29:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.23 on epoch=364
06/12/2022 12:29:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=366
06/12/2022 12:29:11 - INFO - __main__ - Global step 2200 Train loss 0.22 Classification-F1 0.3570698625046451 on epoch=366
06/12/2022 12:29:11 - INFO - __main__ - Saving model with best Classification-F1: 0.35141700404858306 -> 0.3570698625046451 on epoch=366, global_step=2200
06/12/2022 12:29:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=368
06/12/2022 12:29:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=369
06/12/2022 12:29:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=371
06/12/2022 12:29:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=373
06/12/2022 12:29:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=374
06/12/2022 12:29:28 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.2925648601189081 on epoch=374
06/12/2022 12:29:30 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=376
06/12/2022 12:29:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=378
06/12/2022 12:29:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.19 on epoch=379
06/12/2022 12:29:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=381
06/12/2022 12:29:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=383
06/12/2022 12:29:44 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.33693967940543285 on epoch=383
06/12/2022 12:29:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=384
06/12/2022 12:29:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=386
06/12/2022 12:29:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.21 on epoch=388
06/12/2022 12:29:55 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=389
06/12/2022 12:29:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=391
06/12/2022 12:30:01 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.30397022332506207 on epoch=391
06/12/2022 12:30:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.20 on epoch=393
06/12/2022 12:30:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=394
06/12/2022 12:30:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=396
06/12/2022 12:30:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.17 on epoch=398
06/12/2022 12:30:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=399
06/12/2022 12:30:18 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.34242749324101013 on epoch=399
06/12/2022 12:30:20 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=401
06/12/2022 12:30:23 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.20 on epoch=403
06/12/2022 12:30:26 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=404
06/12/2022 12:30:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.21 on epoch=406
06/12/2022 12:30:31 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=408
06/12/2022 12:30:34 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.24791182009526977 on epoch=408
06/12/2022 12:30:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.19 on epoch=409
06/12/2022 12:30:40 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=411
06/12/2022 12:30:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=413
06/12/2022 12:30:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.17 on epoch=414
06/12/2022 12:30:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=416
06/12/2022 12:30:51 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.30454961884842097 on epoch=416
06/12/2022 12:30:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=418
06/12/2022 12:30:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.17 on epoch=419
06/12/2022 12:31:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=421
06/12/2022 12:31:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=423
06/12/2022 12:31:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=424
06/12/2022 12:31:08 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.3159011323262531 on epoch=424
06/12/2022 12:31:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.10 on epoch=426
06/12/2022 12:31:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=428
06/12/2022 12:31:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.13 on epoch=429
06/12/2022 12:31:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=431
06/12/2022 12:31:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.08 on epoch=433
06/12/2022 12:31:25 - INFO - __main__ - Global step 2600 Train loss 0.12 Classification-F1 0.2337284666552959 on epoch=433
06/12/2022 12:31:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=434
06/12/2022 12:31:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.15 on epoch=436
06/12/2022 12:31:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=438
06/12/2022 12:31:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=439
06/12/2022 12:31:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=441
06/12/2022 12:31:42 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.30450918590453474 on epoch=441
06/12/2022 12:31:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=443
06/12/2022 12:31:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=444
06/12/2022 12:31:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=446
06/12/2022 12:31:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=448
06/12/2022 12:31:56 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=449
06/12/2022 12:31:59 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.2802840434419382 on epoch=449
06/12/2022 12:32:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=451
06/12/2022 12:32:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=453
06/12/2022 12:32:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=454
06/12/2022 12:32:10 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=456
06/12/2022 12:32:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.17 on epoch=458
06/12/2022 12:32:16 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.25294573643410856 on epoch=458
06/12/2022 12:32:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=459
06/12/2022 12:32:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=461
06/12/2022 12:32:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=463
06/12/2022 12:32:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=464
06/12/2022 12:32:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=466
06/12/2022 12:32:33 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.28176268548939354 on epoch=466
06/12/2022 12:32:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=468
06/12/2022 12:32:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=469
06/12/2022 12:32:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.14 on epoch=471
06/12/2022 12:32:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=473
06/12/2022 12:32:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=474
06/12/2022 12:32:49 - INFO - __main__ - Global step 2850 Train loss 0.10 Classification-F1 0.3034151034151034 on epoch=474
06/12/2022 12:32:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=476
06/12/2022 12:32:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=478
06/12/2022 12:32:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.14 on epoch=479
06/12/2022 12:33:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=481
06/12/2022 12:33:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=483
06/12/2022 12:33:06 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.2920677540930705 on epoch=483
06/12/2022 12:33:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=484
06/12/2022 12:33:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=486
06/12/2022 12:33:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=488
06/12/2022 12:33:17 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=489
06/12/2022 12:33:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=491
06/12/2022 12:33:22 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.2478614351360905 on epoch=491
06/12/2022 12:33:25 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=493
06/12/2022 12:33:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=494
06/12/2022 12:33:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=496
06/12/2022 12:33:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=498
06/12/2022 12:33:36 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=499
06/12/2022 12:33:37 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:33:37 - INFO - __main__ - Printing 3 examples
06/12/2022 12:33:37 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/12/2022 12:33:37 - INFO - __main__ - ['neutral']
06/12/2022 12:33:37 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/12/2022 12:33:37 - INFO - __main__ - ['neutral']
06/12/2022 12:33:37 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/12/2022 12:33:37 - INFO - __main__ - ['neutral']
06/12/2022 12:33:37 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:33:38 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:33:38 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 12:33:38 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:33:38 - INFO - __main__ - Printing 3 examples
06/12/2022 12:33:38 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
06/12/2022 12:33:38 - INFO - __main__ - ['neutral']
06/12/2022 12:33:38 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
06/12/2022 12:33:38 - INFO - __main__ - ['neutral']
06/12/2022 12:33:38 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
06/12/2022 12:33:38 - INFO - __main__ - ['neutral']
06/12/2022 12:33:38 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:33:38 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:33:38 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 12:33:39 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.23303795202291444 on epoch=499
06/12/2022 12:33:39 - INFO - __main__ - save last model!
06/12/2022 12:33:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 12:33:39 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 12:33:39 - INFO - __main__ - Printing 3 examples
06/12/2022 12:33:39 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 12:33:39 - INFO - __main__ - ['contradiction']
06/12/2022 12:33:39 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 12:33:39 - INFO - __main__ - ['entailment']
06/12/2022 12:33:39 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 12:33:39 - INFO - __main__ - ['contradiction']
06/12/2022 12:33:39 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:33:40 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:33:41 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 12:33:57 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 12:33:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 12:33:58 - INFO - __main__ - Starting training!
06/12/2022 12:34:11 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_21_0.2_8_predictions.txt
06/12/2022 12:34:11 - INFO - __main__ - Classification-F1 on test data: 0.1591
06/12/2022 12:34:12 - INFO - __main__ - prefix=anli_32_21, lr=0.2, bsz=8, dev_performance=0.3570698625046451, test_performance=0.15907747494400803
06/12/2022 12:34:12 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.5, bsz=8 ...
06/12/2022 12:34:13 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:34:13 - INFO - __main__ - Printing 3 examples
06/12/2022 12:34:13 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/12/2022 12:34:13 - INFO - __main__ - ['neutral']
06/12/2022 12:34:13 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/12/2022 12:34:13 - INFO - __main__ - ['neutral']
06/12/2022 12:34:13 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/12/2022 12:34:13 - INFO - __main__ - ['neutral']
06/12/2022 12:34:13 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:34:13 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:34:13 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 12:34:13 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:34:13 - INFO - __main__ - Printing 3 examples
06/12/2022 12:34:13 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
06/12/2022 12:34:13 - INFO - __main__ - ['neutral']
06/12/2022 12:34:13 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
06/12/2022 12:34:13 - INFO - __main__ - ['neutral']
06/12/2022 12:34:13 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
06/12/2022 12:34:13 - INFO - __main__ - ['neutral']
06/12/2022 12:34:13 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:34:13 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:34:13 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 12:34:32 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 12:34:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 12:34:32 - INFO - __main__ - Starting training!
06/12/2022 12:34:36 - INFO - __main__ - Step 10 Global step 10 Train loss 0.99 on epoch=1
06/12/2022 12:34:39 - INFO - __main__ - Step 20 Global step 20 Train loss 0.65 on epoch=3
06/12/2022 12:34:41 - INFO - __main__ - Step 30 Global step 30 Train loss 0.58 on epoch=4
06/12/2022 12:34:44 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=6
06/12/2022 12:34:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.63 on epoch=8
06/12/2022 12:34:49 - INFO - __main__ - Global step 50 Train loss 0.69 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 12:34:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 12:34:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=9
06/12/2022 12:34:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=11
06/12/2022 12:34:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=13
06/12/2022 12:35:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=14
06/12/2022 12:35:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=16
06/12/2022 12:35:06 - INFO - __main__ - Global step 100 Train loss 0.56 Classification-F1 0.29264069264069265 on epoch=16
06/12/2022 12:35:06 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.29264069264069265 on epoch=16, global_step=100
06/12/2022 12:35:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.63 on epoch=18
06/12/2022 12:35:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=19
06/12/2022 12:35:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.60 on epoch=21
06/12/2022 12:35:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=23
06/12/2022 12:35:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=24
06/12/2022 12:35:22 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 12:35:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=26
06/12/2022 12:35:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=28
06/12/2022 12:35:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=29
06/12/2022 12:35:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=31
06/12/2022 12:35:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=33
06/12/2022 12:35:39 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 12:35:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=34
06/12/2022 12:35:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=36
06/12/2022 12:35:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=38
06/12/2022 12:35:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.54 on epoch=39
06/12/2022 12:35:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.62 on epoch=41
06/12/2022 12:35:56 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.16272965879265092 on epoch=41
06/12/2022 12:35:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.63 on epoch=43
06/12/2022 12:36:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.54 on epoch=44
06/12/2022 12:36:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=46
06/12/2022 12:36:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=48
06/12/2022 12:36:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=49
06/12/2022 12:36:13 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.16402116402116398 on epoch=49
06/12/2022 12:36:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=51
06/12/2022 12:36:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=53
06/12/2022 12:36:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=54
06/12/2022 12:36:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=56
06/12/2022 12:36:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=58
06/12/2022 12:36:29 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=58
06/12/2022 12:36:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=59
06/12/2022 12:36:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=61
06/12/2022 12:36:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=63
06/12/2022 12:36:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=64
06/12/2022 12:36:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=66
06/12/2022 12:36:46 - INFO - __main__ - Global step 400 Train loss 0.52 Classification-F1 0.20004099200655875 on epoch=66
06/12/2022 12:36:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=68
06/12/2022 12:36:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=69
06/12/2022 12:36:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=71
06/12/2022 12:36:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=73
06/12/2022 12:37:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=74
06/12/2022 12:37:03 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.18738611649914771 on epoch=74
06/12/2022 12:37:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=76
06/12/2022 12:37:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=78
06/12/2022 12:37:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=79
06/12/2022 12:37:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=81
06/12/2022 12:37:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=83
06/12/2022 12:37:20 - INFO - __main__ - Global step 500 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=83
06/12/2022 12:37:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=84
06/12/2022 12:37:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=86
06/12/2022 12:37:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=88
06/12/2022 12:37:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=89
06/12/2022 12:37:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=91
06/12/2022 12:37:37 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.24231992906691704 on epoch=91
06/12/2022 12:37:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.56 on epoch=93
06/12/2022 12:37:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.54 on epoch=94
06/12/2022 12:37:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=96
06/12/2022 12:37:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=98
06/12/2022 12:37:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=99
06/12/2022 12:37:54 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=99
06/12/2022 12:37:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=101
06/12/2022 12:37:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=103
06/12/2022 12:38:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=104
06/12/2022 12:38:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=106
06/12/2022 12:38:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=108
06/12/2022 12:38:10 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.2697127780775919 on epoch=108
06/12/2022 12:38:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=109
06/12/2022 12:38:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.51 on epoch=111
06/12/2022 12:38:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=113
06/12/2022 12:38:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.49 on epoch=114
06/12/2022 12:38:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=116
06/12/2022 12:38:27 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.17257634761221563 on epoch=116
06/12/2022 12:38:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=118
06/12/2022 12:38:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=119
06/12/2022 12:38:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=121
06/12/2022 12:38:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=123
06/12/2022 12:38:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=124
06/12/2022 12:38:44 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.22207573427085622 on epoch=124
06/12/2022 12:38:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.50 on epoch=126
06/12/2022 12:38:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=128
06/12/2022 12:38:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=129
06/12/2022 12:38:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.53 on epoch=131
06/12/2022 12:38:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=133
06/12/2022 12:39:01 - INFO - __main__ - Global step 800 Train loss 0.50 Classification-F1 0.2697878253433809 on epoch=133
06/12/2022 12:39:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=134
06/12/2022 12:39:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=136
06/12/2022 12:39:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=138
06/12/2022 12:39:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=139
06/12/2022 12:39:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=141
06/12/2022 12:39:17 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=141
06/12/2022 12:39:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=143
06/12/2022 12:39:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.52 on epoch=144
06/12/2022 12:39:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=146
06/12/2022 12:39:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.53 on epoch=148
06/12/2022 12:39:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=149
06/12/2022 12:39:34 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=149
06/12/2022 12:39:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=151
06/12/2022 12:39:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=153
06/12/2022 12:39:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=154
06/12/2022 12:39:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=156
06/12/2022 12:39:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=158
06/12/2022 12:39:50 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.21445221445221443 on epoch=158
06/12/2022 12:39:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=159
06/12/2022 12:39:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=161
06/12/2022 12:39:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=163
06/12/2022 12:40:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=164
06/12/2022 12:40:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=166
06/12/2022 12:40:07 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.2457912457912458 on epoch=166
06/12/2022 12:40:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.52 on epoch=168
06/12/2022 12:40:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=169
06/12/2022 12:40:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=171
06/12/2022 12:40:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=173
06/12/2022 12:40:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=174
06/12/2022 12:40:24 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=174
06/12/2022 12:40:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.47 on epoch=176
06/12/2022 12:40:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=178
06/12/2022 12:40:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.45 on epoch=179
06/12/2022 12:40:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=181
06/12/2022 12:40:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=183
06/12/2022 12:40:41 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.16129032258064516 on epoch=183
06/12/2022 12:40:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=184
06/12/2022 12:40:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=186
06/12/2022 12:40:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.47 on epoch=188
06/12/2022 12:40:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=189
06/12/2022 12:40:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=191
06/12/2022 12:40:57 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.22453102453102455 on epoch=191
06/12/2022 12:41:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=193
06/12/2022 12:41:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=194
06/12/2022 12:41:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=196
06/12/2022 12:41:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=198
06/12/2022 12:41:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.47 on epoch=199
06/12/2022 12:41:14 - INFO - __main__ - Global step 1200 Train loss 0.47 Classification-F1 0.2898148148148148 on epoch=199
06/12/2022 12:41:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.53 on epoch=201
06/12/2022 12:41:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=203
06/12/2022 12:41:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.50 on epoch=204
06/12/2022 12:41:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=206
06/12/2022 12:41:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.61 on epoch=208
06/12/2022 12:41:31 - INFO - __main__ - Global step 1250 Train loss 0.51 Classification-F1 0.2699800306824707 on epoch=208
06/12/2022 12:41:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=209
06/12/2022 12:41:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=211
06/12/2022 12:41:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=213
06/12/2022 12:41:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=214
06/12/2022 12:41:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=216
06/12/2022 12:41:48 - INFO - __main__ - Global step 1300 Train loss 0.46 Classification-F1 0.24356898722791867 on epoch=216
06/12/2022 12:41:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=218
06/12/2022 12:41:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.49 on epoch=219
06/12/2022 12:41:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=221
06/12/2022 12:41:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=223
06/12/2022 12:42:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=224
06/12/2022 12:42:04 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.18414611184334398 on epoch=224
06/12/2022 12:42:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.49 on epoch=226
06/12/2022 12:42:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.49 on epoch=228
06/12/2022 12:42:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.46 on epoch=229
06/12/2022 12:42:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.52 on epoch=231
06/12/2022 12:42:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.51 on epoch=233
06/12/2022 12:42:21 - INFO - __main__ - Global step 1400 Train loss 0.49 Classification-F1 0.19740547548034182 on epoch=233
06/12/2022 12:42:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=234
06/12/2022 12:42:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=236
06/12/2022 12:42:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=238
06/12/2022 12:42:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=239
06/12/2022 12:42:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=241
06/12/2022 12:42:38 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.18938307030129123 on epoch=241
06/12/2022 12:42:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=243
06/12/2022 12:42:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=244
06/12/2022 12:42:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.50 on epoch=246
06/12/2022 12:42:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.47 on epoch=248
06/12/2022 12:42:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=249
06/12/2022 12:42:54 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.2009661835748792 on epoch=249
06/12/2022 12:42:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.52 on epoch=251
06/12/2022 12:43:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=253
06/12/2022 12:43:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=254
06/12/2022 12:43:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=256
06/12/2022 12:43:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=258
06/12/2022 12:43:11 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.24675324675324672 on epoch=258
06/12/2022 12:43:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=259
06/12/2022 12:43:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.48 on epoch=261
06/12/2022 12:43:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=263
06/12/2022 12:43:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=264
06/12/2022 12:43:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.48 on epoch=266
06/12/2022 12:43:28 - INFO - __main__ - Global step 1600 Train loss 0.45 Classification-F1 0.19004581685809327 on epoch=266
06/12/2022 12:43:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=268
06/12/2022 12:43:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=269
06/12/2022 12:43:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.45 on epoch=271
06/12/2022 12:43:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=273
06/12/2022 12:43:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=274
06/12/2022 12:43:45 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.19516908212560388 on epoch=274
06/12/2022 12:43:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=276
06/12/2022 12:43:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=278
06/12/2022 12:43:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.49 on epoch=279
06/12/2022 12:43:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=281
06/12/2022 12:43:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.48 on epoch=283
06/12/2022 12:44:01 - INFO - __main__ - Global step 1700 Train loss 0.46 Classification-F1 0.2722909244648375 on epoch=283
06/12/2022 12:44:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=284
06/12/2022 12:44:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.52 on epoch=286
06/12/2022 12:44:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.45 on epoch=288
06/12/2022 12:44:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=289
06/12/2022 12:44:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=291
06/12/2022 12:44:18 - INFO - __main__ - Global step 1750 Train loss 0.46 Classification-F1 0.2019762845849802 on epoch=291
06/12/2022 12:44:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=293
06/12/2022 12:44:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.47 on epoch=294
06/12/2022 12:44:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=296
06/12/2022 12:44:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.50 on epoch=298
06/12/2022 12:44:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=299
06/12/2022 12:44:34 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.2650163056501943 on epoch=299
06/12/2022 12:44:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=301
06/12/2022 12:44:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=303
06/12/2022 12:44:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=304
06/12/2022 12:44:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.45 on epoch=306
06/12/2022 12:44:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.45 on epoch=308
06/12/2022 12:44:51 - INFO - __main__ - Global step 1850 Train loss 0.44 Classification-F1 0.21794871794871795 on epoch=308
06/12/2022 12:44:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=309
06/12/2022 12:44:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=311
06/12/2022 12:44:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.47 on epoch=313
06/12/2022 12:45:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.46 on epoch=314
06/12/2022 12:45:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=316
06/12/2022 12:45:07 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.25028706518068217 on epoch=316
06/12/2022 12:45:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=318
06/12/2022 12:45:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=319
06/12/2022 12:45:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.49 on epoch=321
06/12/2022 12:45:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=323
06/12/2022 12:45:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.47 on epoch=324
06/12/2022 12:45:24 - INFO - __main__ - Global step 1950 Train loss 0.44 Classification-F1 0.2506493506493507 on epoch=324
06/12/2022 12:45:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=326
06/12/2022 12:45:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=328
06/12/2022 12:45:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=329
06/12/2022 12:45:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.48 on epoch=331
06/12/2022 12:45:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=333
06/12/2022 12:45:40 - INFO - __main__ - Global step 2000 Train loss 0.45 Classification-F1 0.2638888888888889 on epoch=333
06/12/2022 12:45:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.43 on epoch=334
06/12/2022 12:45:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.41 on epoch=336
06/12/2022 12:45:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.46 on epoch=338
06/12/2022 12:45:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.39 on epoch=339
06/12/2022 12:45:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.44 on epoch=341
06/12/2022 12:45:57 - INFO - __main__ - Global step 2050 Train loss 0.43 Classification-F1 0.2778185375397408 on epoch=341
06/12/2022 12:45:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.46 on epoch=343
06/12/2022 12:46:02 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.38 on epoch=344
06/12/2022 12:46:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.46 on epoch=346
06/12/2022 12:46:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.49 on epoch=348
06/12/2022 12:46:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.47 on epoch=349
06/12/2022 12:46:13 - INFO - __main__ - Global step 2100 Train loss 0.45 Classification-F1 0.24064298083905925 on epoch=349
06/12/2022 12:46:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.45 on epoch=351
06/12/2022 12:46:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.44 on epoch=353
06/12/2022 12:46:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=354
06/12/2022 12:46:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.45 on epoch=356
06/12/2022 12:46:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.43 on epoch=358
06/12/2022 12:46:30 - INFO - __main__ - Global step 2150 Train loss 0.44 Classification-F1 0.23192250097498968 on epoch=358
06/12/2022 12:46:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=359
06/12/2022 12:46:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.43 on epoch=361
06/12/2022 12:46:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.42 on epoch=363
06/12/2022 12:46:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.43 on epoch=364
06/12/2022 12:46:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.43 on epoch=366
06/12/2022 12:46:46 - INFO - __main__ - Global step 2200 Train loss 0.43 Classification-F1 0.2860915730185679 on epoch=366
06/12/2022 12:46:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.42 on epoch=368
06/12/2022 12:46:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.45 on epoch=369
06/12/2022 12:46:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.45 on epoch=371
06/12/2022 12:46:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=373
06/12/2022 12:47:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.40 on epoch=374
06/12/2022 12:47:03 - INFO - __main__ - Global step 2250 Train loss 0.43 Classification-F1 0.22790816587411156 on epoch=374
06/12/2022 12:47:05 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.42 on epoch=376
06/12/2022 12:47:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.44 on epoch=378
06/12/2022 12:47:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.39 on epoch=379
06/12/2022 12:47:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.44 on epoch=381
06/12/2022 12:47:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.42 on epoch=383
06/12/2022 12:47:19 - INFO - __main__ - Global step 2300 Train loss 0.42 Classification-F1 0.25770519643967305 on epoch=383
06/12/2022 12:47:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.37 on epoch=384
06/12/2022 12:47:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.41 on epoch=386
06/12/2022 12:47:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.40 on epoch=388
06/12/2022 12:47:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.45 on epoch=389
06/12/2022 12:47:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.44 on epoch=391
06/12/2022 12:47:36 - INFO - __main__ - Global step 2350 Train loss 0.42 Classification-F1 0.23743285621138263 on epoch=391
06/12/2022 12:47:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.40 on epoch=393
06/12/2022 12:47:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.38 on epoch=394
06/12/2022 12:47:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.48 on epoch=396
06/12/2022 12:47:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.41 on epoch=398
06/12/2022 12:47:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.35 on epoch=399
06/12/2022 12:47:52 - INFO - __main__ - Global step 2400 Train loss 0.40 Classification-F1 0.233045770428948 on epoch=399
06/12/2022 12:47:55 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=401
06/12/2022 12:47:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=403
06/12/2022 12:48:00 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.41 on epoch=404
06/12/2022 12:48:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.37 on epoch=406
06/12/2022 12:48:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.43 on epoch=408
06/12/2022 12:48:09 - INFO - __main__ - Global step 2450 Train loss 0.40 Classification-F1 0.18541033434650456 on epoch=408
06/12/2022 12:48:11 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.38 on epoch=409
06/12/2022 12:48:14 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.40 on epoch=411
06/12/2022 12:48:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.41 on epoch=413
06/12/2022 12:48:19 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.37 on epoch=414
06/12/2022 12:48:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.39 on epoch=416
06/12/2022 12:48:25 - INFO - __main__ - Global step 2500 Train loss 0.39 Classification-F1 0.23643892339544514 on epoch=416
06/12/2022 12:48:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.40 on epoch=418
06/12/2022 12:48:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=419
06/12/2022 12:48:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.39 on epoch=421
06/12/2022 12:48:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.40 on epoch=423
06/12/2022 12:48:39 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.41 on epoch=424
06/12/2022 12:48:42 - INFO - __main__ - Global step 2550 Train loss 0.41 Classification-F1 0.25016523463317913 on epoch=424
06/12/2022 12:48:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.43 on epoch=426
06/12/2022 12:48:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.42 on epoch=428
06/12/2022 12:48:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.42 on epoch=429
06/12/2022 12:48:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.45 on epoch=431
06/12/2022 12:48:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.41 on epoch=433
06/12/2022 12:48:58 - INFO - __main__ - Global step 2600 Train loss 0.43 Classification-F1 0.2431090566683787 on epoch=433
06/12/2022 12:49:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.36 on epoch=434
06/12/2022 12:49:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.41 on epoch=436
06/12/2022 12:49:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.41 on epoch=438
06/12/2022 12:49:09 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.43 on epoch=439
06/12/2022 12:49:12 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.40 on epoch=441
06/12/2022 12:49:15 - INFO - __main__ - Global step 2650 Train loss 0.40 Classification-F1 0.20680489101541732 on epoch=441
06/12/2022 12:49:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.37 on epoch=443
06/12/2022 12:49:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.38 on epoch=444
06/12/2022 12:49:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.43 on epoch=446
06/12/2022 12:49:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.41 on epoch=448
06/12/2022 12:49:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.35 on epoch=449
06/12/2022 12:49:31 - INFO - __main__ - Global step 2700 Train loss 0.39 Classification-F1 0.28650479954827784 on epoch=449
06/12/2022 12:49:34 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.42 on epoch=451
06/12/2022 12:49:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.37 on epoch=453
06/12/2022 12:49:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.38 on epoch=454
06/12/2022 12:49:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.39 on epoch=456
06/12/2022 12:49:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.41 on epoch=458
06/12/2022 12:49:48 - INFO - __main__ - Global step 2750 Train loss 0.39 Classification-F1 0.2894873612095441 on epoch=458
06/12/2022 12:49:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.40 on epoch=459
06/12/2022 12:49:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.39 on epoch=461
06/12/2022 12:49:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.35 on epoch=463
06/12/2022 12:49:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.43 on epoch=464
06/12/2022 12:50:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.38 on epoch=466
06/12/2022 12:50:04 - INFO - __main__ - Global step 2800 Train loss 0.39 Classification-F1 0.3062451172653497 on epoch=466
06/12/2022 12:50:04 - INFO - __main__ - Saving model with best Classification-F1: 0.29264069264069265 -> 0.3062451172653497 on epoch=466, global_step=2800
06/12/2022 12:50:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.39 on epoch=468
06/12/2022 12:50:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.36 on epoch=469
06/12/2022 12:50:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.35 on epoch=471
06/12/2022 12:50:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.40 on epoch=473
06/12/2022 12:50:18 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.40 on epoch=474
06/12/2022 12:50:21 - INFO - __main__ - Global step 2850 Train loss 0.38 Classification-F1 0.2800544172358265 on epoch=474
06/12/2022 12:50:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.36 on epoch=476
06/12/2022 12:50:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.33 on epoch=478
06/12/2022 12:50:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.34 on epoch=479
06/12/2022 12:50:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.34 on epoch=481
06/12/2022 12:50:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.41 on epoch=483
06/12/2022 12:50:37 - INFO - __main__ - Global step 2900 Train loss 0.36 Classification-F1 0.3370890905137481 on epoch=483
06/12/2022 12:50:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3062451172653497 -> 0.3370890905137481 on epoch=483, global_step=2900
06/12/2022 12:50:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.33 on epoch=484
06/12/2022 12:50:43 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.38 on epoch=486
06/12/2022 12:50:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.34 on epoch=488
06/12/2022 12:50:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.31 on epoch=489
06/12/2022 12:50:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.36 on epoch=491
06/12/2022 12:50:54 - INFO - __main__ - Global step 2950 Train loss 0.35 Classification-F1 0.3448949707387509 on epoch=491
06/12/2022 12:50:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3370890905137481 -> 0.3448949707387509 on epoch=491, global_step=2950
06/12/2022 12:50:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.37 on epoch=493
06/12/2022 12:50:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.33 on epoch=494
06/12/2022 12:51:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.28 on epoch=496
06/12/2022 12:51:05 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.35 on epoch=498
06/12/2022 12:51:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.31 on epoch=499
06/12/2022 12:51:09 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:51:09 - INFO - __main__ - Printing 3 examples
06/12/2022 12:51:09 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/12/2022 12:51:09 - INFO - __main__ - ['neutral']
06/12/2022 12:51:09 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/12/2022 12:51:09 - INFO - __main__ - ['neutral']
06/12/2022 12:51:09 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/12/2022 12:51:09 - INFO - __main__ - ['neutral']
06/12/2022 12:51:09 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:51:09 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:51:09 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 12:51:09 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:51:09 - INFO - __main__ - Printing 3 examples
06/12/2022 12:51:09 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
06/12/2022 12:51:09 - INFO - __main__ - ['neutral']
06/12/2022 12:51:09 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
06/12/2022 12:51:09 - INFO - __main__ - ['neutral']
06/12/2022 12:51:09 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
06/12/2022 12:51:09 - INFO - __main__ - ['neutral']
06/12/2022 12:51:09 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:51:09 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:51:09 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 12:51:10 - INFO - __main__ - Global step 3000 Train loss 0.33 Classification-F1 0.2504513949560254 on epoch=499
06/12/2022 12:51:10 - INFO - __main__ - save last model!
06/12/2022 12:51:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 12:51:10 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 12:51:10 - INFO - __main__ - Printing 3 examples
06/12/2022 12:51:10 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 12:51:10 - INFO - __main__ - ['contradiction']
06/12/2022 12:51:10 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 12:51:10 - INFO - __main__ - ['entailment']
06/12/2022 12:51:10 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 12:51:10 - INFO - __main__ - ['contradiction']
06/12/2022 12:51:10 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:51:11 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:51:12 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 12:51:28 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 12:51:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 12:51:29 - INFO - __main__ - Starting training!
06/12/2022 12:51:43 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_42_0.5_8_predictions.txt
06/12/2022 12:51:43 - INFO - __main__ - Classification-F1 on test data: 0.2733
06/12/2022 12:51:43 - INFO - __main__ - prefix=anli_32_42, lr=0.5, bsz=8, dev_performance=0.3448949707387509, test_performance=0.27325231582302717
06/12/2022 12:51:43 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.4, bsz=8 ...
06/12/2022 12:51:44 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:51:44 - INFO - __main__ - Printing 3 examples
06/12/2022 12:51:44 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/12/2022 12:51:44 - INFO - __main__ - ['neutral']
06/12/2022 12:51:44 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/12/2022 12:51:44 - INFO - __main__ - ['neutral']
06/12/2022 12:51:44 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/12/2022 12:51:44 - INFO - __main__ - ['neutral']
06/12/2022 12:51:44 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:51:44 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:51:44 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 12:51:44 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 12:51:44 - INFO - __main__ - Printing 3 examples
06/12/2022 12:51:44 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
06/12/2022 12:51:44 - INFO - __main__ - ['neutral']
06/12/2022 12:51:44 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
06/12/2022 12:51:44 - INFO - __main__ - ['neutral']
06/12/2022 12:51:44 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
06/12/2022 12:51:44 - INFO - __main__ - ['neutral']
06/12/2022 12:51:44 - INFO - __main__ - Tokenizing Input ...
06/12/2022 12:51:45 - INFO - __main__ - Tokenizing Output ...
06/12/2022 12:51:45 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 12:52:03 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 12:52:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 12:52:04 - INFO - __main__ - Starting training!
06/12/2022 12:52:08 - INFO - __main__ - Step 10 Global step 10 Train loss 2.89 on epoch=1
06/12/2022 12:52:10 - INFO - __main__ - Step 20 Global step 20 Train loss 4.60 on epoch=3
06/12/2022 12:52:13 - INFO - __main__ - Step 30 Global step 30 Train loss 6.01 on epoch=4
06/12/2022 12:52:16 - INFO - __main__ - Step 40 Global step 40 Train loss 6.15 on epoch=6
06/12/2022 12:52:19 - INFO - __main__ - Step 50 Global step 50 Train loss 5.25 on epoch=8
06/12/2022 12:52:21 - INFO - __main__ - Global step 50 Train loss 4.98 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 12:52:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 12:52:24 - INFO - __main__ - Step 60 Global step 60 Train loss 3.18 on epoch=9
06/12/2022 12:52:26 - INFO - __main__ - Step 70 Global step 70 Train loss 2.74 on epoch=11
06/12/2022 12:52:29 - INFO - __main__ - Step 80 Global step 80 Train loss 2.37 on epoch=13
06/12/2022 12:52:32 - INFO - __main__ - Step 90 Global step 90 Train loss 1.58 on epoch=14
06/12/2022 12:52:35 - INFO - __main__ - Step 100 Global step 100 Train loss 1.51 on epoch=16
06/12/2022 12:52:37 - INFO - __main__ - Global step 100 Train loss 2.27 Classification-F1 0.19999999999999998 on epoch=16
06/12/2022 12:52:37 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.19999999999999998 on epoch=16, global_step=100
06/12/2022 12:52:39 - INFO - __main__ - Step 110 Global step 110 Train loss 1.39 on epoch=18
06/12/2022 12:52:42 - INFO - __main__ - Step 120 Global step 120 Train loss 1.23 on epoch=19
06/12/2022 12:52:45 - INFO - __main__ - Step 130 Global step 130 Train loss 1.35 on epoch=21
06/12/2022 12:52:48 - INFO - __main__ - Step 140 Global step 140 Train loss 1.25 on epoch=23
06/12/2022 12:52:50 - INFO - __main__ - Step 150 Global step 150 Train loss 1.02 on epoch=24
06/12/2022 12:52:52 - INFO - __main__ - Global step 150 Train loss 1.25 Classification-F1 0.2714101918666443 on epoch=24
06/12/2022 12:52:52 - INFO - __main__ - Saving model with best Classification-F1: 0.19999999999999998 -> 0.2714101918666443 on epoch=24, global_step=150
06/12/2022 12:52:55 - INFO - __main__ - Step 160 Global step 160 Train loss 1.13 on epoch=26
06/12/2022 12:52:58 - INFO - __main__ - Step 170 Global step 170 Train loss 1.26 on epoch=28
06/12/2022 12:53:01 - INFO - __main__ - Step 180 Global step 180 Train loss 1.18 on epoch=29
06/12/2022 12:53:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.99 on epoch=31
06/12/2022 12:53:06 - INFO - __main__ - Step 200 Global step 200 Train loss 1.43 on epoch=33
06/12/2022 12:53:08 - INFO - __main__ - Global step 200 Train loss 1.20 Classification-F1 0.1881810228266921 on epoch=33
06/12/2022 12:53:11 - INFO - __main__ - Step 210 Global step 210 Train loss 1.08 on epoch=34
06/12/2022 12:53:14 - INFO - __main__ - Step 220 Global step 220 Train loss 1.05 on epoch=36
06/12/2022 12:53:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=38
06/12/2022 12:53:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.97 on epoch=39
06/12/2022 12:53:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.90 on epoch=41
06/12/2022 12:53:24 - INFO - __main__ - Global step 250 Train loss 0.99 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 12:53:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.98 on epoch=43
06/12/2022 12:53:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.84 on epoch=44
06/12/2022 12:53:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.87 on epoch=46
06/12/2022 12:53:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=48
06/12/2022 12:53:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.83 on epoch=49
06/12/2022 12:53:40 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.16666666666666666 on epoch=49
06/12/2022 12:53:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.81 on epoch=51
06/12/2022 12:53:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.89 on epoch=53
06/12/2022 12:53:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=54
06/12/2022 12:53:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.66 on epoch=56
06/12/2022 12:53:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.81 on epoch=58
06/12/2022 12:53:56 - INFO - __main__ - Global step 350 Train loss 0.80 Classification-F1 0.16666666666666666 on epoch=58
06/12/2022 12:53:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.64 on epoch=59
06/12/2022 12:54:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=61
06/12/2022 12:54:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.75 on epoch=63
06/12/2022 12:54:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.68 on epoch=64
06/12/2022 12:54:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.70 on epoch=66
06/12/2022 12:54:12 - INFO - __main__ - Global step 400 Train loss 0.70 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 12:54:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.71 on epoch=68
06/12/2022 12:54:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.61 on epoch=69
06/12/2022 12:54:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.71 on epoch=71
06/12/2022 12:54:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.73 on epoch=73
06/12/2022 12:54:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.61 on epoch=74
06/12/2022 12:54:29 - INFO - __main__ - Global step 450 Train loss 0.67 Classification-F1 0.16666666666666666 on epoch=74
06/12/2022 12:54:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.70 on epoch=76
06/12/2022 12:54:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.71 on epoch=78
06/12/2022 12:54:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.64 on epoch=79
06/12/2022 12:54:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.75 on epoch=81
06/12/2022 12:54:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.60 on epoch=83
06/12/2022 12:54:46 - INFO - __main__ - Global step 500 Train loss 0.68 Classification-F1 0.16666666666666666 on epoch=83
06/12/2022 12:54:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=84
06/12/2022 12:54:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.62 on epoch=86
06/12/2022 12:54:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.64 on epoch=88
06/12/2022 12:54:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=89
06/12/2022 12:55:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.62 on epoch=91
06/12/2022 12:55:03 - INFO - __main__ - Global step 550 Train loss 0.60 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 12:55:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.61 on epoch=93
06/12/2022 12:55:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.58 on epoch=94
06/12/2022 12:55:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.64 on epoch=96
06/12/2022 12:55:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.64 on epoch=98
06/12/2022 12:55:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.60 on epoch=99
06/12/2022 12:55:19 - INFO - __main__ - Global step 600 Train loss 0.61 Classification-F1 0.16666666666666666 on epoch=99
06/12/2022 12:55:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.55 on epoch=101
06/12/2022 12:55:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.59 on epoch=103
06/12/2022 12:55:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.58 on epoch=104
06/12/2022 12:55:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=106
06/12/2022 12:55:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.64 on epoch=108
06/12/2022 12:55:36 - INFO - __main__ - Global step 650 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=108
06/12/2022 12:55:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.54 on epoch=109
06/12/2022 12:55:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.60 on epoch=111
06/12/2022 12:55:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.56 on epoch=113
06/12/2022 12:55:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.53 on epoch=114
06/12/2022 12:55:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.60 on epoch=116
06/12/2022 12:55:53 - INFO - __main__ - Global step 700 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 12:55:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.62 on epoch=118
06/12/2022 12:55:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.59 on epoch=119
06/12/2022 12:56:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=121
06/12/2022 12:56:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.55 on epoch=123
06/12/2022 12:56:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.56 on epoch=124
06/12/2022 12:56:10 - INFO - __main__ - Global step 750 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=124
06/12/2022 12:56:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.54 on epoch=126
06/12/2022 12:56:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.57 on epoch=128
06/12/2022 12:56:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.54 on epoch=129
06/12/2022 12:56:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.53 on epoch=131
06/12/2022 12:56:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=133
06/12/2022 12:56:26 - INFO - __main__ - Global step 800 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=133
06/12/2022 12:56:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.57 on epoch=134
06/12/2022 12:56:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.50 on epoch=136
06/12/2022 12:56:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.55 on epoch=138
06/12/2022 12:56:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=139
06/12/2022 12:56:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=141
06/12/2022 12:56:43 - INFO - __main__ - Global step 850 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=141
06/12/2022 12:56:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.59 on epoch=143
06/12/2022 12:56:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=144
06/12/2022 12:56:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.57 on epoch=146
06/12/2022 12:56:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.59 on epoch=148
06/12/2022 12:56:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=149
06/12/2022 12:56:59 - INFO - __main__ - Global step 900 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=149
06/12/2022 12:57:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=151
06/12/2022 12:57:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.55 on epoch=153
06/12/2022 12:57:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.53 on epoch=154
06/12/2022 12:57:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=156
06/12/2022 12:57:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.55 on epoch=158
06/12/2022 12:57:16 - INFO - __main__ - Global step 950 Train loss 0.52 Classification-F1 0.30370370370370375 on epoch=158
06/12/2022 12:57:16 - INFO - __main__ - Saving model with best Classification-F1: 0.2714101918666443 -> 0.30370370370370375 on epoch=158, global_step=950
06/12/2022 12:57:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=159
06/12/2022 12:57:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.56 on epoch=161
06/12/2022 12:57:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.54 on epoch=163
06/12/2022 12:57:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.58 on epoch=164
06/12/2022 12:57:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.58 on epoch=166
06/12/2022 12:57:33 - INFO - __main__ - Global step 1000 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=166
06/12/2022 12:57:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.50 on epoch=168
06/12/2022 12:57:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.52 on epoch=169
06/12/2022 12:57:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=171
06/12/2022 12:57:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.55 on epoch=173
06/12/2022 12:57:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.53 on epoch=174
06/12/2022 12:57:50 - INFO - __main__ - Global step 1050 Train loss 0.51 Classification-F1 0.1679790026246719 on epoch=174
06/12/2022 12:57:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.58 on epoch=176
06/12/2022 12:57:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.53 on epoch=178
06/12/2022 12:57:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.51 on epoch=179
06/12/2022 12:58:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.51 on epoch=181
06/12/2022 12:58:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=183
06/12/2022 12:58:06 - INFO - __main__ - Global step 1100 Train loss 0.54 Classification-F1 0.18147828587863102 on epoch=183
06/12/2022 12:58:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=184
06/12/2022 12:58:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.54 on epoch=186
06/12/2022 12:58:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.52 on epoch=188
06/12/2022 12:58:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.51 on epoch=189
06/12/2022 12:58:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=191
06/12/2022 12:58:23 - INFO - __main__ - Global step 1150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=191
06/12/2022 12:58:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.52 on epoch=193
06/12/2022 12:58:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=194
06/12/2022 12:58:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=196
06/12/2022 12:58:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.55 on epoch=198
06/12/2022 12:58:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=199
06/12/2022 12:58:40 - INFO - __main__ - Global step 1200 Train loss 0.50 Classification-F1 0.21684771469563435 on epoch=199
06/12/2022 12:58:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.58 on epoch=201
06/12/2022 12:58:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.58 on epoch=203
06/12/2022 12:58:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.51 on epoch=204
06/12/2022 12:58:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.54 on epoch=206
06/12/2022 12:58:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.53 on epoch=208
06/12/2022 12:58:57 - INFO - __main__ - Global step 1250 Train loss 0.55 Classification-F1 0.1851851851851852 on epoch=208
06/12/2022 12:59:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=209
06/12/2022 12:59:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.55 on epoch=211
06/12/2022 12:59:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=213
06/12/2022 12:59:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.54 on epoch=214
06/12/2022 12:59:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.55 on epoch=216
06/12/2022 12:59:14 - INFO - __main__ - Global step 1300 Train loss 0.54 Classification-F1 0.1881810228266921 on epoch=216
06/12/2022 12:59:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=218
06/12/2022 12:59:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=219
06/12/2022 12:59:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.49 on epoch=221
06/12/2022 12:59:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.54 on epoch=223
06/12/2022 12:59:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=224
06/12/2022 12:59:30 - INFO - __main__ - Global step 1350 Train loss 0.50 Classification-F1 0.22496392496392495 on epoch=224
06/12/2022 12:59:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.52 on epoch=226
06/12/2022 12:59:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.55 on epoch=228
06/12/2022 12:59:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.52 on epoch=229
06/12/2022 12:59:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=231
06/12/2022 12:59:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.50 on epoch=233
06/12/2022 12:59:47 - INFO - __main__ - Global step 1400 Train loss 0.51 Classification-F1 0.21001779811848462 on epoch=233
06/12/2022 12:59:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.49 on epoch=234
06/12/2022 12:59:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.53 on epoch=236
06/12/2022 12:59:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=238
06/12/2022 12:59:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=239
06/12/2022 13:00:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.49 on epoch=241
06/12/2022 13:00:04 - INFO - __main__ - Global step 1450 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=241
06/12/2022 13:00:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.53 on epoch=243
06/12/2022 13:00:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=244
06/12/2022 13:00:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=246
06/12/2022 13:00:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.53 on epoch=248
06/12/2022 13:00:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.51 on epoch=249
06/12/2022 13:00:20 - INFO - __main__ - Global step 1500 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=249
06/12/2022 13:00:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.51 on epoch=251
06/12/2022 13:00:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=253
06/12/2022 13:00:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=254
06/12/2022 13:00:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.52 on epoch=256
06/12/2022 13:00:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.47 on epoch=258
06/12/2022 13:00:37 - INFO - __main__ - Global step 1550 Train loss 0.48 Classification-F1 0.18809318377911996 on epoch=258
06/12/2022 13:00:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.51 on epoch=259
06/12/2022 13:00:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.46 on epoch=261
06/12/2022 13:00:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.57 on epoch=263
06/12/2022 13:00:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.51 on epoch=264
06/12/2022 13:00:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.51 on epoch=266
06/12/2022 13:00:54 - INFO - __main__ - Global step 1600 Train loss 0.51 Classification-F1 0.1679790026246719 on epoch=266
06/12/2022 13:00:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=268
06/12/2022 13:01:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.52 on epoch=269
06/12/2022 13:01:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=271
06/12/2022 13:01:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.54 on epoch=273
06/12/2022 13:01:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.51 on epoch=274
06/12/2022 13:01:11 - INFO - __main__ - Global step 1650 Train loss 0.50 Classification-F1 0.24106816877901216 on epoch=274
06/12/2022 13:01:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=276
06/12/2022 13:01:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.59 on epoch=278
06/12/2022 13:01:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.49 on epoch=279
06/12/2022 13:01:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.55 on epoch=281
06/12/2022 13:01:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.55 on epoch=283
06/12/2022 13:01:29 - INFO - __main__ - Global step 1700 Train loss 0.53 Classification-F1 0.22604378727811705 on epoch=283
06/12/2022 13:01:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.48 on epoch=284
06/12/2022 13:01:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.49 on epoch=286
06/12/2022 13:01:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.54 on epoch=288
06/12/2022 13:01:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.49 on epoch=289
06/12/2022 13:01:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.54 on epoch=291
06/12/2022 13:01:46 - INFO - __main__ - Global step 1750 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=291
06/12/2022 13:01:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.52 on epoch=293
06/12/2022 13:01:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.51 on epoch=294
06/12/2022 13:01:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.55 on epoch=296
06/12/2022 13:01:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.55 on epoch=298
06/12/2022 13:01:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.50 on epoch=299
06/12/2022 13:02:02 - INFO - __main__ - Global step 1800 Train loss 0.53 Classification-F1 0.22588285960378984 on epoch=299
06/12/2022 13:02:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.53 on epoch=301
06/12/2022 13:02:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.50 on epoch=303
06/12/2022 13:02:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=304
06/12/2022 13:02:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.51 on epoch=306
06/12/2022 13:02:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.49 on epoch=308
06/12/2022 13:02:19 - INFO - __main__ - Global step 1850 Train loss 0.51 Classification-F1 0.1986376620522962 on epoch=308
06/12/2022 13:02:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=309
06/12/2022 13:02:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.52 on epoch=311
06/12/2022 13:02:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.52 on epoch=313
06/12/2022 13:02:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.53 on epoch=314
06/12/2022 13:02:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.52 on epoch=316
06/12/2022 13:02:35 - INFO - __main__ - Global step 1900 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=316
06/12/2022 13:02:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.47 on epoch=318
06/12/2022 13:02:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.53 on epoch=319
06/12/2022 13:02:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.52 on epoch=321
06/12/2022 13:02:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.55 on epoch=323
06/12/2022 13:02:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.47 on epoch=324
06/12/2022 13:02:52 - INFO - __main__ - Global step 1950 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=324
06/12/2022 13:02:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.50 on epoch=326
06/12/2022 13:02:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.52 on epoch=328
06/12/2022 13:03:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=329
06/12/2022 13:03:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.50 on epoch=331
06/12/2022 13:03:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.50 on epoch=333
06/12/2022 13:03:09 - INFO - __main__ - Global step 2000 Train loss 0.50 Classification-F1 0.16927852221969872 on epoch=333
06/12/2022 13:03:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.55 on epoch=334
06/12/2022 13:03:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.46 on epoch=336
06/12/2022 13:03:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.49 on epoch=338
06/12/2022 13:03:19 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.49 on epoch=339
06/12/2022 13:03:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.44 on epoch=341
06/12/2022 13:03:25 - INFO - __main__ - Global step 2050 Train loss 0.48 Classification-F1 0.2750231822396771 on epoch=341
06/12/2022 13:03:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.55 on epoch=343
06/12/2022 13:03:30 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.45 on epoch=344
06/12/2022 13:03:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.56 on epoch=346
06/12/2022 13:03:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.52 on epoch=348
06/12/2022 13:03:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.45 on epoch=349
06/12/2022 13:03:41 - INFO - __main__ - Global step 2100 Train loss 0.50 Classification-F1 0.2903091060985798 on epoch=349
06/12/2022 13:03:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.52 on epoch=351
06/12/2022 13:03:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.54 on epoch=353
06/12/2022 13:03:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.48 on epoch=354
06/12/2022 13:03:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.51 on epoch=356
06/12/2022 13:03:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.43 on epoch=358
06/12/2022 13:03:58 - INFO - __main__ - Global step 2150 Train loss 0.49 Classification-F1 0.2303585049580473 on epoch=358
06/12/2022 13:04:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.49 on epoch=359
06/12/2022 13:04:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.53 on epoch=361
06/12/2022 13:04:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.49 on epoch=363
06/12/2022 13:04:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=364
06/12/2022 13:04:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.46 on epoch=366
06/12/2022 13:04:15 - INFO - __main__ - Global step 2200 Train loss 0.48 Classification-F1 0.23778880921738063 on epoch=366
06/12/2022 13:04:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.49 on epoch=368
06/12/2022 13:04:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.45 on epoch=369
06/12/2022 13:04:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.50 on epoch=371
06/12/2022 13:04:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.52 on epoch=373
06/12/2022 13:04:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.50 on epoch=374
06/12/2022 13:04:31 - INFO - __main__ - Global step 2250 Train loss 0.49 Classification-F1 0.16272965879265092 on epoch=374
06/12/2022 13:04:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.49 on epoch=376
06/12/2022 13:04:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.50 on epoch=378
06/12/2022 13:04:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.44 on epoch=379
06/12/2022 13:04:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.47 on epoch=381
06/12/2022 13:04:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.54 on epoch=383
06/12/2022 13:04:48 - INFO - __main__ - Global step 2300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=383
06/12/2022 13:04:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.49 on epoch=384
06/12/2022 13:04:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.50 on epoch=386
06/12/2022 13:04:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.50 on epoch=388
06/12/2022 13:04:59 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.49 on epoch=389
06/12/2022 13:05:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.51 on epoch=391
06/12/2022 13:05:04 - INFO - __main__ - Global step 2350 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=391
06/12/2022 13:05:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.45 on epoch=393
06/12/2022 13:05:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.43 on epoch=394
06/12/2022 13:05:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.49 on epoch=396
06/12/2022 13:05:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.55 on epoch=398
06/12/2022 13:05:18 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.48 on epoch=399
06/12/2022 13:05:20 - INFO - __main__ - Global step 2400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=399
06/12/2022 13:05:23 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.55 on epoch=401
06/12/2022 13:05:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.49 on epoch=403
06/12/2022 13:05:28 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.51 on epoch=404
06/12/2022 13:05:31 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.55 on epoch=406
06/12/2022 13:05:34 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.47 on epoch=408
06/12/2022 13:05:36 - INFO - __main__ - Global step 2450 Train loss 0.51 Classification-F1 0.2336619204089084 on epoch=408
06/12/2022 13:05:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.50 on epoch=409
06/12/2022 13:05:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.48 on epoch=411
06/12/2022 13:05:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.45 on epoch=413
06/12/2022 13:05:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=414
06/12/2022 13:05:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.47 on epoch=416
06/12/2022 13:05:53 - INFO - __main__ - Global step 2500 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=416
06/12/2022 13:05:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.47 on epoch=418
06/12/2022 13:05:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.50 on epoch=419
06/12/2022 13:06:01 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.49 on epoch=421
06/12/2022 13:06:03 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.53 on epoch=423
06/12/2022 13:06:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.44 on epoch=424
06/12/2022 13:06:09 - INFO - __main__ - Global step 2550 Train loss 0.49 Classification-F1 0.2321391158600461 on epoch=424
06/12/2022 13:06:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.46 on epoch=426
06/12/2022 13:06:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.52 on epoch=428
06/12/2022 13:06:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.51 on epoch=429
06/12/2022 13:06:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.52 on epoch=431
06/12/2022 13:06:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.51 on epoch=433
06/12/2022 13:06:25 - INFO - __main__ - Global step 2600 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=433
06/12/2022 13:06:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.47 on epoch=434
06/12/2022 13:06:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.45 on epoch=436
06/12/2022 13:06:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.44 on epoch=438
06/12/2022 13:06:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.46 on epoch=439
06/12/2022 13:06:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.52 on epoch=441
06/12/2022 13:06:42 - INFO - __main__ - Global step 2650 Train loss 0.47 Classification-F1 0.20035778175313057 on epoch=441
06/12/2022 13:06:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.50 on epoch=443
06/12/2022 13:06:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.45 on epoch=444
06/12/2022 13:06:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.46 on epoch=446
06/12/2022 13:06:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.47 on epoch=448
06/12/2022 13:06:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.46 on epoch=449
06/12/2022 13:06:58 - INFO - __main__ - Global step 2700 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=449
06/12/2022 13:07:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.46 on epoch=451
06/12/2022 13:07:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.53 on epoch=453
06/12/2022 13:07:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.46 on epoch=454
06/12/2022 13:07:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.43 on epoch=456
06/12/2022 13:07:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.45 on epoch=458
06/12/2022 13:07:15 - INFO - __main__ - Global step 2750 Train loss 0.47 Classification-F1 0.2318782098312546 on epoch=458
06/12/2022 13:07:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.44 on epoch=459
06/12/2022 13:07:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.44 on epoch=461
06/12/2022 13:07:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.52 on epoch=463
06/12/2022 13:07:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.48 on epoch=464
06/12/2022 13:07:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.47 on epoch=466
06/12/2022 13:07:31 - INFO - __main__ - Global step 2800 Train loss 0.47 Classification-F1 0.16272965879265092 on epoch=466
06/12/2022 13:07:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.46 on epoch=468
06/12/2022 13:07:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.48 on epoch=469
06/12/2022 13:07:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.49 on epoch=471
06/12/2022 13:07:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.49 on epoch=473
06/12/2022 13:07:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.44 on epoch=474
06/12/2022 13:07:47 - INFO - __main__ - Global step 2850 Train loss 0.47 Classification-F1 0.19004581685809327 on epoch=474
06/12/2022 13:07:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.51 on epoch=476
06/12/2022 13:07:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.53 on epoch=478
06/12/2022 13:07:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.49 on epoch=479
06/12/2022 13:07:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.51 on epoch=481
06/12/2022 13:08:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.50 on epoch=483
06/12/2022 13:08:04 - INFO - __main__ - Global step 2900 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=483
06/12/2022 13:08:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.48 on epoch=484
06/12/2022 13:08:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.46 on epoch=486
06/12/2022 13:08:12 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.49 on epoch=488
06/12/2022 13:08:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.45 on epoch=489
06/12/2022 13:08:17 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.47 on epoch=491
06/12/2022 13:08:20 - INFO - __main__ - Global step 2950 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=491
06/12/2022 13:08:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.50 on epoch=493
06/12/2022 13:08:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.49 on epoch=494
06/12/2022 13:08:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.51 on epoch=496
06/12/2022 13:08:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.55 on epoch=498
06/12/2022 13:08:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.47 on epoch=499
06/12/2022 13:08:35 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:08:35 - INFO - __main__ - Printing 3 examples
06/12/2022 13:08:35 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/12/2022 13:08:35 - INFO - __main__ - ['neutral']
06/12/2022 13:08:35 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/12/2022 13:08:35 - INFO - __main__ - ['neutral']
06/12/2022 13:08:35 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/12/2022 13:08:35 - INFO - __main__ - ['neutral']
06/12/2022 13:08:35 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:08:35 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:08:35 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 13:08:35 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:08:35 - INFO - __main__ - Printing 3 examples
06/12/2022 13:08:35 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
06/12/2022 13:08:35 - INFO - __main__ - ['neutral']
06/12/2022 13:08:35 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
06/12/2022 13:08:35 - INFO - __main__ - ['neutral']
06/12/2022 13:08:35 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
06/12/2022 13:08:35 - INFO - __main__ - ['neutral']
06/12/2022 13:08:35 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:08:35 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:08:35 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 13:08:36 - INFO - __main__ - Global step 3000 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=499
06/12/2022 13:08:36 - INFO - __main__ - save last model!
06/12/2022 13:08:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 13:08:36 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 13:08:36 - INFO - __main__ - Printing 3 examples
06/12/2022 13:08:36 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 13:08:36 - INFO - __main__ - ['contradiction']
06/12/2022 13:08:36 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 13:08:36 - INFO - __main__ - ['entailment']
06/12/2022 13:08:36 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 13:08:36 - INFO - __main__ - ['contradiction']
06/12/2022 13:08:36 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:08:37 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:08:38 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 13:08:54 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 13:08:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 13:08:54 - INFO - __main__ - Starting training!
06/12/2022 13:09:08 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_42_0.4_8_predictions.txt
06/12/2022 13:09:08 - INFO - __main__ - Classification-F1 on test data: 0.1660
06/12/2022 13:09:08 - INFO - __main__ - prefix=anli_32_42, lr=0.4, bsz=8, dev_performance=0.30370370370370375, test_performance=0.16603962879357914
06/12/2022 13:09:08 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.3, bsz=8 ...
06/12/2022 13:09:09 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:09:09 - INFO - __main__ - Printing 3 examples
06/12/2022 13:09:09 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/12/2022 13:09:09 - INFO - __main__ - ['neutral']
06/12/2022 13:09:09 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/12/2022 13:09:09 - INFO - __main__ - ['neutral']
06/12/2022 13:09:09 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/12/2022 13:09:09 - INFO - __main__ - ['neutral']
06/12/2022 13:09:09 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:09:09 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:09:09 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 13:09:09 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:09:09 - INFO - __main__ - Printing 3 examples
06/12/2022 13:09:09 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
06/12/2022 13:09:09 - INFO - __main__ - ['neutral']
06/12/2022 13:09:09 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
06/12/2022 13:09:09 - INFO - __main__ - ['neutral']
06/12/2022 13:09:09 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
06/12/2022 13:09:09 - INFO - __main__ - ['neutral']
06/12/2022 13:09:09 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:09:10 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:09:10 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 13:09:25 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 13:09:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 13:09:26 - INFO - __main__ - Starting training!
06/12/2022 13:09:30 - INFO - __main__ - Step 10 Global step 10 Train loss 1.15 on epoch=1
06/12/2022 13:09:32 - INFO - __main__ - Step 20 Global step 20 Train loss 0.82 on epoch=3
06/12/2022 13:09:35 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=4
06/12/2022 13:09:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.76 on epoch=6
06/12/2022 13:09:41 - INFO - __main__ - Step 50 Global step 50 Train loss 1.05 on epoch=8
06/12/2022 13:09:43 - INFO - __main__ - Global step 50 Train loss 0.88 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 13:09:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 13:09:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.86 on epoch=9
06/12/2022 13:09:48 - INFO - __main__ - Step 70 Global step 70 Train loss 1.22 on epoch=11
06/12/2022 13:09:51 - INFO - __main__ - Step 80 Global step 80 Train loss 1.17 on epoch=13
06/12/2022 13:09:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=14
06/12/2022 13:09:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.79 on epoch=16
06/12/2022 13:09:58 - INFO - __main__ - Global step 100 Train loss 0.99 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 13:10:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.73 on epoch=18
06/12/2022 13:10:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.72 on epoch=19
06/12/2022 13:10:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.73 on epoch=21
06/12/2022 13:10:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.63 on epoch=23
06/12/2022 13:10:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.76 on epoch=24
06/12/2022 13:10:15 - INFO - __main__ - Global step 150 Train loss 0.71 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 13:10:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.77 on epoch=26
06/12/2022 13:10:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.67 on epoch=28
06/12/2022 13:10:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=29
06/12/2022 13:10:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=31
06/12/2022 13:10:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=33
06/12/2022 13:10:30 - INFO - __main__ - Global step 200 Train loss 0.69 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 13:10:33 - INFO - __main__ - Step 210 Global step 210 Train loss 1.00 on epoch=34
06/12/2022 13:10:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.89 on epoch=36
06/12/2022 13:10:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.61 on epoch=38
06/12/2022 13:10:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=39
06/12/2022 13:10:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.63 on epoch=41
06/12/2022 13:10:46 - INFO - __main__ - Global step 250 Train loss 0.73 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 13:10:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.63 on epoch=43
06/12/2022 13:10:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.61 on epoch=44
06/12/2022 13:10:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.60 on epoch=46
06/12/2022 13:10:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=48
06/12/2022 13:10:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.66 on epoch=49
06/12/2022 13:11:02 - INFO - __main__ - Global step 300 Train loss 0.62 Classification-F1 0.16666666666666666 on epoch=49
06/12/2022 13:11:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.60 on epoch=51
06/12/2022 13:11:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.60 on epoch=53
06/12/2022 13:11:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=54
06/12/2022 13:11:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=56
06/12/2022 13:11:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=58
06/12/2022 13:11:18 - INFO - __main__ - Global step 350 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=58
06/12/2022 13:11:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.58 on epoch=59
06/12/2022 13:11:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.55 on epoch=61
06/12/2022 13:11:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.60 on epoch=63
06/12/2022 13:11:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.60 on epoch=64
06/12/2022 13:11:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=66
06/12/2022 13:11:35 - INFO - __main__ - Global step 400 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 13:11:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.56 on epoch=68
06/12/2022 13:11:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=69
06/12/2022 13:11:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.64 on epoch=71
06/12/2022 13:11:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.58 on epoch=73
06/12/2022 13:11:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=74
06/12/2022 13:11:52 - INFO - __main__ - Global step 450 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=74
06/12/2022 13:11:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=76
06/12/2022 13:11:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.61 on epoch=78
06/12/2022 13:12:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.52 on epoch=79
06/12/2022 13:12:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.66 on epoch=81
06/12/2022 13:12:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=83
06/12/2022 13:12:09 - INFO - __main__ - Global step 500 Train loss 0.58 Classification-F1 0.15873015873015875 on epoch=83
06/12/2022 13:12:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=84
06/12/2022 13:12:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.60 on epoch=86
06/12/2022 13:12:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.56 on epoch=88
06/12/2022 13:12:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.55 on epoch=89
06/12/2022 13:12:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=91
06/12/2022 13:12:26 - INFO - __main__ - Global step 550 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 13:12:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=93
06/12/2022 13:12:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=94
06/12/2022 13:12:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=96
06/12/2022 13:12:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.59 on epoch=98
06/12/2022 13:12:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=99
06/12/2022 13:12:43 - INFO - __main__ - Global step 600 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=99
06/12/2022 13:12:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.57 on epoch=101
06/12/2022 13:12:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=103
06/12/2022 13:12:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=104
06/12/2022 13:12:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.55 on epoch=106
06/12/2022 13:12:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.55 on epoch=108
06/12/2022 13:12:59 - INFO - __main__ - Global step 650 Train loss 0.54 Classification-F1 0.1626016260162602 on epoch=108
06/12/2022 13:13:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=109
06/12/2022 13:13:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=111
06/12/2022 13:13:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.58 on epoch=113
06/12/2022 13:13:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.58 on epoch=114
06/12/2022 13:13:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.54 on epoch=116
06/12/2022 13:13:16 - INFO - __main__ - Global step 700 Train loss 0.56 Classification-F1 0.18020202020202022 on epoch=116
06/12/2022 13:13:16 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.18020202020202022 on epoch=116, global_step=700
06/12/2022 13:13:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=118
06/12/2022 13:13:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.59 on epoch=119
06/12/2022 13:13:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=121
06/12/2022 13:13:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=123
06/12/2022 13:13:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=124
06/12/2022 13:13:33 - INFO - __main__ - Global step 750 Train loss 0.53 Classification-F1 0.16402116402116398 on epoch=124
06/12/2022 13:13:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.54 on epoch=126
06/12/2022 13:13:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=128
06/12/2022 13:13:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=129
06/12/2022 13:13:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=131
06/12/2022 13:13:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.57 on epoch=133
06/12/2022 13:13:50 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.16272965879265092 on epoch=133
06/12/2022 13:13:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=134
06/12/2022 13:13:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.56 on epoch=136
06/12/2022 13:13:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.51 on epoch=138
06/12/2022 13:14:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=139
06/12/2022 13:14:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.53 on epoch=141
06/12/2022 13:14:06 - INFO - __main__ - Global step 850 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=141
06/12/2022 13:14:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.54 on epoch=143
06/12/2022 13:14:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.52 on epoch=144
06/12/2022 13:14:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.52 on epoch=146
06/12/2022 13:14:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=148
06/12/2022 13:14:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=149
06/12/2022 13:14:22 - INFO - __main__ - Global step 900 Train loss 0.52 Classification-F1 0.16272965879265092 on epoch=149
06/12/2022 13:14:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.54 on epoch=151
06/12/2022 13:14:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.54 on epoch=153
06/12/2022 13:14:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.51 on epoch=154
06/12/2022 13:14:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.56 on epoch=156
06/12/2022 13:14:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=158
06/12/2022 13:14:39 - INFO - __main__ - Global step 950 Train loss 0.53 Classification-F1 0.21666666666666667 on epoch=158
06/12/2022 13:14:39 - INFO - __main__ - Saving model with best Classification-F1: 0.18020202020202022 -> 0.21666666666666667 on epoch=158, global_step=950
06/12/2022 13:14:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.47 on epoch=159
06/12/2022 13:14:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.51 on epoch=161
06/12/2022 13:14:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.53 on epoch=163
06/12/2022 13:14:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=164
06/12/2022 13:14:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.55 on epoch=166
06/12/2022 13:14:56 - INFO - __main__ - Global step 1000 Train loss 0.51 Classification-F1 0.2501571338780641 on epoch=166
06/12/2022 13:14:56 - INFO - __main__ - Saving model with best Classification-F1: 0.21666666666666667 -> 0.2501571338780641 on epoch=166, global_step=1000
06/12/2022 13:14:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=168
06/12/2022 13:15:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=169
06/12/2022 13:15:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=171
06/12/2022 13:15:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.53 on epoch=173
06/12/2022 13:15:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=174
06/12/2022 13:15:13 - INFO - __main__ - Global step 1050 Train loss 0.49 Classification-F1 0.1679790026246719 on epoch=174
06/12/2022 13:15:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.56 on epoch=176
06/12/2022 13:15:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=178
06/12/2022 13:15:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=179
06/12/2022 13:15:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.53 on epoch=181
06/12/2022 13:15:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.61 on epoch=183
06/12/2022 13:15:30 - INFO - __main__ - Global step 1100 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=183
06/12/2022 13:15:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=184
06/12/2022 13:15:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.52 on epoch=186
06/12/2022 13:15:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.52 on epoch=188
06/12/2022 13:15:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.49 on epoch=189
06/12/2022 13:15:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.50 on epoch=191
06/12/2022 13:15:47 - INFO - __main__ - Global step 1150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=191
06/12/2022 13:15:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.52 on epoch=193
06/12/2022 13:15:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=194
06/12/2022 13:15:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.53 on epoch=196
06/12/2022 13:15:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=198
06/12/2022 13:16:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=199
06/12/2022 13:16:04 - INFO - __main__ - Global step 1200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=199
06/12/2022 13:16:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=201
06/12/2022 13:16:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=203
06/12/2022 13:16:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.48 on epoch=204
06/12/2022 13:16:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=206
06/12/2022 13:16:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=208
06/12/2022 13:16:21 - INFO - __main__ - Global step 1250 Train loss 0.48 Classification-F1 0.18809005083514885 on epoch=208
06/12/2022 13:16:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=209
06/12/2022 13:16:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=211
06/12/2022 13:16:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.59 on epoch=213
06/12/2022 13:16:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.50 on epoch=214
06/12/2022 13:16:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.51 on epoch=216
06/12/2022 13:16:38 - INFO - __main__ - Global step 1300 Train loss 0.50 Classification-F1 0.1693121693121693 on epoch=216
06/12/2022 13:16:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=218
06/12/2022 13:16:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=219
06/12/2022 13:16:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.52 on epoch=221
06/12/2022 13:16:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=223
06/12/2022 13:16:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=224
06/12/2022 13:16:55 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=224
06/12/2022 13:16:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=226
06/12/2022 13:17:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=228
06/12/2022 13:17:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.46 on epoch=229
06/12/2022 13:17:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.50 on epoch=231
06/12/2022 13:17:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.49 on epoch=233
06/12/2022 13:17:12 - INFO - __main__ - Global step 1400 Train loss 0.49 Classification-F1 0.25830140485312897 on epoch=233
06/12/2022 13:17:12 - INFO - __main__ - Saving model with best Classification-F1: 0.2501571338780641 -> 0.25830140485312897 on epoch=233, global_step=1400
06/12/2022 13:17:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=234
06/12/2022 13:17:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.51 on epoch=236
06/12/2022 13:17:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=238
06/12/2022 13:17:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=239
06/12/2022 13:17:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=241
06/12/2022 13:17:29 - INFO - __main__ - Global step 1450 Train loss 0.48 Classification-F1 0.21872816212438853 on epoch=241
06/12/2022 13:17:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=243
06/12/2022 13:17:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.47 on epoch=244
06/12/2022 13:17:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.50 on epoch=246
06/12/2022 13:17:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.51 on epoch=248
06/12/2022 13:17:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=249
06/12/2022 13:17:46 - INFO - __main__ - Global step 1500 Train loss 0.49 Classification-F1 0.19444444444444445 on epoch=249
06/12/2022 13:17:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.49 on epoch=251
06/12/2022 13:17:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=253
06/12/2022 13:17:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.52 on epoch=254
06/12/2022 13:17:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.49 on epoch=256
06/12/2022 13:18:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.51 on epoch=258
06/12/2022 13:18:04 - INFO - __main__ - Global step 1550 Train loss 0.49 Classification-F1 0.28247141150366956 on epoch=258
06/12/2022 13:18:04 - INFO - __main__ - Saving model with best Classification-F1: 0.25830140485312897 -> 0.28247141150366956 on epoch=258, global_step=1550
06/12/2022 13:18:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.51 on epoch=259
06/12/2022 13:18:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=261
06/12/2022 13:18:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.47 on epoch=263
06/12/2022 13:18:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=264
06/12/2022 13:18:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.51 on epoch=266
06/12/2022 13:18:21 - INFO - __main__ - Global step 1600 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=266
06/12/2022 13:18:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=268
06/12/2022 13:18:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.54 on epoch=269
06/12/2022 13:18:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.47 on epoch=271
06/12/2022 13:18:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=273
06/12/2022 13:18:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.49 on epoch=274
06/12/2022 13:18:38 - INFO - __main__ - Global step 1650 Train loss 0.49 Classification-F1 0.16533333333333333 on epoch=274
06/12/2022 13:18:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=276
06/12/2022 13:18:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=278
06/12/2022 13:18:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.45 on epoch=279
06/12/2022 13:18:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.49 on epoch=281
06/12/2022 13:18:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.50 on epoch=283
06/12/2022 13:18:55 - INFO - __main__ - Global step 1700 Train loss 0.49 Classification-F1 0.22469324266500135 on epoch=283
06/12/2022 13:18:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=284
06/12/2022 13:19:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=286
06/12/2022 13:19:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.49 on epoch=288
06/12/2022 13:19:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=289
06/12/2022 13:19:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.48 on epoch=291
06/12/2022 13:19:13 - INFO - __main__ - Global step 1750 Train loss 0.48 Classification-F1 0.26713947990543735 on epoch=291
06/12/2022 13:19:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.50 on epoch=293
06/12/2022 13:19:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.47 on epoch=294
06/12/2022 13:19:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=296
06/12/2022 13:19:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.50 on epoch=298
06/12/2022 13:19:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=299
06/12/2022 13:19:30 - INFO - __main__ - Global step 1800 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=299
06/12/2022 13:19:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.47 on epoch=301
06/12/2022 13:19:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.48 on epoch=303
06/12/2022 13:19:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.48 on epoch=304
06/12/2022 13:19:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.47 on epoch=306
06/12/2022 13:19:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.51 on epoch=308
06/12/2022 13:19:47 - INFO - __main__ - Global step 1850 Train loss 0.48 Classification-F1 0.16402116402116398 on epoch=308
06/12/2022 13:19:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=309
06/12/2022 13:19:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=311
06/12/2022 13:19:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=313
06/12/2022 13:19:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.46 on epoch=314
06/12/2022 13:20:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.50 on epoch=316
06/12/2022 13:20:04 - INFO - __main__ - Global step 1900 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=316
06/12/2022 13:20:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.49 on epoch=318
06/12/2022 13:20:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=319
06/12/2022 13:20:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=321
06/12/2022 13:20:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=323
06/12/2022 13:20:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.46 on epoch=324
06/12/2022 13:20:21 - INFO - __main__ - Global step 1950 Train loss 0.46 Classification-F1 0.24547433243085415 on epoch=324
06/12/2022 13:20:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.51 on epoch=326
06/12/2022 13:20:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=328
06/12/2022 13:20:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=329
06/12/2022 13:20:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.51 on epoch=331
06/12/2022 13:20:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.47 on epoch=333
06/12/2022 13:20:38 - INFO - __main__ - Global step 2000 Train loss 0.48 Classification-F1 0.24267676767676769 on epoch=333
06/12/2022 13:20:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.44 on epoch=334
06/12/2022 13:20:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.48 on epoch=336
06/12/2022 13:20:47 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.47 on epoch=338
06/12/2022 13:20:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.44 on epoch=339
06/12/2022 13:20:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.53 on epoch=341
06/12/2022 13:20:55 - INFO - __main__ - Global step 2050 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=341
06/12/2022 13:20:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.47 on epoch=343
06/12/2022 13:21:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.45 on epoch=344
06/12/2022 13:21:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.44 on epoch=346
06/12/2022 13:21:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.43 on epoch=348
06/12/2022 13:21:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.45 on epoch=349
06/12/2022 13:21:12 - INFO - __main__ - Global step 2100 Train loss 0.45 Classification-F1 0.2085278555866791 on epoch=349
06/12/2022 13:21:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.46 on epoch=351
06/12/2022 13:21:18 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.53 on epoch=353
06/12/2022 13:21:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.43 on epoch=354
06/12/2022 13:21:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.47 on epoch=356
06/12/2022 13:21:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.48 on epoch=358
06/12/2022 13:21:29 - INFO - __main__ - Global step 2150 Train loss 0.47 Classification-F1 0.2734231041814316 on epoch=358
06/12/2022 13:21:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.47 on epoch=359
06/12/2022 13:21:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.47 on epoch=361
06/12/2022 13:21:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.45 on epoch=363
06/12/2022 13:21:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.44 on epoch=364
06/12/2022 13:21:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.48 on epoch=366
06/12/2022 13:21:46 - INFO - __main__ - Global step 2200 Train loss 0.46 Classification-F1 0.3288859403410487 on epoch=366
06/12/2022 13:21:46 - INFO - __main__ - Saving model with best Classification-F1: 0.28247141150366956 -> 0.3288859403410487 on epoch=366, global_step=2200
06/12/2022 13:21:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.50 on epoch=368
06/12/2022 13:21:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.45 on epoch=369
06/12/2022 13:21:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.45 on epoch=371
06/12/2022 13:21:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.47 on epoch=373
06/12/2022 13:22:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.45 on epoch=374
06/12/2022 13:22:03 - INFO - __main__ - Global step 2250 Train loss 0.46 Classification-F1 0.23602162008392005 on epoch=374
06/12/2022 13:22:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.45 on epoch=376
06/12/2022 13:22:09 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.46 on epoch=378
06/12/2022 13:22:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.41 on epoch=379
06/12/2022 13:22:14 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.45 on epoch=381
06/12/2022 13:22:17 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.45 on epoch=383
06/12/2022 13:22:20 - INFO - __main__ - Global step 2300 Train loss 0.45 Classification-F1 0.28153717627401836 on epoch=383
06/12/2022 13:22:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.45 on epoch=384
06/12/2022 13:22:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.45 on epoch=386
06/12/2022 13:22:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.42 on epoch=388
06/12/2022 13:22:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.46 on epoch=389
06/12/2022 13:22:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.44 on epoch=391
06/12/2022 13:22:37 - INFO - __main__ - Global step 2350 Train loss 0.44 Classification-F1 0.34957124612297025 on epoch=391
06/12/2022 13:22:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3288859403410487 -> 0.34957124612297025 on epoch=391, global_step=2350
06/12/2022 13:22:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.44 on epoch=393
06/12/2022 13:22:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.41 on epoch=394
06/12/2022 13:22:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=396
06/12/2022 13:22:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.47 on epoch=398
06/12/2022 13:22:51 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.37 on epoch=399
06/12/2022 13:22:54 - INFO - __main__ - Global step 2400 Train loss 0.42 Classification-F1 0.3196959954274283 on epoch=399
06/12/2022 13:22:57 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.49 on epoch=401
06/12/2022 13:22:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.47 on epoch=403
06/12/2022 13:23:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.40 on epoch=404
06/12/2022 13:23:05 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.50 on epoch=406
06/12/2022 13:23:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.46 on epoch=408
06/12/2022 13:23:10 - INFO - __main__ - Global step 2450 Train loss 0.46 Classification-F1 0.30901960784313726 on epoch=408
06/12/2022 13:23:13 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.45 on epoch=409
06/12/2022 13:23:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.44 on epoch=411
06/12/2022 13:23:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.43 on epoch=413
06/12/2022 13:23:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=414
06/12/2022 13:23:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.45 on epoch=416
06/12/2022 13:23:28 - INFO - __main__ - Global step 2500 Train loss 0.44 Classification-F1 0.27443181818181817 on epoch=416
06/12/2022 13:23:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.43 on epoch=418
06/12/2022 13:23:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.39 on epoch=419
06/12/2022 13:23:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.39 on epoch=421
06/12/2022 13:23:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=423
06/12/2022 13:23:42 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.46 on epoch=424
06/12/2022 13:23:45 - INFO - __main__ - Global step 2550 Train loss 0.42 Classification-F1 0.2931216931216931 on epoch=424
06/12/2022 13:23:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.48 on epoch=426
06/12/2022 13:23:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.41 on epoch=428
06/12/2022 13:23:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.40 on epoch=429
06/12/2022 13:23:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.47 on epoch=431
06/12/2022 13:23:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.37 on epoch=433
06/12/2022 13:24:02 - INFO - __main__ - Global step 2600 Train loss 0.43 Classification-F1 0.2876523582405935 on epoch=433
06/12/2022 13:24:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.36 on epoch=434
06/12/2022 13:24:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.43 on epoch=436
06/12/2022 13:24:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.44 on epoch=438
06/12/2022 13:24:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.40 on epoch=439
06/12/2022 13:24:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.35 on epoch=441
06/12/2022 13:24:19 - INFO - __main__ - Global step 2650 Train loss 0.40 Classification-F1 0.36497842380195317 on epoch=441
06/12/2022 13:24:19 - INFO - __main__ - Saving model with best Classification-F1: 0.34957124612297025 -> 0.36497842380195317 on epoch=441, global_step=2650
06/12/2022 13:24:22 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.41 on epoch=443
06/12/2022 13:24:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.33 on epoch=444
06/12/2022 13:24:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.40 on epoch=446
06/12/2022 13:24:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.32 on epoch=448
06/12/2022 13:24:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.40 on epoch=449
06/12/2022 13:24:36 - INFO - __main__ - Global step 2700 Train loss 0.37 Classification-F1 0.2865932605051731 on epoch=449
06/12/2022 13:24:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.39 on epoch=451
06/12/2022 13:24:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.39 on epoch=453
06/12/2022 13:24:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.39 on epoch=454
06/12/2022 13:24:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.38 on epoch=456
06/12/2022 13:24:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.35 on epoch=458
06/12/2022 13:24:53 - INFO - __main__ - Global step 2750 Train loss 0.38 Classification-F1 0.29991769547325103 on epoch=458
06/12/2022 13:24:56 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.37 on epoch=459
06/12/2022 13:24:59 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.31 on epoch=461
06/12/2022 13:25:01 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.32 on epoch=463
06/12/2022 13:25:04 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.32 on epoch=464
06/12/2022 13:25:07 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.32 on epoch=466
06/12/2022 13:25:10 - INFO - __main__ - Global step 2800 Train loss 0.33 Classification-F1 0.2698412698412698 on epoch=466
06/12/2022 13:25:13 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.29 on epoch=468
06/12/2022 13:25:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.29 on epoch=469
06/12/2022 13:25:18 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.34 on epoch=471
06/12/2022 13:25:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.31 on epoch=473
06/12/2022 13:25:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.39 on epoch=474
06/12/2022 13:25:26 - INFO - __main__ - Global step 2850 Train loss 0.32 Classification-F1 0.270526606010477 on epoch=474
06/12/2022 13:25:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.34 on epoch=476
06/12/2022 13:25:32 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.35 on epoch=478
06/12/2022 13:25:35 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.30 on epoch=479
06/12/2022 13:25:38 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.38 on epoch=481
06/12/2022 13:25:41 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.31 on epoch=483
06/12/2022 13:25:44 - INFO - __main__ - Global step 2900 Train loss 0.34 Classification-F1 0.26462194073031514 on epoch=483
06/12/2022 13:25:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.30 on epoch=484
06/12/2022 13:25:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.26 on epoch=486
06/12/2022 13:25:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.27 on epoch=488
06/12/2022 13:25:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.25 on epoch=489
06/12/2022 13:25:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.26 on epoch=491
06/12/2022 13:26:01 - INFO - __main__ - Global step 2950 Train loss 0.27 Classification-F1 0.3075924433700421 on epoch=491
06/12/2022 13:26:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.27 on epoch=493
06/12/2022 13:26:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.29 on epoch=494
06/12/2022 13:26:09 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.23 on epoch=496
06/12/2022 13:26:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.22 on epoch=498
06/12/2022 13:26:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.24 on epoch=499
06/12/2022 13:26:16 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:26:16 - INFO - __main__ - Printing 3 examples
06/12/2022 13:26:16 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/12/2022 13:26:16 - INFO - __main__ - ['neutral']
06/12/2022 13:26:16 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/12/2022 13:26:16 - INFO - __main__ - ['neutral']
06/12/2022 13:26:16 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/12/2022 13:26:16 - INFO - __main__ - ['neutral']
06/12/2022 13:26:16 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:26:16 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:26:16 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 13:26:16 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:26:16 - INFO - __main__ - Printing 3 examples
06/12/2022 13:26:16 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
06/12/2022 13:26:16 - INFO - __main__ - ['neutral']
06/12/2022 13:26:16 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
06/12/2022 13:26:16 - INFO - __main__ - ['neutral']
06/12/2022 13:26:16 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
06/12/2022 13:26:16 - INFO - __main__ - ['neutral']
06/12/2022 13:26:16 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:26:16 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:26:16 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 13:26:18 - INFO - __main__ - Global step 3000 Train loss 0.25 Classification-F1 0.3089811155028546 on epoch=499
06/12/2022 13:26:18 - INFO - __main__ - save last model!
06/12/2022 13:26:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 13:26:18 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 13:26:18 - INFO - __main__ - Printing 3 examples
06/12/2022 13:26:18 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 13:26:18 - INFO - __main__ - ['contradiction']
06/12/2022 13:26:18 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 13:26:18 - INFO - __main__ - ['entailment']
06/12/2022 13:26:18 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 13:26:18 - INFO - __main__ - ['contradiction']
06/12/2022 13:26:18 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:26:18 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:26:19 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 13:26:36 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 13:26:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 13:26:36 - INFO - __main__ - Starting training!
06/12/2022 13:26:50 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_42_0.3_8_predictions.txt
06/12/2022 13:26:50 - INFO - __main__ - Classification-F1 on test data: 0.3018
06/12/2022 13:26:51 - INFO - __main__ - prefix=anli_32_42, lr=0.3, bsz=8, dev_performance=0.36497842380195317, test_performance=0.30181256870590545
06/12/2022 13:26:51 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.2, bsz=8 ...
06/12/2022 13:26:52 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:26:52 - INFO - __main__ - Printing 3 examples
06/12/2022 13:26:52 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/12/2022 13:26:52 - INFO - __main__ - ['neutral']
06/12/2022 13:26:52 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/12/2022 13:26:52 - INFO - __main__ - ['neutral']
06/12/2022 13:26:52 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/12/2022 13:26:52 - INFO - __main__ - ['neutral']
06/12/2022 13:26:52 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:26:52 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:26:52 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 13:26:52 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:26:52 - INFO - __main__ - Printing 3 examples
06/12/2022 13:26:52 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
06/12/2022 13:26:52 - INFO - __main__ - ['neutral']
06/12/2022 13:26:52 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
06/12/2022 13:26:52 - INFO - __main__ - ['neutral']
06/12/2022 13:26:52 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
06/12/2022 13:26:52 - INFO - __main__ - ['neutral']
06/12/2022 13:26:52 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:26:52 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:26:52 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 13:27:11 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 13:27:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 13:27:11 - INFO - __main__ - Starting training!
06/12/2022 13:27:15 - INFO - __main__ - Step 10 Global step 10 Train loss 1.30 on epoch=1
06/12/2022 13:27:18 - INFO - __main__ - Step 20 Global step 20 Train loss 0.80 on epoch=3
06/12/2022 13:27:20 - INFO - __main__ - Step 30 Global step 30 Train loss 0.72 on epoch=4
06/12/2022 13:27:23 - INFO - __main__ - Step 40 Global step 40 Train loss 0.68 on epoch=6
06/12/2022 13:27:26 - INFO - __main__ - Step 50 Global step 50 Train loss 0.61 on epoch=8
06/12/2022 13:27:28 - INFO - __main__ - Global step 50 Train loss 0.82 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 13:27:28 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 13:27:31 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=9
06/12/2022 13:27:34 - INFO - __main__ - Step 70 Global step 70 Train loss 0.60 on epoch=11
06/12/2022 13:27:36 - INFO - __main__ - Step 80 Global step 80 Train loss 0.67 on epoch=13
06/12/2022 13:27:39 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=14
06/12/2022 13:27:42 - INFO - __main__ - Step 100 Global step 100 Train loss 0.62 on epoch=16
06/12/2022 13:27:45 - INFO - __main__ - Global step 100 Train loss 0.60 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 13:27:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=18
06/12/2022 13:27:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=19
06/12/2022 13:27:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.60 on epoch=21
06/12/2022 13:27:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.62 on epoch=23
06/12/2022 13:27:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=24
06/12/2022 13:28:01 - INFO - __main__ - Global step 150 Train loss 0.57 Classification-F1 0.2709057791025004 on epoch=24
06/12/2022 13:28:01 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2709057791025004 on epoch=24, global_step=150
06/12/2022 13:28:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=26
06/12/2022 13:28:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=28
06/12/2022 13:28:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=29
06/12/2022 13:28:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.59 on epoch=31
06/12/2022 13:28:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=33
06/12/2022 13:28:18 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 13:28:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=34
06/12/2022 13:28:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=36
06/12/2022 13:28:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=38
06/12/2022 13:28:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=39
06/12/2022 13:28:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=41
06/12/2022 13:28:34 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 13:28:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=43
06/12/2022 13:28:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=44
06/12/2022 13:28:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.58 on epoch=46
06/12/2022 13:28:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=48
06/12/2022 13:28:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=49
06/12/2022 13:28:51 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.24404761904761907 on epoch=49
06/12/2022 13:28:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=51
06/12/2022 13:28:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.60 on epoch=53
06/12/2022 13:28:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=54
06/12/2022 13:29:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=56
06/12/2022 13:29:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=58
06/12/2022 13:29:08 - INFO - __main__ - Global step 350 Train loss 0.54 Classification-F1 0.21001779811848462 on epoch=58
06/12/2022 13:29:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=59
06/12/2022 13:29:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=61
06/12/2022 13:29:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.55 on epoch=63
06/12/2022 13:29:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.56 on epoch=64
06/12/2022 13:29:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=66
06/12/2022 13:29:24 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 13:29:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=68
06/12/2022 13:29:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=69
06/12/2022 13:29:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=71
06/12/2022 13:29:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.54 on epoch=73
06/12/2022 13:29:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=74
06/12/2022 13:29:41 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.18061964403427822 on epoch=74
06/12/2022 13:29:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=76
06/12/2022 13:29:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=78
06/12/2022 13:29:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=79
06/12/2022 13:29:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=81
06/12/2022 13:29:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=83
06/12/2022 13:29:58 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.16402116402116398 on epoch=83
06/12/2022 13:30:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=84
06/12/2022 13:30:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=86
06/12/2022 13:30:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=88
06/12/2022 13:30:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=89
06/12/2022 13:30:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=91
06/12/2022 13:30:15 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 13:30:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=93
06/12/2022 13:30:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=94
06/12/2022 13:30:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=96
06/12/2022 13:30:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.56 on epoch=98
06/12/2022 13:30:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=99
06/12/2022 13:30:31 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.22500440839358138 on epoch=99
06/12/2022 13:30:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.52 on epoch=101
06/12/2022 13:30:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=103
06/12/2022 13:30:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=104
06/12/2022 13:30:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=106
06/12/2022 13:30:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=108
06/12/2022 13:30:48 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.1881810228266921 on epoch=108
06/12/2022 13:30:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=109
06/12/2022 13:30:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=111
06/12/2022 13:30:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.53 on epoch=113
06/12/2022 13:30:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=114
06/12/2022 13:31:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.52 on epoch=116
06/12/2022 13:31:04 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=116
06/12/2022 13:31:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=118
06/12/2022 13:31:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=119
06/12/2022 13:31:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.56 on epoch=121
06/12/2022 13:31:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.52 on epoch=123
06/12/2022 13:31:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=124
06/12/2022 13:31:21 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.27054154995331464 on epoch=124
06/12/2022 13:31:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=126
06/12/2022 13:31:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=128
06/12/2022 13:31:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=129
06/12/2022 13:31:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=131
06/12/2022 13:31:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=133
06/12/2022 13:31:38 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.20370370370370372 on epoch=133
06/12/2022 13:31:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=134
06/12/2022 13:31:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=136
06/12/2022 13:31:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=138
06/12/2022 13:31:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=139
06/12/2022 13:31:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=141
06/12/2022 13:31:54 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=141
06/12/2022 13:31:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=143
06/12/2022 13:31:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=144
06/12/2022 13:32:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.51 on epoch=146
06/12/2022 13:32:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=148
06/12/2022 13:32:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=149
06/12/2022 13:32:10 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.15873015873015875 on epoch=149
06/12/2022 13:32:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=151
06/12/2022 13:32:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=153
06/12/2022 13:32:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=154
06/12/2022 13:32:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=156
06/12/2022 13:32:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=158
06/12/2022 13:32:27 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=158
06/12/2022 13:32:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=159
06/12/2022 13:32:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=161
06/12/2022 13:32:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.50 on epoch=163
06/12/2022 13:32:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=164
06/12/2022 13:32:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.48 on epoch=166
06/12/2022 13:32:43 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.18694276675294788 on epoch=166
06/12/2022 13:32:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=168
06/12/2022 13:32:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=169
06/12/2022 13:32:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.49 on epoch=171
06/12/2022 13:32:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.52 on epoch=173
06/12/2022 13:32:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=174
06/12/2022 13:33:00 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.3186649099692578 on epoch=174
06/12/2022 13:33:00 - INFO - __main__ - Saving model with best Classification-F1: 0.2709057791025004 -> 0.3186649099692578 on epoch=174, global_step=1050
06/12/2022 13:33:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.50 on epoch=176
06/12/2022 13:33:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=178
06/12/2022 13:33:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.51 on epoch=179
06/12/2022 13:33:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=181
06/12/2022 13:33:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.48 on epoch=183
06/12/2022 13:33:17 - INFO - __main__ - Global step 1100 Train loss 0.48 Classification-F1 0.21711057304277648 on epoch=183
06/12/2022 13:33:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=184
06/12/2022 13:33:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=186
06/12/2022 13:33:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.51 on epoch=188
06/12/2022 13:33:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=189
06/12/2022 13:33:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.50 on epoch=191
06/12/2022 13:33:33 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.2638667375509481 on epoch=191
06/12/2022 13:33:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=193
06/12/2022 13:33:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=194
06/12/2022 13:33:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=196
06/12/2022 13:33:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=198
06/12/2022 13:33:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=199
06/12/2022 13:33:50 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.27116670324043596 on epoch=199
06/12/2022 13:33:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=201
06/12/2022 13:33:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.52 on epoch=203
06/12/2022 13:33:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.48 on epoch=204
06/12/2022 13:34:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=206
06/12/2022 13:34:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.47 on epoch=208
06/12/2022 13:34:06 - INFO - __main__ - Global step 1250 Train loss 0.49 Classification-F1 0.31656419629998894 on epoch=208
06/12/2022 13:34:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=209
06/12/2022 13:34:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=211
06/12/2022 13:34:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=213
06/12/2022 13:34:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=214
06/12/2022 13:34:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.49 on epoch=216
06/12/2022 13:34:22 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.26846819645732695 on epoch=216
06/12/2022 13:34:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=218
06/12/2022 13:34:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.49 on epoch=219
06/12/2022 13:34:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=221
06/12/2022 13:34:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=223
06/12/2022 13:34:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=224
06/12/2022 13:34:39 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.2861594670105308 on epoch=224
06/12/2022 13:34:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.48 on epoch=226
06/12/2022 13:34:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=228
06/12/2022 13:34:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=229
06/12/2022 13:34:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=231
06/12/2022 13:34:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=233
06/12/2022 13:34:55 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.24897054165346852 on epoch=233
06/12/2022 13:34:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=234
06/12/2022 13:35:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=236
06/12/2022 13:35:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=238
06/12/2022 13:35:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=239
06/12/2022 13:35:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=241
06/12/2022 13:35:12 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.2302466371431889 on epoch=241
06/12/2022 13:35:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=243
06/12/2022 13:35:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=244
06/12/2022 13:35:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=246
06/12/2022 13:35:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.48 on epoch=248
06/12/2022 13:35:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=249
06/12/2022 13:35:28 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.24836158192090396 on epoch=249
06/12/2022 13:35:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.48 on epoch=251
06/12/2022 13:35:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.46 on epoch=253
06/12/2022 13:35:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=254
06/12/2022 13:35:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=256
06/12/2022 13:35:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=258
06/12/2022 13:35:45 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.2808781169572702 on epoch=258
06/12/2022 13:35:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=259
06/12/2022 13:35:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=261
06/12/2022 13:35:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=263
06/12/2022 13:35:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=264
06/12/2022 13:35:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=266
06/12/2022 13:36:01 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.2851376812846796 on epoch=266
06/12/2022 13:36:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=268
06/12/2022 13:36:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.34 on epoch=269
06/12/2022 13:36:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=271
06/12/2022 13:36:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=273
06/12/2022 13:36:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=274
06/12/2022 13:36:18 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.2760760760760761 on epoch=274
06/12/2022 13:36:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=276
06/12/2022 13:36:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=278
06/12/2022 13:36:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=279
06/12/2022 13:36:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=281
06/12/2022 13:36:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=283
06/12/2022 13:36:34 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.24774344270999257 on epoch=283
06/12/2022 13:36:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=284
06/12/2022 13:36:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.39 on epoch=286
06/12/2022 13:36:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=288
06/12/2022 13:36:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=289
06/12/2022 13:36:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=291
06/12/2022 13:36:50 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.2710056995771281 on epoch=291
06/12/2022 13:36:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=293
06/12/2022 13:36:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=294
06/12/2022 13:36:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=296
06/12/2022 13:37:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=298
06/12/2022 13:37:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=299
06/12/2022 13:37:06 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.2944734345351044 on epoch=299
06/12/2022 13:37:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=301
06/12/2022 13:37:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=303
06/12/2022 13:37:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=304
06/12/2022 13:37:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=306
06/12/2022 13:37:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=308
06/12/2022 13:37:22 - INFO - __main__ - Global step 1850 Train loss 0.37 Classification-F1 0.31086070791953146 on epoch=308
06/12/2022 13:37:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=309
06/12/2022 13:37:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=311
06/12/2022 13:37:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=313
06/12/2022 13:37:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=314
06/12/2022 13:37:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=316
06/12/2022 13:37:39 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.27152683543119177 on epoch=316
06/12/2022 13:37:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=318
06/12/2022 13:37:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=319
06/12/2022 13:37:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=321
06/12/2022 13:37:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=323
06/12/2022 13:37:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=324
06/12/2022 13:37:55 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.2830180474016091 on epoch=324
06/12/2022 13:37:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.30 on epoch=326
06/12/2022 13:38:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=328
06/12/2022 13:38:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.34 on epoch=329
06/12/2022 13:38:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=331
06/12/2022 13:38:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=333
06/12/2022 13:38:12 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.25045580038547693 on epoch=333
06/12/2022 13:38:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.31 on epoch=334
06/12/2022 13:38:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.29 on epoch=336
06/12/2022 13:38:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.31 on epoch=338
06/12/2022 13:38:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.29 on epoch=339
06/12/2022 13:38:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.27 on epoch=341
06/12/2022 13:38:29 - INFO - __main__ - Global step 2050 Train loss 0.30 Classification-F1 0.23504961115580583 on epoch=341
06/12/2022 13:38:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.27 on epoch=343
06/12/2022 13:38:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.33 on epoch=344
06/12/2022 13:38:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=346
06/12/2022 13:38:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.30 on epoch=348
06/12/2022 13:38:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.28 on epoch=349
06/12/2022 13:38:45 - INFO - __main__ - Global step 2100 Train loss 0.30 Classification-F1 0.30805571367467766 on epoch=349
06/12/2022 13:38:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=351
06/12/2022 13:38:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.27 on epoch=353
06/12/2022 13:38:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.25 on epoch=354
06/12/2022 13:38:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=356
06/12/2022 13:38:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=358
06/12/2022 13:39:02 - INFO - __main__ - Global step 2150 Train loss 0.26 Classification-F1 0.26850663243474543 on epoch=358
06/12/2022 13:39:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=359
06/12/2022 13:39:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.31 on epoch=361
06/12/2022 13:39:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.28 on epoch=363
06/12/2022 13:39:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=364
06/12/2022 13:39:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.25 on epoch=366
06/12/2022 13:39:19 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.26122136569897764 on epoch=366
06/12/2022 13:39:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=368
06/12/2022 13:39:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.26 on epoch=369
06/12/2022 13:39:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.25 on epoch=371
06/12/2022 13:39:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.24 on epoch=373
06/12/2022 13:39:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=374
06/12/2022 13:39:35 - INFO - __main__ - Global step 2250 Train loss 0.24 Classification-F1 0.2973448773448773 on epoch=374
06/12/2022 13:39:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.24 on epoch=376
06/12/2022 13:39:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.25 on epoch=378
06/12/2022 13:39:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=379
06/12/2022 13:39:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=381
06/12/2022 13:39:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=383
06/12/2022 13:39:52 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.2670256572695597 on epoch=383
06/12/2022 13:39:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.25 on epoch=384
06/12/2022 13:39:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=386
06/12/2022 13:40:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=388
06/12/2022 13:40:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=389
06/12/2022 13:40:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=391
06/12/2022 13:40:08 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.3373639216335846 on epoch=391
06/12/2022 13:40:08 - INFO - __main__ - Saving model with best Classification-F1: 0.3186649099692578 -> 0.3373639216335846 on epoch=391, global_step=2350
06/12/2022 13:40:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=393
06/12/2022 13:40:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=394
06/12/2022 13:40:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=396
06/12/2022 13:40:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.17 on epoch=398
06/12/2022 13:40:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=399
06/12/2022 13:40:25 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.33719862434052966 on epoch=399
06/12/2022 13:40:28 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=401
06/12/2022 13:40:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=403
06/12/2022 13:40:33 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=404
06/12/2022 13:40:36 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=406
06/12/2022 13:40:38 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=408
06/12/2022 13:40:41 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.29169960474308304 on epoch=408
06/12/2022 13:40:44 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=409
06/12/2022 13:40:47 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=411
06/12/2022 13:40:49 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.11 on epoch=413
06/12/2022 13:40:52 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=414
06/12/2022 13:40:55 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=416
06/12/2022 13:40:58 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.25112075814323004 on epoch=416
06/12/2022 13:41:00 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=418
06/12/2022 13:41:03 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=419
06/12/2022 13:41:06 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=421
06/12/2022 13:41:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.16 on epoch=423
06/12/2022 13:41:11 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.14 on epoch=424
06/12/2022 13:41:14 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.23994726433750824 on epoch=424
06/12/2022 13:41:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=426
06/12/2022 13:41:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=428
06/12/2022 13:41:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.18 on epoch=429
06/12/2022 13:41:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.14 on epoch=431
06/12/2022 13:41:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=433
06/12/2022 13:41:31 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.26653210830426016 on epoch=433
06/12/2022 13:41:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=434
06/12/2022 13:41:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.16 on epoch=436
06/12/2022 13:41:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.16 on epoch=438
06/12/2022 13:41:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=439
06/12/2022 13:41:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=441
06/12/2022 13:41:47 - INFO - __main__ - Global step 2650 Train loss 0.15 Classification-F1 0.1903567447045708 on epoch=441
06/12/2022 13:41:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=443
06/12/2022 13:41:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=444
06/12/2022 13:41:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=446
06/12/2022 13:41:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=448
06/12/2022 13:42:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=449
06/12/2022 13:42:04 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.25406205181271896 on epoch=449
06/12/2022 13:42:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=451
06/12/2022 13:42:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.10 on epoch=453
06/12/2022 13:42:12 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=454
06/12/2022 13:42:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=456
06/12/2022 13:42:17 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=458
06/12/2022 13:42:20 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.20851666926717638 on epoch=458
06/12/2022 13:42:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=459
06/12/2022 13:42:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=461
06/12/2022 13:42:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=463
06/12/2022 13:42:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=464
06/12/2022 13:42:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=466
06/12/2022 13:42:36 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.1981200897867565 on epoch=466
06/12/2022 13:42:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=468
06/12/2022 13:42:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=469
06/12/2022 13:42:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=471
06/12/2022 13:42:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=473
06/12/2022 13:42:50 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.12 on epoch=474
06/12/2022 13:42:53 - INFO - __main__ - Global step 2850 Train loss 0.10 Classification-F1 0.19879719345138658 on epoch=474
06/12/2022 13:42:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=476
06/12/2022 13:42:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.12 on epoch=478
06/12/2022 13:43:01 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=479
06/12/2022 13:43:04 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.11 on epoch=481
06/12/2022 13:43:06 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=483
06/12/2022 13:43:09 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.18729180546302465 on epoch=483
06/12/2022 13:43:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.14 on epoch=484
06/12/2022 13:43:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=486
06/12/2022 13:43:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=488
06/12/2022 13:43:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=489
06/12/2022 13:43:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=491
06/12/2022 13:43:25 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.102141099055656 on epoch=491
06/12/2022 13:43:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=493
06/12/2022 13:43:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=494
06/12/2022 13:43:34 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=496
06/12/2022 13:43:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=498
06/12/2022 13:43:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=499
06/12/2022 13:43:41 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:43:41 - INFO - __main__ - Printing 3 examples
06/12/2022 13:43:41 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/12/2022 13:43:41 - INFO - __main__ - ['contradiction']
06/12/2022 13:43:41 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/12/2022 13:43:41 - INFO - __main__ - ['contradiction']
06/12/2022 13:43:41 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/12/2022 13:43:41 - INFO - __main__ - ['contradiction']
06/12/2022 13:43:41 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:43:41 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:43:41 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 13:43:41 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:43:41 - INFO - __main__ - Printing 3 examples
06/12/2022 13:43:41 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
06/12/2022 13:43:41 - INFO - __main__ - ['contradiction']
06/12/2022 13:43:41 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
06/12/2022 13:43:41 - INFO - __main__ - ['contradiction']
06/12/2022 13:43:41 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
06/12/2022 13:43:41 - INFO - __main__ - ['contradiction']
06/12/2022 13:43:41 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:43:41 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:43:41 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 13:43:42 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.13055597797986554 on epoch=499
06/12/2022 13:43:42 - INFO - __main__ - save last model!
06/12/2022 13:43:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 13:43:42 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 13:43:42 - INFO - __main__ - Printing 3 examples
06/12/2022 13:43:42 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 13:43:42 - INFO - __main__ - ['contradiction']
06/12/2022 13:43:42 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 13:43:42 - INFO - __main__ - ['entailment']
06/12/2022 13:43:42 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 13:43:42 - INFO - __main__ - ['contradiction']
06/12/2022 13:43:42 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:43:43 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:43:44 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 13:43:59 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 13:44:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 13:44:00 - INFO - __main__ - Starting training!
06/12/2022 13:44:15 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_42_0.2_8_predictions.txt
06/12/2022 13:44:15 - INFO - __main__ - Classification-F1 on test data: 0.0663
06/12/2022 13:44:16 - INFO - __main__ - prefix=anli_32_42, lr=0.2, bsz=8, dev_performance=0.3373639216335846, test_performance=0.06627922567837195
06/12/2022 13:44:16 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.5, bsz=8 ...
06/12/2022 13:44:17 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:44:17 - INFO - __main__ - Printing 3 examples
06/12/2022 13:44:17 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/12/2022 13:44:17 - INFO - __main__ - ['contradiction']
06/12/2022 13:44:17 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/12/2022 13:44:17 - INFO - __main__ - ['contradiction']
06/12/2022 13:44:17 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/12/2022 13:44:17 - INFO - __main__ - ['contradiction']
06/12/2022 13:44:17 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:44:17 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:44:17 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 13:44:17 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 13:44:17 - INFO - __main__ - Printing 3 examples
06/12/2022 13:44:17 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
06/12/2022 13:44:17 - INFO - __main__ - ['contradiction']
06/12/2022 13:44:17 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
06/12/2022 13:44:17 - INFO - __main__ - ['contradiction']
06/12/2022 13:44:17 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
06/12/2022 13:44:17 - INFO - __main__ - ['contradiction']
06/12/2022 13:44:17 - INFO - __main__ - Tokenizing Input ...
06/12/2022 13:44:17 - INFO - __main__ - Tokenizing Output ...
06/12/2022 13:44:17 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 13:44:33 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 13:44:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 13:44:34 - INFO - __main__ - Starting training!
06/12/2022 13:44:38 - INFO - __main__ - Step 10 Global step 10 Train loss 1.58 on epoch=1
06/12/2022 13:44:41 - INFO - __main__ - Step 20 Global step 20 Train loss 0.94 on epoch=3
06/12/2022 13:44:43 - INFO - __main__ - Step 30 Global step 30 Train loss 0.67 on epoch=4
06/12/2022 13:44:46 - INFO - __main__ - Step 40 Global step 40 Train loss 0.60 on epoch=6
06/12/2022 13:44:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=8
06/12/2022 13:44:51 - INFO - __main__ - Global step 50 Train loss 0.86 Classification-F1 0.1990221455277538 on epoch=8
06/12/2022 13:44:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1990221455277538 on epoch=8, global_step=50
06/12/2022 13:44:54 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=9
06/12/2022 13:44:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=11
06/12/2022 13:44:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.53 on epoch=13
06/12/2022 13:45:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=14
06/12/2022 13:45:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=16
06/12/2022 13:45:07 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 13:45:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=18
06/12/2022 13:45:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=19
06/12/2022 13:45:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=21
06/12/2022 13:45:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=23
06/12/2022 13:45:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=24
06/12/2022 13:45:23 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 13:45:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=26
06/12/2022 13:45:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=28
06/12/2022 13:45:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=29
06/12/2022 13:45:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=31
06/12/2022 13:45:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=33
06/12/2022 13:45:40 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.1679790026246719 on epoch=33
06/12/2022 13:45:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=34
06/12/2022 13:45:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=36
06/12/2022 13:45:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=38
06/12/2022 13:45:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=39
06/12/2022 13:45:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=41
06/12/2022 13:45:57 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 13:45:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=43
06/12/2022 13:46:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=44
06/12/2022 13:46:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=46
06/12/2022 13:46:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=48
06/12/2022 13:46:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=49
06/12/2022 13:46:13 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.18845800663982484 on epoch=49
06/12/2022 13:46:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=51
06/12/2022 13:46:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=53
06/12/2022 13:46:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=54
06/12/2022 13:46:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=56
06/12/2022 13:46:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=58
06/12/2022 13:46:30 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.25537954919661615 on epoch=58
06/12/2022 13:46:30 - INFO - __main__ - Saving model with best Classification-F1: 0.1990221455277538 -> 0.25537954919661615 on epoch=58, global_step=350
06/12/2022 13:46:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=59
06/12/2022 13:46:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=61
06/12/2022 13:46:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=63
06/12/2022 13:46:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=64
06/12/2022 13:46:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=66
06/12/2022 13:46:46 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.18892001244942422 on epoch=66
06/12/2022 13:46:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
06/12/2022 13:46:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=69
06/12/2022 13:46:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=71
06/12/2022 13:46:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=73
06/12/2022 13:47:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=74
06/12/2022 13:47:03 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.2810314685314685 on epoch=74
06/12/2022 13:47:03 - INFO - __main__ - Saving model with best Classification-F1: 0.25537954919661615 -> 0.2810314685314685 on epoch=74, global_step=450
06/12/2022 13:47:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=76
06/12/2022 13:47:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=78
06/12/2022 13:47:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=79
06/12/2022 13:47:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=81
06/12/2022 13:47:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=83
06/12/2022 13:47:20 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.3 on epoch=83
06/12/2022 13:47:20 - INFO - __main__ - Saving model with best Classification-F1: 0.2810314685314685 -> 0.3 on epoch=83, global_step=500
06/12/2022 13:47:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.42 on epoch=84
06/12/2022 13:47:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=86
06/12/2022 13:47:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.37 on epoch=88
06/12/2022 13:47:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=89
06/12/2022 13:47:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=91
06/12/2022 13:47:37 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.17519958983373618 on epoch=91
06/12/2022 13:47:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=93
06/12/2022 13:47:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=94
06/12/2022 13:47:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=96
06/12/2022 13:47:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=98
06/12/2022 13:47:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=99
06/12/2022 13:47:54 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.2831016936280094 on epoch=99
06/12/2022 13:47:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=101
06/12/2022 13:47:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=103
06/12/2022 13:48:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=104
06/12/2022 13:48:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=106
06/12/2022 13:48:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=108
06/12/2022 13:48:10 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.1842884046273877 on epoch=108
06/12/2022 13:48:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=109
06/12/2022 13:48:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=111
06/12/2022 13:48:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.60 on epoch=113
06/12/2022 13:48:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.80 on epoch=114
06/12/2022 13:48:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.66 on epoch=116
06/12/2022 13:48:27 - INFO - __main__ - Global step 700 Train loss 0.61 Classification-F1 0.19393939393939394 on epoch=116
06/12/2022 13:48:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.55 on epoch=118
06/12/2022 13:48:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.53 on epoch=119
06/12/2022 13:48:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=121
06/12/2022 13:48:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=123
06/12/2022 13:48:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=124
06/12/2022 13:48:44 - INFO - __main__ - Global step 750 Train loss 0.47 Classification-F1 0.32744238046095636 on epoch=124
06/12/2022 13:48:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3 -> 0.32744238046095636 on epoch=124, global_step=750
06/12/2022 13:48:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=126
06/12/2022 13:48:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=128
06/12/2022 13:48:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=129
06/12/2022 13:48:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=131
06/12/2022 13:48:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=133
06/12/2022 13:49:00 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.2536363636363636 on epoch=133
06/12/2022 13:49:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=134
06/12/2022 13:49:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=136
06/12/2022 13:49:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=138
06/12/2022 13:49:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.65 on epoch=139
06/12/2022 13:49:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=141
06/12/2022 13:49:17 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.32665920119447556 on epoch=141
06/12/2022 13:49:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=143
06/12/2022 13:49:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=144
06/12/2022 13:49:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=146
06/12/2022 13:49:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=148
06/12/2022 13:49:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=149
06/12/2022 13:49:34 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.2307164307164307 on epoch=149
06/12/2022 13:49:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=151
06/12/2022 13:49:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=153
06/12/2022 13:49:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=154
06/12/2022 13:49:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=156
06/12/2022 13:49:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=158
06/12/2022 13:49:51 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.32972729321613703 on epoch=158
06/12/2022 13:49:51 - INFO - __main__ - Saving model with best Classification-F1: 0.32744238046095636 -> 0.32972729321613703 on epoch=158, global_step=950
06/12/2022 13:49:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=159
06/12/2022 13:49:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=161
06/12/2022 13:49:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=163
06/12/2022 13:50:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.55 on epoch=164
06/12/2022 13:50:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=166
06/12/2022 13:50:07 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.24477863899782806 on epoch=166
06/12/2022 13:50:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=168
06/12/2022 13:50:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=169
06/12/2022 13:50:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=171
06/12/2022 13:50:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=173
06/12/2022 13:50:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=174
06/12/2022 13:50:24 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.2843478260869566 on epoch=174
06/12/2022 13:50:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=176
06/12/2022 13:50:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=178
06/12/2022 13:50:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=179
06/12/2022 13:50:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=181
06/12/2022 13:50:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=183
06/12/2022 13:50:41 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.3052129569776629 on epoch=183
06/12/2022 13:50:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=184
06/12/2022 13:50:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=186
06/12/2022 13:50:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=188
06/12/2022 13:50:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=189
06/12/2022 13:50:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=191
06/12/2022 13:50:57 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.36641010081653946 on epoch=191
06/12/2022 13:50:57 - INFO - __main__ - Saving model with best Classification-F1: 0.32972729321613703 -> 0.36641010081653946 on epoch=191, global_step=1150
06/12/2022 13:51:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=193
06/12/2022 13:51:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=194
06/12/2022 13:51:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=196
06/12/2022 13:51:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=198
06/12/2022 13:51:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=199
06/12/2022 13:51:14 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.29193293479007765 on epoch=199
06/12/2022 13:51:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=201
06/12/2022 13:51:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=203
06/12/2022 13:51:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=204
06/12/2022 13:51:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=206
06/12/2022 13:51:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=208
06/12/2022 13:51:31 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.3031988476732412 on epoch=208
06/12/2022 13:51:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=209
06/12/2022 13:51:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=211
06/12/2022 13:51:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=213
06/12/2022 13:51:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=214
06/12/2022 13:51:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=216
06/12/2022 13:51:48 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.30501089324618735 on epoch=216
06/12/2022 13:51:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=218
06/12/2022 13:51:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=219
06/12/2022 13:51:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=221
06/12/2022 13:51:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=223
06/12/2022 13:52:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=224
06/12/2022 13:52:05 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.2971196377430915 on epoch=224
06/12/2022 13:52:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=226
06/12/2022 13:52:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=228
06/12/2022 13:52:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=229
06/12/2022 13:52:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=231
06/12/2022 13:52:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=233
06/12/2022 13:52:22 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.22222222222222218 on epoch=233
06/12/2022 13:52:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=234
06/12/2022 13:52:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=236
06/12/2022 13:52:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=238
06/12/2022 13:52:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=239
06/12/2022 13:52:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=241
06/12/2022 13:52:38 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.3196217494089834 on epoch=241
06/12/2022 13:52:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.28 on epoch=243
06/12/2022 13:52:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=244
06/12/2022 13:52:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=246
06/12/2022 13:52:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=248
06/12/2022 13:52:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=249
06/12/2022 13:52:55 - INFO - __main__ - Global step 1500 Train loss 0.32 Classification-F1 0.31262411347517727 on epoch=249
06/12/2022 13:52:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.31 on epoch=251
06/12/2022 13:53:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.29 on epoch=253
06/12/2022 13:53:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=254
06/12/2022 13:53:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=256
06/12/2022 13:53:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=258
06/12/2022 13:53:12 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.28444989106753815 on epoch=258
06/12/2022 13:53:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=259
06/12/2022 13:53:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=261
06/12/2022 13:53:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=263
06/12/2022 13:53:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=264
06/12/2022 13:53:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=266
06/12/2022 13:53:28 - INFO - __main__ - Global step 1600 Train loss 0.32 Classification-F1 0.35109056735728644 on epoch=266
06/12/2022 13:53:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=268
06/12/2022 13:53:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.33 on epoch=269
06/12/2022 13:53:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.29 on epoch=271
06/12/2022 13:53:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=273
06/12/2022 13:53:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=274
06/12/2022 13:53:44 - INFO - __main__ - Global step 1650 Train loss 0.31 Classification-F1 0.34217684296508066 on epoch=274
06/12/2022 13:53:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=276
06/12/2022 13:53:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=278
06/12/2022 13:53:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=279
06/12/2022 13:53:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=281
06/12/2022 13:53:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.29 on epoch=283
06/12/2022 13:54:01 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.32129068378413583 on epoch=283
06/12/2022 13:54:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=284
06/12/2022 13:54:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=286
06/12/2022 13:54:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.28 on epoch=288
06/12/2022 13:54:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.29 on epoch=289
06/12/2022 13:54:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.28 on epoch=291
06/12/2022 13:54:17 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.3186354055919273 on epoch=291
06/12/2022 13:54:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=293
06/12/2022 13:54:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=294
06/12/2022 13:54:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=296
06/12/2022 13:54:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=298
06/12/2022 13:54:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=299
06/12/2022 13:54:34 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.33414852857259314 on epoch=299
06/12/2022 13:54:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.26 on epoch=301
06/12/2022 13:54:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=303
06/12/2022 13:54:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=304
06/12/2022 13:54:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=306
06/12/2022 13:54:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=308
06/12/2022 13:54:50 - INFO - __main__ - Global step 1850 Train loss 0.27 Classification-F1 0.22346269714690767 on epoch=308
06/12/2022 13:54:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.31 on epoch=309
06/12/2022 13:54:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=311
06/12/2022 13:54:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=313
06/12/2022 13:55:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=314
06/12/2022 13:55:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.28 on epoch=316
06/12/2022 13:55:07 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.36915213485733256 on epoch=316
06/12/2022 13:55:07 - INFO - __main__ - Saving model with best Classification-F1: 0.36641010081653946 -> 0.36915213485733256 on epoch=316, global_step=1900
06/12/2022 13:55:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=318
06/12/2022 13:55:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=319
06/12/2022 13:55:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.32 on epoch=321
06/12/2022 13:55:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=323
06/12/2022 13:55:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=324
06/12/2022 13:55:24 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.3353835693458335 on epoch=324
06/12/2022 13:55:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=326
06/12/2022 13:55:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=328
06/12/2022 13:55:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.22 on epoch=329
06/12/2022 13:55:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=331
06/12/2022 13:55:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=333
06/12/2022 13:55:40 - INFO - __main__ - Global step 2000 Train loss 0.25 Classification-F1 0.3222663770928638 on epoch=333
06/12/2022 13:55:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=334
06/12/2022 13:55:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.26 on epoch=336
06/12/2022 13:55:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.23 on epoch=338
06/12/2022 13:55:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.24 on epoch=339
06/12/2022 13:55:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.25 on epoch=341
06/12/2022 13:55:57 - INFO - __main__ - Global step 2050 Train loss 0.26 Classification-F1 0.3451874847076095 on epoch=341
06/12/2022 13:56:00 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=343
06/12/2022 13:56:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=344
06/12/2022 13:56:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.24 on epoch=346
06/12/2022 13:56:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=348
06/12/2022 13:56:11 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.26 on epoch=349
06/12/2022 13:56:14 - INFO - __main__ - Global step 2100 Train loss 0.23 Classification-F1 0.28161576177409925 on epoch=349
06/12/2022 13:56:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=351
06/12/2022 13:56:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.31 on epoch=353
06/12/2022 13:56:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.28 on epoch=354
06/12/2022 13:56:25 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=356
06/12/2022 13:56:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.31 on epoch=358
06/12/2022 13:56:30 - INFO - __main__ - Global step 2150 Train loss 0.27 Classification-F1 0.19821562453141403 on epoch=358
06/12/2022 13:56:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.26 on epoch=359
06/12/2022 13:56:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=361
06/12/2022 13:56:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.31 on epoch=363
06/12/2022 13:56:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=364
06/12/2022 13:56:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.25 on epoch=366
06/12/2022 13:56:47 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.3733211233211233 on epoch=366
06/12/2022 13:56:47 - INFO - __main__ - Saving model with best Classification-F1: 0.36915213485733256 -> 0.3733211233211233 on epoch=366, global_step=2200
06/12/2022 13:56:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=368
06/12/2022 13:56:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.26 on epoch=369
06/12/2022 13:56:55 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.24 on epoch=371
06/12/2022 13:56:58 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=373
06/12/2022 13:57:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.27 on epoch=374
06/12/2022 13:57:03 - INFO - __main__ - Global step 2250 Train loss 0.24 Classification-F1 0.2920634920634921 on epoch=374
06/12/2022 13:57:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.23 on epoch=376
06/12/2022 13:57:09 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=378
06/12/2022 13:57:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 1.99 on epoch=379
06/12/2022 13:57:14 - INFO - __main__ - Step 2290 Global step 2290 Train loss 2.59 on epoch=381
06/12/2022 13:57:17 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.71 on epoch=383
06/12/2022 13:57:20 - INFO - __main__ - Global step 2300 Train loss 1.15 Classification-F1 0.23116042533518263 on epoch=383
06/12/2022 13:57:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.29 on epoch=384
06/12/2022 13:57:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.31 on epoch=386
06/12/2022 13:57:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.29 on epoch=388
06/12/2022 13:57:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.33 on epoch=389
06/12/2022 13:57:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=391
06/12/2022 13:57:36 - INFO - __main__ - Global step 2350 Train loss 0.30 Classification-F1 0.35147527910685805 on epoch=391
06/12/2022 13:57:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=393
06/12/2022 13:57:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.24 on epoch=394
06/12/2022 13:57:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.30 on epoch=396
06/12/2022 13:57:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.39 on epoch=398
06/12/2022 13:57:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.28 on epoch=399
06/12/2022 13:57:53 - INFO - __main__ - Global step 2400 Train loss 0.29 Classification-F1 0.2957253835588253 on epoch=399
06/12/2022 13:57:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.23 on epoch=401
06/12/2022 13:57:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.21 on epoch=403
06/12/2022 13:58:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.29 on epoch=404
06/12/2022 13:58:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.24 on epoch=406
06/12/2022 13:58:07 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.28 on epoch=408
06/12/2022 13:58:10 - INFO - __main__ - Global step 2450 Train loss 0.25 Classification-F1 0.37142857142857144 on epoch=408
06/12/2022 13:58:13 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.24 on epoch=409
06/12/2022 13:58:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.26 on epoch=411
06/12/2022 13:58:18 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.24 on epoch=413
06/12/2022 13:58:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=414
06/12/2022 13:58:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.25 on epoch=416
06/12/2022 13:58:27 - INFO - __main__ - Global step 2500 Train loss 0.25 Classification-F1 0.308038889564994 on epoch=416
06/12/2022 13:58:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.22 on epoch=418
06/12/2022 13:58:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=419
06/12/2022 13:58:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.19 on epoch=421
06/12/2022 13:58:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.14 on epoch=423
06/12/2022 13:58:41 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=424
06/12/2022 13:58:44 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.3498235782482358 on epoch=424
06/12/2022 13:58:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=426
06/12/2022 13:58:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.17 on epoch=428
06/12/2022 13:58:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=429
06/12/2022 13:58:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=431
06/12/2022 13:58:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=433
06/12/2022 13:59:01 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.3205837173579109 on epoch=433
06/12/2022 13:59:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.21 on epoch=434
06/12/2022 13:59:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.22 on epoch=436
06/12/2022 13:59:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.15 on epoch=438
06/12/2022 13:59:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=439
06/12/2022 13:59:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.23 on epoch=441
06/12/2022 13:59:17 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.3478835978835979 on epoch=441
06/12/2022 13:59:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.19 on epoch=443
06/12/2022 13:59:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.22 on epoch=444
06/12/2022 13:59:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.15 on epoch=446
06/12/2022 13:59:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.15 on epoch=448
06/12/2022 13:59:31 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=449
06/12/2022 13:59:34 - INFO - __main__ - Global step 2700 Train loss 0.18 Classification-F1 0.27840438754981484 on epoch=449
06/12/2022 13:59:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=451
06/12/2022 13:59:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=453
06/12/2022 13:59:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.21 on epoch=454
06/12/2022 13:59:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.21 on epoch=456
06/12/2022 13:59:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.21 on epoch=458
06/12/2022 13:59:50 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.3461955105790722 on epoch=458
06/12/2022 13:59:53 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=459
06/12/2022 13:59:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.23 on epoch=461
06/12/2022 13:59:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=463
06/12/2022 14:00:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.17 on epoch=464
06/12/2022 14:00:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=466
06/12/2022 14:00:07 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.34806151180420186 on epoch=466
06/12/2022 14:00:10 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.16 on epoch=468
06/12/2022 14:00:13 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=469
06/12/2022 14:00:15 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=471
06/12/2022 14:00:18 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.19 on epoch=473
06/12/2022 14:00:21 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=474
06/12/2022 14:00:24 - INFO - __main__ - Global step 2850 Train loss 0.17 Classification-F1 0.3446819846819847 on epoch=474
06/12/2022 14:00:27 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=476
06/12/2022 14:00:30 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.19 on epoch=478
06/12/2022 14:00:32 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.22 on epoch=479
06/12/2022 14:00:35 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.14 on epoch=481
06/12/2022 14:00:38 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.17 on epoch=483
06/12/2022 14:00:41 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.37924881403142274 on epoch=483
06/12/2022 14:00:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3733211233211233 -> 0.37924881403142274 on epoch=483, global_step=2900
06/12/2022 14:00:44 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=484
06/12/2022 14:00:47 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.17 on epoch=486
06/12/2022 14:00:49 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.14 on epoch=488
06/12/2022 14:00:52 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=489
06/12/2022 14:00:55 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=491
06/12/2022 14:00:58 - INFO - __main__ - Global step 2950 Train loss 0.16 Classification-F1 0.33241121672104795 on epoch=491
06/12/2022 14:01:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=493
06/12/2022 14:01:03 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=494
06/12/2022 14:01:06 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.19 on epoch=496
06/12/2022 14:01:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.18 on epoch=498
06/12/2022 14:01:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=499
06/12/2022 14:01:13 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:01:13 - INFO - __main__ - Printing 3 examples
06/12/2022 14:01:13 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/12/2022 14:01:13 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:13 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/12/2022 14:01:13 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:13 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/12/2022 14:01:13 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:13 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:01:13 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:01:13 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 14:01:13 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:01:13 - INFO - __main__ - Printing 3 examples
06/12/2022 14:01:13 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
06/12/2022 14:01:13 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:13 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
06/12/2022 14:01:13 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:13 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
06/12/2022 14:01:13 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:13 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:01:13 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:01:13 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 14:01:14 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.3496762496762496 on epoch=499
06/12/2022 14:01:14 - INFO - __main__ - save last model!
06/12/2022 14:01:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 14:01:15 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 14:01:15 - INFO - __main__ - Printing 3 examples
06/12/2022 14:01:15 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 14:01:15 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:15 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 14:01:15 - INFO - __main__ - ['entailment']
06/12/2022 14:01:15 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 14:01:15 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:15 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:01:15 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:01:16 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 14:01:29 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 14:01:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 14:01:30 - INFO - __main__ - Starting training!
06/12/2022 14:01:48 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_87_0.5_8_predictions.txt
06/12/2022 14:01:48 - INFO - __main__ - Classification-F1 on test data: 0.3182
06/12/2022 14:01:48 - INFO - __main__ - prefix=anli_32_87, lr=0.5, bsz=8, dev_performance=0.37924881403142274, test_performance=0.3181663436324251
06/12/2022 14:01:48 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.4, bsz=8 ...
06/12/2022 14:01:49 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:01:49 - INFO - __main__ - Printing 3 examples
06/12/2022 14:01:49 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/12/2022 14:01:49 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:49 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/12/2022 14:01:49 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:49 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/12/2022 14:01:49 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:49 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:01:49 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:01:49 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 14:01:49 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:01:49 - INFO - __main__ - Printing 3 examples
06/12/2022 14:01:49 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
06/12/2022 14:01:49 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:49 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
06/12/2022 14:01:49 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:49 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
06/12/2022 14:01:49 - INFO - __main__ - ['contradiction']
06/12/2022 14:01:49 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:01:49 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:01:49 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 14:02:08 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 14:02:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 14:02:09 - INFO - __main__ - Starting training!
06/12/2022 14:02:13 - INFO - __main__ - Step 10 Global step 10 Train loss 0.93 on epoch=1
06/12/2022 14:02:15 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=3
06/12/2022 14:02:18 - INFO - __main__ - Step 30 Global step 30 Train loss 0.63 on epoch=4
06/12/2022 14:02:21 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=6
06/12/2022 14:02:24 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=8
06/12/2022 14:02:27 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.24962233682346202 on epoch=8
06/12/2022 14:02:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.24962233682346202 on epoch=8, global_step=50
06/12/2022 14:02:30 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=9
06/12/2022 14:02:32 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=11
06/12/2022 14:02:35 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=13
06/12/2022 14:02:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=14
06/12/2022 14:02:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=16
06/12/2022 14:02:43 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 14:02:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=18
06/12/2022 14:02:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=19
06/12/2022 14:02:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=21
06/12/2022 14:02:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=23
06/12/2022 14:02:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=24
06/12/2022 14:03:00 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 14:03:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=26
06/12/2022 14:03:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=28
06/12/2022 14:03:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=29
06/12/2022 14:03:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=31
06/12/2022 14:03:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=33
06/12/2022 14:03:17 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
06/12/2022 14:03:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=34
06/12/2022 14:03:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=36
06/12/2022 14:03:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=38
06/12/2022 14:03:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=39
06/12/2022 14:03:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=41
06/12/2022 14:03:34 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
06/12/2022 14:03:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=43
06/12/2022 14:03:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=44
06/12/2022 14:03:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=46
06/12/2022 14:03:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=48
06/12/2022 14:03:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=49
06/12/2022 14:03:50 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.1679790026246719 on epoch=49
06/12/2022 14:03:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=51
06/12/2022 14:03:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=53
06/12/2022 14:03:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=54
06/12/2022 14:04:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=56
06/12/2022 14:04:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=58
06/12/2022 14:04:07 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.20768789443488242 on epoch=58
06/12/2022 14:04:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=59
06/12/2022 14:04:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=61
06/12/2022 14:04:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=63
06/12/2022 14:04:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=64
06/12/2022 14:04:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=66
06/12/2022 14:04:23 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 14:04:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=68
06/12/2022 14:04:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=69
06/12/2022 14:04:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=71
06/12/2022 14:04:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=73
06/12/2022 14:04:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=74
06/12/2022 14:04:40 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.2212397531546468 on epoch=74
06/12/2022 14:04:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=76
06/12/2022 14:04:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=78
06/12/2022 14:04:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=79
06/12/2022 14:04:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=81
06/12/2022 14:04:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=83
06/12/2022 14:04:57 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.22453102453102455 on epoch=83
06/12/2022 14:05:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=84
06/12/2022 14:05:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=86
06/12/2022 14:05:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=88
06/12/2022 14:05:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=89
06/12/2022 14:05:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=91
06/12/2022 14:05:14 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.2821678321678322 on epoch=91
06/12/2022 14:05:14 - INFO - __main__ - Saving model with best Classification-F1: 0.24962233682346202 -> 0.2821678321678322 on epoch=91, global_step=550
06/12/2022 14:05:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=93
06/12/2022 14:05:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.35 on epoch=94
06/12/2022 14:05:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=96
06/12/2022 14:05:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=98
06/12/2022 14:05:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=99
06/12/2022 14:05:31 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.1731229076996652 on epoch=99
06/12/2022 14:05:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=101
06/12/2022 14:05:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=103
06/12/2022 14:05:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=104
06/12/2022 14:05:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=106
06/12/2022 14:05:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=108
06/12/2022 14:05:47 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.31567883673146835 on epoch=108
06/12/2022 14:05:47 - INFO - __main__ - Saving model with best Classification-F1: 0.2821678321678322 -> 0.31567883673146835 on epoch=108, global_step=650
06/12/2022 14:05:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=109
06/12/2022 14:05:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=111
06/12/2022 14:05:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=113
06/12/2022 14:05:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=114
06/12/2022 14:06:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=116
06/12/2022 14:06:04 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.2846373825658195 on epoch=116
06/12/2022 14:06:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=118
06/12/2022 14:06:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=119
06/12/2022 14:06:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=121
06/12/2022 14:06:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=123
06/12/2022 14:06:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=124
06/12/2022 14:06:21 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.16402116402116398 on epoch=124
06/12/2022 14:06:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=126
06/12/2022 14:06:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=128
06/12/2022 14:06:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=129
06/12/2022 14:06:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=131
06/12/2022 14:06:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=133
06/12/2022 14:06:38 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.22207697893972403 on epoch=133
06/12/2022 14:06:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=134
06/12/2022 14:06:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=136
06/12/2022 14:06:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=138
06/12/2022 14:06:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=139
06/12/2022 14:06:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=141
06/12/2022 14:06:54 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.2850856202008891 on epoch=141
06/12/2022 14:06:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=143
06/12/2022 14:07:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=144
06/12/2022 14:07:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.33 on epoch=146
06/12/2022 14:07:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=148
06/12/2022 14:07:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=149
06/12/2022 14:07:11 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.21358858858858856 on epoch=149
06/12/2022 14:07:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=151
06/12/2022 14:07:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=153
06/12/2022 14:07:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=154
06/12/2022 14:07:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=156
06/12/2022 14:07:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=158
06/12/2022 14:07:28 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.237995337995338 on epoch=158
06/12/2022 14:07:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.34 on epoch=159
06/12/2022 14:07:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=161
06/12/2022 14:07:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=163
06/12/2022 14:07:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=164
06/12/2022 14:07:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=166
06/12/2022 14:07:45 - INFO - __main__ - Global step 1000 Train loss 0.34 Classification-F1 0.319922830792396 on epoch=166
06/12/2022 14:07:45 - INFO - __main__ - Saving model with best Classification-F1: 0.31567883673146835 -> 0.319922830792396 on epoch=166, global_step=1000
06/12/2022 14:07:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=168
06/12/2022 14:07:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=169
06/12/2022 14:07:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=171
06/12/2022 14:07:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=173
06/12/2022 14:07:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=174
06/12/2022 14:08:02 - INFO - __main__ - Global step 1050 Train loss 0.33 Classification-F1 0.22900934161212735 on epoch=174
06/12/2022 14:08:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=176
06/12/2022 14:08:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.32 on epoch=178
06/12/2022 14:08:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=179
06/12/2022 14:08:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=181
06/12/2022 14:08:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=183
06/12/2022 14:08:19 - INFO - __main__ - Global step 1100 Train loss 0.31 Classification-F1 0.26079717328859403 on epoch=183
06/12/2022 14:08:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=184
06/12/2022 14:08:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=186
06/12/2022 14:08:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=188
06/12/2022 14:08:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=189
06/12/2022 14:08:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=191
06/12/2022 14:08:35 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.3476529160739687 on epoch=191
06/12/2022 14:08:35 - INFO - __main__ - Saving model with best Classification-F1: 0.319922830792396 -> 0.3476529160739687 on epoch=191, global_step=1150
06/12/2022 14:08:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=193
06/12/2022 14:08:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=194
06/12/2022 14:08:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=196
06/12/2022 14:08:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=198
06/12/2022 14:08:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=199
06/12/2022 14:08:52 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.24970432617491442 on epoch=199
06/12/2022 14:08:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=201
06/12/2022 14:08:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=203
06/12/2022 14:09:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=204
06/12/2022 14:09:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.27 on epoch=206
06/12/2022 14:09:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=208
06/12/2022 14:09:09 - INFO - __main__ - Global step 1250 Train loss 0.26 Classification-F1 0.2434920634920635 on epoch=208
06/12/2022 14:09:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=209
06/12/2022 14:09:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=211
06/12/2022 14:09:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=213
06/12/2022 14:09:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=214
06/12/2022 14:09:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=216
06/12/2022 14:09:26 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.3050165520753756 on epoch=216
06/12/2022 14:09:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=218
06/12/2022 14:09:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=219
06/12/2022 14:09:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=221
06/12/2022 14:09:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=223
06/12/2022 14:09:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=224
06/12/2022 14:09:42 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.21979925491777993 on epoch=224
06/12/2022 14:09:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=226
06/12/2022 14:09:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=228
06/12/2022 14:09:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=229
06/12/2022 14:09:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=231
06/12/2022 14:09:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=233
06/12/2022 14:09:59 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.2921477495810734 on epoch=233
06/12/2022 14:10:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=234
06/12/2022 14:10:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=236
06/12/2022 14:10:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=238
06/12/2022 14:10:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=239
06/12/2022 14:10:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=241
06/12/2022 14:10:16 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.3163148054981025 on epoch=241
06/12/2022 14:10:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=243
06/12/2022 14:10:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=244
06/12/2022 14:10:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=246
06/12/2022 14:10:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=248
06/12/2022 14:10:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=249
06/12/2022 14:10:32 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.32875860537419493 on epoch=249
06/12/2022 14:10:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=251
06/12/2022 14:10:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=253
06/12/2022 14:10:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=254
06/12/2022 14:10:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=256
06/12/2022 14:10:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=258
06/12/2022 14:10:49 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.2892116447767872 on epoch=258
06/12/2022 14:10:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=259
06/12/2022 14:10:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=261
06/12/2022 14:10:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=263
06/12/2022 14:11:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=264
06/12/2022 14:11:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=266
06/12/2022 14:11:06 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.2866038071517523 on epoch=266
06/12/2022 14:11:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.17 on epoch=268
06/12/2022 14:11:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=269
06/12/2022 14:11:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=271
06/12/2022 14:11:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=273
06/12/2022 14:11:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=274
06/12/2022 14:11:23 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.34512503956948404 on epoch=274
06/12/2022 14:11:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.16 on epoch=276
06/12/2022 14:11:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=278
06/12/2022 14:11:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=279
06/12/2022 14:11:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=281
06/12/2022 14:11:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=283
06/12/2022 14:11:39 - INFO - __main__ - Global step 1700 Train loss 0.16 Classification-F1 0.17161234616477583 on epoch=283
06/12/2022 14:11:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=284
06/12/2022 14:11:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=286
06/12/2022 14:11:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=288
06/12/2022 14:11:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=289
06/12/2022 14:11:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=291
06/12/2022 14:11:56 - INFO - __main__ - Global step 1750 Train loss 0.12 Classification-F1 0.2246589195118607 on epoch=291
06/12/2022 14:11:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=293
06/12/2022 14:12:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=294
06/12/2022 14:12:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=296
06/12/2022 14:12:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=298
06/12/2022 14:12:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=299
06/12/2022 14:12:13 - INFO - __main__ - Global step 1800 Train loss 0.10 Classification-F1 0.19574286600496277 on epoch=299
06/12/2022 14:12:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=301
06/12/2022 14:12:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=303
06/12/2022 14:12:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=304
06/12/2022 14:12:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=306
06/12/2022 14:12:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=308
06/12/2022 14:12:30 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.29461013814230386 on epoch=308
06/12/2022 14:12:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=309
06/12/2022 14:12:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=311
06/12/2022 14:12:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=313
06/12/2022 14:12:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=314
06/12/2022 14:12:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=316
06/12/2022 14:12:47 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.29734592892487627 on epoch=316
06/12/2022 14:12:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=318
06/12/2022 14:12:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=319
06/12/2022 14:12:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=321
06/12/2022 14:12:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=323
06/12/2022 14:13:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=324
06/12/2022 14:13:04 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.23988689168257626 on epoch=324
06/12/2022 14:13:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=326
06/12/2022 14:13:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=328
06/12/2022 14:13:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=329
06/12/2022 14:13:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=331
06/12/2022 14:13:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=333
06/12/2022 14:13:21 - INFO - __main__ - Global step 2000 Train loss 0.10 Classification-F1 0.2261503928170595 on epoch=333
06/12/2022 14:13:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=334
06/12/2022 14:13:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=336
06/12/2022 14:13:29 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=338
06/12/2022 14:13:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=339
06/12/2022 14:13:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=341
06/12/2022 14:13:37 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.28700606961476527 on epoch=341
06/12/2022 14:13:40 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=343
06/12/2022 14:13:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=344
06/12/2022 14:13:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=346
06/12/2022 14:13:49 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=348
06/12/2022 14:13:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=349
06/12/2022 14:13:54 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.29052429052429046 on epoch=349
06/12/2022 14:13:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=351
06/12/2022 14:14:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=353
06/12/2022 14:14:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=354
06/12/2022 14:14:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=356
06/12/2022 14:14:08 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=358
06/12/2022 14:14:11 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.2818185627487953 on epoch=358
06/12/2022 14:14:14 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=359
06/12/2022 14:14:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=361
06/12/2022 14:14:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=363
06/12/2022 14:14:22 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=364
06/12/2022 14:14:25 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.10 on epoch=366
06/12/2022 14:14:28 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.2892920317577852 on epoch=366
06/12/2022 14:14:30 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.16 on epoch=368
06/12/2022 14:14:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=369
06/12/2022 14:14:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=371
06/12/2022 14:14:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=373
06/12/2022 14:14:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=374
06/12/2022 14:14:44 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.3034064164498947 on epoch=374
06/12/2022 14:14:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=376
06/12/2022 14:14:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=378
06/12/2022 14:14:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=379
06/12/2022 14:14:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=381
06/12/2022 14:14:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=383
06/12/2022 14:15:01 - INFO - __main__ - Global step 2300 Train loss 0.08 Classification-F1 0.2103989703989704 on epoch=383
06/12/2022 14:15:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=384
06/12/2022 14:15:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=386
06/12/2022 14:15:09 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=388
06/12/2022 14:15:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=389
06/12/2022 14:15:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=391
06/12/2022 14:15:17 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.3633023735810113 on epoch=391
06/12/2022 14:15:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3476529160739687 -> 0.3633023735810113 on epoch=391, global_step=2350
06/12/2022 14:15:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=393
06/12/2022 14:15:23 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=394
06/12/2022 14:15:26 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=396
06/12/2022 14:15:28 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=398
06/12/2022 14:15:31 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=399
06/12/2022 14:15:34 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.22933604336043362 on epoch=399
06/12/2022 14:15:37 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=401
06/12/2022 14:15:40 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=403
06/12/2022 14:15:42 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=404
06/12/2022 14:15:45 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=406
06/12/2022 14:15:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=408
06/12/2022 14:15:51 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.29384289538574293 on epoch=408
06/12/2022 14:15:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=409
06/12/2022 14:15:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=411
06/12/2022 14:15:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=413
06/12/2022 14:16:02 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=414
06/12/2022 14:16:05 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=416
06/12/2022 14:16:08 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.328963938341724 on epoch=416
06/12/2022 14:16:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=418
06/12/2022 14:16:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=419
06/12/2022 14:16:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=421
06/12/2022 14:16:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=423
06/12/2022 14:16:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=424
06/12/2022 14:16:24 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.2609627883461555 on epoch=424
06/12/2022 14:16:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=426
06/12/2022 14:16:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=428
06/12/2022 14:16:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=429
06/12/2022 14:16:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=431
06/12/2022 14:16:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=433
06/12/2022 14:16:41 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.3611229971948842 on epoch=433
06/12/2022 14:16:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=434
06/12/2022 14:16:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=436
06/12/2022 14:16:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=438
06/12/2022 14:16:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=439
06/12/2022 14:16:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=441
06/12/2022 14:16:58 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.1868336314847943 on epoch=441
06/12/2022 14:17:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=443
06/12/2022 14:17:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=444
06/12/2022 14:17:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=446
06/12/2022 14:17:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=448
06/12/2022 14:17:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=449
06/12/2022 14:17:15 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.16332595053525287 on epoch=449
06/12/2022 14:17:18 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=451
06/12/2022 14:17:21 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=453
06/12/2022 14:17:24 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=454
06/12/2022 14:17:26 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=456
06/12/2022 14:17:29 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=458
06/12/2022 14:17:32 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.23351416515973478 on epoch=458
06/12/2022 14:17:35 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=459
06/12/2022 14:17:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=461
06/12/2022 14:17:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=463
06/12/2022 14:17:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=464
06/12/2022 14:17:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=466
06/12/2022 14:17:49 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.21419402966414253 on epoch=466
06/12/2022 14:17:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=468
06/12/2022 14:17:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=469
06/12/2022 14:17:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=471
06/12/2022 14:18:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=473
06/12/2022 14:18:03 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=474
06/12/2022 14:18:06 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.21458058849363199 on epoch=474
06/12/2022 14:18:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=476
06/12/2022 14:18:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=478
06/12/2022 14:18:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=479
06/12/2022 14:18:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=481
06/12/2022 14:18:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=483
06/12/2022 14:18:24 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.21047077922077922 on epoch=483
06/12/2022 14:18:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=484
06/12/2022 14:18:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=486
06/12/2022 14:18:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=488
06/12/2022 14:18:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=489
06/12/2022 14:18:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=491
06/12/2022 14:18:41 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.20199761478831246 on epoch=491
06/12/2022 14:18:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=493
06/12/2022 14:18:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=494
06/12/2022 14:18:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=496
06/12/2022 14:18:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=498
06/12/2022 14:18:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=499
06/12/2022 14:18:56 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:18:56 - INFO - __main__ - Printing 3 examples
06/12/2022 14:18:56 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/12/2022 14:18:56 - INFO - __main__ - ['contradiction']
06/12/2022 14:18:56 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/12/2022 14:18:56 - INFO - __main__ - ['contradiction']
06/12/2022 14:18:56 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/12/2022 14:18:56 - INFO - __main__ - ['contradiction']
06/12/2022 14:18:56 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:18:57 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:18:57 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 14:18:57 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:18:57 - INFO - __main__ - Printing 3 examples
06/12/2022 14:18:57 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
06/12/2022 14:18:57 - INFO - __main__ - ['contradiction']
06/12/2022 14:18:57 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
06/12/2022 14:18:57 - INFO - __main__ - ['contradiction']
06/12/2022 14:18:57 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
06/12/2022 14:18:57 - INFO - __main__ - ['contradiction']
06/12/2022 14:18:57 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:18:57 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:18:57 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 14:18:58 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.27298773390634795 on epoch=499
06/12/2022 14:18:58 - INFO - __main__ - save last model!
06/12/2022 14:18:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 14:18:58 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 14:18:58 - INFO - __main__ - Printing 3 examples
06/12/2022 14:18:58 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 14:18:58 - INFO - __main__ - ['contradiction']
06/12/2022 14:18:58 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 14:18:58 - INFO - __main__ - ['entailment']
06/12/2022 14:18:58 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 14:18:58 - INFO - __main__ - ['contradiction']
06/12/2022 14:18:58 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:18:59 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:19:00 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 14:19:12 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 14:19:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 14:19:13 - INFO - __main__ - Starting training!
06/12/2022 14:19:31 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_87_0.4_8_predictions.txt
06/12/2022 14:19:31 - INFO - __main__ - Classification-F1 on test data: 0.1088
06/12/2022 14:19:31 - INFO - __main__ - prefix=anli_32_87, lr=0.4, bsz=8, dev_performance=0.3633023735810113, test_performance=0.10878219670676219
06/12/2022 14:19:31 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.3, bsz=8 ...
06/12/2022 14:19:32 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:19:32 - INFO - __main__ - Printing 3 examples
06/12/2022 14:19:32 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/12/2022 14:19:32 - INFO - __main__ - ['contradiction']
06/12/2022 14:19:32 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/12/2022 14:19:32 - INFO - __main__ - ['contradiction']
06/12/2022 14:19:32 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/12/2022 14:19:32 - INFO - __main__ - ['contradiction']
06/12/2022 14:19:32 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:19:32 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:19:32 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 14:19:32 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:19:32 - INFO - __main__ - Printing 3 examples
06/12/2022 14:19:32 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
06/12/2022 14:19:32 - INFO - __main__ - ['contradiction']
06/12/2022 14:19:32 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
06/12/2022 14:19:32 - INFO - __main__ - ['contradiction']
06/12/2022 14:19:32 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
06/12/2022 14:19:32 - INFO - __main__ - ['contradiction']
06/12/2022 14:19:32 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:19:32 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:19:32 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 14:19:52 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 14:19:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 14:19:53 - INFO - __main__ - Starting training!
06/12/2022 14:19:57 - INFO - __main__ - Step 10 Global step 10 Train loss 0.97 on epoch=1
06/12/2022 14:20:00 - INFO - __main__ - Step 20 Global step 20 Train loss 0.54 on epoch=3
06/12/2022 14:20:03 - INFO - __main__ - Step 30 Global step 30 Train loss 0.62 on epoch=4
06/12/2022 14:20:05 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=6
06/12/2022 14:20:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=8
06/12/2022 14:20:11 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 14:20:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 14:20:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=9
06/12/2022 14:20:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=11
06/12/2022 14:20:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=13
06/12/2022 14:20:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=14
06/12/2022 14:20:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=16
06/12/2022 14:20:28 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=16
06/12/2022 14:20:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=18
06/12/2022 14:20:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=19
06/12/2022 14:20:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=21
06/12/2022 14:20:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=23
06/12/2022 14:20:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=24
06/12/2022 14:20:45 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 14:20:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=26
06/12/2022 14:20:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=28
06/12/2022 14:20:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=29
06/12/2022 14:20:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=31
06/12/2022 14:20:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=33
06/12/2022 14:21:01 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.21333333333333335 on epoch=33
06/12/2022 14:21:01 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.21333333333333335 on epoch=33, global_step=200
06/12/2022 14:21:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=34
06/12/2022 14:21:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=36
06/12/2022 14:21:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=38
06/12/2022 14:21:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.59 on epoch=39
06/12/2022 14:21:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=41
06/12/2022 14:21:18 - INFO - __main__ - Global step 250 Train loss 0.52 Classification-F1 0.18545454545454546 on epoch=41
06/12/2022 14:21:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=43
06/12/2022 14:21:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=44
06/12/2022 14:21:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=46
06/12/2022 14:21:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=48
06/12/2022 14:21:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=49
06/12/2022 14:21:35 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=49
06/12/2022 14:21:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=51
06/12/2022 14:21:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=53
06/12/2022 14:21:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=54
06/12/2022 14:21:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=56
06/12/2022 14:21:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=58
06/12/2022 14:21:51 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=58
06/12/2022 14:21:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=59
06/12/2022 14:21:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=61
06/12/2022 14:21:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=63
06/12/2022 14:22:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=64
06/12/2022 14:22:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=66
06/12/2022 14:22:06 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 14:22:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=68
06/12/2022 14:22:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=69
06/12/2022 14:22:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=71
06/12/2022 14:22:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=73
06/12/2022 14:22:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=74
06/12/2022 14:22:23 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.23214285714285718 on epoch=74
06/12/2022 14:22:24 - INFO - __main__ - Saving model with best Classification-F1: 0.21333333333333335 -> 0.23214285714285718 on epoch=74, global_step=450
06/12/2022 14:22:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=76
06/12/2022 14:22:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=78
06/12/2022 14:22:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=79
06/12/2022 14:22:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=81
06/12/2022 14:22:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=83
06/12/2022 14:22:40 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.2294895815590757 on epoch=83
06/12/2022 14:22:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.42 on epoch=84
06/12/2022 14:22:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=86
06/12/2022 14:22:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=88
06/12/2022 14:22:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=89
06/12/2022 14:22:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=91
06/12/2022 14:22:58 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.264828738512949 on epoch=91
06/12/2022 14:22:58 - INFO - __main__ - Saving model with best Classification-F1: 0.23214285714285718 -> 0.264828738512949 on epoch=91, global_step=550
06/12/2022 14:23:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=93
06/12/2022 14:23:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=94
06/12/2022 14:23:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=96
06/12/2022 14:23:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=98
06/12/2022 14:23:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=99
06/12/2022 14:23:15 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.16792717086834738 on epoch=99
06/12/2022 14:23:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=101
06/12/2022 14:23:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=103
06/12/2022 14:23:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=104
06/12/2022 14:23:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=106
06/12/2022 14:23:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=108
06/12/2022 14:23:31 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.21212121212121213 on epoch=108
06/12/2022 14:23:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=109
06/12/2022 14:23:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=111
06/12/2022 14:23:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=113
06/12/2022 14:23:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=114
06/12/2022 14:23:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=116
06/12/2022 14:23:46 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.2191919191919192 on epoch=116
06/12/2022 14:23:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=118
06/12/2022 14:23:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=119
06/12/2022 14:23:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=121
06/12/2022 14:23:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=123
06/12/2022 14:24:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=124
06/12/2022 14:24:02 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.18414611184334398 on epoch=124
06/12/2022 14:24:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=126
06/12/2022 14:24:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=128
06/12/2022 14:24:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=129
06/12/2022 14:24:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=131
06/12/2022 14:24:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=133
06/12/2022 14:24:19 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.29429322580007516 on epoch=133
06/12/2022 14:24:19 - INFO - __main__ - Saving model with best Classification-F1: 0.264828738512949 -> 0.29429322580007516 on epoch=133, global_step=800
06/12/2022 14:24:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=134
06/12/2022 14:24:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=136
06/12/2022 14:24:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=138
06/12/2022 14:24:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=139
06/12/2022 14:24:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=141
06/12/2022 14:24:35 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.25822510822510825 on epoch=141
06/12/2022 14:24:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=143
06/12/2022 14:24:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=144
06/12/2022 14:24:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=146
06/12/2022 14:24:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=148
06/12/2022 14:24:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=149
06/12/2022 14:24:51 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.17333333333333334 on epoch=149
06/12/2022 14:24:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.34 on epoch=151
06/12/2022 14:24:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=153
06/12/2022 14:24:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=154
06/12/2022 14:25:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=156
06/12/2022 14:25:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=158
06/12/2022 14:25:07 - INFO - __main__ - Global step 950 Train loss 0.35 Classification-F1 0.20066844919786098 on epoch=158
06/12/2022 14:25:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.34 on epoch=159
06/12/2022 14:25:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=161
06/12/2022 14:25:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=163
06/12/2022 14:25:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=164
06/12/2022 14:25:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=166
06/12/2022 14:25:23 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.2772404900064474 on epoch=166
06/12/2022 14:25:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=168
06/12/2022 14:25:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=169
06/12/2022 14:25:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=171
06/12/2022 14:25:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=173
06/12/2022 14:25:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=174
06/12/2022 14:25:39 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.19907657252503444 on epoch=174
06/12/2022 14:25:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=176
06/12/2022 14:25:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.32 on epoch=178
06/12/2022 14:25:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=179
06/12/2022 14:25:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=181
06/12/2022 14:25:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=183
06/12/2022 14:25:56 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.19646338067390698 on epoch=183
06/12/2022 14:25:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.30 on epoch=184
06/12/2022 14:26:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=186
06/12/2022 14:26:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=188
06/12/2022 14:26:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=189
06/12/2022 14:26:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=191
06/12/2022 14:26:12 - INFO - __main__ - Global step 1150 Train loss 0.29 Classification-F1 0.22354497354497357 on epoch=191
06/12/2022 14:26:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.27 on epoch=193
06/12/2022 14:26:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=194
06/12/2022 14:26:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=196
06/12/2022 14:26:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=198
06/12/2022 14:26:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=199
06/12/2022 14:26:28 - INFO - __main__ - Global step 1200 Train loss 0.30 Classification-F1 0.3321537219842305 on epoch=199
06/12/2022 14:26:28 - INFO - __main__ - Saving model with best Classification-F1: 0.29429322580007516 -> 0.3321537219842305 on epoch=199, global_step=1200
06/12/2022 14:26:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=201
06/12/2022 14:26:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=203
06/12/2022 14:26:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=204
06/12/2022 14:26:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=206
06/12/2022 14:26:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=208
06/12/2022 14:26:45 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.23499780318589134 on epoch=208
06/12/2022 14:26:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=209
06/12/2022 14:26:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=211
06/12/2022 14:26:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=213
06/12/2022 14:26:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=214
06/12/2022 14:26:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=216
06/12/2022 14:27:01 - INFO - __main__ - Global step 1300 Train loss 0.27 Classification-F1 0.2892979872361316 on epoch=216
06/12/2022 14:27:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=218
06/12/2022 14:27:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=219
06/12/2022 14:27:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=221
06/12/2022 14:27:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=223
06/12/2022 14:27:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=224
06/12/2022 14:27:17 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.23584560093994056 on epoch=224
06/12/2022 14:27:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=226
06/12/2022 14:27:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=228
06/12/2022 14:27:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=229
06/12/2022 14:27:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=231
06/12/2022 14:27:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=233
06/12/2022 14:27:33 - INFO - __main__ - Global step 1400 Train loss 0.23 Classification-F1 0.2825228967991691 on epoch=233
06/12/2022 14:27:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=234
06/12/2022 14:27:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=236
06/12/2022 14:27:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=238
06/12/2022 14:27:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=239
06/12/2022 14:27:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=241
06/12/2022 14:27:49 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.26632553606237813 on epoch=241
06/12/2022 14:27:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=243
06/12/2022 14:27:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=244
06/12/2022 14:27:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=246
06/12/2022 14:28:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=248
06/12/2022 14:28:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=249
06/12/2022 14:28:06 - INFO - __main__ - Global step 1500 Train loss 0.22 Classification-F1 0.33266056436788144 on epoch=249
06/12/2022 14:28:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3321537219842305 -> 0.33266056436788144 on epoch=249, global_step=1500
06/12/2022 14:28:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.21 on epoch=251
06/12/2022 14:28:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=253
06/12/2022 14:28:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=254
06/12/2022 14:28:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=256
06/12/2022 14:28:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=258
06/12/2022 14:28:22 - INFO - __main__ - Global step 1550 Train loss 0.17 Classification-F1 0.2673992673992674 on epoch=258
06/12/2022 14:28:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=259
06/12/2022 14:28:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=261
06/12/2022 14:28:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=263
06/12/2022 14:28:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=264
06/12/2022 14:28:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=266
06/12/2022 14:28:38 - INFO - __main__ - Global step 1600 Train loss 0.18 Classification-F1 0.2773580661443589 on epoch=266
06/12/2022 14:28:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=268
06/12/2022 14:28:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.17 on epoch=269
06/12/2022 14:28:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=271
06/12/2022 14:28:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=273
06/12/2022 14:28:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=274
06/12/2022 14:28:54 - INFO - __main__ - Global step 1650 Train loss 0.17 Classification-F1 0.3319998785006986 on epoch=274
06/12/2022 14:28:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=276
06/12/2022 14:29:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=278
06/12/2022 14:29:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=279
06/12/2022 14:29:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=281
06/12/2022 14:29:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=283
06/12/2022 14:29:11 - INFO - __main__ - Global step 1700 Train loss 0.14 Classification-F1 0.2919838238987175 on epoch=283
06/12/2022 14:29:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=284
06/12/2022 14:29:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=286
06/12/2022 14:29:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=288
06/12/2022 14:29:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=289
06/12/2022 14:29:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=291
06/12/2022 14:29:27 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.22702197288452547 on epoch=291
06/12/2022 14:29:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=293
06/12/2022 14:29:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=294
06/12/2022 14:29:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=296
06/12/2022 14:29:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=298
06/12/2022 14:29:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=299
06/12/2022 14:29:43 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.1790379757255828 on epoch=299
06/12/2022 14:29:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=301
06/12/2022 14:29:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=303
06/12/2022 14:29:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=304
06/12/2022 14:29:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=306
06/12/2022 14:29:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=308
06/12/2022 14:30:00 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.22366818873668187 on epoch=308
06/12/2022 14:30:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=309
06/12/2022 14:30:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=311
06/12/2022 14:30:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=313
06/12/2022 14:30:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=314
06/12/2022 14:30:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=316
06/12/2022 14:30:16 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.2910477322242029 on epoch=316
06/12/2022 14:30:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.15 on epoch=318
06/12/2022 14:30:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=319
06/12/2022 14:30:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=321
06/12/2022 14:30:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=323
06/12/2022 14:30:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=324
06/12/2022 14:30:32 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.30437146316751923 on epoch=324
06/12/2022 14:30:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=326
06/12/2022 14:30:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.15 on epoch=328
06/12/2022 14:30:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=329
06/12/2022 14:30:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=331
06/12/2022 14:30:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=333
06/12/2022 14:30:49 - INFO - __main__ - Global step 2000 Train loss 0.10 Classification-F1 0.313203323431917 on epoch=333
06/12/2022 14:30:51 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=334
06/12/2022 14:30:54 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=336
06/12/2022 14:30:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=338
06/12/2022 14:31:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=339
06/12/2022 14:31:02 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=341
06/12/2022 14:31:05 - INFO - __main__ - Global step 2050 Train loss 0.10 Classification-F1 0.2802579365079365 on epoch=341
06/12/2022 14:31:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=343
06/12/2022 14:31:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=344
06/12/2022 14:31:13 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=346
06/12/2022 14:31:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=348
06/12/2022 14:31:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.12 on epoch=349
06/12/2022 14:31:22 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.22915064102564103 on epoch=349
06/12/2022 14:31:24 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=351
06/12/2022 14:31:27 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=353
06/12/2022 14:31:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=354
06/12/2022 14:31:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.10 on epoch=356
06/12/2022 14:31:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=358
06/12/2022 14:31:38 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.20909877490020257 on epoch=358
06/12/2022 14:31:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=359
06/12/2022 14:31:43 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=361
06/12/2022 14:31:46 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=363
06/12/2022 14:31:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=364
06/12/2022 14:31:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=366
06/12/2022 14:31:55 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.3276612517716519 on epoch=366
06/12/2022 14:31:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=368
06/12/2022 14:32:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=369
06/12/2022 14:32:03 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=371
06/12/2022 14:32:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=373
06/12/2022 14:32:08 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=374
06/12/2022 14:32:11 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.24173502665156907 on epoch=374
06/12/2022 14:32:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=376
06/12/2022 14:32:17 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=378
06/12/2022 14:32:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=379
06/12/2022 14:32:22 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=381
06/12/2022 14:32:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=383
06/12/2022 14:32:28 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.17677383504008892 on epoch=383
06/12/2022 14:32:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.10 on epoch=384
06/12/2022 14:32:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=386
06/12/2022 14:32:36 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=388
06/12/2022 14:32:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=389
06/12/2022 14:32:41 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=391
06/12/2022 14:32:44 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.21300797869110605 on epoch=391
06/12/2022 14:32:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=393
06/12/2022 14:32:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=394
06/12/2022 14:32:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=396
06/12/2022 14:32:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=398
06/12/2022 14:32:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.08 on epoch=399
06/12/2022 14:33:01 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.26654761904761903 on epoch=399
06/12/2022 14:33:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=401
06/12/2022 14:33:07 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=403
06/12/2022 14:33:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=404
06/12/2022 14:33:12 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=406
06/12/2022 14:33:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=408
06/12/2022 14:33:18 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.20599415204678362 on epoch=408
06/12/2022 14:33:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=409
06/12/2022 14:33:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=411
06/12/2022 14:33:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=413
06/12/2022 14:33:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=414
06/12/2022 14:33:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=416
06/12/2022 14:33:35 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.24925399803448584 on epoch=416
06/12/2022 14:33:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=418
06/12/2022 14:33:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=419
06/12/2022 14:33:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=421
06/12/2022 14:33:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=423
06/12/2022 14:33:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=424
06/12/2022 14:33:52 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.18971448228414173 on epoch=424
06/12/2022 14:33:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=426
06/12/2022 14:33:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=428
06/12/2022 14:34:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=429
06/12/2022 14:34:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=431
06/12/2022 14:34:06 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=433
06/12/2022 14:34:09 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.2101998243302591 on epoch=433
06/12/2022 14:34:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=434
06/12/2022 14:34:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=436
06/12/2022 14:34:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=438
06/12/2022 14:34:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=439
06/12/2022 14:34:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=441
06/12/2022 14:34:26 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.2333175505050505 on epoch=441
06/12/2022 14:34:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=443
06/12/2022 14:34:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=444
06/12/2022 14:34:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=446
06/12/2022 14:34:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=448
06/12/2022 14:34:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=449
06/12/2022 14:34:43 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.1948262361721978 on epoch=449
06/12/2022 14:34:45 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=451
06/12/2022 14:34:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=453
06/12/2022 14:34:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=454
06/12/2022 14:34:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=456
06/12/2022 14:34:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=458
06/12/2022 14:34:59 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.33267932637283065 on epoch=458
06/12/2022 14:34:59 - INFO - __main__ - Saving model with best Classification-F1: 0.33266056436788144 -> 0.33267932637283065 on epoch=458, global_step=2750
06/12/2022 14:35:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=459
06/12/2022 14:35:05 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=461
06/12/2022 14:35:08 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=463
06/12/2022 14:35:11 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=464
06/12/2022 14:35:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=466
06/12/2022 14:35:16 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.3204151043907636 on epoch=466
06/12/2022 14:35:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=468
06/12/2022 14:35:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=469
06/12/2022 14:35:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=471
06/12/2022 14:35:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=473
06/12/2022 14:35:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=474
06/12/2022 14:35:33 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.23100802854594113 on epoch=474
06/12/2022 14:35:36 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=476
06/12/2022 14:35:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=478
06/12/2022 14:35:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=479
06/12/2022 14:35:44 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=481
06/12/2022 14:35:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=483
06/12/2022 14:35:50 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.2227383764359114 on epoch=483
06/12/2022 14:35:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=484
06/12/2022 14:35:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=486
06/12/2022 14:35:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=488
06/12/2022 14:36:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=489
06/12/2022 14:36:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=491
06/12/2022 14:36:07 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.22973410770020936 on epoch=491
06/12/2022 14:36:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=493
06/12/2022 14:36:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=494
06/12/2022 14:36:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=496
06/12/2022 14:36:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=498
06/12/2022 14:36:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=499
06/12/2022 14:36:22 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:36:22 - INFO - __main__ - Printing 3 examples
06/12/2022 14:36:22 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/12/2022 14:36:22 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:22 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/12/2022 14:36:22 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:22 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/12/2022 14:36:22 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:22 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:36:22 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:36:22 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 14:36:22 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:36:22 - INFO - __main__ - Printing 3 examples
06/12/2022 14:36:22 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
06/12/2022 14:36:22 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:22 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
06/12/2022 14:36:22 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:22 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
06/12/2022 14:36:22 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:22 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:36:22 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:36:23 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 14:36:24 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.32042594136231267 on epoch=499
06/12/2022 14:36:24 - INFO - __main__ - save last model!
06/12/2022 14:36:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 14:36:24 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 14:36:24 - INFO - __main__ - Printing 3 examples
06/12/2022 14:36:24 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 14:36:24 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:24 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 14:36:24 - INFO - __main__ - ['entailment']
06/12/2022 14:36:24 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 14:36:24 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:24 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:36:24 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:36:25 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 14:36:39 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 14:36:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 14:36:40 - INFO - __main__ - Starting training!
06/12/2022 14:36:56 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_87_0.3_8_predictions.txt
06/12/2022 14:36:56 - INFO - __main__ - Classification-F1 on test data: 0.1952
06/12/2022 14:36:57 - INFO - __main__ - prefix=anli_32_87, lr=0.3, bsz=8, dev_performance=0.33267932637283065, test_performance=0.1951622764443545
06/12/2022 14:36:57 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.2, bsz=8 ...
06/12/2022 14:36:58 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:36:58 - INFO - __main__ - Printing 3 examples
06/12/2022 14:36:58 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/12/2022 14:36:58 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:58 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/12/2022 14:36:58 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:58 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/12/2022 14:36:58 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:58 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:36:58 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:36:58 - INFO - __main__ - Loaded 96 examples from train data
06/12/2022 14:36:58 - INFO - __main__ - Start tokenizing ... 96 instances
06/12/2022 14:36:58 - INFO - __main__ - Printing 3 examples
06/12/2022 14:36:58 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
06/12/2022 14:36:58 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:58 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
06/12/2022 14:36:58 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:58 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
06/12/2022 14:36:58 - INFO - __main__ - ['contradiction']
06/12/2022 14:36:58 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:36:58 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:36:58 - INFO - __main__ - Loaded 96 examples from dev data
06/12/2022 14:37:17 - INFO - __main__ - load prompt embedding from ckpt
06/12/2022 14:37:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/12/2022 14:37:17 - INFO - __main__ - Starting training!
06/12/2022 14:37:21 - INFO - __main__ - Step 10 Global step 10 Train loss 1.14 on epoch=1
06/12/2022 14:37:24 - INFO - __main__ - Step 20 Global step 20 Train loss 0.66 on epoch=3
06/12/2022 14:37:27 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=4
06/12/2022 14:37:29 - INFO - __main__ - Step 40 Global step 40 Train loss 0.59 on epoch=6
06/12/2022 14:37:32 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=8
06/12/2022 14:37:34 - INFO - __main__ - Global step 50 Train loss 0.71 Classification-F1 0.16666666666666666 on epoch=8
06/12/2022 14:37:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
06/12/2022 14:37:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.56 on epoch=9
06/12/2022 14:37:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=11
06/12/2022 14:37:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.53 on epoch=13
06/12/2022 14:37:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=14
06/12/2022 14:37:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=16
06/12/2022 14:37:51 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.2014652014652015 on epoch=16
06/12/2022 14:37:51 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2014652014652015 on epoch=16, global_step=100
06/12/2022 14:37:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=18
06/12/2022 14:37:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=19
06/12/2022 14:38:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=21
06/12/2022 14:38:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=23
06/12/2022 14:38:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=24
06/12/2022 14:38:08 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=24
06/12/2022 14:38:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=26
06/12/2022 14:38:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=28
06/12/2022 14:38:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=29
06/12/2022 14:38:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=31
06/12/2022 14:38:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=33
06/12/2022 14:38:26 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.19396764851310308 on epoch=33
06/12/2022 14:38:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=34
06/12/2022 14:38:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=36
06/12/2022 14:38:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=38
06/12/2022 14:38:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=39
06/12/2022 14:38:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=41
06/12/2022 14:38:43 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.27429635882061876 on epoch=41
06/12/2022 14:38:43 - INFO - __main__ - Saving model with best Classification-F1: 0.2014652014652015 -> 0.27429635882061876 on epoch=41, global_step=250
06/12/2022 14:38:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=43
06/12/2022 14:38:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=44
06/12/2022 14:38:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=46
06/12/2022 14:38:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=48
06/12/2022 14:38:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=49
06/12/2022 14:38:59 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.1836290071584189 on epoch=49
06/12/2022 14:39:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=51
06/12/2022 14:39:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=53
06/12/2022 14:39:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=54
06/12/2022 14:39:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=56
06/12/2022 14:39:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=58
06/12/2022 14:39:16 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.15300546448087435 on epoch=58
06/12/2022 14:39:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=59
06/12/2022 14:39:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=61
06/12/2022 14:39:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=63
06/12/2022 14:39:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=64
06/12/2022 14:39:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=66
06/12/2022 14:39:33 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
06/12/2022 14:39:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
06/12/2022 14:39:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=69
06/12/2022 14:39:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=71
06/12/2022 14:39:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=73
06/12/2022 14:39:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=74
06/12/2022 14:39:50 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.22431431619908362 on epoch=74
06/12/2022 14:39:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=76
06/12/2022 14:39:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=78
06/12/2022 14:39:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=79
06/12/2022 14:40:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=81
06/12/2022 14:40:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=83
06/12/2022 14:40:06 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.21825396825396826 on epoch=83
06/12/2022 14:40:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=84
06/12/2022 14:40:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=86
06/12/2022 14:40:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=88
06/12/2022 14:40:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=89
06/12/2022 14:40:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=91
06/12/2022 14:40:23 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=91
06/12/2022 14:40:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=93
06/12/2022 14:40:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=94
06/12/2022 14:40:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=96
06/12/2022 14:40:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=98
06/12/2022 14:40:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=99
06/12/2022 14:40:40 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.1627656712402475 on epoch=99
06/12/2022 14:40:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=101
06/12/2022 14:40:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=103
06/12/2022 14:40:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=104
06/12/2022 14:40:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=106
06/12/2022 14:40:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=108
06/12/2022 14:40:56 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.19143916413886083 on epoch=108
06/12/2022 14:40:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=109
06/12/2022 14:41:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=111
06/12/2022 14:41:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=113
06/12/2022 14:41:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=114
06/12/2022 14:41:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=116
06/12/2022 14:41:13 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.31500304519302974 on epoch=116
06/12/2022 14:41:13 - INFO - __main__ - Saving model with best Classification-F1: 0.27429635882061876 -> 0.31500304519302974 on epoch=116, global_step=700
06/12/2022 14:41:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=118
06/12/2022 14:41:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=119
06/12/2022 14:41:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=121
06/12/2022 14:41:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=123
06/12/2022 14:41:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=124
06/12/2022 14:41:30 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.17222222222222225 on epoch=124
06/12/2022 14:41:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=126
06/12/2022 14:41:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=128
06/12/2022 14:41:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=129
06/12/2022 14:41:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=131
06/12/2022 14:41:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=133
06/12/2022 14:41:47 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.18603736479842672 on epoch=133
06/12/2022 14:41:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=134
06/12/2022 14:41:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=136
06/12/2022 14:41:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=138
06/12/2022 14:41:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=139
06/12/2022 14:42:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=141
06/12/2022 14:42:04 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.19022423784328546 on epoch=141
06/12/2022 14:42:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=143
06/12/2022 14:42:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=144
06/12/2022 14:42:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=146
06/12/2022 14:42:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=148
06/12/2022 14:42:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=149
06/12/2022 14:42:20 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.16844116844116844 on epoch=149
06/12/2022 14:42:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=151
06/12/2022 14:42:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=153
06/12/2022 14:42:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=154
06/12/2022 14:42:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=156
06/12/2022 14:42:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=158
06/12/2022 14:42:37 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.19004581685809327 on epoch=158
06/12/2022 14:42:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=159
06/12/2022 14:42:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=161
06/12/2022 14:42:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=163
06/12/2022 14:42:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=164
06/12/2022 14:42:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=166
06/12/2022 14:42:54 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.3420047031158142 on epoch=166
06/12/2022 14:42:55 - INFO - __main__ - Saving model with best Classification-F1: 0.31500304519302974 -> 0.3420047031158142 on epoch=166, global_step=1000
06/12/2022 14:42:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=168
06/12/2022 14:43:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=169
06/12/2022 14:43:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=171
06/12/2022 14:43:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=173
06/12/2022 14:43:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=174
06/12/2022 14:43:11 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.190404984423676 on epoch=174
06/12/2022 14:43:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=176
06/12/2022 14:43:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=178
06/12/2022 14:43:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=179
06/12/2022 14:43:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=181
06/12/2022 14:43:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.33 on epoch=183
06/12/2022 14:43:28 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.21001356517449746 on epoch=183
06/12/2022 14:43:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=184
06/12/2022 14:43:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=186
06/12/2022 14:43:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=188
06/12/2022 14:43:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=189
06/12/2022 14:43:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=191
06/12/2022 14:43:44 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.3531625028976979 on epoch=191
06/12/2022 14:43:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3420047031158142 -> 0.3531625028976979 on epoch=191, global_step=1150
06/12/2022 14:43:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=193
06/12/2022 14:43:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=194
06/12/2022 14:43:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=196
06/12/2022 14:43:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=198
06/12/2022 14:43:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.28 on epoch=199
06/12/2022 14:44:01 - INFO - __main__ - Global step 1200 Train loss 0.30 Classification-F1 0.260952380952381 on epoch=199
06/12/2022 14:44:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=201
06/12/2022 14:44:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=203
06/12/2022 14:44:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=204
06/12/2022 14:44:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=206
06/12/2022 14:44:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=208
06/12/2022 14:44:18 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.23094419131004496 on epoch=208
06/12/2022 14:44:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=209
06/12/2022 14:44:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=211
06/12/2022 14:44:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.30 on epoch=213
06/12/2022 14:44:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=214
06/12/2022 14:44:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=216
06/12/2022 14:44:34 - INFO - __main__ - Global step 1300 Train loss 0.28 Classification-F1 0.36473773615817745 on epoch=216
06/12/2022 14:44:34 - INFO - __main__ - Saving model with best Classification-F1: 0.3531625028976979 -> 0.36473773615817745 on epoch=216, global_step=1300
06/12/2022 14:44:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=218
06/12/2022 14:44:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=219
06/12/2022 14:44:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=221
06/12/2022 14:44:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=223
06/12/2022 14:44:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=224
06/12/2022 14:44:51 - INFO - __main__ - Global step 1350 Train loss 0.27 Classification-F1 0.21707165109034268 on epoch=224
06/12/2022 14:44:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=226
06/12/2022 14:44:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=228
06/12/2022 14:44:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=229
06/12/2022 14:45:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=231
06/12/2022 14:45:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=233
06/12/2022 14:45:08 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.2617091632436901 on epoch=233
06/12/2022 14:45:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=234
06/12/2022 14:45:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=236
06/12/2022 14:45:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=238
06/12/2022 14:45:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=239
06/12/2022 14:45:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=241
06/12/2022 14:45:24 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.2966905901116427 on epoch=241
06/12/2022 14:45:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=243
06/12/2022 14:45:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=244
06/12/2022 14:45:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=246
06/12/2022 14:45:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=248
06/12/2022 14:45:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=249
06/12/2022 14:45:41 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.2862918164977662 on epoch=249
06/12/2022 14:45:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=251
06/12/2022 14:45:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.24 on epoch=253
06/12/2022 14:45:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=254
06/12/2022 14:45:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=256
06/12/2022 14:45:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=258
06/12/2022 14:45:57 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.313151324359531 on epoch=258
06/12/2022 14:46:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=259
06/12/2022 14:46:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.23 on epoch=261
06/12/2022 14:46:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=263
06/12/2022 14:46:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=264
06/12/2022 14:46:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=266
06/12/2022 14:46:13 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.22842778136895783 on epoch=266
06/12/2022 14:46:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=268
06/12/2022 14:46:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=269
06/12/2022 14:46:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=271
06/12/2022 14:46:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=273
06/12/2022 14:46:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=274
06/12/2022 14:46:30 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.2196825396825397 on epoch=274
06/12/2022 14:46:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=276
06/12/2022 14:46:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=278
06/12/2022 14:46:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=279
06/12/2022 14:46:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=281
06/12/2022 14:46:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=283
06/12/2022 14:46:46 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.3045986177565125 on epoch=283
06/12/2022 14:46:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=284
06/12/2022 14:46:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.17 on epoch=286
06/12/2022 14:46:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=288
06/12/2022 14:46:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=289
06/12/2022 14:47:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=291
06/12/2022 14:47:03 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.3214638895685507 on epoch=291
06/12/2022 14:47:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=293
06/12/2022 14:47:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=294
06/12/2022 14:47:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=296
06/12/2022 14:47:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=298
06/12/2022 14:47:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=299
06/12/2022 14:47:19 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.25302941924290695 on epoch=299
06/12/2022 14:47:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=301
06/12/2022 14:47:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=303
06/12/2022 14:47:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=304
06/12/2022 14:47:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=306
06/12/2022 14:47:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=308
06/12/2022 14:47:36 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.29181199752628323 on epoch=308
06/12/2022 14:47:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.17 on epoch=309
06/12/2022 14:47:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=311
06/12/2022 14:47:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=313
06/12/2022 14:47:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=314
06/12/2022 14:47:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.15 on epoch=316
06/12/2022 14:47:53 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.2556723207004825 on epoch=316
06/12/2022 14:47:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=318
06/12/2022 14:47:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=319
06/12/2022 14:48:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.15 on epoch=321
06/12/2022 14:48:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=323
06/12/2022 14:48:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=324
06/12/2022 14:48:09 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.337584803256445 on epoch=324
06/12/2022 14:48:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=326
06/12/2022 14:48:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=328
06/12/2022 14:48:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=329
06/12/2022 14:48:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=331
06/12/2022 14:48:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=333
06/12/2022 14:48:26 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.32022960699431285 on epoch=333
06/12/2022 14:48:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=334
06/12/2022 14:48:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=336
06/12/2022 14:48:34 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=338
06/12/2022 14:48:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=339
06/12/2022 14:48:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=341
06/12/2022 14:48:42 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.2945349669487601 on epoch=341
06/12/2022 14:48:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.14 on epoch=343
06/12/2022 14:48:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.15 on epoch=344
06/12/2022 14:48:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=346
06/12/2022 14:48:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=348
06/12/2022 14:48:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.12 on epoch=349
06/12/2022 14:48:59 - INFO - __main__ - Global step 2100 Train loss 0.11 Classification-F1 0.3558739796262565 on epoch=349
06/12/2022 14:49:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.13 on epoch=351
06/12/2022 14:49:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=353
06/12/2022 14:49:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=354
06/12/2022 14:49:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=356
06/12/2022 14:49:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=358
06/12/2022 14:49:16 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.1901754385964912 on epoch=358
06/12/2022 14:49:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=359
06/12/2022 14:49:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=361
06/12/2022 14:49:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.11 on epoch=363
06/12/2022 14:49:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=364
06/12/2022 14:49:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=366
06/12/2022 14:49:32 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.25396825396825395 on epoch=366
06/12/2022 14:49:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=368
06/12/2022 14:49:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.09 on epoch=369
06/12/2022 14:49:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.12 on epoch=371
06/12/2022 14:49:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=373
06/12/2022 14:49:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.13 on epoch=374
06/12/2022 14:49:49 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.23752234993614302 on epoch=374
06/12/2022 14:49:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.10 on epoch=376
06/12/2022 14:49:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=378
06/12/2022 14:49:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.11 on epoch=379
06/12/2022 14:50:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=381
06/12/2022 14:50:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=383
06/12/2022 14:50:05 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.31783111567478145 on epoch=383
06/12/2022 14:50:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.10 on epoch=384
06/12/2022 14:50:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=386
06/12/2022 14:50:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.09 on epoch=388
06/12/2022 14:50:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.10 on epoch=389
06/12/2022 14:50:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=391
06/12/2022 14:50:22 - INFO - __main__ - Global step 2350 Train loss 0.10 Classification-F1 0.14270916407703801 on epoch=391
06/12/2022 14:50:25 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=393
06/12/2022 14:50:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=394
06/12/2022 14:50:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=396
06/12/2022 14:50:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=398
06/12/2022 14:50:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=399
06/12/2022 14:50:39 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.28248089359200473 on epoch=399
06/12/2022 14:50:41 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=401
06/12/2022 14:50:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.07 on epoch=403
06/12/2022 14:50:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=404
06/12/2022 14:50:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=406
06/12/2022 14:50:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=408
06/12/2022 14:50:55 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.30157533098709566 on epoch=408
06/12/2022 14:50:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=409
06/12/2022 14:51:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=411
06/12/2022 14:51:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=413
06/12/2022 14:51:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.17 on epoch=414
06/12/2022 14:51:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=416
06/12/2022 14:51:11 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.3317320394879062 on epoch=416
06/12/2022 14:51:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=418
06/12/2022 14:51:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=419
06/12/2022 14:51:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=421
06/12/2022 14:51:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.11 on epoch=423
06/12/2022 14:51:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=424
06/12/2022 14:51:28 - INFO - __main__ - Global step 2550 Train loss 0.08 Classification-F1 0.18682432432432433 on epoch=424
06/12/2022 14:51:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=426
06/12/2022 14:51:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=428
06/12/2022 14:51:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=429
06/12/2022 14:51:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=431
06/12/2022 14:51:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.08 on epoch=433
06/12/2022 14:51:45 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.2617094017094017 on epoch=433
06/12/2022 14:51:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=434
06/12/2022 14:51:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=436
06/12/2022 14:51:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=438
06/12/2022 14:51:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=439
06/12/2022 14:51:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=441
06/12/2022 14:52:02 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.2639169472502806 on epoch=441
06/12/2022 14:52:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=443
06/12/2022 14:52:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=444
06/12/2022 14:52:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=446
06/12/2022 14:52:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=448
06/12/2022 14:52:16 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=449
06/12/2022 14:52:19 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.24110286720321933 on epoch=449
06/12/2022 14:52:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=451
06/12/2022 14:52:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=453
06/12/2022 14:52:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=454
06/12/2022 14:52:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=456
06/12/2022 14:52:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=458
06/12/2022 14:52:35 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.29085987764869153 on epoch=458
06/12/2022 14:52:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=459
06/12/2022 14:52:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=461
06/12/2022 14:52:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=463
06/12/2022 14:52:46 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=464
06/12/2022 14:52:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=466
06/12/2022 14:52:52 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.14382837501481568 on epoch=466
06/12/2022 14:52:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=468
06/12/2022 14:52:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=469
06/12/2022 14:53:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=471
06/12/2022 14:53:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=473
06/12/2022 14:53:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=474
06/12/2022 14:53:09 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.2725376763294137 on epoch=474
06/12/2022 14:53:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=476
06/12/2022 14:53:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=478
06/12/2022 14:53:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=479
06/12/2022 14:53:20 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=481
06/12/2022 14:53:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=483
06/12/2022 14:53:25 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.23998731568098935 on epoch=483
06/12/2022 14:53:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=484
06/12/2022 14:53:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=486
06/12/2022 14:53:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=488
06/12/2022 14:53:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.10 on epoch=489
06/12/2022 14:53:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=491
06/12/2022 14:53:42 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.2322209653092006 on epoch=491
06/12/2022 14:53:45 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=493
06/12/2022 14:53:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=494
06/12/2022 14:53:50 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=496
06/12/2022 14:53:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=498
06/12/2022 14:53:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=499
06/12/2022 14:53:59 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.17937033968516986 on epoch=499
06/12/2022 14:53:59 - INFO - __main__ - save last model!
06/12/2022 14:53:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/12/2022 14:53:59 - INFO - __main__ - Start tokenizing ... 1000 instances
06/12/2022 14:53:59 - INFO - __main__ - Printing 3 examples
06/12/2022 14:53:59 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/12/2022 14:53:59 - INFO - __main__ - ['contradiction']
06/12/2022 14:53:59 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/12/2022 14:53:59 - INFO - __main__ - ['entailment']
06/12/2022 14:53:59 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/12/2022 14:53:59 - INFO - __main__ - ['contradiction']
06/12/2022 14:53:59 - INFO - __main__ - Tokenizing Input ...
06/12/2022 14:53:59 - INFO - __main__ - Tokenizing Output ...
06/12/2022 14:54:01 - INFO - __main__ - Loaded 1000 examples from test data
06/12/2022 14:54:32 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down32shot/singletask-anli/anli_32_87_0.2_8_predictions.txt
06/12/2022 14:54:32 - INFO - __main__ - Classification-F1 on test data: 0.1220
06/12/2022 14:54:32 - INFO - __main__ - prefix=anli_32_87, lr=0.2, bsz=8, dev_performance=0.36473773615817745, test_performance=0.12197243507316557
