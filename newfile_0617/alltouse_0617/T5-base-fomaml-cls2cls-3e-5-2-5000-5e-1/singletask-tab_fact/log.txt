05/17/2022 01:19:17 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-base-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
05/17/2022 01:19:17 - INFO - __main__ - models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact
05/17/2022 01:19:17 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-base-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
05/17/2022 01:19:17 - INFO - __main__ - models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact
05/17/2022 01:19:21 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/17/2022 01:19:21 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/17/2022 01:19:21 - INFO - __main__ - args.device: cuda:0
05/17/2022 01:19:21 - INFO - __main__ - Using 2 gpus
05/17/2022 01:19:21 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
05/17/2022 01:19:21 - INFO - __main__ - args.device: cuda:1
05/17/2022 01:19:21 - INFO - __main__ - Using 2 gpus
05/17/2022 01:19:21 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
05/17/2022 01:19:25 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.5, bsz=8 ...
05/17/2022 01:19:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:19:26 - INFO - __main__ - Printing 3 examples
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:19:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:19:26 - INFO - __main__ - Printing 3 examples
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:19:26 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:19:26 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:19:26 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 01:19:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:19:26 - INFO - __main__ - Printing 3 examples
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:19:26 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 01:19:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:19:26 - INFO - __main__ - Printing 3 examples
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/17/2022 01:19:26 - INFO - __main__ - ['refuted']
05/17/2022 01:19:26 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:19:27 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:19:27 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:19:27 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 01:19:27 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 01:19:32 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 01:19:32 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 01:19:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 01:19:33 - INFO - __main__ - Starting training!
05/17/2022 01:19:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 01:19:38 - INFO - __main__ - Starting training!
05/17/2022 01:19:40 - INFO - __main__ - Step 10 Global step 10 Train loss 5.01 on epoch=4
05/17/2022 01:19:42 - INFO - __main__ - Step 20 Global step 20 Train loss 4.89 on epoch=9
05/17/2022 01:19:44 - INFO - __main__ - Step 30 Global step 30 Train loss 4.82 on epoch=14
05/17/2022 01:19:46 - INFO - __main__ - Step 40 Global step 40 Train loss 4.63 on epoch=19
05/17/2022 01:19:48 - INFO - __main__ - Step 50 Global step 50 Train loss 4.47 on epoch=24
05/17/2022 01:19:49 - INFO - __main__ - Global step 50 Train loss 4.76 Classification-F1 0.0 on epoch=24
05/17/2022 01:19:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 01:19:51 - INFO - __main__ - Step 60 Global step 60 Train loss 4.52 on epoch=29
05/17/2022 01:19:53 - INFO - __main__ - Step 70 Global step 70 Train loss 4.22 on epoch=34
05/17/2022 01:19:55 - INFO - __main__ - Step 80 Global step 80 Train loss 4.14 on epoch=39
05/17/2022 01:19:57 - INFO - __main__ - Step 90 Global step 90 Train loss 4.04 on epoch=44
05/17/2022 01:19:59 - INFO - __main__ - Step 100 Global step 100 Train loss 3.88 on epoch=49
05/17/2022 01:20:01 - INFO - __main__ - Global step 100 Train loss 4.16 Classification-F1 0.0 on epoch=49
05/17/2022 01:20:03 - INFO - __main__ - Step 110 Global step 110 Train loss 3.72 on epoch=54
05/17/2022 01:20:05 - INFO - __main__ - Step 120 Global step 120 Train loss 3.64 on epoch=59
05/17/2022 01:20:07 - INFO - __main__ - Step 130 Global step 130 Train loss 3.54 on epoch=64
05/17/2022 01:20:09 - INFO - __main__ - Step 140 Global step 140 Train loss 3.41 on epoch=69
05/17/2022 01:20:11 - INFO - __main__ - Step 150 Global step 150 Train loss 3.34 on epoch=74
05/17/2022 01:20:14 - INFO - __main__ - Global step 150 Train loss 3.53 Classification-F1 0.21276595744680848 on epoch=74
05/17/2022 01:20:14 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.21276595744680848 on epoch=74, global_step=150
05/17/2022 01:20:16 - INFO - __main__ - Step 160 Global step 160 Train loss 3.17 on epoch=79
05/17/2022 01:20:18 - INFO - __main__ - Step 170 Global step 170 Train loss 2.98 on epoch=84
05/17/2022 01:20:19 - INFO - __main__ - Step 180 Global step 180 Train loss 2.84 on epoch=89
05/17/2022 01:20:21 - INFO - __main__ - Step 190 Global step 190 Train loss 2.77 on epoch=94
05/17/2022 01:20:23 - INFO - __main__ - Step 200 Global step 200 Train loss 2.64 on epoch=99
05/17/2022 01:20:27 - INFO - __main__ - Global step 200 Train loss 2.88 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 01:20:27 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.3333333333333333 on epoch=99, global_step=200
05/17/2022 01:20:29 - INFO - __main__ - Step 210 Global step 210 Train loss 2.52 on epoch=104
05/17/2022 01:20:31 - INFO - __main__ - Step 220 Global step 220 Train loss 2.41 on epoch=109
05/17/2022 01:20:32 - INFO - __main__ - Step 230 Global step 230 Train loss 2.36 on epoch=114
05/17/2022 01:20:34 - INFO - __main__ - Step 240 Global step 240 Train loss 2.26 on epoch=119
05/17/2022 01:20:36 - INFO - __main__ - Step 250 Global step 250 Train loss 2.11 on epoch=124
05/17/2022 01:20:40 - INFO - __main__ - Global step 250 Train loss 2.33 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 01:20:42 - INFO - __main__ - Step 260 Global step 260 Train loss 2.13 on epoch=129
05/17/2022 01:20:44 - INFO - __main__ - Step 270 Global step 270 Train loss 1.91 on epoch=134
05/17/2022 01:20:46 - INFO - __main__ - Step 280 Global step 280 Train loss 1.90 on epoch=139
05/17/2022 01:20:48 - INFO - __main__ - Step 290 Global step 290 Train loss 1.84 on epoch=144
05/17/2022 01:20:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.67 on epoch=149
05/17/2022 01:20:53 - INFO - __main__ - Global step 300 Train loss 1.89 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 01:20:55 - INFO - __main__ - Step 310 Global step 310 Train loss 1.73 on epoch=154
05/17/2022 01:20:57 - INFO - __main__ - Step 320 Global step 320 Train loss 1.55 on epoch=159
05/17/2022 01:20:59 - INFO - __main__ - Step 330 Global step 330 Train loss 1.60 on epoch=164
05/17/2022 01:21:00 - INFO - __main__ - Step 340 Global step 340 Train loss 1.55 on epoch=169
05/17/2022 01:21:02 - INFO - __main__ - Step 350 Global step 350 Train loss 1.38 on epoch=174
05/17/2022 01:21:05 - INFO - __main__ - Global step 350 Train loss 1.56 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 01:21:07 - INFO - __main__ - Step 360 Global step 360 Train loss 1.35 on epoch=179
05/17/2022 01:21:09 - INFO - __main__ - Step 370 Global step 370 Train loss 1.26 on epoch=184
05/17/2022 01:21:11 - INFO - __main__ - Step 380 Global step 380 Train loss 1.30 on epoch=189
05/17/2022 01:21:13 - INFO - __main__ - Step 390 Global step 390 Train loss 1.13 on epoch=194
05/17/2022 01:21:15 - INFO - __main__ - Step 400 Global step 400 Train loss 1.07 on epoch=199
05/17/2022 01:21:16 - INFO - __main__ - Global step 400 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 01:21:17 - INFO - __main__ - Step 410 Global step 410 Train loss 1.04 on epoch=204
05/17/2022 01:21:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.98 on epoch=209
05/17/2022 01:21:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.95 on epoch=214
05/17/2022 01:21:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=219
05/17/2022 01:21:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.83 on epoch=224
05/17/2022 01:21:27 - INFO - __main__ - Global step 450 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 01:21:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.88 on epoch=229
05/17/2022 01:21:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.87 on epoch=234
05/17/2022 01:21:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.79 on epoch=239
05/17/2022 01:21:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.73 on epoch=244
05/17/2022 01:21:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=249
05/17/2022 01:21:38 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 01:21:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.62 on epoch=254
05/17/2022 01:21:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=259
05/17/2022 01:21:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.68 on epoch=264
05/17/2022 01:21:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.67 on epoch=269
05/17/2022 01:21:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.63 on epoch=274
05/17/2022 01:21:48 - INFO - __main__ - Global step 550 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 01:21:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=279
05/17/2022 01:21:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.58 on epoch=284
05/17/2022 01:21:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=289
05/17/2022 01:21:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.60 on epoch=294
05/17/2022 01:21:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=299
05/17/2022 01:21:58 - INFO - __main__ - Global step 600 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 01:22:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.54 on epoch=304
05/17/2022 01:22:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=309
05/17/2022 01:22:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=314
05/17/2022 01:22:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=319
05/17/2022 01:22:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=324
05/17/2022 01:22:08 - INFO - __main__ - Global step 650 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 01:22:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=329
05/17/2022 01:22:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=334
05/17/2022 01:22:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=339
05/17/2022 01:22:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=344
05/17/2022 01:22:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=349
05/17/2022 01:22:18 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 01:22:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=354
05/17/2022 01:22:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=359
05/17/2022 01:22:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=364
05/17/2022 01:22:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=369
05/17/2022 01:22:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=374
05/17/2022 01:22:29 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 01:22:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=379
05/17/2022 01:22:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=384
05/17/2022 01:22:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=389
05/17/2022 01:22:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=394
05/17/2022 01:22:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=399
05/17/2022 01:22:39 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 01:22:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=404
05/17/2022 01:22:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=409
05/17/2022 01:22:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
05/17/2022 01:22:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=419
05/17/2022 01:22:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=424
05/17/2022 01:22:49 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 01:22:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=429
05/17/2022 01:22:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=434
05/17/2022 01:22:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=439
05/17/2022 01:22:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
05/17/2022 01:22:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=449
05/17/2022 01:22:59 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 01:23:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=454
05/17/2022 01:23:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=459
05/17/2022 01:23:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=464
05/17/2022 01:23:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=469
05/17/2022 01:23:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=474
05/17/2022 01:23:09 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 01:23:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=479
05/17/2022 01:23:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=484
05/17/2022 01:23:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=489
05/17/2022 01:23:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=494
05/17/2022 01:23:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
05/17/2022 01:23:19 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 01:23:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=504
05/17/2022 01:23:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=509
05/17/2022 01:23:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=514
05/17/2022 01:23:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=519
05/17/2022 01:23:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=524
05/17/2022 01:23:29 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 01:23:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=529
05/17/2022 01:23:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=534
05/17/2022 01:23:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=539
05/17/2022 01:23:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=544
05/17/2022 01:23:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=549
05/17/2022 01:23:39 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 01:23:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=554
05/17/2022 01:23:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=559
05/17/2022 01:23:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=564
05/17/2022 01:23:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=569
05/17/2022 01:23:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=574
05/17/2022 01:23:49 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 01:23:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=579
05/17/2022 01:23:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=584
05/17/2022 01:23:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
05/17/2022 01:23:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=594
05/17/2022 01:23:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=599
05/17/2022 01:23:59 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 01:24:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
05/17/2022 01:24:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.31 on epoch=609
05/17/2022 01:24:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=614
05/17/2022 01:24:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.31 on epoch=619
05/17/2022 01:24:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=624
05/17/2022 01:24:09 - INFO - __main__ - Global step 1250 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 01:24:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=629
05/17/2022 01:24:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=634
05/17/2022 01:24:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=639
05/17/2022 01:24:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=644
05/17/2022 01:24:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=649
05/17/2022 01:24:19 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 01:24:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=654
05/17/2022 01:24:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=659
05/17/2022 01:24:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=664
05/17/2022 01:24:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=669
05/17/2022 01:24:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=674
05/17/2022 01:24:29 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 01:24:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=679
05/17/2022 01:24:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=684
05/17/2022 01:24:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=689
05/17/2022 01:24:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=694
05/17/2022 01:24:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=699
05/17/2022 01:24:39 - INFO - __main__ - Global step 1400 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 01:24:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=704
05/17/2022 01:24:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=709
05/17/2022 01:24:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=714
05/17/2022 01:24:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.28 on epoch=719
05/17/2022 01:24:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.27 on epoch=724
05/17/2022 01:24:49 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 01:24:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=729
05/17/2022 01:24:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=734
05/17/2022 01:24:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=739
05/17/2022 01:24:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=744
05/17/2022 01:24:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=749
05/17/2022 01:24:59 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 01:25:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=754
05/17/2022 01:25:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=759
05/17/2022 01:25:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=764
05/17/2022 01:25:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=769
05/17/2022 01:25:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=774
05/17/2022 01:25:09 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 01:25:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=779
05/17/2022 01:25:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=784
05/17/2022 01:25:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.30 on epoch=789
05/17/2022 01:25:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=794
05/17/2022 01:25:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=799
05/17/2022 01:25:20 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 01:25:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=804
05/17/2022 01:25:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=809
05/17/2022 01:25:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=814
05/17/2022 01:25:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=819
05/17/2022 01:25:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=824
05/17/2022 01:25:30 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 01:25:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=829
05/17/2022 01:25:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=834
05/17/2022 01:25:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=839
05/17/2022 01:25:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=844
05/17/2022 01:25:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=849
05/17/2022 01:25:40 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 01:25:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=854
05/17/2022 01:25:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=859
05/17/2022 01:25:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=864
05/17/2022 01:25:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=869
05/17/2022 01:25:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.30 on epoch=874
05/17/2022 01:25:50 - INFO - __main__ - Global step 1750 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 01:25:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=879
05/17/2022 01:25:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=884
05/17/2022 01:25:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=889
05/17/2022 01:25:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=894
05/17/2022 01:25:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=899
05/17/2022 01:26:00 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 01:26:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=904
05/17/2022 01:26:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=909
05/17/2022 01:26:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=914
05/17/2022 01:26:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.31 on epoch=919
05/17/2022 01:26:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=924
05/17/2022 01:26:10 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 01:26:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=929
05/17/2022 01:26:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=934
05/17/2022 01:26:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=939
05/17/2022 01:26:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.28 on epoch=944
05/17/2022 01:26:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=949
05/17/2022 01:26:20 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 01:26:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=954
05/17/2022 01:26:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=959
05/17/2022 01:26:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=964
05/17/2022 01:26:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=969
05/17/2022 01:26:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=974
05/17/2022 01:26:30 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 01:26:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=979
05/17/2022 01:26:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=984
05/17/2022 01:26:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
05/17/2022 01:26:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=994
05/17/2022 01:26:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=999
05/17/2022 01:26:40 - INFO - __main__ - Global step 2000 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 01:26:40 - INFO - __main__ - save last model!
05/17/2022 01:26:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 01:26:40 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 01:26:40 - INFO - __main__ - Printing 3 examples
05/17/2022 01:26:40 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 01:26:40 - INFO - __main__ - ['entailed']
05/17/2022 01:26:40 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 01:26:40 - INFO - __main__ - ['entailed']
05/17/2022 01:26:40 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 01:26:40 - INFO - __main__ - ['entailed']
05/17/2022 01:26:40 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:26:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:26:40 - INFO - __main__ - Printing 3 examples
05/17/2022 01:26:40 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/17/2022 01:26:40 - INFO - __main__ - ['refuted']
05/17/2022 01:26:40 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/17/2022 01:26:40 - INFO - __main__ - ['refuted']
05/17/2022 01:26:40 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/17/2022 01:26:40 - INFO - __main__ - ['refuted']
05/17/2022 01:26:40 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:26:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:26:41 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 01:26:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:26:41 - INFO - __main__ - Printing 3 examples
05/17/2022 01:26:41 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/17/2022 01:26:41 - INFO - __main__ - ['refuted']
05/17/2022 01:26:41 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/17/2022 01:26:41 - INFO - __main__ - ['refuted']
05/17/2022 01:26:41 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/17/2022 01:26:41 - INFO - __main__ - ['refuted']
05/17/2022 01:26:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:26:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:26:41 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 01:26:47 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 01:26:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 01:26:47 - INFO - __main__ - Starting training!
05/17/2022 01:27:04 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:27:16 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 01:31:25 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.5_8_predictions.txt
05/17/2022 01:31:25 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 01:31:25 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 01:31:25 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.4, bsz=8 ...
05/17/2022 01:31:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:31:26 - INFO - __main__ - Printing 3 examples
05/17/2022 01:31:26 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/17/2022 01:31:26 - INFO - __main__ - ['refuted']
05/17/2022 01:31:26 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/17/2022 01:31:26 - INFO - __main__ - ['refuted']
05/17/2022 01:31:26 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/17/2022 01:31:26 - INFO - __main__ - ['refuted']
05/17/2022 01:31:26 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:31:26 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:31:26 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 01:31:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:31:26 - INFO - __main__ - Printing 3 examples
05/17/2022 01:31:26 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/17/2022 01:31:26 - INFO - __main__ - ['refuted']
05/17/2022 01:31:26 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/17/2022 01:31:26 - INFO - __main__ - ['refuted']
05/17/2022 01:31:26 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/17/2022 01:31:26 - INFO - __main__ - ['refuted']
05/17/2022 01:31:26 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:31:26 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:31:26 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 01:31:32 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 01:31:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 01:31:33 - INFO - __main__ - Starting training!
05/17/2022 01:31:35 - INFO - __main__ - Step 10 Global step 10 Train loss 5.06 on epoch=4
05/17/2022 01:31:37 - INFO - __main__ - Step 20 Global step 20 Train loss 4.99 on epoch=9
05/17/2022 01:31:39 - INFO - __main__ - Step 30 Global step 30 Train loss 4.88 on epoch=14
05/17/2022 01:31:41 - INFO - __main__ - Step 40 Global step 40 Train loss 4.85 on epoch=19
05/17/2022 01:31:43 - INFO - __main__ - Step 50 Global step 50 Train loss 4.59 on epoch=24
05/17/2022 01:31:45 - INFO - __main__ - Global step 50 Train loss 4.88 Classification-F1 0.0 on epoch=24
05/17/2022 01:31:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 01:31:47 - INFO - __main__ - Step 60 Global step 60 Train loss 4.69 on epoch=29
05/17/2022 01:31:49 - INFO - __main__ - Step 70 Global step 70 Train loss 4.53 on epoch=34
05/17/2022 01:31:51 - INFO - __main__ - Step 80 Global step 80 Train loss 4.48 on epoch=39
05/17/2022 01:31:53 - INFO - __main__ - Step 90 Global step 90 Train loss 4.24 on epoch=44
05/17/2022 01:31:55 - INFO - __main__ - Step 100 Global step 100 Train loss 4.29 on epoch=49
05/17/2022 01:31:58 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/17/2022 01:32:00 - INFO - __main__ - Step 110 Global step 110 Train loss 4.08 on epoch=54
05/17/2022 01:32:02 - INFO - __main__ - Step 120 Global step 120 Train loss 4.21 on epoch=59
05/17/2022 01:32:03 - INFO - __main__ - Step 130 Global step 130 Train loss 3.97 on epoch=64
05/17/2022 01:32:05 - INFO - __main__ - Step 140 Global step 140 Train loss 3.87 on epoch=69
05/17/2022 01:32:07 - INFO - __main__ - Step 150 Global step 150 Train loss 3.72 on epoch=74
05/17/2022 01:32:11 - INFO - __main__ - Global step 150 Train loss 3.97 Classification-F1 0.0 on epoch=74
05/17/2022 01:32:13 - INFO - __main__ - Step 160 Global step 160 Train loss 3.66 on epoch=79
05/17/2022 01:32:15 - INFO - __main__ - Step 170 Global step 170 Train loss 3.57 on epoch=84
05/17/2022 01:32:17 - INFO - __main__ - Step 180 Global step 180 Train loss 3.59 on epoch=89
05/17/2022 01:32:19 - INFO - __main__ - Step 190 Global step 190 Train loss 3.51 on epoch=94
05/17/2022 01:32:21 - INFO - __main__ - Step 200 Global step 200 Train loss 3.35 on epoch=99
05/17/2022 01:32:26 - INFO - __main__ - Global step 200 Train loss 3.53 Classification-F1 0.07973421926910298 on epoch=99
05/17/2022 01:32:26 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.07973421926910298 on epoch=99, global_step=200
05/17/2022 01:32:28 - INFO - __main__ - Step 210 Global step 210 Train loss 3.34 on epoch=104
05/17/2022 01:32:30 - INFO - __main__ - Step 220 Global step 220 Train loss 3.29 on epoch=109
05/17/2022 01:32:32 - INFO - __main__ - Step 230 Global step 230 Train loss 3.04 on epoch=114
05/17/2022 01:32:33 - INFO - __main__ - Step 240 Global step 240 Train loss 3.09 on epoch=119
05/17/2022 01:32:35 - INFO - __main__ - Step 250 Global step 250 Train loss 2.88 on epoch=124
05/17/2022 01:32:39 - INFO - __main__ - Global step 250 Train loss 3.13 Classification-F1 0.1739130434782609 on epoch=124
05/17/2022 01:32:39 - INFO - __main__ - Saving model with best Classification-F1: 0.07973421926910298 -> 0.1739130434782609 on epoch=124, global_step=250
05/17/2022 01:32:41 - INFO - __main__ - Step 260 Global step 260 Train loss 2.89 on epoch=129
05/17/2022 01:32:42 - INFO - __main__ - Step 270 Global step 270 Train loss 2.84 on epoch=134
05/17/2022 01:32:44 - INFO - __main__ - Step 280 Global step 280 Train loss 2.83 on epoch=139
05/17/2022 01:32:46 - INFO - __main__ - Step 290 Global step 290 Train loss 2.72 on epoch=144
05/17/2022 01:32:48 - INFO - __main__ - Step 300 Global step 300 Train loss 2.50 on epoch=149
05/17/2022 01:32:51 - INFO - __main__ - Global step 300 Train loss 2.75 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 01:32:51 - INFO - __main__ - Saving model with best Classification-F1: 0.1739130434782609 -> 0.3333333333333333 on epoch=149, global_step=300
05/17/2022 01:32:53 - INFO - __main__ - Step 310 Global step 310 Train loss 2.59 on epoch=154
05/17/2022 01:32:55 - INFO - __main__ - Step 320 Global step 320 Train loss 2.30 on epoch=159
05/17/2022 01:32:57 - INFO - __main__ - Step 330 Global step 330 Train loss 2.41 on epoch=164
05/17/2022 01:32:59 - INFO - __main__ - Step 340 Global step 340 Train loss 2.39 on epoch=169
05/17/2022 01:33:01 - INFO - __main__ - Step 350 Global step 350 Train loss 2.35 on epoch=174
05/17/2022 01:33:05 - INFO - __main__ - Global step 350 Train loss 2.41 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 01:33:07 - INFO - __main__ - Step 360 Global step 360 Train loss 2.29 on epoch=179
05/17/2022 01:33:08 - INFO - __main__ - Step 370 Global step 370 Train loss 2.17 on epoch=184
05/17/2022 01:33:10 - INFO - __main__ - Step 380 Global step 380 Train loss 2.15 on epoch=189
05/17/2022 01:33:12 - INFO - __main__ - Step 390 Global step 390 Train loss 2.05 on epoch=194
05/17/2022 01:33:14 - INFO - __main__ - Step 400 Global step 400 Train loss 1.97 on epoch=199
05/17/2022 01:33:21 - INFO - __main__ - Global step 400 Train loss 2.13 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 01:33:23 - INFO - __main__ - Step 410 Global step 410 Train loss 2.00 on epoch=204
05/17/2022 01:33:25 - INFO - __main__ - Step 420 Global step 420 Train loss 1.99 on epoch=209
05/17/2022 01:33:27 - INFO - __main__ - Step 430 Global step 430 Train loss 2.04 on epoch=214
05/17/2022 01:33:29 - INFO - __main__ - Step 440 Global step 440 Train loss 1.99 on epoch=219
05/17/2022 01:33:30 - INFO - __main__ - Step 450 Global step 450 Train loss 1.96 on epoch=224
05/17/2022 01:33:37 - INFO - __main__ - Global step 450 Train loss 2.00 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 01:33:39 - INFO - __main__ - Step 460 Global step 460 Train loss 1.81 on epoch=229
05/17/2022 01:33:41 - INFO - __main__ - Step 470 Global step 470 Train loss 1.71 on epoch=234
05/17/2022 01:33:43 - INFO - __main__ - Step 480 Global step 480 Train loss 1.64 on epoch=239
05/17/2022 01:33:45 - INFO - __main__ - Step 490 Global step 490 Train loss 1.71 on epoch=244
05/17/2022 01:33:47 - INFO - __main__ - Step 500 Global step 500 Train loss 1.60 on epoch=249
05/17/2022 01:33:50 - INFO - __main__ - Global step 500 Train loss 1.69 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 01:33:52 - INFO - __main__ - Step 510 Global step 510 Train loss 1.59 on epoch=254
05/17/2022 01:33:54 - INFO - __main__ - Step 520 Global step 520 Train loss 1.53 on epoch=259
05/17/2022 01:33:56 - INFO - __main__ - Step 530 Global step 530 Train loss 1.49 on epoch=264
05/17/2022 01:33:57 - INFO - __main__ - Step 540 Global step 540 Train loss 1.47 on epoch=269
05/17/2022 01:33:59 - INFO - __main__ - Step 550 Global step 550 Train loss 1.48 on epoch=274
05/17/2022 01:34:02 - INFO - __main__ - Global step 550 Train loss 1.51 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 01:34:04 - INFO - __main__ - Step 560 Global step 560 Train loss 1.47 on epoch=279
05/17/2022 01:34:06 - INFO - __main__ - Step 570 Global step 570 Train loss 1.39 on epoch=284
05/17/2022 01:34:08 - INFO - __main__ - Step 580 Global step 580 Train loss 1.33 on epoch=289
05/17/2022 01:34:10 - INFO - __main__ - Step 590 Global step 590 Train loss 1.24 on epoch=294
05/17/2022 01:34:12 - INFO - __main__ - Step 600 Global step 600 Train loss 1.30 on epoch=299
05/17/2022 01:34:13 - INFO - __main__ - Global step 600 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 01:34:14 - INFO - __main__ - Step 610 Global step 610 Train loss 1.25 on epoch=304
05/17/2022 01:34:16 - INFO - __main__ - Step 620 Global step 620 Train loss 1.20 on epoch=309
05/17/2022 01:34:18 - INFO - __main__ - Step 630 Global step 630 Train loss 1.18 on epoch=314
05/17/2022 01:34:20 - INFO - __main__ - Step 640 Global step 640 Train loss 1.29 on epoch=319
05/17/2022 01:34:22 - INFO - __main__ - Step 650 Global step 650 Train loss 1.13 on epoch=324
05/17/2022 01:34:23 - INFO - __main__ - Global step 650 Train loss 1.21 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 01:34:25 - INFO - __main__ - Step 660 Global step 660 Train loss 1.14 on epoch=329
05/17/2022 01:34:27 - INFO - __main__ - Step 670 Global step 670 Train loss 1.08 on epoch=334
05/17/2022 01:34:29 - INFO - __main__ - Step 680 Global step 680 Train loss 1.15 on epoch=339
05/17/2022 01:34:31 - INFO - __main__ - Step 690 Global step 690 Train loss 1.15 on epoch=344
05/17/2022 01:34:33 - INFO - __main__ - Step 700 Global step 700 Train loss 1.08 on epoch=349
05/17/2022 01:34:33 - INFO - __main__ - Global step 700 Train loss 1.12 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 01:34:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.95 on epoch=354
05/17/2022 01:34:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.98 on epoch=359
05/17/2022 01:34:39 - INFO - __main__ - Step 730 Global step 730 Train loss 1.05 on epoch=364
05/17/2022 01:34:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.99 on epoch=369
05/17/2022 01:34:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.97 on epoch=374
05/17/2022 01:34:44 - INFO - __main__ - Global step 750 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 01:34:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.88 on epoch=379
05/17/2022 01:34:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=384
05/17/2022 01:34:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.91 on epoch=389
05/17/2022 01:34:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.93 on epoch=394
05/17/2022 01:34:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.92 on epoch=399
05/17/2022 01:34:54 - INFO - __main__ - Global step 800 Train loss 0.91 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 01:34:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=404
05/17/2022 01:34:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.88 on epoch=409
05/17/2022 01:35:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.85 on epoch=414
05/17/2022 01:35:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.81 on epoch=419
05/17/2022 01:35:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.85 on epoch=424
05/17/2022 01:35:05 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 01:35:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.74 on epoch=429
05/17/2022 01:35:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=434
05/17/2022 01:35:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.82 on epoch=439
05/17/2022 01:35:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.73 on epoch=444
05/17/2022 01:35:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.76 on epoch=449
05/17/2022 01:35:15 - INFO - __main__ - Global step 900 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 01:35:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.73 on epoch=454
05/17/2022 01:35:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.83 on epoch=459
05/17/2022 01:35:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.69 on epoch=464
05/17/2022 01:35:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.74 on epoch=469
05/17/2022 01:35:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.72 on epoch=474
05/17/2022 01:35:26 - INFO - __main__ - Global step 950 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 01:35:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.72 on epoch=479
05/17/2022 01:35:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.74 on epoch=484
05/17/2022 01:35:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.69 on epoch=489
05/17/2022 01:35:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.68 on epoch=494
05/17/2022 01:35:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.78 on epoch=499
05/17/2022 01:35:36 - INFO - __main__ - Global step 1000 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 01:35:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.62 on epoch=504
05/17/2022 01:35:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.78 on epoch=509
05/17/2022 01:35:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.77 on epoch=514
05/17/2022 01:35:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.62 on epoch=519
05/17/2022 01:35:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.66 on epoch=524
05/17/2022 01:35:46 - INFO - __main__ - Global step 1050 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 01:35:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.64 on epoch=529
05/17/2022 01:35:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.65 on epoch=534
05/17/2022 01:35:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.62 on epoch=539
05/17/2022 01:35:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.57 on epoch=544
05/17/2022 01:35:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.63 on epoch=549
05/17/2022 01:35:57 - INFO - __main__ - Global step 1100 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 01:35:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=554
05/17/2022 01:36:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.65 on epoch=559
05/17/2022 01:36:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.56 on epoch=564
05/17/2022 01:36:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.53 on epoch=569
05/17/2022 01:36:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.60 on epoch=574
05/17/2022 01:36:07 - INFO - __main__ - Global step 1150 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 01:36:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=579
05/17/2022 01:36:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.58 on epoch=584
05/17/2022 01:36:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=589
05/17/2022 01:36:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.63 on epoch=594
05/17/2022 01:36:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.53 on epoch=599
05/17/2022 01:36:18 - INFO - __main__ - Global step 1200 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 01:36:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.52 on epoch=604
05/17/2022 01:36:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=609
05/17/2022 01:36:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.53 on epoch=614
05/17/2022 01:36:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.58 on epoch=619
05/17/2022 01:36:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=624
05/17/2022 01:36:28 - INFO - __main__ - Global step 1250 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 01:36:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=629
05/17/2022 01:36:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.58 on epoch=634
05/17/2022 01:36:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=639
05/17/2022 01:36:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.57 on epoch=644
05/17/2022 01:36:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.54 on epoch=649
05/17/2022 01:36:39 - INFO - __main__ - Global step 1300 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 01:36:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=654
05/17/2022 01:36:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=659
05/17/2022 01:36:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.50 on epoch=664
05/17/2022 01:36:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.53 on epoch=669
05/17/2022 01:36:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=674
05/17/2022 01:36:49 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 01:36:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=679
05/17/2022 01:36:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.53 on epoch=684
05/17/2022 01:36:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=689
05/17/2022 01:36:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=694
05/17/2022 01:36:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=699
05/17/2022 01:36:59 - INFO - __main__ - Global step 1400 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 01:37:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=704
05/17/2022 01:37:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
05/17/2022 01:37:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=714
05/17/2022 01:37:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.53 on epoch=719
05/17/2022 01:37:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
05/17/2022 01:37:10 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 01:37:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=729
05/17/2022 01:37:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=734
05/17/2022 01:37:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/17/2022 01:37:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=744
05/17/2022 01:37:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=749
05/17/2022 01:37:20 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 01:37:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.47 on epoch=754
05/17/2022 01:37:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=759
05/17/2022 01:37:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=764
05/17/2022 01:37:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=769
05/17/2022 01:37:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=774
05/17/2022 01:37:30 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 01:37:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.47 on epoch=779
05/17/2022 01:37:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
05/17/2022 01:37:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=789
05/17/2022 01:37:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=794
05/17/2022 01:37:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=799
05/17/2022 01:37:40 - INFO - __main__ - Global step 1600 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 01:37:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=804
05/17/2022 01:37:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=809
05/17/2022 01:37:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=814
05/17/2022 01:37:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
05/17/2022 01:37:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=824
05/17/2022 01:37:51 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 01:37:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=829
05/17/2022 01:37:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=834
05/17/2022 01:37:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
05/17/2022 01:37:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=844
05/17/2022 01:38:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
05/17/2022 01:38:01 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 01:38:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=854
05/17/2022 01:38:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=859
05/17/2022 01:38:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/17/2022 01:38:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=869
05/17/2022 01:38:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=874
05/17/2022 01:38:11 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 01:38:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=879
05/17/2022 01:38:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=884
05/17/2022 01:38:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
05/17/2022 01:38:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=894
05/17/2022 01:38:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
05/17/2022 01:38:22 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 01:38:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=904
05/17/2022 01:38:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=909
05/17/2022 01:38:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=914
05/17/2022 01:38:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=919
05/17/2022 01:38:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=924
05/17/2022 01:38:32 - INFO - __main__ - Global step 1850 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 01:38:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
05/17/2022 01:38:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=934
05/17/2022 01:38:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=939
05/17/2022 01:38:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=944
05/17/2022 01:38:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/17/2022 01:38:42 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 01:38:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
05/17/2022 01:38:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
05/17/2022 01:38:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.34 on epoch=964
05/17/2022 01:38:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=969
05/17/2022 01:38:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=974
05/17/2022 01:38:53 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 01:38:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
05/17/2022 01:38:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/17/2022 01:38:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/17/2022 01:39:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=994
05/17/2022 01:39:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=999
05/17/2022 01:39:03 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 01:39:03 - INFO - __main__ - save last model!
05/17/2022 01:39:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 01:39:03 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 01:39:03 - INFO - __main__ - Printing 3 examples
05/17/2022 01:39:03 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 01:39:03 - INFO - __main__ - ['entailed']
05/17/2022 01:39:03 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 01:39:03 - INFO - __main__ - ['entailed']
05/17/2022 01:39:03 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 01:39:03 - INFO - __main__ - ['entailed']
05/17/2022 01:39:03 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:39:04 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:39:04 - INFO - __main__ - Printing 3 examples
05/17/2022 01:39:04 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/17/2022 01:39:04 - INFO - __main__ - ['refuted']
05/17/2022 01:39:04 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/17/2022 01:39:04 - INFO - __main__ - ['refuted']
05/17/2022 01:39:04 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/17/2022 01:39:04 - INFO - __main__ - ['refuted']
05/17/2022 01:39:04 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:39:04 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:39:04 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 01:39:04 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:39:04 - INFO - __main__ - Printing 3 examples
05/17/2022 01:39:04 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/17/2022 01:39:04 - INFO - __main__ - ['refuted']
05/17/2022 01:39:04 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/17/2022 01:39:04 - INFO - __main__ - ['refuted']
05/17/2022 01:39:04 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/17/2022 01:39:04 - INFO - __main__ - ['refuted']
05/17/2022 01:39:04 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:39:04 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:39:04 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 01:39:10 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 01:39:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 01:39:10 - INFO - __main__ - Starting training!
05/17/2022 01:39:28 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:39:41 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 01:43:43 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.4_8_predictions.txt
05/17/2022 01:43:43 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 01:43:43 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 01:43:43 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.3, bsz=8 ...
05/17/2022 01:43:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:43:44 - INFO - __main__ - Printing 3 examples
05/17/2022 01:43:44 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/17/2022 01:43:44 - INFO - __main__ - ['refuted']
05/17/2022 01:43:44 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/17/2022 01:43:44 - INFO - __main__ - ['refuted']
05/17/2022 01:43:44 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/17/2022 01:43:44 - INFO - __main__ - ['refuted']
05/17/2022 01:43:44 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:43:44 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:43:44 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 01:43:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:43:44 - INFO - __main__ - Printing 3 examples
05/17/2022 01:43:44 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/17/2022 01:43:44 - INFO - __main__ - ['refuted']
05/17/2022 01:43:44 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/17/2022 01:43:44 - INFO - __main__ - ['refuted']
05/17/2022 01:43:44 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/17/2022 01:43:44 - INFO - __main__ - ['refuted']
05/17/2022 01:43:44 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:43:44 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:43:44 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 01:43:50 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 01:43:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 01:43:51 - INFO - __main__ - Starting training!
05/17/2022 01:43:53 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/17/2022 01:43:55 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/17/2022 01:43:57 - INFO - __main__ - Step 30 Global step 30 Train loss 4.98 on epoch=14
05/17/2022 01:43:58 - INFO - __main__ - Step 40 Global step 40 Train loss 4.81 on epoch=19
05/17/2022 01:44:00 - INFO - __main__ - Step 50 Global step 50 Train loss 4.73 on epoch=24
05/17/2022 01:44:02 - INFO - __main__ - Global step 50 Train loss 4.90 Classification-F1 0.0 on epoch=24
05/17/2022 01:44:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 01:44:04 - INFO - __main__ - Step 60 Global step 60 Train loss 4.69 on epoch=29
05/17/2022 01:44:06 - INFO - __main__ - Step 70 Global step 70 Train loss 4.64 on epoch=34
05/17/2022 01:44:08 - INFO - __main__ - Step 80 Global step 80 Train loss 4.54 on epoch=39
05/17/2022 01:44:09 - INFO - __main__ - Step 90 Global step 90 Train loss 4.52 on epoch=44
05/17/2022 01:44:11 - INFO - __main__ - Step 100 Global step 100 Train loss 4.46 on epoch=49
05/17/2022 01:44:12 - INFO - __main__ - Global step 100 Train loss 4.57 Classification-F1 0.0 on epoch=49
05/17/2022 01:44:14 - INFO - __main__ - Step 110 Global step 110 Train loss 4.43 on epoch=54
05/17/2022 01:44:16 - INFO - __main__ - Step 120 Global step 120 Train loss 4.40 on epoch=59
05/17/2022 01:44:18 - INFO - __main__ - Step 130 Global step 130 Train loss 4.23 on epoch=64
05/17/2022 01:44:20 - INFO - __main__ - Step 140 Global step 140 Train loss 4.11 on epoch=69
05/17/2022 01:44:22 - INFO - __main__ - Step 150 Global step 150 Train loss 4.04 on epoch=74
05/17/2022 01:44:23 - INFO - __main__ - Global step 150 Train loss 4.24 Classification-F1 0.0 on epoch=74
05/17/2022 01:44:25 - INFO - __main__ - Step 160 Global step 160 Train loss 4.00 on epoch=79
05/17/2022 01:44:27 - INFO - __main__ - Step 170 Global step 170 Train loss 3.86 on epoch=84
05/17/2022 01:44:29 - INFO - __main__ - Step 180 Global step 180 Train loss 3.70 on epoch=89
05/17/2022 01:44:31 - INFO - __main__ - Step 190 Global step 190 Train loss 3.73 on epoch=94
05/17/2022 01:44:32 - INFO - __main__ - Step 200 Global step 200 Train loss 3.67 on epoch=99
05/17/2022 01:44:34 - INFO - __main__ - Global step 200 Train loss 3.79 Classification-F1 0.0 on epoch=99
05/17/2022 01:44:36 - INFO - __main__ - Step 210 Global step 210 Train loss 3.64 on epoch=104
05/17/2022 01:44:38 - INFO - __main__ - Step 220 Global step 220 Train loss 3.49 on epoch=109
05/17/2022 01:44:40 - INFO - __main__ - Step 230 Global step 230 Train loss 3.41 on epoch=114
05/17/2022 01:44:42 - INFO - __main__ - Step 240 Global step 240 Train loss 3.38 on epoch=119
05/17/2022 01:44:43 - INFO - __main__ - Step 250 Global step 250 Train loss 3.28 on epoch=124
05/17/2022 01:44:45 - INFO - __main__ - Global step 250 Train loss 3.44 Classification-F1 0.16666666666666669 on epoch=124
05/17/2022 01:44:45 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.16666666666666669 on epoch=124, global_step=250
05/17/2022 01:44:47 - INFO - __main__ - Step 260 Global step 260 Train loss 3.10 on epoch=129
05/17/2022 01:44:49 - INFO - __main__ - Step 270 Global step 270 Train loss 3.12 on epoch=134
05/17/2022 01:44:50 - INFO - __main__ - Step 280 Global step 280 Train loss 3.04 on epoch=139
05/17/2022 01:44:52 - INFO - __main__ - Step 290 Global step 290 Train loss 2.97 on epoch=144
05/17/2022 01:44:54 - INFO - __main__ - Step 300 Global step 300 Train loss 2.84 on epoch=149
05/17/2022 01:44:57 - INFO - __main__ - Global step 300 Train loss 3.01 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 01:44:57 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666669 -> 0.3333333333333333 on epoch=149, global_step=300
05/17/2022 01:44:58 - INFO - __main__ - Step 310 Global step 310 Train loss 2.76 on epoch=154
05/17/2022 01:45:00 - INFO - __main__ - Step 320 Global step 320 Train loss 2.74 on epoch=159
05/17/2022 01:45:02 - INFO - __main__ - Step 330 Global step 330 Train loss 2.53 on epoch=164
05/17/2022 01:45:04 - INFO - __main__ - Step 340 Global step 340 Train loss 2.35 on epoch=169
05/17/2022 01:45:06 - INFO - __main__ - Step 350 Global step 350 Train loss 2.26 on epoch=174
05/17/2022 01:45:09 - INFO - __main__ - Global step 350 Train loss 2.53 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 01:45:10 - INFO - __main__ - Step 360 Global step 360 Train loss 2.25 on epoch=179
05/17/2022 01:45:12 - INFO - __main__ - Step 370 Global step 370 Train loss 2.33 on epoch=184
05/17/2022 01:45:14 - INFO - __main__ - Step 380 Global step 380 Train loss 2.10 on epoch=189
05/17/2022 01:45:16 - INFO - __main__ - Step 390 Global step 390 Train loss 2.02 on epoch=194
05/17/2022 01:45:18 - INFO - __main__ - Step 400 Global step 400 Train loss 1.97 on epoch=199
05/17/2022 01:45:21 - INFO - __main__ - Global step 400 Train loss 2.14 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 01:45:23 - INFO - __main__ - Step 410 Global step 410 Train loss 1.92 on epoch=204
05/17/2022 01:45:25 - INFO - __main__ - Step 420 Global step 420 Train loss 1.88 on epoch=209
05/17/2022 01:45:26 - INFO - __main__ - Step 430 Global step 430 Train loss 1.84 on epoch=214
05/17/2022 01:45:28 - INFO - __main__ - Step 440 Global step 440 Train loss 1.75 on epoch=219
05/17/2022 01:45:30 - INFO - __main__ - Step 450 Global step 450 Train loss 1.66 on epoch=224
05/17/2022 01:45:32 - INFO - __main__ - Global step 450 Train loss 1.81 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 01:45:34 - INFO - __main__ - Step 460 Global step 460 Train loss 1.61 on epoch=229
05/17/2022 01:45:36 - INFO - __main__ - Step 470 Global step 470 Train loss 1.55 on epoch=234
05/17/2022 01:45:37 - INFO - __main__ - Step 480 Global step 480 Train loss 1.51 on epoch=239
05/17/2022 01:45:39 - INFO - __main__ - Step 490 Global step 490 Train loss 1.53 on epoch=244
05/17/2022 01:45:41 - INFO - __main__ - Step 500 Global step 500 Train loss 1.37 on epoch=249
05/17/2022 01:45:43 - INFO - __main__ - Global step 500 Train loss 1.51 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 01:45:45 - INFO - __main__ - Step 510 Global step 510 Train loss 1.41 on epoch=254
05/17/2022 01:45:47 - INFO - __main__ - Step 520 Global step 520 Train loss 1.32 on epoch=259
05/17/2022 01:45:48 - INFO - __main__ - Step 530 Global step 530 Train loss 1.32 on epoch=264
05/17/2022 01:45:50 - INFO - __main__ - Step 540 Global step 540 Train loss 1.21 on epoch=269
05/17/2022 01:45:52 - INFO - __main__ - Step 550 Global step 550 Train loss 1.13 on epoch=274
05/17/2022 01:45:53 - INFO - __main__ - Global step 550 Train loss 1.28 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 01:45:55 - INFO - __main__ - Step 560 Global step 560 Train loss 1.17 on epoch=279
05/17/2022 01:45:57 - INFO - __main__ - Step 570 Global step 570 Train loss 1.10 on epoch=284
05/17/2022 01:45:59 - INFO - __main__ - Step 580 Global step 580 Train loss 1.06 on epoch=289
05/17/2022 01:46:00 - INFO - __main__ - Step 590 Global step 590 Train loss 1.13 on epoch=294
05/17/2022 01:46:02 - INFO - __main__ - Step 600 Global step 600 Train loss 1.13 on epoch=299
05/17/2022 01:46:03 - INFO - __main__ - Global step 600 Train loss 1.12 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 01:46:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.99 on epoch=304
05/17/2022 01:46:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.92 on epoch=309
05/17/2022 01:46:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.96 on epoch=314
05/17/2022 01:46:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.87 on epoch=319
05/17/2022 01:46:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.94 on epoch=324
05/17/2022 01:46:13 - INFO - __main__ - Global step 650 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 01:46:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.95 on epoch=329
05/17/2022 01:46:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.84 on epoch=334
05/17/2022 01:46:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.87 on epoch=339
05/17/2022 01:46:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.79 on epoch=344
05/17/2022 01:46:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.79 on epoch=349
05/17/2022 01:46:23 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 01:46:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.78 on epoch=354
05/17/2022 01:46:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.76 on epoch=359
05/17/2022 01:46:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.76 on epoch=364
05/17/2022 01:46:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.75 on epoch=369
05/17/2022 01:46:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.72 on epoch=374
05/17/2022 01:46:34 - INFO - __main__ - Global step 750 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 01:46:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.73 on epoch=379
05/17/2022 01:46:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=384
05/17/2022 01:46:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.71 on epoch=389
05/17/2022 01:46:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.58 on epoch=394
05/17/2022 01:46:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.63 on epoch=399
05/17/2022 01:46:44 - INFO - __main__ - Global step 800 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 01:46:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.68 on epoch=404
05/17/2022 01:46:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.69 on epoch=409
05/17/2022 01:46:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.71 on epoch=414
05/17/2022 01:46:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.65 on epoch=419
05/17/2022 01:46:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.69 on epoch=424
05/17/2022 01:46:54 - INFO - __main__ - Global step 850 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 01:46:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.55 on epoch=429
05/17/2022 01:46:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.57 on epoch=434
05/17/2022 01:47:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.56 on epoch=439
05/17/2022 01:47:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.59 on epoch=444
05/17/2022 01:47:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=449
05/17/2022 01:47:04 - INFO - __main__ - Global step 900 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 01:47:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.60 on epoch=454
05/17/2022 01:47:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.60 on epoch=459
05/17/2022 01:47:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.56 on epoch=464
05/17/2022 01:47:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.53 on epoch=469
05/17/2022 01:47:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.57 on epoch=474
05/17/2022 01:47:14 - INFO - __main__ - Global step 950 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 01:47:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.53 on epoch=479
05/17/2022 01:47:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=484
05/17/2022 01:47:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=489
05/17/2022 01:47:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=494
05/17/2022 01:47:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.55 on epoch=499
05/17/2022 01:47:25 - INFO - __main__ - Global step 1000 Train loss 0.51 Classification-F1 0.3992490613266583 on epoch=499
05/17/2022 01:47:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=499, global_step=1000
05/17/2022 01:47:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=504
05/17/2022 01:47:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.53 on epoch=509
05/17/2022 01:47:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=514
05/17/2022 01:47:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=519
05/17/2022 01:47:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=524
05/17/2022 01:47:35 - INFO - __main__ - Global step 1050 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 01:47:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.52 on epoch=529
05/17/2022 01:47:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.50 on epoch=534
05/17/2022 01:47:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.56 on epoch=539
05/17/2022 01:47:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=544
05/17/2022 01:47:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=549
05/17/2022 01:47:45 - INFO - __main__ - Global step 1100 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 01:47:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=554
05/17/2022 01:47:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=559
05/17/2022 01:47:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.49 on epoch=564
05/17/2022 01:47:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=569
05/17/2022 01:47:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=574
05/17/2022 01:47:55 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 01:47:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=579
05/17/2022 01:47:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=584
05/17/2022 01:48:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
05/17/2022 01:48:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.50 on epoch=594
05/17/2022 01:48:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=599
05/17/2022 01:48:05 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 01:48:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.48 on epoch=604
05/17/2022 01:48:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
05/17/2022 01:48:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=614
05/17/2022 01:48:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=619
05/17/2022 01:48:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=624
05/17/2022 01:48:16 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 01:48:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=629
05/17/2022 01:48:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.52 on epoch=634
05/17/2022 01:48:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=639
05/17/2022 01:48:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=644
05/17/2022 01:48:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.49 on epoch=649
05/17/2022 01:48:26 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 01:48:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=654
05/17/2022 01:48:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=659
05/17/2022 01:48:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
05/17/2022 01:48:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=669
05/17/2022 01:48:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=674
05/17/2022 01:48:36 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 01:48:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=679
05/17/2022 01:48:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
05/17/2022 01:48:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=689
05/17/2022 01:48:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=694
05/17/2022 01:48:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=699
05/17/2022 01:48:46 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 01:48:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=704
05/17/2022 01:48:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=709
05/17/2022 01:48:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
05/17/2022 01:48:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=719
05/17/2022 01:48:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
05/17/2022 01:48:56 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 01:48:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=729
05/17/2022 01:48:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=734
05/17/2022 01:49:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/17/2022 01:49:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
05/17/2022 01:49:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=749
05/17/2022 01:49:06 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 01:49:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=754
05/17/2022 01:49:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=759
05/17/2022 01:49:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=764
05/17/2022 01:49:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=769
05/17/2022 01:49:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=774
05/17/2022 01:49:16 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 01:49:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=779
05/17/2022 01:49:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=784
05/17/2022 01:49:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
05/17/2022 01:49:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=794
05/17/2022 01:49:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=799
05/17/2022 01:49:26 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 01:49:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=804
05/17/2022 01:49:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=809
05/17/2022 01:49:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=814
05/17/2022 01:49:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
05/17/2022 01:49:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=824
05/17/2022 01:49:36 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 01:49:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
05/17/2022 01:49:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=834
05/17/2022 01:49:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
05/17/2022 01:49:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
05/17/2022 01:49:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=849
05/17/2022 01:49:46 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.4589371980676329 on epoch=849
05/17/2022 01:49:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4589371980676329 on epoch=849, global_step=1700
05/17/2022 01:49:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/17/2022 01:49:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=859
05/17/2022 01:49:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=864
05/17/2022 01:49:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=869
05/17/2022 01:49:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
05/17/2022 01:49:56 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.4181818181818182 on epoch=874
05/17/2022 01:49:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=879
05/17/2022 01:50:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=884
05/17/2022 01:50:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=889
05/17/2022 01:50:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
05/17/2022 01:50:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=899
05/17/2022 01:50:06 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.3816425120772947 on epoch=899
05/17/2022 01:50:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=904
05/17/2022 01:50:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=909
05/17/2022 01:50:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=914
05/17/2022 01:50:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=919
05/17/2022 01:50:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
05/17/2022 01:50:16 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.4385964912280702 on epoch=924
05/17/2022 01:50:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=929
05/17/2022 01:50:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=934
05/17/2022 01:50:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=939
05/17/2022 01:50:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
05/17/2022 01:50:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=949
05/17/2022 01:50:26 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.46843853820598 on epoch=949
05/17/2022 01:50:26 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.46843853820598 on epoch=949, global_step=1900
05/17/2022 01:50:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
05/17/2022 01:50:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=959
05/17/2022 01:50:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
05/17/2022 01:50:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
05/17/2022 01:50:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/17/2022 01:50:36 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 01:50:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=979
05/17/2022 01:50:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=984
05/17/2022 01:50:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=989
05/17/2022 01:50:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.34 on epoch=994
05/17/2022 01:50:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=999
05/17/2022 01:50:46 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 01:50:46 - INFO - __main__ - save last model!
05/17/2022 01:50:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 01:50:46 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 01:50:46 - INFO - __main__ - Printing 3 examples
05/17/2022 01:50:46 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 01:50:46 - INFO - __main__ - ['entailed']
05/17/2022 01:50:46 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 01:50:46 - INFO - __main__ - ['entailed']
05/17/2022 01:50:46 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 01:50:46 - INFO - __main__ - ['entailed']
05/17/2022 01:50:46 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:50:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:50:47 - INFO - __main__ - Printing 3 examples
05/17/2022 01:50:47 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/17/2022 01:50:47 - INFO - __main__ - ['refuted']
05/17/2022 01:50:47 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/17/2022 01:50:47 - INFO - __main__ - ['refuted']
05/17/2022 01:50:47 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/17/2022 01:50:47 - INFO - __main__ - ['refuted']
05/17/2022 01:50:47 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:50:47 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:50:47 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 01:50:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:50:47 - INFO - __main__ - Printing 3 examples
05/17/2022 01:50:47 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/17/2022 01:50:47 - INFO - __main__ - ['refuted']
05/17/2022 01:50:47 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/17/2022 01:50:47 - INFO - __main__ - ['refuted']
05/17/2022 01:50:47 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/17/2022 01:50:47 - INFO - __main__ - ['refuted']
05/17/2022 01:50:47 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:50:47 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:50:47 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 01:50:53 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 01:50:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 01:50:53 - INFO - __main__ - Starting training!
05/17/2022 01:51:10 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:51:23 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 01:55:29 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.3_8_predictions.txt
05/17/2022 01:55:29 - INFO - __main__ - Classification-F1 on test data: 0.3312
05/17/2022 01:55:29 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.3, bsz=8, dev_performance=0.46843853820598, test_performance=0.3311539798652483
05/17/2022 01:55:29 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.2, bsz=8 ...
05/17/2022 01:55:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:55:30 - INFO - __main__ - Printing 3 examples
05/17/2022 01:55:30 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/17/2022 01:55:30 - INFO - __main__ - ['refuted']
05/17/2022 01:55:30 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/17/2022 01:55:30 - INFO - __main__ - ['refuted']
05/17/2022 01:55:30 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/17/2022 01:55:30 - INFO - __main__ - ['refuted']
05/17/2022 01:55:30 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:55:30 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:55:30 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 01:55:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 01:55:30 - INFO - __main__ - Printing 3 examples
05/17/2022 01:55:30 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/17/2022 01:55:30 - INFO - __main__ - ['refuted']
05/17/2022 01:55:30 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/17/2022 01:55:30 - INFO - __main__ - ['refuted']
05/17/2022 01:55:30 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/17/2022 01:55:30 - INFO - __main__ - ['refuted']
05/17/2022 01:55:30 - INFO - __main__ - Tokenizing Input ...
05/17/2022 01:55:30 - INFO - __main__ - Tokenizing Output ...
05/17/2022 01:55:30 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 01:55:36 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 01:55:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 01:55:36 - INFO - __main__ - Starting training!
05/17/2022 01:55:38 - INFO - __main__ - Step 10 Global step 10 Train loss 5.08 on epoch=4
05/17/2022 01:55:40 - INFO - __main__ - Step 20 Global step 20 Train loss 4.89 on epoch=9
05/17/2022 01:55:42 - INFO - __main__ - Step 30 Global step 30 Train loss 4.90 on epoch=14
05/17/2022 01:55:44 - INFO - __main__ - Step 40 Global step 40 Train loss 4.80 on epoch=19
05/17/2022 01:55:46 - INFO - __main__ - Step 50 Global step 50 Train loss 4.85 on epoch=24
05/17/2022 01:55:47 - INFO - __main__ - Global step 50 Train loss 4.90 Classification-F1 0.0 on epoch=24
05/17/2022 01:55:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 01:55:49 - INFO - __main__ - Step 60 Global step 60 Train loss 4.85 on epoch=29
05/17/2022 01:55:51 - INFO - __main__ - Step 70 Global step 70 Train loss 4.71 on epoch=34
05/17/2022 01:55:53 - INFO - __main__ - Step 80 Global step 80 Train loss 4.64 on epoch=39
05/17/2022 01:55:54 - INFO - __main__ - Step 90 Global step 90 Train loss 4.51 on epoch=44
05/17/2022 01:55:56 - INFO - __main__ - Step 100 Global step 100 Train loss 4.61 on epoch=49
05/17/2022 01:55:57 - INFO - __main__ - Global step 100 Train loss 4.66 Classification-F1 0.0 on epoch=49
05/17/2022 01:55:59 - INFO - __main__ - Step 110 Global step 110 Train loss 4.45 on epoch=54
05/17/2022 01:56:01 - INFO - __main__ - Step 120 Global step 120 Train loss 4.38 on epoch=59
05/17/2022 01:56:03 - INFO - __main__ - Step 130 Global step 130 Train loss 4.29 on epoch=64
05/17/2022 01:56:05 - INFO - __main__ - Step 140 Global step 140 Train loss 4.30 on epoch=69
05/17/2022 01:56:07 - INFO - __main__ - Step 150 Global step 150 Train loss 4.19 on epoch=74
05/17/2022 01:56:10 - INFO - __main__ - Global step 150 Train loss 4.32 Classification-F1 0.0 on epoch=74
05/17/2022 01:56:12 - INFO - __main__ - Step 160 Global step 160 Train loss 4.07 on epoch=79
05/17/2022 01:56:14 - INFO - __main__ - Step 170 Global step 170 Train loss 4.13 on epoch=84
05/17/2022 01:56:16 - INFO - __main__ - Step 180 Global step 180 Train loss 4.10 on epoch=89
05/17/2022 01:56:18 - INFO - __main__ - Step 190 Global step 190 Train loss 4.02 on epoch=94
05/17/2022 01:56:19 - INFO - __main__ - Step 200 Global step 200 Train loss 4.03 on epoch=99
05/17/2022 01:56:21 - INFO - __main__ - Global step 200 Train loss 4.07 Classification-F1 0.0 on epoch=99
05/17/2022 01:56:23 - INFO - __main__ - Step 210 Global step 210 Train loss 3.87 on epoch=104
05/17/2022 01:56:25 - INFO - __main__ - Step 220 Global step 220 Train loss 3.78 on epoch=109
05/17/2022 01:56:27 - INFO - __main__ - Step 230 Global step 230 Train loss 3.77 on epoch=114
05/17/2022 01:56:29 - INFO - __main__ - Step 240 Global step 240 Train loss 3.81 on epoch=119
05/17/2022 01:56:31 - INFO - __main__ - Step 250 Global step 250 Train loss 3.79 on epoch=124
05/17/2022 01:56:32 - INFO - __main__ - Global step 250 Train loss 3.80 Classification-F1 0.0 on epoch=124
05/17/2022 01:56:34 - INFO - __main__ - Step 260 Global step 260 Train loss 3.74 on epoch=129
05/17/2022 01:56:35 - INFO - __main__ - Step 270 Global step 270 Train loss 3.69 on epoch=134
05/17/2022 01:56:37 - INFO - __main__ - Step 280 Global step 280 Train loss 3.61 on epoch=139
05/17/2022 01:56:39 - INFO - __main__ - Step 290 Global step 290 Train loss 3.55 on epoch=144
05/17/2022 01:56:41 - INFO - __main__ - Step 300 Global step 300 Train loss 3.56 on epoch=149
05/17/2022 01:56:42 - INFO - __main__ - Global step 300 Train loss 3.63 Classification-F1 0.0 on epoch=149
05/17/2022 01:56:44 - INFO - __main__ - Step 310 Global step 310 Train loss 3.53 on epoch=154
05/17/2022 01:56:46 - INFO - __main__ - Step 320 Global step 320 Train loss 3.50 on epoch=159
05/17/2022 01:56:48 - INFO - __main__ - Step 330 Global step 330 Train loss 3.40 on epoch=164
05/17/2022 01:56:50 - INFO - __main__ - Step 340 Global step 340 Train loss 3.39 on epoch=169
05/17/2022 01:56:52 - INFO - __main__ - Step 350 Global step 350 Train loss 3.28 on epoch=174
05/17/2022 01:57:03 - INFO - __main__ - Global step 350 Train loss 3.42 Classification-F1 0.019047619047619046 on epoch=174
05/17/2022 01:57:03 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.019047619047619046 on epoch=174, global_step=350
05/17/2022 01:57:05 - INFO - __main__ - Step 360 Global step 360 Train loss 3.39 on epoch=179
05/17/2022 01:57:07 - INFO - __main__ - Step 370 Global step 370 Train loss 3.30 on epoch=184
05/17/2022 01:57:09 - INFO - __main__ - Step 380 Global step 380 Train loss 3.25 on epoch=189
05/17/2022 01:57:11 - INFO - __main__ - Step 390 Global step 390 Train loss 3.12 on epoch=194
05/17/2022 01:57:13 - INFO - __main__ - Step 400 Global step 400 Train loss 3.18 on epoch=199
05/17/2022 01:57:20 - INFO - __main__ - Global step 400 Train loss 3.25 Classification-F1 0.10077519379844961 on epoch=199
05/17/2022 01:57:20 - INFO - __main__ - Saving model with best Classification-F1: 0.019047619047619046 -> 0.10077519379844961 on epoch=199, global_step=400
05/17/2022 01:57:22 - INFO - __main__ - Step 410 Global step 410 Train loss 3.02 on epoch=204
05/17/2022 01:57:24 - INFO - __main__ - Step 420 Global step 420 Train loss 3.11 on epoch=209
05/17/2022 01:57:26 - INFO - __main__ - Step 430 Global step 430 Train loss 2.97 on epoch=214
05/17/2022 01:57:28 - INFO - __main__ - Step 440 Global step 440 Train loss 2.92 on epoch=219
05/17/2022 01:57:29 - INFO - __main__ - Step 450 Global step 450 Train loss 2.92 on epoch=224
05/17/2022 01:57:33 - INFO - __main__ - Global step 450 Train loss 2.99 Classification-F1 0.09677419354838708 on epoch=224
05/17/2022 01:57:35 - INFO - __main__ - Step 460 Global step 460 Train loss 2.84 on epoch=229
05/17/2022 01:57:36 - INFO - __main__ - Step 470 Global step 470 Train loss 2.84 on epoch=234
05/17/2022 01:57:38 - INFO - __main__ - Step 480 Global step 480 Train loss 2.77 on epoch=239
05/17/2022 01:57:40 - INFO - __main__ - Step 490 Global step 490 Train loss 2.70 on epoch=244
05/17/2022 01:57:42 - INFO - __main__ - Step 500 Global step 500 Train loss 2.73 on epoch=249
05/17/2022 01:57:45 - INFO - __main__ - Global step 500 Train loss 2.78 Classification-F1 0.1627906976744186 on epoch=249
05/17/2022 01:57:45 - INFO - __main__ - Saving model with best Classification-F1: 0.10077519379844961 -> 0.1627906976744186 on epoch=249, global_step=500
05/17/2022 01:57:47 - INFO - __main__ - Step 510 Global step 510 Train loss 2.57 on epoch=254
05/17/2022 01:57:49 - INFO - __main__ - Step 520 Global step 520 Train loss 2.52 on epoch=259
05/17/2022 01:57:51 - INFO - __main__ - Step 530 Global step 530 Train loss 2.47 on epoch=264
05/17/2022 01:57:53 - INFO - __main__ - Step 540 Global step 540 Train loss 2.53 on epoch=269
05/17/2022 01:57:55 - INFO - __main__ - Step 550 Global step 550 Train loss 2.37 on epoch=274
05/17/2022 01:57:58 - INFO - __main__ - Global step 550 Train loss 2.49 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 01:57:58 - INFO - __main__ - Saving model with best Classification-F1: 0.1627906976744186 -> 0.3333333333333333 on epoch=274, global_step=550
05/17/2022 01:58:00 - INFO - __main__ - Step 560 Global step 560 Train loss 2.39 on epoch=279
05/17/2022 01:58:02 - INFO - __main__ - Step 570 Global step 570 Train loss 2.23 on epoch=284
05/17/2022 01:58:04 - INFO - __main__ - Step 580 Global step 580 Train loss 2.35 on epoch=289
05/17/2022 01:58:05 - INFO - __main__ - Step 590 Global step 590 Train loss 2.20 on epoch=294
05/17/2022 01:58:07 - INFO - __main__ - Step 600 Global step 600 Train loss 2.25 on epoch=299
05/17/2022 01:58:10 - INFO - __main__ - Global step 600 Train loss 2.28 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 01:58:12 - INFO - __main__ - Step 610 Global step 610 Train loss 2.21 on epoch=304
05/17/2022 01:58:14 - INFO - __main__ - Step 620 Global step 620 Train loss 2.17 on epoch=309
05/17/2022 01:58:16 - INFO - __main__ - Step 630 Global step 630 Train loss 2.05 on epoch=314
05/17/2022 01:58:18 - INFO - __main__ - Step 640 Global step 640 Train loss 2.15 on epoch=319
05/17/2022 01:58:20 - INFO - __main__ - Step 650 Global step 650 Train loss 1.94 on epoch=324
05/17/2022 01:58:22 - INFO - __main__ - Global step 650 Train loss 2.10 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 01:58:24 - INFO - __main__ - Step 660 Global step 660 Train loss 1.86 on epoch=329
05/17/2022 01:58:26 - INFO - __main__ - Step 670 Global step 670 Train loss 1.91 on epoch=334
05/17/2022 01:58:28 - INFO - __main__ - Step 680 Global step 680 Train loss 1.90 on epoch=339
05/17/2022 01:58:30 - INFO - __main__ - Step 690 Global step 690 Train loss 1.85 on epoch=344
05/17/2022 01:58:32 - INFO - __main__ - Step 700 Global step 700 Train loss 1.86 on epoch=349
05/17/2022 01:58:34 - INFO - __main__ - Global step 700 Train loss 1.88 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 01:58:36 - INFO - __main__ - Step 710 Global step 710 Train loss 1.78 on epoch=354
05/17/2022 01:58:38 - INFO - __main__ - Step 720 Global step 720 Train loss 1.74 on epoch=359
05/17/2022 01:58:40 - INFO - __main__ - Step 730 Global step 730 Train loss 1.64 on epoch=364
05/17/2022 01:58:42 - INFO - __main__ - Step 740 Global step 740 Train loss 1.68 on epoch=369
05/17/2022 01:58:43 - INFO - __main__ - Step 750 Global step 750 Train loss 1.70 on epoch=374
05/17/2022 01:58:45 - INFO - __main__ - Global step 750 Train loss 1.71 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 01:58:47 - INFO - __main__ - Step 760 Global step 760 Train loss 1.58 on epoch=379
05/17/2022 01:58:49 - INFO - __main__ - Step 770 Global step 770 Train loss 1.58 on epoch=384
05/17/2022 01:58:51 - INFO - __main__ - Step 780 Global step 780 Train loss 1.53 on epoch=389
05/17/2022 01:58:53 - INFO - __main__ - Step 790 Global step 790 Train loss 1.54 on epoch=394
05/17/2022 01:58:55 - INFO - __main__ - Step 800 Global step 800 Train loss 1.53 on epoch=399
05/17/2022 01:58:55 - INFO - __main__ - Global step 800 Train loss 1.55 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 01:58:57 - INFO - __main__ - Step 810 Global step 810 Train loss 1.34 on epoch=404
05/17/2022 01:58:59 - INFO - __main__ - Step 820 Global step 820 Train loss 1.51 on epoch=409
05/17/2022 01:59:01 - INFO - __main__ - Step 830 Global step 830 Train loss 1.44 on epoch=414
05/17/2022 01:59:03 - INFO - __main__ - Step 840 Global step 840 Train loss 1.47 on epoch=419
05/17/2022 01:59:05 - INFO - __main__ - Step 850 Global step 850 Train loss 1.53 on epoch=424
05/17/2022 01:59:06 - INFO - __main__ - Global step 850 Train loss 1.46 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 01:59:08 - INFO - __main__ - Step 860 Global step 860 Train loss 1.37 on epoch=429
05/17/2022 01:59:10 - INFO - __main__ - Step 870 Global step 870 Train loss 1.39 on epoch=434
05/17/2022 01:59:12 - INFO - __main__ - Step 880 Global step 880 Train loss 1.44 on epoch=439
05/17/2022 01:59:14 - INFO - __main__ - Step 890 Global step 890 Train loss 1.29 on epoch=444
05/17/2022 01:59:15 - INFO - __main__ - Step 900 Global step 900 Train loss 1.30 on epoch=449
05/17/2022 01:59:16 - INFO - __main__ - Global step 900 Train loss 1.36 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 01:59:18 - INFO - __main__ - Step 910 Global step 910 Train loss 1.47 on epoch=454
05/17/2022 01:59:20 - INFO - __main__ - Step 920 Global step 920 Train loss 1.41 on epoch=459
05/17/2022 01:59:22 - INFO - __main__ - Step 930 Global step 930 Train loss 1.41 on epoch=464
05/17/2022 01:59:24 - INFO - __main__ - Step 940 Global step 940 Train loss 1.38 on epoch=469
05/17/2022 01:59:26 - INFO - __main__ - Step 950 Global step 950 Train loss 1.31 on epoch=474
05/17/2022 01:59:27 - INFO - __main__ - Global step 950 Train loss 1.40 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 01:59:29 - INFO - __main__ - Step 960 Global step 960 Train loss 1.34 on epoch=479
05/17/2022 01:59:31 - INFO - __main__ - Step 970 Global step 970 Train loss 1.22 on epoch=484
05/17/2022 01:59:32 - INFO - __main__ - Step 980 Global step 980 Train loss 1.37 on epoch=489
05/17/2022 01:59:34 - INFO - __main__ - Step 990 Global step 990 Train loss 1.30 on epoch=494
05/17/2022 01:59:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.21 on epoch=499
05/17/2022 01:59:38 - INFO - __main__ - Global step 1000 Train loss 1.29 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 01:59:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.26 on epoch=504
05/17/2022 01:59:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.11 on epoch=509
05/17/2022 01:59:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.13 on epoch=514
05/17/2022 01:59:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.15 on epoch=519
05/17/2022 01:59:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.04 on epoch=524
05/17/2022 01:59:49 - INFO - __main__ - Global step 1050 Train loss 1.14 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 01:59:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.10 on epoch=529
05/17/2022 01:59:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.01 on epoch=534
05/17/2022 01:59:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.10 on epoch=539
05/17/2022 01:59:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.09 on epoch=544
05/17/2022 01:59:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.05 on epoch=549
05/17/2022 01:59:59 - INFO - __main__ - Global step 1100 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 02:00:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.17 on epoch=554
05/17/2022 02:00:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.16 on epoch=559
05/17/2022 02:00:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.12 on epoch=564
05/17/2022 02:00:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.98 on epoch=569
05/17/2022 02:00:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.09 on epoch=574
05/17/2022 02:00:10 - INFO - __main__ - Global step 1150 Train loss 1.10 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 02:00:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.02 on epoch=579
05/17/2022 02:00:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.16 on epoch=584
05/17/2022 02:00:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.99 on epoch=589
05/17/2022 02:00:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.92 on epoch=594
05/17/2022 02:00:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.94 on epoch=599
05/17/2022 02:00:20 - INFO - __main__ - Global step 1200 Train loss 1.01 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 02:00:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.97 on epoch=604
05/17/2022 02:00:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.92 on epoch=609
05/17/2022 02:00:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.98 on epoch=614
05/17/2022 02:00:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.92 on epoch=619
05/17/2022 02:00:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.01 on epoch=624
05/17/2022 02:00:31 - INFO - __main__ - Global step 1250 Train loss 0.96 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 02:00:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.93 on epoch=629
05/17/2022 02:00:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.96 on epoch=634
05/17/2022 02:00:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.88 on epoch=639
05/17/2022 02:00:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.87 on epoch=644
05/17/2022 02:00:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.81 on epoch=649
05/17/2022 02:00:41 - INFO - __main__ - Global step 1300 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 02:00:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.84 on epoch=654
05/17/2022 02:00:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.79 on epoch=659
05/17/2022 02:00:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.77 on epoch=664
05/17/2022 02:00:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.82 on epoch=669
05/17/2022 02:00:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.90 on epoch=674
05/17/2022 02:00:52 - INFO - __main__ - Global step 1350 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 02:00:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.83 on epoch=679
05/17/2022 02:00:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.81 on epoch=684
05/17/2022 02:00:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.81 on epoch=689
05/17/2022 02:00:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.78 on epoch=694
05/17/2022 02:01:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.78 on epoch=699
05/17/2022 02:01:02 - INFO - __main__ - Global step 1400 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 02:01:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.73 on epoch=704
05/17/2022 02:01:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.74 on epoch=709
05/17/2022 02:01:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.84 on epoch=714
05/17/2022 02:01:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.83 on epoch=719
05/17/2022 02:01:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.82 on epoch=724
05/17/2022 02:01:12 - INFO - __main__ - Global step 1450 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 02:01:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.73 on epoch=729
05/17/2022 02:01:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.71 on epoch=734
05/17/2022 02:01:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.69 on epoch=739
05/17/2022 02:01:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.77 on epoch=744
05/17/2022 02:01:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.68 on epoch=749
05/17/2022 02:01:23 - INFO - __main__ - Global step 1500 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 02:01:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.69 on epoch=754
05/17/2022 02:01:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.71 on epoch=759
05/17/2022 02:01:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.65 on epoch=764
05/17/2022 02:01:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.72 on epoch=769
05/17/2022 02:01:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.62 on epoch=774
05/17/2022 02:01:33 - INFO - __main__ - Global step 1550 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 02:01:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.73 on epoch=779
05/17/2022 02:01:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.63 on epoch=784
05/17/2022 02:01:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.63 on epoch=789
05/17/2022 02:01:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.61 on epoch=794
05/17/2022 02:01:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.60 on epoch=799
05/17/2022 02:01:43 - INFO - __main__ - Global step 1600 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 02:01:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.65 on epoch=804
05/17/2022 02:01:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.64 on epoch=809
05/17/2022 02:01:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.58 on epoch=814
05/17/2022 02:01:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.65 on epoch=819
05/17/2022 02:01:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.63 on epoch=824
05/17/2022 02:01:54 - INFO - __main__ - Global step 1650 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 02:01:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.61 on epoch=829
05/17/2022 02:01:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.57 on epoch=834
05/17/2022 02:01:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.62 on epoch=839
05/17/2022 02:02:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.62 on epoch=844
05/17/2022 02:02:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.59 on epoch=849
05/17/2022 02:02:04 - INFO - __main__ - Global step 1700 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 02:02:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.54 on epoch=854
05/17/2022 02:02:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.62 on epoch=859
05/17/2022 02:02:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.57 on epoch=864
05/17/2022 02:02:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=869
05/17/2022 02:02:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.54 on epoch=874
05/17/2022 02:02:14 - INFO - __main__ - Global step 1750 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 02:02:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.51 on epoch=879
05/17/2022 02:02:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.59 on epoch=884
05/17/2022 02:02:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.54 on epoch=889
05/17/2022 02:02:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.51 on epoch=894
05/17/2022 02:02:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.50 on epoch=899
05/17/2022 02:02:25 - INFO - __main__ - Global step 1800 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 02:02:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.47 on epoch=904
05/17/2022 02:02:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.53 on epoch=909
05/17/2022 02:02:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.49 on epoch=914
05/17/2022 02:02:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.48 on epoch=919
05/17/2022 02:02:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.56 on epoch=924
05/17/2022 02:02:35 - INFO - __main__ - Global step 1850 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 02:02:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.51 on epoch=929
05/17/2022 02:02:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.52 on epoch=934
05/17/2022 02:02:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=939
05/17/2022 02:02:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
05/17/2022 02:02:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=949
05/17/2022 02:02:46 - INFO - __main__ - Global step 1900 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 02:02:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.57 on epoch=954
05/17/2022 02:02:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=959
05/17/2022 02:02:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.48 on epoch=964
05/17/2022 02:02:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.54 on epoch=969
05/17/2022 02:02:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=974
05/17/2022 02:02:56 - INFO - __main__ - Global step 1950 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 02:02:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=979
05/17/2022 02:03:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
05/17/2022 02:03:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=989
05/17/2022 02:03:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=994
05/17/2022 02:03:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=999
05/17/2022 02:03:06 - INFO - __main__ - Global step 2000 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 02:03:06 - INFO - __main__ - save last model!
05/17/2022 02:03:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 02:03:06 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 02:03:06 - INFO - __main__ - Printing 3 examples
05/17/2022 02:03:06 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:03:06 - INFO - __main__ - ['entailed']
05/17/2022 02:03:06 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:03:06 - INFO - __main__ - ['entailed']
05/17/2022 02:03:06 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:03:06 - INFO - __main__ - ['entailed']
05/17/2022 02:03:06 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:03:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:03:07 - INFO - __main__ - Printing 3 examples
05/17/2022 02:03:07 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/17/2022 02:03:07 - INFO - __main__ - ['refuted']
05/17/2022 02:03:07 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/17/2022 02:03:07 - INFO - __main__ - ['refuted']
05/17/2022 02:03:07 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/17/2022 02:03:07 - INFO - __main__ - ['refuted']
05/17/2022 02:03:07 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:03:07 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:03:07 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 02:03:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:03:07 - INFO - __main__ - Printing 3 examples
05/17/2022 02:03:07 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/17/2022 02:03:07 - INFO - __main__ - ['refuted']
05/17/2022 02:03:07 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/17/2022 02:03:07 - INFO - __main__ - ['refuted']
05/17/2022 02:03:07 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/17/2022 02:03:07 - INFO - __main__ - ['refuted']
05/17/2022 02:03:07 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:03:07 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:03:07 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 02:03:13 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 02:03:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 02:03:13 - INFO - __main__ - Starting training!
05/17/2022 02:03:30 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:03:43 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 02:08:57 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.2_8_predictions.txt
05/17/2022 02:08:57 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 02:08:57 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.2, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 02:08:57 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.5, bsz=8 ...
05/17/2022 02:08:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:08:58 - INFO - __main__ - Printing 3 examples
05/17/2022 02:08:58 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/17/2022 02:08:58 - INFO - __main__ - ['refuted']
05/17/2022 02:08:58 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/17/2022 02:08:58 - INFO - __main__ - ['refuted']
05/17/2022 02:08:58 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/17/2022 02:08:58 - INFO - __main__ - ['refuted']
05/17/2022 02:08:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:08:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:08:58 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 02:08:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:08:58 - INFO - __main__ - Printing 3 examples
05/17/2022 02:08:58 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/17/2022 02:08:58 - INFO - __main__ - ['refuted']
05/17/2022 02:08:58 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/17/2022 02:08:58 - INFO - __main__ - ['refuted']
05/17/2022 02:08:58 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/17/2022 02:08:58 - INFO - __main__ - ['refuted']
05/17/2022 02:08:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:08:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:08:58 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 02:09:04 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 02:09:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 02:09:04 - INFO - __main__ - Starting training!
05/17/2022 02:09:06 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/17/2022 02:09:08 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/17/2022 02:09:10 - INFO - __main__ - Step 30 Global step 30 Train loss 4.85 on epoch=14
05/17/2022 02:09:12 - INFO - __main__ - Step 40 Global step 40 Train loss 4.77 on epoch=19
05/17/2022 02:09:14 - INFO - __main__ - Step 50 Global step 50 Train loss 4.65 on epoch=24
05/17/2022 02:09:15 - INFO - __main__ - Global step 50 Train loss 4.85 Classification-F1 0.0 on epoch=24
05/17/2022 02:09:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 02:09:17 - INFO - __main__ - Step 60 Global step 60 Train loss 4.53 on epoch=29
05/17/2022 02:09:19 - INFO - __main__ - Step 70 Global step 70 Train loss 4.49 on epoch=34
05/17/2022 02:09:21 - INFO - __main__ - Step 80 Global step 80 Train loss 4.27 on epoch=39
05/17/2022 02:09:23 - INFO - __main__ - Step 90 Global step 90 Train loss 4.13 on epoch=44
05/17/2022 02:09:25 - INFO - __main__ - Step 100 Global step 100 Train loss 3.96 on epoch=49
05/17/2022 02:09:28 - INFO - __main__ - Global step 100 Train loss 4.28 Classification-F1 0.0 on epoch=49
05/17/2022 02:09:30 - INFO - __main__ - Step 110 Global step 110 Train loss 3.87 on epoch=54
05/17/2022 02:09:32 - INFO - __main__ - Step 120 Global step 120 Train loss 3.84 on epoch=59
05/17/2022 02:09:34 - INFO - __main__ - Step 130 Global step 130 Train loss 3.74 on epoch=64
05/17/2022 02:09:35 - INFO - __main__ - Step 140 Global step 140 Train loss 3.65 on epoch=69
05/17/2022 02:09:37 - INFO - __main__ - Step 150 Global step 150 Train loss 3.45 on epoch=74
05/17/2022 02:09:47 - INFO - __main__ - Global step 150 Train loss 3.71 Classification-F1 0.0 on epoch=74
05/17/2022 02:09:49 - INFO - __main__ - Step 160 Global step 160 Train loss 3.42 on epoch=79
05/17/2022 02:09:50 - INFO - __main__ - Step 170 Global step 170 Train loss 3.31 on epoch=84
05/17/2022 02:09:52 - INFO - __main__ - Step 180 Global step 180 Train loss 3.06 on epoch=89
05/17/2022 02:09:54 - INFO - __main__ - Step 190 Global step 190 Train loss 2.98 on epoch=94
05/17/2022 02:09:56 - INFO - __main__ - Step 200 Global step 200 Train loss 2.67 on epoch=99
05/17/2022 02:09:58 - INFO - __main__ - Global step 200 Train loss 3.09 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 02:09:58 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=99, global_step=200
05/17/2022 02:10:00 - INFO - __main__ - Step 210 Global step 210 Train loss 2.55 on epoch=104
05/17/2022 02:10:02 - INFO - __main__ - Step 220 Global step 220 Train loss 2.43 on epoch=109
05/17/2022 02:10:04 - INFO - __main__ - Step 230 Global step 230 Train loss 2.14 on epoch=114
05/17/2022 02:10:06 - INFO - __main__ - Step 240 Global step 240 Train loss 2.14 on epoch=119
05/17/2022 02:10:08 - INFO - __main__ - Step 250 Global step 250 Train loss 1.96 on epoch=124
05/17/2022 02:10:10 - INFO - __main__ - Global step 250 Train loss 2.24 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 02:10:12 - INFO - __main__ - Step 260 Global step 260 Train loss 1.86 on epoch=129
05/17/2022 02:10:13 - INFO - __main__ - Step 270 Global step 270 Train loss 1.60 on epoch=134
05/17/2022 02:10:15 - INFO - __main__ - Step 280 Global step 280 Train loss 1.55 on epoch=139
05/17/2022 02:10:17 - INFO - __main__ - Step 290 Global step 290 Train loss 1.55 on epoch=144
05/17/2022 02:10:19 - INFO - __main__ - Step 300 Global step 300 Train loss 1.52 on epoch=149
05/17/2022 02:10:21 - INFO - __main__ - Global step 300 Train loss 1.62 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 02:10:23 - INFO - __main__ - Step 310 Global step 310 Train loss 1.33 on epoch=154
05/17/2022 02:10:25 - INFO - __main__ - Step 320 Global step 320 Train loss 1.41 on epoch=159
05/17/2022 02:10:27 - INFO - __main__ - Step 330 Global step 330 Train loss 1.30 on epoch=164
05/17/2022 02:10:29 - INFO - __main__ - Step 340 Global step 340 Train loss 1.25 on epoch=169
05/17/2022 02:10:31 - INFO - __main__ - Step 350 Global step 350 Train loss 1.22 on epoch=174
05/17/2022 02:10:32 - INFO - __main__ - Global step 350 Train loss 1.30 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 02:10:34 - INFO - __main__ - Step 360 Global step 360 Train loss 1.21 on epoch=179
05/17/2022 02:10:36 - INFO - __main__ - Step 370 Global step 370 Train loss 1.16 on epoch=184
05/17/2022 02:10:38 - INFO - __main__ - Step 380 Global step 380 Train loss 1.19 on epoch=189
05/17/2022 02:10:39 - INFO - __main__ - Step 390 Global step 390 Train loss 1.19 on epoch=194
05/17/2022 02:10:41 - INFO - __main__ - Step 400 Global step 400 Train loss 1.01 on epoch=199
05/17/2022 02:10:42 - INFO - __main__ - Global step 400 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 02:10:44 - INFO - __main__ - Step 410 Global step 410 Train loss 1.00 on epoch=204
05/17/2022 02:10:46 - INFO - __main__ - Step 420 Global step 420 Train loss 1.01 on epoch=209
05/17/2022 02:10:48 - INFO - __main__ - Step 430 Global step 430 Train loss 1.01 on epoch=214
05/17/2022 02:10:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.85 on epoch=219
05/17/2022 02:10:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.86 on epoch=224
05/17/2022 02:10:53 - INFO - __main__ - Global step 450 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 02:10:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=229
05/17/2022 02:10:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.85 on epoch=234
05/17/2022 02:10:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.78 on epoch=239
05/17/2022 02:11:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.73 on epoch=244
05/17/2022 02:11:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.79 on epoch=249
05/17/2022 02:11:03 - INFO - __main__ - Global step 500 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 02:11:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.67 on epoch=254
05/17/2022 02:11:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.69 on epoch=259
05/17/2022 02:11:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.71 on epoch=264
05/17/2022 02:11:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=269
05/17/2022 02:11:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.68 on epoch=274
05/17/2022 02:11:13 - INFO - __main__ - Global step 550 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 02:11:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=279
05/17/2022 02:11:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.71 on epoch=284
05/17/2022 02:11:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.63 on epoch=289
05/17/2022 02:11:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.65 on epoch=294
05/17/2022 02:11:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.62 on epoch=299
05/17/2022 02:11:23 - INFO - __main__ - Global step 600 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 02:11:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.64 on epoch=304
05/17/2022 02:11:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.63 on epoch=309
05/17/2022 02:11:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.61 on epoch=314
05/17/2022 02:11:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.55 on epoch=319
05/17/2022 02:11:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.52 on epoch=324
05/17/2022 02:11:34 - INFO - __main__ - Global step 650 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 02:11:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=329
05/17/2022 02:11:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=334
05/17/2022 02:11:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.57 on epoch=339
05/17/2022 02:11:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.54 on epoch=344
05/17/2022 02:11:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=349
05/17/2022 02:11:44 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 02:11:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=354
05/17/2022 02:11:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.50 on epoch=359
05/17/2022 02:11:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=364
05/17/2022 02:11:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=369
05/17/2022 02:11:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=374
05/17/2022 02:11:54 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 02:11:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=379
05/17/2022 02:11:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.51 on epoch=384
05/17/2022 02:12:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.54 on epoch=389
05/17/2022 02:12:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=394
05/17/2022 02:12:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.53 on epoch=399
05/17/2022 02:12:04 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 02:12:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=404
05/17/2022 02:12:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=409
05/17/2022 02:12:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
05/17/2022 02:12:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=419
05/17/2022 02:12:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=424
05/17/2022 02:12:14 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 02:12:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=429
05/17/2022 02:12:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
05/17/2022 02:12:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=439
05/17/2022 02:12:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
05/17/2022 02:12:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=449
05/17/2022 02:12:24 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 02:12:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.49 on epoch=454
05/17/2022 02:12:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=459
05/17/2022 02:12:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=464
05/17/2022 02:12:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=469
05/17/2022 02:12:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
05/17/2022 02:12:34 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 02:12:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=479
05/17/2022 02:12:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=484
05/17/2022 02:12:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=489
05/17/2022 02:12:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=494
05/17/2022 02:12:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=499
05/17/2022 02:12:44 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 02:12:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
05/17/2022 02:12:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=509
05/17/2022 02:12:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=514
05/17/2022 02:12:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=519
05/17/2022 02:12:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=524
05/17/2022 02:12:54 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 02:12:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=529
05/17/2022 02:12:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=534
05/17/2022 02:13:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=539
05/17/2022 02:13:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=544
05/17/2022 02:13:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=549
05/17/2022 02:13:04 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 02:13:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=554
05/17/2022 02:13:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=559
05/17/2022 02:13:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=564
05/17/2022 02:13:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=569
05/17/2022 02:13:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=574
05/17/2022 02:13:15 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 02:13:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=579
05/17/2022 02:13:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=584
05/17/2022 02:13:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=589
05/17/2022 02:13:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=594
05/17/2022 02:13:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=599
05/17/2022 02:13:25 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 02:13:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=604
05/17/2022 02:13:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=609
05/17/2022 02:13:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=614
05/17/2022 02:13:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=619
05/17/2022 02:13:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=624
05/17/2022 02:13:36 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 02:13:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=629
05/17/2022 02:13:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=634
05/17/2022 02:13:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=639
05/17/2022 02:13:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=644
05/17/2022 02:13:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=649
05/17/2022 02:13:46 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 02:13:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=654
05/17/2022 02:13:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=659
05/17/2022 02:13:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
05/17/2022 02:13:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=669
05/17/2022 02:13:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=674
05/17/2022 02:13:56 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 02:13:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=679
05/17/2022 02:14:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=684
05/17/2022 02:14:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=689
05/17/2022 02:14:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=694
05/17/2022 02:14:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=699
05/17/2022 02:14:07 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 02:14:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=704
05/17/2022 02:14:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=709
05/17/2022 02:14:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=714
05/17/2022 02:14:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=719
05/17/2022 02:14:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=724
05/17/2022 02:14:17 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 02:14:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=729
05/17/2022 02:14:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=734
05/17/2022 02:14:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=739
05/17/2022 02:14:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=744
05/17/2022 02:14:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=749
05/17/2022 02:14:27 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 02:14:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=754
05/17/2022 02:14:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=759
05/17/2022 02:14:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=764
05/17/2022 02:14:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=769
05/17/2022 02:14:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=774
05/17/2022 02:14:37 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 02:14:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=779
05/17/2022 02:14:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=784
05/17/2022 02:14:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
05/17/2022 02:14:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=794
05/17/2022 02:14:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=799
05/17/2022 02:14:47 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 02:14:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=804
05/17/2022 02:14:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=809
05/17/2022 02:14:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=814
05/17/2022 02:14:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=819
05/17/2022 02:14:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=824
05/17/2022 02:14:57 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 02:14:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=829
05/17/2022 02:15:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=834
05/17/2022 02:15:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=839
05/17/2022 02:15:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=844
05/17/2022 02:15:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=849
05/17/2022 02:15:07 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 02:15:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/17/2022 02:15:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=859
05/17/2022 02:15:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/17/2022 02:15:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=869
05/17/2022 02:15:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/17/2022 02:15:17 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 02:15:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=879
05/17/2022 02:15:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
05/17/2022 02:15:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.31 on epoch=889
05/17/2022 02:15:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=894
05/17/2022 02:15:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=899
05/17/2022 02:15:28 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 02:15:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=904
05/17/2022 02:15:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/17/2022 02:15:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=914
05/17/2022 02:15:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=919
05/17/2022 02:15:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=924
05/17/2022 02:15:38 - INFO - __main__ - Global step 1850 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 02:15:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=929
05/17/2022 02:15:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=934
05/17/2022 02:15:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=939
05/17/2022 02:15:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=944
05/17/2022 02:15:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/17/2022 02:15:48 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 02:15:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
05/17/2022 02:15:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=959
05/17/2022 02:15:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/17/2022 02:15:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=969
05/17/2022 02:15:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=974
05/17/2022 02:15:58 - INFO - __main__ - Global step 1950 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 02:16:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.33 on epoch=979
05/17/2022 02:16:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=984
05/17/2022 02:16:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/17/2022 02:16:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=994
05/17/2022 02:16:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
05/17/2022 02:16:08 - INFO - __main__ - Global step 2000 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 02:16:08 - INFO - __main__ - save last model!
05/17/2022 02:16:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 02:16:08 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 02:16:08 - INFO - __main__ - Printing 3 examples
05/17/2022 02:16:08 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:16:08 - INFO - __main__ - ['entailed']
05/17/2022 02:16:08 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:16:08 - INFO - __main__ - ['entailed']
05/17/2022 02:16:08 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:16:08 - INFO - __main__ - ['entailed']
05/17/2022 02:16:08 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:16:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:16:09 - INFO - __main__ - Printing 3 examples
05/17/2022 02:16:09 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/17/2022 02:16:09 - INFO - __main__ - ['refuted']
05/17/2022 02:16:09 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/17/2022 02:16:09 - INFO - __main__ - ['refuted']
05/17/2022 02:16:09 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/17/2022 02:16:09 - INFO - __main__ - ['refuted']
05/17/2022 02:16:09 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:16:09 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:16:09 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 02:16:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:16:09 - INFO - __main__ - Printing 3 examples
05/17/2022 02:16:09 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/17/2022 02:16:09 - INFO - __main__ - ['refuted']
05/17/2022 02:16:09 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/17/2022 02:16:09 - INFO - __main__ - ['refuted']
05/17/2022 02:16:09 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/17/2022 02:16:09 - INFO - __main__ - ['refuted']
05/17/2022 02:16:09 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:16:09 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:16:09 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 02:16:14 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 02:16:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 02:16:15 - INFO - __main__ - Starting training!
05/17/2022 02:16:32 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:16:45 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 02:20:57 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.5_8_predictions.txt
05/17/2022 02:20:57 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 02:20:57 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 02:20:57 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.4, bsz=8 ...
05/17/2022 02:20:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:20:58 - INFO - __main__ - Printing 3 examples
05/17/2022 02:20:58 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/17/2022 02:20:58 - INFO - __main__ - ['refuted']
05/17/2022 02:20:58 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/17/2022 02:20:58 - INFO - __main__ - ['refuted']
05/17/2022 02:20:58 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/17/2022 02:20:58 - INFO - __main__ - ['refuted']
05/17/2022 02:20:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:20:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:20:58 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 02:20:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:20:58 - INFO - __main__ - Printing 3 examples
05/17/2022 02:20:58 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/17/2022 02:20:58 - INFO - __main__ - ['refuted']
05/17/2022 02:20:58 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/17/2022 02:20:58 - INFO - __main__ - ['refuted']
05/17/2022 02:20:58 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/17/2022 02:20:58 - INFO - __main__ - ['refuted']
05/17/2022 02:20:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:20:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:20:58 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 02:21:04 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 02:21:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 02:21:04 - INFO - __main__ - Starting training!
05/17/2022 02:21:06 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/17/2022 02:21:08 - INFO - __main__ - Step 20 Global step 20 Train loss 4.91 on epoch=9
05/17/2022 02:21:10 - INFO - __main__ - Step 30 Global step 30 Train loss 4.75 on epoch=14
05/17/2022 02:21:12 - INFO - __main__ - Step 40 Global step 40 Train loss 4.66 on epoch=19
05/17/2022 02:21:14 - INFO - __main__ - Step 50 Global step 50 Train loss 4.53 on epoch=24
05/17/2022 02:21:15 - INFO - __main__ - Global step 50 Train loss 4.78 Classification-F1 0.0 on epoch=24
05/17/2022 02:21:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 02:21:17 - INFO - __main__ - Step 60 Global step 60 Train loss 4.33 on epoch=29
05/17/2022 02:21:19 - INFO - __main__ - Step 70 Global step 70 Train loss 4.24 on epoch=34
05/17/2022 02:21:21 - INFO - __main__ - Step 80 Global step 80 Train loss 4.09 on epoch=39
05/17/2022 02:21:23 - INFO - __main__ - Step 90 Global step 90 Train loss 3.92 on epoch=44
05/17/2022 02:21:25 - INFO - __main__ - Step 100 Global step 100 Train loss 3.90 on epoch=49
05/17/2022 02:21:27 - INFO - __main__ - Global step 100 Train loss 4.10 Classification-F1 0.0 on epoch=49
05/17/2022 02:21:29 - INFO - __main__ - Step 110 Global step 110 Train loss 3.89 on epoch=54
05/17/2022 02:21:30 - INFO - __main__ - Step 120 Global step 120 Train loss 3.71 on epoch=59
05/17/2022 02:21:32 - INFO - __main__ - Step 130 Global step 130 Train loss 3.64 on epoch=64
05/17/2022 02:21:34 - INFO - __main__ - Step 140 Global step 140 Train loss 3.52 on epoch=69
05/17/2022 02:21:36 - INFO - __main__ - Step 150 Global step 150 Train loss 3.43 on epoch=74
05/17/2022 02:21:39 - INFO - __main__ - Global step 150 Train loss 3.64 Classification-F1 0.02 on epoch=74
05/17/2022 02:21:39 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.02 on epoch=74, global_step=150
05/17/2022 02:21:41 - INFO - __main__ - Step 160 Global step 160 Train loss 3.32 on epoch=79
05/17/2022 02:21:43 - INFO - __main__ - Step 170 Global step 170 Train loss 3.19 on epoch=84
05/17/2022 02:21:45 - INFO - __main__ - Step 180 Global step 180 Train loss 3.01 on epoch=89
05/17/2022 02:21:47 - INFO - __main__ - Step 190 Global step 190 Train loss 2.93 on epoch=94
05/17/2022 02:21:49 - INFO - __main__ - Step 200 Global step 200 Train loss 2.92 on epoch=99
05/17/2022 02:21:51 - INFO - __main__ - Global step 200 Train loss 3.07 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 02:21:51 - INFO - __main__ - Saving model with best Classification-F1: 0.02 -> 0.3333333333333333 on epoch=99, global_step=200
05/17/2022 02:21:53 - INFO - __main__ - Step 210 Global step 210 Train loss 2.77 on epoch=104
05/17/2022 02:21:55 - INFO - __main__ - Step 220 Global step 220 Train loss 2.58 on epoch=109
05/17/2022 02:21:56 - INFO - __main__ - Step 230 Global step 230 Train loss 2.53 on epoch=114
05/17/2022 02:21:58 - INFO - __main__ - Step 240 Global step 240 Train loss 2.47 on epoch=119
05/17/2022 02:22:00 - INFO - __main__ - Step 250 Global step 250 Train loss 2.33 on epoch=124
05/17/2022 02:22:03 - INFO - __main__ - Global step 250 Train loss 2.54 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 02:22:05 - INFO - __main__ - Step 260 Global step 260 Train loss 2.39 on epoch=129
05/17/2022 02:22:07 - INFO - __main__ - Step 270 Global step 270 Train loss 2.31 on epoch=134
05/17/2022 02:22:08 - INFO - __main__ - Step 280 Global step 280 Train loss 2.09 on epoch=139
05/17/2022 02:22:10 - INFO - __main__ - Step 290 Global step 290 Train loss 2.08 on epoch=144
05/17/2022 02:22:12 - INFO - __main__ - Step 300 Global step 300 Train loss 2.05 on epoch=149
05/17/2022 02:22:14 - INFO - __main__ - Global step 300 Train loss 2.18 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 02:22:15 - INFO - __main__ - Step 310 Global step 310 Train loss 1.99 on epoch=154
05/17/2022 02:22:17 - INFO - __main__ - Step 320 Global step 320 Train loss 1.95 on epoch=159
05/17/2022 02:22:19 - INFO - __main__ - Step 330 Global step 330 Train loss 1.73 on epoch=164
05/17/2022 02:22:21 - INFO - __main__ - Step 340 Global step 340 Train loss 1.70 on epoch=169
05/17/2022 02:22:23 - INFO - __main__ - Step 350 Global step 350 Train loss 1.65 on epoch=174
05/17/2022 02:22:25 - INFO - __main__ - Global step 350 Train loss 1.80 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 02:22:26 - INFO - __main__ - Step 360 Global step 360 Train loss 1.68 on epoch=179
05/17/2022 02:22:28 - INFO - __main__ - Step 370 Global step 370 Train loss 1.65 on epoch=184
05/17/2022 02:22:30 - INFO - __main__ - Step 380 Global step 380 Train loss 1.53 on epoch=189
05/17/2022 02:22:32 - INFO - __main__ - Step 390 Global step 390 Train loss 1.47 on epoch=194
05/17/2022 02:22:34 - INFO - __main__ - Step 400 Global step 400 Train loss 1.40 on epoch=199
05/17/2022 02:22:36 - INFO - __main__ - Global step 400 Train loss 1.55 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 02:22:38 - INFO - __main__ - Step 410 Global step 410 Train loss 1.40 on epoch=204
05/17/2022 02:22:40 - INFO - __main__ - Step 420 Global step 420 Train loss 1.36 on epoch=209
05/17/2022 02:22:42 - INFO - __main__ - Step 430 Global step 430 Train loss 1.24 on epoch=214
05/17/2022 02:22:44 - INFO - __main__ - Step 440 Global step 440 Train loss 1.19 on epoch=219
05/17/2022 02:22:46 - INFO - __main__ - Step 450 Global step 450 Train loss 1.18 on epoch=224
05/17/2022 02:22:47 - INFO - __main__ - Global step 450 Train loss 1.27 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 02:22:49 - INFO - __main__ - Step 460 Global step 460 Train loss 1.21 on epoch=229
05/17/2022 02:22:51 - INFO - __main__ - Step 470 Global step 470 Train loss 1.17 on epoch=234
05/17/2022 02:22:53 - INFO - __main__ - Step 480 Global step 480 Train loss 1.03 on epoch=239
05/17/2022 02:22:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.99 on epoch=244
05/17/2022 02:22:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.98 on epoch=249
05/17/2022 02:22:57 - INFO - __main__ - Global step 500 Train loss 1.08 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 02:22:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.87 on epoch=254
05/17/2022 02:23:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.85 on epoch=259
05/17/2022 02:23:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.91 on epoch=264
05/17/2022 02:23:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=269
05/17/2022 02:23:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.71 on epoch=274
05/17/2022 02:23:08 - INFO - __main__ - Global step 550 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 02:23:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=279
05/17/2022 02:23:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.78 on epoch=284
05/17/2022 02:23:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.87 on epoch=289
05/17/2022 02:23:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.75 on epoch=294
05/17/2022 02:23:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.72 on epoch=299
05/17/2022 02:23:18 - INFO - __main__ - Global step 600 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 02:23:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.71 on epoch=304
05/17/2022 02:23:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.68 on epoch=309
05/17/2022 02:23:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.79 on epoch=314
05/17/2022 02:23:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.80 on epoch=319
05/17/2022 02:23:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.70 on epoch=324
05/17/2022 02:23:29 - INFO - __main__ - Global step 650 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 02:23:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.65 on epoch=329
05/17/2022 02:23:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.59 on epoch=334
05/17/2022 02:23:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.66 on epoch=339
05/17/2022 02:23:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=344
05/17/2022 02:23:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.60 on epoch=349
05/17/2022 02:23:39 - INFO - __main__ - Global step 700 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 02:23:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.66 on epoch=354
05/17/2022 02:23:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=359
05/17/2022 02:23:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=364
05/17/2022 02:23:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.52 on epoch=369
05/17/2022 02:23:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.56 on epoch=374
05/17/2022 02:23:49 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 02:23:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=379
05/17/2022 02:23:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.54 on epoch=384
05/17/2022 02:23:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=389
05/17/2022 02:23:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.49 on epoch=394
05/17/2022 02:23:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=399
05/17/2022 02:24:00 - INFO - __main__ - Global step 800 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 02:24:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.50 on epoch=404
05/17/2022 02:24:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.54 on epoch=409
05/17/2022 02:24:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.58 on epoch=414
05/17/2022 02:24:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=419
05/17/2022 02:24:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=424
05/17/2022 02:24:10 - INFO - __main__ - Global step 850 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 02:24:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=429
05/17/2022 02:24:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=434
05/17/2022 02:24:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=439
05/17/2022 02:24:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
05/17/2022 02:24:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=449
05/17/2022 02:24:20 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 02:24:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=454
05/17/2022 02:24:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.52 on epoch=459
05/17/2022 02:24:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=464
05/17/2022 02:24:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=469
05/17/2022 02:24:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=474
05/17/2022 02:24:30 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 02:24:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=479
05/17/2022 02:24:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=484
05/17/2022 02:24:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=489
05/17/2022 02:24:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=494
05/17/2022 02:24:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=499
05/17/2022 02:24:41 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 02:24:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=504
05/17/2022 02:24:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.51 on epoch=509
05/17/2022 02:24:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=514
05/17/2022 02:24:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=519
05/17/2022 02:24:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=524
05/17/2022 02:24:51 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 02:24:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=529
05/17/2022 02:24:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=534
05/17/2022 02:24:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=539
05/17/2022 02:24:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=544
05/17/2022 02:25:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=549
05/17/2022 02:25:01 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 02:25:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=554
05/17/2022 02:25:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=559
05/17/2022 02:25:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=564
05/17/2022 02:25:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=569
05/17/2022 02:25:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=574
05/17/2022 02:25:11 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 02:25:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=579
05/17/2022 02:25:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=584
05/17/2022 02:25:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
05/17/2022 02:25:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.29 on epoch=594
05/17/2022 02:25:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.47 on epoch=599
05/17/2022 02:25:21 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 02:25:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=604
05/17/2022 02:25:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=609
05/17/2022 02:25:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.87 on epoch=614
05/17/2022 02:25:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.80 on epoch=619
05/17/2022 02:25:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=624
05/17/2022 02:25:32 - INFO - __main__ - Global step 1250 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 02:25:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=629
05/17/2022 02:25:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.52 on epoch=634
05/17/2022 02:25:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
05/17/2022 02:25:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=644
05/17/2022 02:25:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=649
05/17/2022 02:25:42 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 02:25:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=654
05/17/2022 02:25:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=659
05/17/2022 02:25:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=664
05/17/2022 02:25:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=669
05/17/2022 02:25:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=674
05/17/2022 02:25:52 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 02:25:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=679
05/17/2022 02:25:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=684
05/17/2022 02:25:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=689
05/17/2022 02:26:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=694
05/17/2022 02:26:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=699
05/17/2022 02:26:02 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 02:26:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=704
05/17/2022 02:26:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=709
05/17/2022 02:26:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
05/17/2022 02:26:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=719
05/17/2022 02:26:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=724
05/17/2022 02:26:13 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 02:26:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=729
05/17/2022 02:26:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=734
05/17/2022 02:26:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=739
05/17/2022 02:26:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=744
05/17/2022 02:26:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=749
05/17/2022 02:26:23 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 02:26:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=754
05/17/2022 02:26:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=759
05/17/2022 02:26:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.29 on epoch=764
05/17/2022 02:26:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=769
05/17/2022 02:26:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=774
05/17/2022 02:26:33 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 02:26:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=779
05/17/2022 02:26:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=784
05/17/2022 02:26:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=789
05/17/2022 02:26:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=794
05/17/2022 02:26:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=799
05/17/2022 02:26:43 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 02:26:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.28 on epoch=804
05/17/2022 02:26:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=809
05/17/2022 02:26:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=814
05/17/2022 02:26:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=819
05/17/2022 02:26:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=824
05/17/2022 02:26:53 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 02:26:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=829
05/17/2022 02:26:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=834
05/17/2022 02:26:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=839
05/17/2022 02:27:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=844
05/17/2022 02:27:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=849
05/17/2022 02:27:04 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 02:27:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=854
05/17/2022 02:27:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=859
05/17/2022 02:27:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=864
05/17/2022 02:27:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=869
05/17/2022 02:27:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/17/2022 02:27:14 - INFO - __main__ - Global step 1750 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 02:27:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.30 on epoch=879
05/17/2022 02:27:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=884
05/17/2022 02:27:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=889
05/17/2022 02:27:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=894
05/17/2022 02:27:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=899
05/17/2022 02:27:24 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 02:27:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=904
05/17/2022 02:27:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/17/2022 02:27:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=914
05/17/2022 02:27:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=919
05/17/2022 02:27:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=924
05/17/2022 02:27:34 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 02:27:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=929
05/17/2022 02:27:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=934
05/17/2022 02:27:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=939
05/17/2022 02:27:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=944
05/17/2022 02:27:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/17/2022 02:27:44 - INFO - __main__ - Global step 1900 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 02:27:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=954
05/17/2022 02:27:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=959
05/17/2022 02:27:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/17/2022 02:27:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
05/17/2022 02:27:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=974
05/17/2022 02:27:55 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 02:27:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=979
05/17/2022 02:27:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=984
05/17/2022 02:28:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
05/17/2022 02:28:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
05/17/2022 02:28:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=999
05/17/2022 02:28:05 - INFO - __main__ - Global step 2000 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 02:28:05 - INFO - __main__ - save last model!
05/17/2022 02:28:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 02:28:05 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 02:28:05 - INFO - __main__ - Printing 3 examples
05/17/2022 02:28:05 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:28:05 - INFO - __main__ - ['entailed']
05/17/2022 02:28:05 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:28:05 - INFO - __main__ - ['entailed']
05/17/2022 02:28:05 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:28:05 - INFO - __main__ - ['entailed']
05/17/2022 02:28:05 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:28:05 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:28:05 - INFO - __main__ - Printing 3 examples
05/17/2022 02:28:05 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/17/2022 02:28:05 - INFO - __main__ - ['refuted']
05/17/2022 02:28:05 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/17/2022 02:28:05 - INFO - __main__ - ['refuted']
05/17/2022 02:28:05 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/17/2022 02:28:05 - INFO - __main__ - ['refuted']
05/17/2022 02:28:05 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:28:05 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:28:05 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 02:28:05 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:28:05 - INFO - __main__ - Printing 3 examples
05/17/2022 02:28:05 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/17/2022 02:28:05 - INFO - __main__ - ['refuted']
05/17/2022 02:28:05 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/17/2022 02:28:05 - INFO - __main__ - ['refuted']
05/17/2022 02:28:05 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/17/2022 02:28:05 - INFO - __main__ - ['refuted']
05/17/2022 02:28:05 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:28:05 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:28:06 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 02:28:11 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 02:28:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 02:28:11 - INFO - __main__ - Starting training!
05/17/2022 02:28:29 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:28:42 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 02:32:45 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.4_8_predictions.txt
05/17/2022 02:32:45 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 02:32:46 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 02:32:46 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.3, bsz=8 ...
05/17/2022 02:32:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:32:47 - INFO - __main__ - Printing 3 examples
05/17/2022 02:32:47 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/17/2022 02:32:47 - INFO - __main__ - ['refuted']
05/17/2022 02:32:47 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/17/2022 02:32:47 - INFO - __main__ - ['refuted']
05/17/2022 02:32:47 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/17/2022 02:32:47 - INFO - __main__ - ['refuted']
05/17/2022 02:32:47 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:32:47 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:32:47 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 02:32:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:32:47 - INFO - __main__ - Printing 3 examples
05/17/2022 02:32:47 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/17/2022 02:32:47 - INFO - __main__ - ['refuted']
05/17/2022 02:32:47 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/17/2022 02:32:47 - INFO - __main__ - ['refuted']
05/17/2022 02:32:47 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/17/2022 02:32:47 - INFO - __main__ - ['refuted']
05/17/2022 02:32:47 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:32:47 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:32:47 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 02:32:53 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 02:32:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 02:32:53 - INFO - __main__ - Starting training!
05/17/2022 02:32:56 - INFO - __main__ - Step 10 Global step 10 Train loss 4.98 on epoch=4
05/17/2022 02:32:57 - INFO - __main__ - Step 20 Global step 20 Train loss 4.94 on epoch=9
05/17/2022 02:32:59 - INFO - __main__ - Step 30 Global step 30 Train loss 4.93 on epoch=14
05/17/2022 02:33:01 - INFO - __main__ - Step 40 Global step 40 Train loss 4.88 on epoch=19
05/17/2022 02:33:03 - INFO - __main__ - Step 50 Global step 50 Train loss 4.83 on epoch=24
05/17/2022 02:33:05 - INFO - __main__ - Global step 50 Train loss 4.91 Classification-F1 0.0 on epoch=24
05/17/2022 02:33:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 02:33:06 - INFO - __main__ - Step 60 Global step 60 Train loss 4.75 on epoch=29
05/17/2022 02:33:08 - INFO - __main__ - Step 70 Global step 70 Train loss 4.59 on epoch=34
05/17/2022 02:33:10 - INFO - __main__ - Step 80 Global step 80 Train loss 4.61 on epoch=39
05/17/2022 02:33:12 - INFO - __main__ - Step 90 Global step 90 Train loss 4.43 on epoch=44
05/17/2022 02:33:14 - INFO - __main__ - Step 100 Global step 100 Train loss 4.46 on epoch=49
05/17/2022 02:33:15 - INFO - __main__ - Global step 100 Train loss 4.57 Classification-F1 0.0 on epoch=49
05/17/2022 02:33:17 - INFO - __main__ - Step 110 Global step 110 Train loss 4.39 on epoch=54
05/17/2022 02:33:19 - INFO - __main__ - Step 120 Global step 120 Train loss 4.11 on epoch=59
05/17/2022 02:33:21 - INFO - __main__ - Step 130 Global step 130 Train loss 4.18 on epoch=64
05/17/2022 02:33:23 - INFO - __main__ - Step 140 Global step 140 Train loss 3.98 on epoch=69
05/17/2022 02:33:24 - INFO - __main__ - Step 150 Global step 150 Train loss 3.96 on epoch=74
05/17/2022 02:33:27 - INFO - __main__ - Global step 150 Train loss 4.12 Classification-F1 0.0 on epoch=74
05/17/2022 02:33:29 - INFO - __main__ - Step 160 Global step 160 Train loss 3.85 on epoch=79
05/17/2022 02:33:31 - INFO - __main__ - Step 170 Global step 170 Train loss 3.87 on epoch=84
05/17/2022 02:33:33 - INFO - __main__ - Step 180 Global step 180 Train loss 3.88 on epoch=89
05/17/2022 02:33:34 - INFO - __main__ - Step 190 Global step 190 Train loss 3.78 on epoch=94
05/17/2022 02:33:36 - INFO - __main__ - Step 200 Global step 200 Train loss 3.64 on epoch=99
05/17/2022 02:33:39 - INFO - __main__ - Global step 200 Train loss 3.80 Classification-F1 0.038461538461538464 on epoch=99
05/17/2022 02:33:39 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.038461538461538464 on epoch=99, global_step=200
05/17/2022 02:33:41 - INFO - __main__ - Step 210 Global step 210 Train loss 3.56 on epoch=104
05/17/2022 02:33:43 - INFO - __main__ - Step 220 Global step 220 Train loss 3.37 on epoch=109
05/17/2022 02:33:45 - INFO - __main__ - Step 230 Global step 230 Train loss 3.41 on epoch=114
05/17/2022 02:33:47 - INFO - __main__ - Step 240 Global step 240 Train loss 3.28 on epoch=119
05/17/2022 02:33:49 - INFO - __main__ - Step 250 Global step 250 Train loss 3.26 on epoch=124
05/17/2022 02:33:50 - INFO - __main__ - Global step 250 Train loss 3.38 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 02:33:50 - INFO - __main__ - Saving model with best Classification-F1: 0.038461538461538464 -> 0.3333333333333333 on epoch=124, global_step=250
05/17/2022 02:33:52 - INFO - __main__ - Step 260 Global step 260 Train loss 3.09 on epoch=129
05/17/2022 02:33:54 - INFO - __main__ - Step 270 Global step 270 Train loss 3.06 on epoch=134
05/17/2022 02:33:56 - INFO - __main__ - Step 280 Global step 280 Train loss 3.08 on epoch=139
05/17/2022 02:33:58 - INFO - __main__ - Step 290 Global step 290 Train loss 2.93 on epoch=144
05/17/2022 02:34:00 - INFO - __main__ - Step 300 Global step 300 Train loss 2.87 on epoch=149
05/17/2022 02:34:06 - INFO - __main__ - Global step 300 Train loss 3.01 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 02:34:08 - INFO - __main__ - Step 310 Global step 310 Train loss 2.63 on epoch=154
05/17/2022 02:34:09 - INFO - __main__ - Step 320 Global step 320 Train loss 2.63 on epoch=159
05/17/2022 02:34:11 - INFO - __main__ - Step 330 Global step 330 Train loss 2.56 on epoch=164
05/17/2022 02:34:13 - INFO - __main__ - Step 340 Global step 340 Train loss 2.44 on epoch=169
05/17/2022 02:34:15 - INFO - __main__ - Step 350 Global step 350 Train loss 2.40 on epoch=174
05/17/2022 02:34:17 - INFO - __main__ - Global step 350 Train loss 2.53 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 02:34:19 - INFO - __main__ - Step 360 Global step 360 Train loss 2.41 on epoch=179
05/17/2022 02:34:21 - INFO - __main__ - Step 370 Global step 370 Train loss 2.35 on epoch=184
05/17/2022 02:34:23 - INFO - __main__ - Step 380 Global step 380 Train loss 2.19 on epoch=189
05/17/2022 02:34:25 - INFO - __main__ - Step 390 Global step 390 Train loss 2.18 on epoch=194
05/17/2022 02:34:27 - INFO - __main__ - Step 400 Global step 400 Train loss 2.07 on epoch=199
05/17/2022 02:34:29 - INFO - __main__ - Global step 400 Train loss 2.24 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 02:34:31 - INFO - __main__ - Step 410 Global step 410 Train loss 2.03 on epoch=204
05/17/2022 02:34:33 - INFO - __main__ - Step 420 Global step 420 Train loss 2.06 on epoch=209
05/17/2022 02:34:34 - INFO - __main__ - Step 430 Global step 430 Train loss 2.07 on epoch=214
05/17/2022 02:34:36 - INFO - __main__ - Step 440 Global step 440 Train loss 1.92 on epoch=219
05/17/2022 02:34:38 - INFO - __main__ - Step 450 Global step 450 Train loss 2.05 on epoch=224
05/17/2022 02:34:44 - INFO - __main__ - Global step 450 Train loss 2.03 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 02:34:46 - INFO - __main__ - Step 460 Global step 460 Train loss 2.05 on epoch=229
05/17/2022 02:34:48 - INFO - __main__ - Step 470 Global step 470 Train loss 1.83 on epoch=234
05/17/2022 02:34:49 - INFO - __main__ - Step 480 Global step 480 Train loss 1.72 on epoch=239
05/17/2022 02:34:51 - INFO - __main__ - Step 490 Global step 490 Train loss 1.77 on epoch=244
05/17/2022 02:34:53 - INFO - __main__ - Step 500 Global step 500 Train loss 1.70 on epoch=249
05/17/2022 02:34:54 - INFO - __main__ - Global step 500 Train loss 1.81 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 02:34:56 - INFO - __main__ - Step 510 Global step 510 Train loss 1.59 on epoch=254
05/17/2022 02:34:58 - INFO - __main__ - Step 520 Global step 520 Train loss 1.62 on epoch=259
05/17/2022 02:35:00 - INFO - __main__ - Step 530 Global step 530 Train loss 1.78 on epoch=264
05/17/2022 02:35:02 - INFO - __main__ - Step 540 Global step 540 Train loss 1.49 on epoch=269
05/17/2022 02:35:04 - INFO - __main__ - Step 550 Global step 550 Train loss 1.53 on epoch=274
05/17/2022 02:35:06 - INFO - __main__ - Global step 550 Train loss 1.60 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 02:35:08 - INFO - __main__ - Step 560 Global step 560 Train loss 1.45 on epoch=279
05/17/2022 02:35:10 - INFO - __main__ - Step 570 Global step 570 Train loss 1.48 on epoch=284
05/17/2022 02:35:12 - INFO - __main__ - Step 580 Global step 580 Train loss 1.20 on epoch=289
05/17/2022 02:35:14 - INFO - __main__ - Step 590 Global step 590 Train loss 1.32 on epoch=294
05/17/2022 02:35:16 - INFO - __main__ - Step 600 Global step 600 Train loss 1.30 on epoch=299
05/17/2022 02:35:19 - INFO - __main__ - Global step 600 Train loss 1.35 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 02:35:21 - INFO - __main__ - Step 610 Global step 610 Train loss 1.17 on epoch=304
05/17/2022 02:35:23 - INFO - __main__ - Step 620 Global step 620 Train loss 1.25 on epoch=309
05/17/2022 02:35:25 - INFO - __main__ - Step 630 Global step 630 Train loss 1.18 on epoch=314
05/17/2022 02:35:26 - INFO - __main__ - Step 640 Global step 640 Train loss 1.17 on epoch=319
05/17/2022 02:35:28 - INFO - __main__ - Step 650 Global step 650 Train loss 1.21 on epoch=324
05/17/2022 02:35:30 - INFO - __main__ - Global step 650 Train loss 1.20 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 02:35:32 - INFO - __main__ - Step 660 Global step 660 Train loss 1.13 on epoch=329
05/17/2022 02:35:34 - INFO - __main__ - Step 670 Global step 670 Train loss 1.01 on epoch=334
05/17/2022 02:35:36 - INFO - __main__ - Step 680 Global step 680 Train loss 1.03 on epoch=339
05/17/2022 02:35:38 - INFO - __main__ - Step 690 Global step 690 Train loss 1.02 on epoch=344
05/17/2022 02:35:39 - INFO - __main__ - Step 700 Global step 700 Train loss 1.02 on epoch=349
05/17/2022 02:35:40 - INFO - __main__ - Global step 700 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 02:35:42 - INFO - __main__ - Step 710 Global step 710 Train loss 1.08 on epoch=354
05/17/2022 02:35:44 - INFO - __main__ - Step 720 Global step 720 Train loss 1.00 on epoch=359
05/17/2022 02:35:46 - INFO - __main__ - Step 730 Global step 730 Train loss 1.16 on epoch=364
05/17/2022 02:35:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.96 on epoch=369
05/17/2022 02:35:50 - INFO - __main__ - Step 750 Global step 750 Train loss 1.01 on epoch=374
05/17/2022 02:35:50 - INFO - __main__ - Global step 750 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 02:35:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.90 on epoch=379
05/17/2022 02:35:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.91 on epoch=384
05/17/2022 02:35:56 - INFO - __main__ - Step 780 Global step 780 Train loss 1.03 on epoch=389
05/17/2022 02:35:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.89 on epoch=394
05/17/2022 02:36:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.87 on epoch=399
05/17/2022 02:36:05 - INFO - __main__ - Global step 800 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 02:36:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.92 on epoch=404
05/17/2022 02:36:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.90 on epoch=409
05/17/2022 02:36:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.80 on epoch=414
05/17/2022 02:36:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.84 on epoch=419
05/17/2022 02:36:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=424
05/17/2022 02:36:15 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 02:36:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.79 on epoch=429
05/17/2022 02:36:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.83 on epoch=434
05/17/2022 02:36:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.74 on epoch=439
05/17/2022 02:36:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.85 on epoch=444
05/17/2022 02:36:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.76 on epoch=449
05/17/2022 02:36:25 - INFO - __main__ - Global step 900 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 02:36:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.80 on epoch=454
05/17/2022 02:36:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.73 on epoch=459
05/17/2022 02:36:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.78 on epoch=464
05/17/2022 02:36:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.79 on epoch=469
05/17/2022 02:36:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.78 on epoch=474
05/17/2022 02:36:36 - INFO - __main__ - Global step 950 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 02:36:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.71 on epoch=479
05/17/2022 02:36:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.82 on epoch=484
05/17/2022 02:36:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.84 on epoch=489
05/17/2022 02:36:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=494
05/17/2022 02:36:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.77 on epoch=499
05/17/2022 02:36:46 - INFO - __main__ - Global step 1000 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 02:36:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.76 on epoch=504
05/17/2022 02:36:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.71 on epoch=509
05/17/2022 02:36:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.78 on epoch=514
05/17/2022 02:36:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.68 on epoch=519
05/17/2022 02:36:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.72 on epoch=524
05/17/2022 02:36:57 - INFO - __main__ - Global step 1050 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 02:36:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.73 on epoch=529
05/17/2022 02:37:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=534
05/17/2022 02:37:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.66 on epoch=539
05/17/2022 02:37:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.62 on epoch=544
05/17/2022 02:37:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.74 on epoch=549
05/17/2022 02:37:07 - INFO - __main__ - Global step 1100 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 02:37:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.71 on epoch=554
05/17/2022 02:37:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.78 on epoch=559
05/17/2022 02:37:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.70 on epoch=564
05/17/2022 02:37:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.72 on epoch=569
05/17/2022 02:37:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.71 on epoch=574
05/17/2022 02:37:17 - INFO - __main__ - Global step 1150 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 02:37:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.67 on epoch=579
05/17/2022 02:37:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.72 on epoch=584
05/17/2022 02:37:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.74 on epoch=589
05/17/2022 02:37:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.70 on epoch=594
05/17/2022 02:37:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.61 on epoch=599
05/17/2022 02:37:28 - INFO - __main__ - Global step 1200 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 02:37:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.66 on epoch=604
05/17/2022 02:37:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.63 on epoch=609
05/17/2022 02:37:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.66 on epoch=614
05/17/2022 02:37:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.66 on epoch=619
05/17/2022 02:37:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.59 on epoch=624
05/17/2022 02:37:38 - INFO - __main__ - Global step 1250 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 02:37:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.60 on epoch=629
05/17/2022 02:37:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.62 on epoch=634
05/17/2022 02:37:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.70 on epoch=639
05/17/2022 02:37:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.59 on epoch=644
05/17/2022 02:37:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.61 on epoch=649
05/17/2022 02:37:49 - INFO - __main__ - Global step 1300 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 02:37:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.54 on epoch=654
05/17/2022 02:37:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.54 on epoch=659
05/17/2022 02:37:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.55 on epoch=664
05/17/2022 02:37:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.54 on epoch=669
05/17/2022 02:37:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.62 on epoch=674
05/17/2022 02:37:59 - INFO - __main__ - Global step 1350 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 02:38:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.60 on epoch=679
05/17/2022 02:38:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.54 on epoch=684
05/17/2022 02:38:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
05/17/2022 02:38:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.53 on epoch=694
05/17/2022 02:38:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.58 on epoch=699
05/17/2022 02:38:09 - INFO - __main__ - Global step 1400 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 02:38:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.50 on epoch=704
05/17/2022 02:38:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=709
05/17/2022 02:38:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=714
05/17/2022 02:38:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.55 on epoch=719
05/17/2022 02:38:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.54 on epoch=724
05/17/2022 02:38:20 - INFO - __main__ - Global step 1450 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 02:38:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=729
05/17/2022 02:38:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.57 on epoch=734
05/17/2022 02:38:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.50 on epoch=739
05/17/2022 02:38:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.53 on epoch=744
05/17/2022 02:38:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.51 on epoch=749
05/17/2022 02:38:30 - INFO - __main__ - Global step 1500 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 02:38:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.50 on epoch=754
05/17/2022 02:38:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=759
05/17/2022 02:38:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.49 on epoch=764
05/17/2022 02:38:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.52 on epoch=769
05/17/2022 02:38:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.50 on epoch=774
05/17/2022 02:38:40 - INFO - __main__ - Global step 1550 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 02:38:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=779
05/17/2022 02:38:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.52 on epoch=784
05/17/2022 02:38:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
05/17/2022 02:38:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.52 on epoch=794
05/17/2022 02:38:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.46 on epoch=799
05/17/2022 02:38:51 - INFO - __main__ - Global step 1600 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 02:38:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
05/17/2022 02:38:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=809
05/17/2022 02:38:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=814
05/17/2022 02:38:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=819
05/17/2022 02:39:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=824
05/17/2022 02:39:01 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 02:39:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=829
05/17/2022 02:39:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=834
05/17/2022 02:39:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.48 on epoch=839
05/17/2022 02:39:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
05/17/2022 02:39:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=849
05/17/2022 02:39:11 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 02:39:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=854
05/17/2022 02:39:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=859
05/17/2022 02:39:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=864
05/17/2022 02:39:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=869
05/17/2022 02:39:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=874
05/17/2022 02:39:21 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 02:39:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=879
05/17/2022 02:39:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
05/17/2022 02:39:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=889
05/17/2022 02:39:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
05/17/2022 02:39:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
05/17/2022 02:39:32 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 02:39:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=904
05/17/2022 02:39:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
05/17/2022 02:39:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=914
05/17/2022 02:39:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=919
05/17/2022 02:39:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=924
05/17/2022 02:39:42 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 02:39:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
05/17/2022 02:39:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=934
05/17/2022 02:39:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.44 on epoch=939
05/17/2022 02:39:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.48 on epoch=944
05/17/2022 02:39:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=949
05/17/2022 02:39:52 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 02:39:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=954
05/17/2022 02:39:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.46 on epoch=959
05/17/2022 02:39:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.43 on epoch=964
05/17/2022 02:40:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
05/17/2022 02:40:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/17/2022 02:40:02 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 02:40:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=979
05/17/2022 02:40:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=984
05/17/2022 02:40:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=989
05/17/2022 02:40:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/17/2022 02:40:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=999
05/17/2022 02:40:13 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 02:40:13 - INFO - __main__ - save last model!
05/17/2022 02:40:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 02:40:13 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 02:40:13 - INFO - __main__ - Printing 3 examples
05/17/2022 02:40:13 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:40:13 - INFO - __main__ - ['entailed']
05/17/2022 02:40:13 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:40:13 - INFO - __main__ - ['entailed']
05/17/2022 02:40:13 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:40:13 - INFO - __main__ - ['entailed']
05/17/2022 02:40:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:40:13 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:40:13 - INFO - __main__ - Printing 3 examples
05/17/2022 02:40:13 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/17/2022 02:40:13 - INFO - __main__ - ['refuted']
05/17/2022 02:40:13 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/17/2022 02:40:13 - INFO - __main__ - ['refuted']
05/17/2022 02:40:13 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/17/2022 02:40:13 - INFO - __main__ - ['refuted']
05/17/2022 02:40:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:40:13 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:40:13 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 02:40:13 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:40:13 - INFO - __main__ - Printing 3 examples
05/17/2022 02:40:13 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/17/2022 02:40:13 - INFO - __main__ - ['refuted']
05/17/2022 02:40:13 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/17/2022 02:40:13 - INFO - __main__ - ['refuted']
05/17/2022 02:40:13 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/17/2022 02:40:13 - INFO - __main__ - ['refuted']
05/17/2022 02:40:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:40:13 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:40:13 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 02:40:19 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 02:40:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 02:40:19 - INFO - __main__ - Starting training!
05/17/2022 02:40:37 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:40:49 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 02:44:57 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.3_8_predictions.txt
05/17/2022 02:44:57 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 02:44:57 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 02:44:57 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.2, bsz=8 ...
05/17/2022 02:44:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:44:58 - INFO - __main__ - Printing 3 examples
05/17/2022 02:44:58 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/17/2022 02:44:58 - INFO - __main__ - ['refuted']
05/17/2022 02:44:58 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/17/2022 02:44:58 - INFO - __main__ - ['refuted']
05/17/2022 02:44:58 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/17/2022 02:44:58 - INFO - __main__ - ['refuted']
05/17/2022 02:44:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:44:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:44:58 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 02:44:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:44:58 - INFO - __main__ - Printing 3 examples
05/17/2022 02:44:58 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/17/2022 02:44:58 - INFO - __main__ - ['refuted']
05/17/2022 02:44:58 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/17/2022 02:44:58 - INFO - __main__ - ['refuted']
05/17/2022 02:44:58 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/17/2022 02:44:58 - INFO - __main__ - ['refuted']
05/17/2022 02:44:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:44:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:44:58 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 02:45:03 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 02:45:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 02:45:04 - INFO - __main__ - Starting training!
05/17/2022 02:45:06 - INFO - __main__ - Step 10 Global step 10 Train loss 5.02 on epoch=4
05/17/2022 02:45:08 - INFO - __main__ - Step 20 Global step 20 Train loss 4.94 on epoch=9
05/17/2022 02:45:10 - INFO - __main__ - Step 30 Global step 30 Train loss 5.06 on epoch=14
05/17/2022 02:45:12 - INFO - __main__ - Step 40 Global step 40 Train loss 4.93 on epoch=19
05/17/2022 02:45:13 - INFO - __main__ - Step 50 Global step 50 Train loss 4.78 on epoch=24
05/17/2022 02:45:15 - INFO - __main__ - Global step 50 Train loss 4.95 Classification-F1 0.0 on epoch=24
05/17/2022 02:45:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 02:45:16 - INFO - __main__ - Step 60 Global step 60 Train loss 4.64 on epoch=29
05/17/2022 02:45:18 - INFO - __main__ - Step 70 Global step 70 Train loss 4.87 on epoch=34
05/17/2022 02:45:20 - INFO - __main__ - Step 80 Global step 80 Train loss 4.69 on epoch=39
05/17/2022 02:45:22 - INFO - __main__ - Step 90 Global step 90 Train loss 4.57 on epoch=44
05/17/2022 02:45:24 - INFO - __main__ - Step 100 Global step 100 Train loss 4.59 on epoch=49
05/17/2022 02:45:25 - INFO - __main__ - Global step 100 Train loss 4.67 Classification-F1 0.0 on epoch=49
05/17/2022 02:45:27 - INFO - __main__ - Step 110 Global step 110 Train loss 4.47 on epoch=54
05/17/2022 02:45:29 - INFO - __main__ - Step 120 Global step 120 Train loss 4.57 on epoch=59
05/17/2022 02:45:31 - INFO - __main__ - Step 130 Global step 130 Train loss 4.49 on epoch=64
05/17/2022 02:45:33 - INFO - __main__ - Step 140 Global step 140 Train loss 4.45 on epoch=69
05/17/2022 02:45:35 - INFO - __main__ - Step 150 Global step 150 Train loss 4.39 on epoch=74
05/17/2022 02:45:36 - INFO - __main__ - Global step 150 Train loss 4.47 Classification-F1 0.0 on epoch=74
05/17/2022 02:45:38 - INFO - __main__ - Step 160 Global step 160 Train loss 4.35 on epoch=79
05/17/2022 02:45:40 - INFO - __main__ - Step 170 Global step 170 Train loss 4.32 on epoch=84
05/17/2022 02:45:42 - INFO - __main__ - Step 180 Global step 180 Train loss 4.30 on epoch=89
05/17/2022 02:45:43 - INFO - __main__ - Step 190 Global step 190 Train loss 4.26 on epoch=94
05/17/2022 02:45:45 - INFO - __main__ - Step 200 Global step 200 Train loss 4.22 on epoch=99
05/17/2022 02:45:47 - INFO - __main__ - Global step 200 Train loss 4.29 Classification-F1 0.0 on epoch=99
05/17/2022 02:45:48 - INFO - __main__ - Step 210 Global step 210 Train loss 4.14 on epoch=104
05/17/2022 02:45:50 - INFO - __main__ - Step 220 Global step 220 Train loss 4.06 on epoch=109
05/17/2022 02:45:52 - INFO - __main__ - Step 230 Global step 230 Train loss 4.18 on epoch=114
05/17/2022 02:45:54 - INFO - __main__ - Step 240 Global step 240 Train loss 4.07 on epoch=119
05/17/2022 02:45:56 - INFO - __main__ - Step 250 Global step 250 Train loss 3.99 on epoch=124
05/17/2022 02:45:57 - INFO - __main__ - Global step 250 Train loss 4.09 Classification-F1 0.0 on epoch=124
05/17/2022 02:45:59 - INFO - __main__ - Step 260 Global step 260 Train loss 3.92 on epoch=129
05/17/2022 02:46:01 - INFO - __main__ - Step 270 Global step 270 Train loss 3.87 on epoch=134
05/17/2022 02:46:03 - INFO - __main__ - Step 280 Global step 280 Train loss 3.95 on epoch=139
05/17/2022 02:46:05 - INFO - __main__ - Step 290 Global step 290 Train loss 3.87 on epoch=144
05/17/2022 02:46:07 - INFO - __main__ - Step 300 Global step 300 Train loss 3.83 on epoch=149
05/17/2022 02:46:08 - INFO - __main__ - Global step 300 Train loss 3.89 Classification-F1 0.0 on epoch=149
05/17/2022 02:46:10 - INFO - __main__ - Step 310 Global step 310 Train loss 3.68 on epoch=154
05/17/2022 02:46:12 - INFO - __main__ - Step 320 Global step 320 Train loss 3.76 on epoch=159
05/17/2022 02:46:14 - INFO - __main__ - Step 330 Global step 330 Train loss 3.62 on epoch=164
05/17/2022 02:46:16 - INFO - __main__ - Step 340 Global step 340 Train loss 3.68 on epoch=169
05/17/2022 02:46:18 - INFO - __main__ - Step 350 Global step 350 Train loss 3.69 on epoch=174
05/17/2022 02:46:20 - INFO - __main__ - Global step 350 Train loss 3.69 Classification-F1 0.0 on epoch=174
05/17/2022 02:46:21 - INFO - __main__ - Step 360 Global step 360 Train loss 3.58 on epoch=179
05/17/2022 02:46:23 - INFO - __main__ - Step 370 Global step 370 Train loss 3.53 on epoch=184
05/17/2022 02:46:25 - INFO - __main__ - Step 380 Global step 380 Train loss 3.50 on epoch=189
05/17/2022 02:46:27 - INFO - __main__ - Step 390 Global step 390 Train loss 3.41 on epoch=194
05/17/2022 02:46:29 - INFO - __main__ - Step 400 Global step 400 Train loss 3.41 on epoch=199
05/17/2022 02:46:31 - INFO - __main__ - Global step 400 Train loss 3.48 Classification-F1 0.07777777777777778 on epoch=199
05/17/2022 02:46:31 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.07777777777777778 on epoch=199, global_step=400
05/17/2022 02:46:33 - INFO - __main__ - Step 410 Global step 410 Train loss 3.31 on epoch=204
05/17/2022 02:46:35 - INFO - __main__ - Step 420 Global step 420 Train loss 3.31 on epoch=209
05/17/2022 02:46:36 - INFO - __main__ - Step 430 Global step 430 Train loss 3.24 on epoch=214
05/17/2022 02:46:38 - INFO - __main__ - Step 440 Global step 440 Train loss 3.21 on epoch=219
05/17/2022 02:46:40 - INFO - __main__ - Step 450 Global step 450 Train loss 3.19 on epoch=224
05/17/2022 02:46:42 - INFO - __main__ - Global step 450 Train loss 3.25 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 02:46:42 - INFO - __main__ - Saving model with best Classification-F1: 0.07777777777777778 -> 0.3333333333333333 on epoch=224, global_step=450
05/17/2022 02:46:44 - INFO - __main__ - Step 460 Global step 460 Train loss 3.14 on epoch=229
05/17/2022 02:46:46 - INFO - __main__ - Step 470 Global step 470 Train loss 3.10 on epoch=234
05/17/2022 02:46:47 - INFO - __main__ - Step 480 Global step 480 Train loss 2.98 on epoch=239
05/17/2022 02:46:49 - INFO - __main__ - Step 490 Global step 490 Train loss 3.04 on epoch=244
05/17/2022 02:46:51 - INFO - __main__ - Step 500 Global step 500 Train loss 2.94 on epoch=249
05/17/2022 02:46:56 - INFO - __main__ - Global step 500 Train loss 3.04 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 02:46:58 - INFO - __main__ - Step 510 Global step 510 Train loss 2.93 on epoch=254
05/17/2022 02:46:59 - INFO - __main__ - Step 520 Global step 520 Train loss 2.88 on epoch=259
05/17/2022 02:47:01 - INFO - __main__ - Step 530 Global step 530 Train loss 2.80 on epoch=264
05/17/2022 02:47:03 - INFO - __main__ - Step 540 Global step 540 Train loss 2.80 on epoch=269
05/17/2022 02:47:05 - INFO - __main__ - Step 550 Global step 550 Train loss 2.69 on epoch=274
05/17/2022 02:47:08 - INFO - __main__ - Global step 550 Train loss 2.82 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 02:47:10 - INFO - __main__ - Step 560 Global step 560 Train loss 2.67 on epoch=279
05/17/2022 02:47:12 - INFO - __main__ - Step 570 Global step 570 Train loss 2.59 on epoch=284
05/17/2022 02:47:14 - INFO - __main__ - Step 580 Global step 580 Train loss 2.49 on epoch=289
05/17/2022 02:47:16 - INFO - __main__ - Step 590 Global step 590 Train loss 2.65 on epoch=294
05/17/2022 02:47:18 - INFO - __main__ - Step 600 Global step 600 Train loss 2.58 on epoch=299
05/17/2022 02:47:22 - INFO - __main__ - Global step 600 Train loss 2.60 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 02:47:24 - INFO - __main__ - Step 610 Global step 610 Train loss 2.54 on epoch=304
05/17/2022 02:47:25 - INFO - __main__ - Step 620 Global step 620 Train loss 2.37 on epoch=309
05/17/2022 02:47:27 - INFO - __main__ - Step 630 Global step 630 Train loss 2.29 on epoch=314
05/17/2022 02:47:29 - INFO - __main__ - Step 640 Global step 640 Train loss 2.32 on epoch=319
05/17/2022 02:47:31 - INFO - __main__ - Step 650 Global step 650 Train loss 2.40 on epoch=324
05/17/2022 02:47:35 - INFO - __main__ - Global step 650 Train loss 2.38 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 02:47:36 - INFO - __main__ - Step 660 Global step 660 Train loss 2.33 on epoch=329
05/17/2022 02:47:38 - INFO - __main__ - Step 670 Global step 670 Train loss 2.19 on epoch=334
05/17/2022 02:47:40 - INFO - __main__ - Step 680 Global step 680 Train loss 2.20 on epoch=339
05/17/2022 02:47:42 - INFO - __main__ - Step 690 Global step 690 Train loss 2.23 on epoch=344
05/17/2022 02:47:44 - INFO - __main__ - Step 700 Global step 700 Train loss 2.26 on epoch=349
05/17/2022 02:47:47 - INFO - __main__ - Global step 700 Train loss 2.24 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 02:47:49 - INFO - __main__ - Step 710 Global step 710 Train loss 2.04 on epoch=354
05/17/2022 02:47:51 - INFO - __main__ - Step 720 Global step 720 Train loss 2.10 on epoch=359
05/17/2022 02:47:53 - INFO - __main__ - Step 730 Global step 730 Train loss 2.03 on epoch=364
05/17/2022 02:47:55 - INFO - __main__ - Step 740 Global step 740 Train loss 2.04 on epoch=369
05/17/2022 02:47:57 - INFO - __main__ - Step 750 Global step 750 Train loss 2.06 on epoch=374
05/17/2022 02:48:00 - INFO - __main__ - Global step 750 Train loss 2.06 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 02:48:02 - INFO - __main__ - Step 760 Global step 760 Train loss 2.04 on epoch=379
05/17/2022 02:48:03 - INFO - __main__ - Step 770 Global step 770 Train loss 1.92 on epoch=384
05/17/2022 02:48:05 - INFO - __main__ - Step 780 Global step 780 Train loss 1.88 on epoch=389
05/17/2022 02:48:07 - INFO - __main__ - Step 790 Global step 790 Train loss 1.88 on epoch=394
05/17/2022 02:48:09 - INFO - __main__ - Step 800 Global step 800 Train loss 1.80 on epoch=399
05/17/2022 02:48:10 - INFO - __main__ - Global step 800 Train loss 1.91 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 02:48:12 - INFO - __main__ - Step 810 Global step 810 Train loss 1.77 on epoch=404
05/17/2022 02:48:14 - INFO - __main__ - Step 820 Global step 820 Train loss 1.66 on epoch=409
05/17/2022 02:48:15 - INFO - __main__ - Step 830 Global step 830 Train loss 1.76 on epoch=414
05/17/2022 02:48:17 - INFO - __main__ - Step 840 Global step 840 Train loss 1.59 on epoch=419
05/17/2022 02:48:19 - INFO - __main__ - Step 850 Global step 850 Train loss 1.57 on epoch=424
05/17/2022 02:48:21 - INFO - __main__ - Global step 850 Train loss 1.67 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 02:48:22 - INFO - __main__ - Step 860 Global step 860 Train loss 1.65 on epoch=429
05/17/2022 02:48:24 - INFO - __main__ - Step 870 Global step 870 Train loss 1.80 on epoch=434
05/17/2022 02:48:26 - INFO - __main__ - Step 880 Global step 880 Train loss 1.54 on epoch=439
05/17/2022 02:48:28 - INFO - __main__ - Step 890 Global step 890 Train loss 1.56 on epoch=444
05/17/2022 02:48:30 - INFO - __main__ - Step 900 Global step 900 Train loss 1.53 on epoch=449
05/17/2022 02:48:32 - INFO - __main__ - Global step 900 Train loss 1.62 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 02:48:34 - INFO - __main__ - Step 910 Global step 910 Train loss 1.41 on epoch=454
05/17/2022 02:48:36 - INFO - __main__ - Step 920 Global step 920 Train loss 1.49 on epoch=459
05/17/2022 02:48:38 - INFO - __main__ - Step 930 Global step 930 Train loss 1.50 on epoch=464
05/17/2022 02:48:40 - INFO - __main__ - Step 940 Global step 940 Train loss 1.32 on epoch=469
05/17/2022 02:48:42 - INFO - __main__ - Step 950 Global step 950 Train loss 1.31 on epoch=474
05/17/2022 02:48:44 - INFO - __main__ - Global step 950 Train loss 1.40 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 02:48:46 - INFO - __main__ - Step 960 Global step 960 Train loss 1.35 on epoch=479
05/17/2022 02:48:48 - INFO - __main__ - Step 970 Global step 970 Train loss 1.42 on epoch=484
05/17/2022 02:48:50 - INFO - __main__ - Step 980 Global step 980 Train loss 1.28 on epoch=489
05/17/2022 02:48:52 - INFO - __main__ - Step 990 Global step 990 Train loss 1.29 on epoch=494
05/17/2022 02:48:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.29 on epoch=499
05/17/2022 02:48:56 - INFO - __main__ - Global step 1000 Train loss 1.33 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 02:48:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.32 on epoch=504
05/17/2022 02:49:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.29 on epoch=509
05/17/2022 02:49:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.15 on epoch=514
05/17/2022 02:49:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.21 on epoch=519
05/17/2022 02:49:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.14 on epoch=524
05/17/2022 02:49:08 - INFO - __main__ - Global step 1050 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 02:49:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.13 on epoch=529
05/17/2022 02:49:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.14 on epoch=534
05/17/2022 02:49:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.12 on epoch=539
05/17/2022 02:49:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.16 on epoch=544
05/17/2022 02:49:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.12 on epoch=549
05/17/2022 02:49:20 - INFO - __main__ - Global step 1100 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 02:49:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.11 on epoch=554
05/17/2022 02:49:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.20 on epoch=559
05/17/2022 02:49:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.03 on epoch=564
05/17/2022 02:49:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.09 on epoch=569
05/17/2022 02:49:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.08 on epoch=574
05/17/2022 02:49:31 - INFO - __main__ - Global step 1150 Train loss 1.10 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 02:49:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.01 on epoch=579
05/17/2022 02:49:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.02 on epoch=584
05/17/2022 02:49:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.97 on epoch=589
05/17/2022 02:49:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.93 on epoch=594
05/17/2022 02:49:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.97 on epoch=599
05/17/2022 02:49:42 - INFO - __main__ - Global step 1200 Train loss 0.98 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 02:49:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.96 on epoch=604
05/17/2022 02:49:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.94 on epoch=609
05/17/2022 02:49:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.88 on epoch=614
05/17/2022 02:49:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.87 on epoch=619
05/17/2022 02:49:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.91 on epoch=624
05/17/2022 02:49:52 - INFO - __main__ - Global step 1250 Train loss 0.91 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 02:49:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.00 on epoch=629
05/17/2022 02:49:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.98 on epoch=634
05/17/2022 02:49:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.82 on epoch=639
05/17/2022 02:50:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.94 on epoch=644
05/17/2022 02:50:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.88 on epoch=649
05/17/2022 02:50:03 - INFO - __main__ - Global step 1300 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 02:50:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.82 on epoch=654
05/17/2022 02:50:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.88 on epoch=659
05/17/2022 02:50:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.83 on epoch=664
05/17/2022 02:50:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.97 on epoch=669
05/17/2022 02:50:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=674
05/17/2022 02:50:13 - INFO - __main__ - Global step 1350 Train loss 0.87 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 02:50:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.80 on epoch=679
05/17/2022 02:50:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.73 on epoch=684
05/17/2022 02:50:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.78 on epoch=689
05/17/2022 02:50:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.78 on epoch=694
05/17/2022 02:50:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.69 on epoch=699
05/17/2022 02:50:23 - INFO - __main__ - Global step 1400 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 02:50:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.81 on epoch=704
05/17/2022 02:50:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.73 on epoch=709
05/17/2022 02:50:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.69 on epoch=714
05/17/2022 02:50:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.73 on epoch=719
05/17/2022 02:50:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.70 on epoch=724
05/17/2022 02:50:34 - INFO - __main__ - Global step 1450 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 02:50:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.70 on epoch=729
05/17/2022 02:50:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.68 on epoch=734
05/17/2022 02:50:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.76 on epoch=739
05/17/2022 02:50:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.70 on epoch=744
05/17/2022 02:50:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.72 on epoch=749
05/17/2022 02:50:44 - INFO - __main__ - Global step 1500 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 02:50:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.72 on epoch=754
05/17/2022 02:50:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.66 on epoch=759
05/17/2022 02:50:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.67 on epoch=764
05/17/2022 02:50:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.68 on epoch=769
05/17/2022 02:50:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.68 on epoch=774
05/17/2022 02:50:55 - INFO - __main__ - Global step 1550 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 02:50:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.66 on epoch=779
05/17/2022 02:50:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.73 on epoch=784
05/17/2022 02:51:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.59 on epoch=789
05/17/2022 02:51:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.64 on epoch=794
05/17/2022 02:51:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.71 on epoch=799
05/17/2022 02:51:05 - INFO - __main__ - Global step 1600 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 02:51:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.71 on epoch=804
05/17/2022 02:51:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.64 on epoch=809
05/17/2022 02:51:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.55 on epoch=814
05/17/2022 02:51:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.64 on epoch=819
05/17/2022 02:51:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.62 on epoch=824
05/17/2022 02:51:15 - INFO - __main__ - Global step 1650 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 02:51:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.67 on epoch=829
05/17/2022 02:51:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.63 on epoch=834
05/17/2022 02:51:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.63 on epoch=839
05/17/2022 02:51:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.65 on epoch=844
05/17/2022 02:51:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.57 on epoch=849
05/17/2022 02:51:26 - INFO - __main__ - Global step 1700 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 02:51:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.53 on epoch=854
05/17/2022 02:51:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.59 on epoch=859
05/17/2022 02:51:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.67 on epoch=864
05/17/2022 02:51:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.57 on epoch=869
05/17/2022 02:51:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.57 on epoch=874
05/17/2022 02:51:36 - INFO - __main__ - Global step 1750 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 02:51:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.61 on epoch=879
05/17/2022 02:51:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.58 on epoch=884
05/17/2022 02:51:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.57 on epoch=889
05/17/2022 02:51:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=894
05/17/2022 02:51:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.55 on epoch=899
05/17/2022 02:51:47 - INFO - __main__ - Global step 1800 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 02:51:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.59 on epoch=904
05/17/2022 02:51:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.56 on epoch=909
05/17/2022 02:51:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=914
05/17/2022 02:51:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.54 on epoch=919
05/17/2022 02:51:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.50 on epoch=924
05/17/2022 02:51:57 - INFO - __main__ - Global step 1850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 02:51:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.52 on epoch=929
05/17/2022 02:52:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.49 on epoch=934
05/17/2022 02:52:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=939
05/17/2022 02:52:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
05/17/2022 02:52:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.57 on epoch=949
05/17/2022 02:52:08 - INFO - __main__ - Global step 1900 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 02:52:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.59 on epoch=954
05/17/2022 02:52:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.60 on epoch=959
05/17/2022 02:52:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.53 on epoch=964
05/17/2022 02:52:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.52 on epoch=969
05/17/2022 02:52:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.54 on epoch=974
05/17/2022 02:52:18 - INFO - __main__ - Global step 1950 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 02:52:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.48 on epoch=979
05/17/2022 02:52:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.51 on epoch=984
05/17/2022 02:52:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.56 on epoch=989
05/17/2022 02:52:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.51 on epoch=994
05/17/2022 02:52:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.50 on epoch=999
05/17/2022 02:52:28 - INFO - __main__ - Global step 2000 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 02:52:28 - INFO - __main__ - save last model!
05/17/2022 02:52:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 02:52:28 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 02:52:28 - INFO - __main__ - Printing 3 examples
05/17/2022 02:52:28 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:52:28 - INFO - __main__ - ['entailed']
05/17/2022 02:52:28 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:52:28 - INFO - __main__ - ['entailed']
05/17/2022 02:52:28 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 02:52:28 - INFO - __main__ - ['entailed']
05/17/2022 02:52:28 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:52:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:52:29 - INFO - __main__ - Printing 3 examples
05/17/2022 02:52:29 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/17/2022 02:52:29 - INFO - __main__ - ['entailed']
05/17/2022 02:52:29 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/17/2022 02:52:29 - INFO - __main__ - ['entailed']
05/17/2022 02:52:29 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/17/2022 02:52:29 - INFO - __main__ - ['entailed']
05/17/2022 02:52:29 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:52:29 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:52:29 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 02:52:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:52:29 - INFO - __main__ - Printing 3 examples
05/17/2022 02:52:29 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/17/2022 02:52:29 - INFO - __main__ - ['entailed']
05/17/2022 02:52:29 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/17/2022 02:52:29 - INFO - __main__ - ['entailed']
05/17/2022 02:52:29 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/17/2022 02:52:29 - INFO - __main__ - ['entailed']
05/17/2022 02:52:29 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:52:29 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:52:29 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 02:52:34 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 02:52:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 02:52:35 - INFO - __main__ - Starting training!
05/17/2022 02:52:52 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:53:05 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 02:58:46 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.2_8_predictions.txt
05/17/2022 02:58:46 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 02:58:47 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.2, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 02:58:47 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.5, bsz=8 ...
05/17/2022 02:58:48 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:58:48 - INFO - __main__ - Printing 3 examples
05/17/2022 02:58:48 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/17/2022 02:58:48 - INFO - __main__ - ['entailed']
05/17/2022 02:58:48 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/17/2022 02:58:48 - INFO - __main__ - ['entailed']
05/17/2022 02:58:48 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/17/2022 02:58:48 - INFO - __main__ - ['entailed']
05/17/2022 02:58:48 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:58:48 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:58:48 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 02:58:48 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 02:58:48 - INFO - __main__ - Printing 3 examples
05/17/2022 02:58:48 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/17/2022 02:58:48 - INFO - __main__ - ['entailed']
05/17/2022 02:58:48 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/17/2022 02:58:48 - INFO - __main__ - ['entailed']
05/17/2022 02:58:48 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/17/2022 02:58:48 - INFO - __main__ - ['entailed']
05/17/2022 02:58:48 - INFO - __main__ - Tokenizing Input ...
05/17/2022 02:58:48 - INFO - __main__ - Tokenizing Output ...
05/17/2022 02:58:48 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 02:58:53 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 02:58:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 02:58:54 - INFO - __main__ - Starting training!
05/17/2022 02:58:56 - INFO - __main__ - Step 10 Global step 10 Train loss 5.10 on epoch=4
05/17/2022 02:58:58 - INFO - __main__ - Step 20 Global step 20 Train loss 4.95 on epoch=9
05/17/2022 02:58:59 - INFO - __main__ - Step 30 Global step 30 Train loss 4.84 on epoch=14
05/17/2022 02:59:01 - INFO - __main__ - Step 40 Global step 40 Train loss 4.85 on epoch=19
05/17/2022 02:59:03 - INFO - __main__ - Step 50 Global step 50 Train loss 4.77 on epoch=24
05/17/2022 02:59:15 - INFO - __main__ - Global step 50 Train loss 4.90 Classification-F1 0.0 on epoch=24
05/17/2022 02:59:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 02:59:16 - INFO - __main__ - Step 60 Global step 60 Train loss 4.63 on epoch=29
05/17/2022 02:59:18 - INFO - __main__ - Step 70 Global step 70 Train loss 4.45 on epoch=34
05/17/2022 02:59:20 - INFO - __main__ - Step 80 Global step 80 Train loss 4.54 on epoch=39
05/17/2022 02:59:22 - INFO - __main__ - Step 90 Global step 90 Train loss 4.29 on epoch=44
05/17/2022 02:59:24 - INFO - __main__ - Step 100 Global step 100 Train loss 4.30 on epoch=49
05/17/2022 02:59:35 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/17/2022 02:59:37 - INFO - __main__ - Step 110 Global step 110 Train loss 4.17 on epoch=54
05/17/2022 02:59:39 - INFO - __main__ - Step 120 Global step 120 Train loss 3.95 on epoch=59
05/17/2022 02:59:41 - INFO - __main__ - Step 130 Global step 130 Train loss 4.03 on epoch=64
05/17/2022 02:59:43 - INFO - __main__ - Step 140 Global step 140 Train loss 3.80 on epoch=69
05/17/2022 02:59:45 - INFO - __main__ - Step 150 Global step 150 Train loss 3.72 on epoch=74
05/17/2022 02:59:51 - INFO - __main__ - Global step 150 Train loss 3.93 Classification-F1 0.04105571847507331 on epoch=74
05/17/2022 02:59:51 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.04105571847507331 on epoch=74, global_step=150
05/17/2022 02:59:53 - INFO - __main__ - Step 160 Global step 160 Train loss 3.54 on epoch=79
05/17/2022 02:59:55 - INFO - __main__ - Step 170 Global step 170 Train loss 3.37 on epoch=84
05/17/2022 02:59:57 - INFO - __main__ - Step 180 Global step 180 Train loss 3.27 on epoch=89
05/17/2022 02:59:59 - INFO - __main__ - Step 190 Global step 190 Train loss 3.11 on epoch=94
05/17/2022 03:00:01 - INFO - __main__ - Step 200 Global step 200 Train loss 2.94 on epoch=99
05/17/2022 03:00:06 - INFO - __main__ - Global step 200 Train loss 3.25 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 03:00:06 - INFO - __main__ - Saving model with best Classification-F1: 0.04105571847507331 -> 0.3333333333333333 on epoch=99, global_step=200
05/17/2022 03:00:08 - INFO - __main__ - Step 210 Global step 210 Train loss 2.80 on epoch=104
05/17/2022 03:00:10 - INFO - __main__ - Step 220 Global step 220 Train loss 2.64 on epoch=109
05/17/2022 03:00:11 - INFO - __main__ - Step 230 Global step 230 Train loss 2.63 on epoch=114
05/17/2022 03:00:13 - INFO - __main__ - Step 240 Global step 240 Train loss 2.53 on epoch=119
05/17/2022 03:00:15 - INFO - __main__ - Step 250 Global step 250 Train loss 2.28 on epoch=124
05/17/2022 03:00:19 - INFO - __main__ - Global step 250 Train loss 2.58 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 03:00:21 - INFO - __main__ - Step 260 Global step 260 Train loss 2.17 on epoch=129
05/17/2022 03:00:23 - INFO - __main__ - Step 270 Global step 270 Train loss 2.15 on epoch=134
05/17/2022 03:00:25 - INFO - __main__ - Step 280 Global step 280 Train loss 1.98 on epoch=139
05/17/2022 03:00:27 - INFO - __main__ - Step 290 Global step 290 Train loss 1.82 on epoch=144
05/17/2022 03:00:29 - INFO - __main__ - Step 300 Global step 300 Train loss 1.75 on epoch=149
05/17/2022 03:00:31 - INFO - __main__ - Global step 300 Train loss 1.97 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 03:00:33 - INFO - __main__ - Step 310 Global step 310 Train loss 1.81 on epoch=154
05/17/2022 03:00:35 - INFO - __main__ - Step 320 Global step 320 Train loss 1.71 on epoch=159
05/17/2022 03:00:37 - INFO - __main__ - Step 330 Global step 330 Train loss 1.73 on epoch=164
05/17/2022 03:00:39 - INFO - __main__ - Step 340 Global step 340 Train loss 1.57 on epoch=169
05/17/2022 03:00:41 - INFO - __main__ - Step 350 Global step 350 Train loss 1.63 on epoch=174
05/17/2022 03:00:43 - INFO - __main__ - Global step 350 Train loss 1.69 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 03:00:45 - INFO - __main__ - Step 360 Global step 360 Train loss 1.44 on epoch=179
05/17/2022 03:00:47 - INFO - __main__ - Step 370 Global step 370 Train loss 1.26 on epoch=184
05/17/2022 03:00:49 - INFO - __main__ - Step 380 Global step 380 Train loss 1.31 on epoch=189
05/17/2022 03:00:51 - INFO - __main__ - Step 390 Global step 390 Train loss 1.25 on epoch=194
05/17/2022 03:00:53 - INFO - __main__ - Step 400 Global step 400 Train loss 1.09 on epoch=199
05/17/2022 03:00:54 - INFO - __main__ - Global step 400 Train loss 1.27 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 03:00:56 - INFO - __main__ - Step 410 Global step 410 Train loss 1.02 on epoch=204
05/17/2022 03:00:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.94 on epoch=209
05/17/2022 03:01:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.97 on epoch=214
05/17/2022 03:01:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=219
05/17/2022 03:01:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=224
05/17/2022 03:01:06 - INFO - __main__ - Global step 450 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 03:01:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.86 on epoch=229
05/17/2022 03:01:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.81 on epoch=234
05/17/2022 03:01:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.83 on epoch=239
05/17/2022 03:01:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.78 on epoch=244
05/17/2022 03:01:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.74 on epoch=249
05/17/2022 03:01:16 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 03:01:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.67 on epoch=254
05/17/2022 03:01:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.76 on epoch=259
05/17/2022 03:01:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.64 on epoch=264
05/17/2022 03:01:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=269
05/17/2022 03:01:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.60 on epoch=274
05/17/2022 03:01:27 - INFO - __main__ - Global step 550 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 03:01:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=279
05/17/2022 03:01:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=284
05/17/2022 03:01:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=289
05/17/2022 03:01:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.57 on epoch=294
05/17/2022 03:01:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=299
05/17/2022 03:01:37 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 03:01:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.50 on epoch=304
05/17/2022 03:01:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=309
05/17/2022 03:01:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.63 on epoch=314
05/17/2022 03:01:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=319
05/17/2022 03:01:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.57 on epoch=324
05/17/2022 03:01:47 - INFO - __main__ - Global step 650 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 03:01:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=329
05/17/2022 03:01:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=334
05/17/2022 03:01:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.53 on epoch=339
05/17/2022 03:01:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=344
05/17/2022 03:01:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.53 on epoch=349
05/17/2022 03:01:58 - INFO - __main__ - Global step 700 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 03:02:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=354
05/17/2022 03:02:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=359
05/17/2022 03:02:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=364
05/17/2022 03:02:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=369
05/17/2022 03:02:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=374
05/17/2022 03:02:08 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 03:02:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=379
05/17/2022 03:02:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=384
05/17/2022 03:02:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=389
05/17/2022 03:02:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=394
05/17/2022 03:02:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=399
05/17/2022 03:02:18 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 03:02:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=404
05/17/2022 03:02:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=409
05/17/2022 03:02:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=414
05/17/2022 03:02:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=419
05/17/2022 03:02:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=424
05/17/2022 03:02:28 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 03:02:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=429
05/17/2022 03:02:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
05/17/2022 03:02:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=439
05/17/2022 03:02:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=444
05/17/2022 03:02:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=449
05/17/2022 03:02:38 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 03:02:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=454
05/17/2022 03:02:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=459
05/17/2022 03:02:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=464
05/17/2022 03:02:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=469
05/17/2022 03:02:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=474
05/17/2022 03:02:48 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 03:02:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=479
05/17/2022 03:02:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=484
05/17/2022 03:02:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=489
05/17/2022 03:02:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=494
05/17/2022 03:02:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=499
05/17/2022 03:02:58 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 03:03:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=504
05/17/2022 03:03:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=509
05/17/2022 03:03:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=514
05/17/2022 03:03:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=519
05/17/2022 03:03:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=524
05/17/2022 03:03:09 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 03:03:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=529
05/17/2022 03:03:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=534
05/17/2022 03:03:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=539
05/17/2022 03:03:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=544
05/17/2022 03:03:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=549
05/17/2022 03:03:19 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 03:03:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=554
05/17/2022 03:03:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=559
05/17/2022 03:03:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=564
05/17/2022 03:03:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=569
05/17/2022 03:03:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=574
05/17/2022 03:03:29 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 03:03:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=579
05/17/2022 03:03:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=584
05/17/2022 03:03:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
05/17/2022 03:03:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=594
05/17/2022 03:03:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=599
05/17/2022 03:03:39 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 03:03:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=604
05/17/2022 03:03:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=609
05/17/2022 03:03:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=614
05/17/2022 03:03:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=619
05/17/2022 03:03:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=624
05/17/2022 03:03:49 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 03:03:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=629
05/17/2022 03:03:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=634
05/17/2022 03:03:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=639
05/17/2022 03:03:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=644
05/17/2022 03:03:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=649
05/17/2022 03:03:59 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 03:04:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=654
05/17/2022 03:04:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=659
05/17/2022 03:04:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=664
05/17/2022 03:04:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=669
05/17/2022 03:04:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=674
05/17/2022 03:04:09 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 03:04:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=679
05/17/2022 03:04:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=684
05/17/2022 03:04:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=689
05/17/2022 03:04:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=694
05/17/2022 03:04:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=699
05/17/2022 03:04:19 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 03:04:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=704
05/17/2022 03:04:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=709
05/17/2022 03:04:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=714
05/17/2022 03:04:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=719
05/17/2022 03:04:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=724
05/17/2022 03:04:30 - INFO - __main__ - Global step 1450 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 03:04:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=729
05/17/2022 03:04:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=734
05/17/2022 03:04:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=739
05/17/2022 03:04:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=744
05/17/2022 03:04:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.28 on epoch=749
05/17/2022 03:04:40 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 03:04:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=754
05/17/2022 03:04:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=759
05/17/2022 03:04:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=764
05/17/2022 03:04:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=769
05/17/2022 03:04:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=774
05/17/2022 03:04:50 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 03:04:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=779
05/17/2022 03:04:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=784
05/17/2022 03:04:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=789
05/17/2022 03:04:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
05/17/2022 03:04:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=799
05/17/2022 03:05:00 - INFO - __main__ - Global step 1600 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 03:05:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=804
05/17/2022 03:05:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=809
05/17/2022 03:05:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.25 on epoch=814
05/17/2022 03:05:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.29 on epoch=819
05/17/2022 03:05:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=824
05/17/2022 03:05:10 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 03:05:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=829
05/17/2022 03:05:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=834
05/17/2022 03:05:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=839
05/17/2022 03:05:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=844
05/17/2022 03:05:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=849
05/17/2022 03:05:20 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 03:05:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.32 on epoch=854
05/17/2022 03:05:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=859
05/17/2022 03:05:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=864
05/17/2022 03:05:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=869
05/17/2022 03:05:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/17/2022 03:05:30 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 03:05:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=879
05/17/2022 03:05:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=884
05/17/2022 03:05:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=889
05/17/2022 03:05:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.29 on epoch=894
05/17/2022 03:05:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=899
05/17/2022 03:05:40 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 03:05:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=904
05/17/2022 03:05:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=909
05/17/2022 03:05:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.28 on epoch=914
05/17/2022 03:05:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=919
05/17/2022 03:05:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=924
05/17/2022 03:05:51 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 03:05:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=929
05/17/2022 03:05:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=934
05/17/2022 03:05:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=939
05/17/2022 03:05:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=944
05/17/2022 03:06:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.28 on epoch=949
05/17/2022 03:06:01 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 03:06:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=954
05/17/2022 03:06:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=959
05/17/2022 03:06:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=964
05/17/2022 03:06:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=969
05/17/2022 03:06:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=974
05/17/2022 03:06:11 - INFO - __main__ - Global step 1950 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 03:06:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=979
05/17/2022 03:06:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=984
05/17/2022 03:06:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.30 on epoch=989
05/17/2022 03:06:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=994
05/17/2022 03:06:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=999
05/17/2022 03:06:21 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 03:06:21 - INFO - __main__ - save last model!
05/17/2022 03:06:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 03:06:21 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 03:06:21 - INFO - __main__ - Printing 3 examples
05/17/2022 03:06:21 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:06:21 - INFO - __main__ - ['entailed']
05/17/2022 03:06:21 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:06:21 - INFO - __main__ - ['entailed']
05/17/2022 03:06:21 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:06:21 - INFO - __main__ - ['entailed']
05/17/2022 03:06:21 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:06:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:06:21 - INFO - __main__ - Printing 3 examples
05/17/2022 03:06:21 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/17/2022 03:06:21 - INFO - __main__ - ['entailed']
05/17/2022 03:06:21 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/17/2022 03:06:21 - INFO - __main__ - ['entailed']
05/17/2022 03:06:21 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/17/2022 03:06:21 - INFO - __main__ - ['entailed']
05/17/2022 03:06:21 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:06:22 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:06:22 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 03:06:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:06:22 - INFO - __main__ - Printing 3 examples
05/17/2022 03:06:22 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/17/2022 03:06:22 - INFO - __main__ - ['entailed']
05/17/2022 03:06:22 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/17/2022 03:06:22 - INFO - __main__ - ['entailed']
05/17/2022 03:06:22 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/17/2022 03:06:22 - INFO - __main__ - ['entailed']
05/17/2022 03:06:22 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:06:22 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:06:22 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 03:06:27 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 03:06:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 03:06:27 - INFO - __main__ - Starting training!
05/17/2022 03:06:45 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:06:57 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 03:11:06 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.5_8_predictions.txt
05/17/2022 03:11:06 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 03:11:06 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 03:11:06 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.4, bsz=8 ...
05/17/2022 03:11:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:11:07 - INFO - __main__ - Printing 3 examples
05/17/2022 03:11:07 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/17/2022 03:11:07 - INFO - __main__ - ['entailed']
05/17/2022 03:11:07 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/17/2022 03:11:07 - INFO - __main__ - ['entailed']
05/17/2022 03:11:07 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/17/2022 03:11:07 - INFO - __main__ - ['entailed']
05/17/2022 03:11:07 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:11:07 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:11:07 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 03:11:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:11:07 - INFO - __main__ - Printing 3 examples
05/17/2022 03:11:07 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/17/2022 03:11:07 - INFO - __main__ - ['entailed']
05/17/2022 03:11:07 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/17/2022 03:11:07 - INFO - __main__ - ['entailed']
05/17/2022 03:11:07 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/17/2022 03:11:07 - INFO - __main__ - ['entailed']
05/17/2022 03:11:07 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:11:07 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:11:07 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 03:11:13 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 03:11:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 03:11:13 - INFO - __main__ - Starting training!
05/17/2022 03:11:15 - INFO - __main__ - Step 10 Global step 10 Train loss 5.05 on epoch=4
05/17/2022 03:11:17 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/17/2022 03:11:19 - INFO - __main__ - Step 30 Global step 30 Train loss 4.91 on epoch=14
05/17/2022 03:11:21 - INFO - __main__ - Step 40 Global step 40 Train loss 4.82 on epoch=19
05/17/2022 03:11:23 - INFO - __main__ - Step 50 Global step 50 Train loss 4.74 on epoch=24
05/17/2022 03:11:23 - INFO - __main__ - Global step 50 Train loss 4.91 Classification-F1 0.0 on epoch=24
05/17/2022 03:11:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 03:11:25 - INFO - __main__ - Step 60 Global step 60 Train loss 4.68 on epoch=29
05/17/2022 03:11:27 - INFO - __main__ - Step 70 Global step 70 Train loss 4.55 on epoch=34
05/17/2022 03:11:29 - INFO - __main__ - Step 80 Global step 80 Train loss 4.47 on epoch=39
05/17/2022 03:11:31 - INFO - __main__ - Step 90 Global step 90 Train loss 4.28 on epoch=44
05/17/2022 03:11:33 - INFO - __main__ - Step 100 Global step 100 Train loss 4.26 on epoch=49
05/17/2022 03:11:34 - INFO - __main__ - Global step 100 Train loss 4.45 Classification-F1 0.0 on epoch=49
05/17/2022 03:11:36 - INFO - __main__ - Step 110 Global step 110 Train loss 4.06 on epoch=54
05/17/2022 03:11:38 - INFO - __main__ - Step 120 Global step 120 Train loss 3.95 on epoch=59
05/17/2022 03:11:40 - INFO - __main__ - Step 130 Global step 130 Train loss 3.91 on epoch=64
05/17/2022 03:11:41 - INFO - __main__ - Step 140 Global step 140 Train loss 3.88 on epoch=69
05/17/2022 03:11:43 - INFO - __main__ - Step 150 Global step 150 Train loss 3.79 on epoch=74
05/17/2022 03:11:44 - INFO - __main__ - Global step 150 Train loss 3.92 Classification-F1 0.13333333333333336 on epoch=74
05/17/2022 03:11:44 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.13333333333333336 on epoch=74, global_step=150
05/17/2022 03:11:46 - INFO - __main__ - Step 160 Global step 160 Train loss 3.65 on epoch=79
05/17/2022 03:11:48 - INFO - __main__ - Step 170 Global step 170 Train loss 3.44 on epoch=84
05/17/2022 03:11:50 - INFO - __main__ - Step 180 Global step 180 Train loss 3.48 on epoch=89
05/17/2022 03:11:52 - INFO - __main__ - Step 190 Global step 190 Train loss 3.32 on epoch=94
05/17/2022 03:11:54 - INFO - __main__ - Step 200 Global step 200 Train loss 3.28 on epoch=99
05/17/2022 03:11:55 - INFO - __main__ - Global step 200 Train loss 3.43 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 03:11:55 - INFO - __main__ - Saving model with best Classification-F1: 0.13333333333333336 -> 0.3333333333333333 on epoch=99, global_step=200
05/17/2022 03:11:57 - INFO - __main__ - Step 210 Global step 210 Train loss 3.14 on epoch=104
05/17/2022 03:11:59 - INFO - __main__ - Step 220 Global step 220 Train loss 3.05 on epoch=109
05/17/2022 03:12:00 - INFO - __main__ - Step 230 Global step 230 Train loss 2.92 on epoch=114
05/17/2022 03:12:02 - INFO - __main__ - Step 240 Global step 240 Train loss 2.86 on epoch=119
05/17/2022 03:12:04 - INFO - __main__ - Step 250 Global step 250 Train loss 2.73 on epoch=124
05/17/2022 03:12:06 - INFO - __main__ - Global step 250 Train loss 2.94 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 03:12:08 - INFO - __main__ - Step 260 Global step 260 Train loss 2.49 on epoch=129
05/17/2022 03:12:10 - INFO - __main__ - Step 270 Global step 270 Train loss 2.55 on epoch=134
05/17/2022 03:12:12 - INFO - __main__ - Step 280 Global step 280 Train loss 2.44 on epoch=139
05/17/2022 03:12:14 - INFO - __main__ - Step 290 Global step 290 Train loss 2.42 on epoch=144
05/17/2022 03:12:16 - INFO - __main__ - Step 300 Global step 300 Train loss 2.22 on epoch=149
05/17/2022 03:12:18 - INFO - __main__ - Global step 300 Train loss 2.42 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 03:12:20 - INFO - __main__ - Step 310 Global step 310 Train loss 2.33 on epoch=154
05/17/2022 03:12:21 - INFO - __main__ - Step 320 Global step 320 Train loss 2.20 on epoch=159
05/17/2022 03:12:23 - INFO - __main__ - Step 330 Global step 330 Train loss 2.22 on epoch=164
05/17/2022 03:12:25 - INFO - __main__ - Step 340 Global step 340 Train loss 2.09 on epoch=169
05/17/2022 03:12:27 - INFO - __main__ - Step 350 Global step 350 Train loss 2.01 on epoch=174
05/17/2022 03:12:29 - INFO - __main__ - Global step 350 Train loss 2.17 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 03:12:31 - INFO - __main__ - Step 360 Global step 360 Train loss 1.94 on epoch=179
05/17/2022 03:12:33 - INFO - __main__ - Step 370 Global step 370 Train loss 1.76 on epoch=184
05/17/2022 03:12:35 - INFO - __main__ - Step 380 Global step 380 Train loss 1.66 on epoch=189
05/17/2022 03:12:37 - INFO - __main__ - Step 390 Global step 390 Train loss 1.67 on epoch=194
05/17/2022 03:12:39 - INFO - __main__ - Step 400 Global step 400 Train loss 1.61 on epoch=199
05/17/2022 03:12:41 - INFO - __main__ - Global step 400 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 03:12:43 - INFO - __main__ - Step 410 Global step 410 Train loss 1.52 on epoch=204
05/17/2022 03:12:44 - INFO - __main__ - Step 420 Global step 420 Train loss 1.40 on epoch=209
05/17/2022 03:12:46 - INFO - __main__ - Step 430 Global step 430 Train loss 1.27 on epoch=214
05/17/2022 03:12:48 - INFO - __main__ - Step 440 Global step 440 Train loss 1.21 on epoch=219
05/17/2022 03:12:50 - INFO - __main__ - Step 450 Global step 450 Train loss 1.30 on epoch=224
05/17/2022 03:12:51 - INFO - __main__ - Global step 450 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 03:12:53 - INFO - __main__ - Step 460 Global step 460 Train loss 1.18 on epoch=229
05/17/2022 03:12:55 - INFO - __main__ - Step 470 Global step 470 Train loss 1.13 on epoch=234
05/17/2022 03:12:57 - INFO - __main__ - Step 480 Global step 480 Train loss 1.06 on epoch=239
05/17/2022 03:12:59 - INFO - __main__ - Step 490 Global step 490 Train loss 1.01 on epoch=244
05/17/2022 03:13:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.95 on epoch=249
05/17/2022 03:13:01 - INFO - __main__ - Global step 500 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 03:13:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.94 on epoch=254
05/17/2022 03:13:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=259
05/17/2022 03:13:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.83 on epoch=264
05/17/2022 03:13:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.76 on epoch=269
05/17/2022 03:13:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=274
05/17/2022 03:13:12 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 03:13:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.81 on epoch=279
05/17/2022 03:13:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.93 on epoch=284
05/17/2022 03:13:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.69 on epoch=289
05/17/2022 03:13:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.86 on epoch=294
05/17/2022 03:13:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.77 on epoch=299
05/17/2022 03:13:22 - INFO - __main__ - Global step 600 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 03:13:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=304
05/17/2022 03:13:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.72 on epoch=309
05/17/2022 03:13:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.73 on epoch=314
05/17/2022 03:13:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.72 on epoch=319
05/17/2022 03:13:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.70 on epoch=324
05/17/2022 03:13:33 - INFO - __main__ - Global step 650 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 03:13:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.66 on epoch=329
05/17/2022 03:13:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=334
05/17/2022 03:13:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=339
05/17/2022 03:13:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=344
05/17/2022 03:13:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.66 on epoch=349
05/17/2022 03:13:43 - INFO - __main__ - Global step 700 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 03:13:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.56 on epoch=354
05/17/2022 03:13:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
05/17/2022 03:13:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.69 on epoch=364
05/17/2022 03:13:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.57 on epoch=369
05/17/2022 03:13:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=374
05/17/2022 03:13:53 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 03:13:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.62 on epoch=379
05/17/2022 03:13:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=384
05/17/2022 03:13:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.67 on epoch=389
05/17/2022 03:14:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.54 on epoch=394
05/17/2022 03:14:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.56 on epoch=399
05/17/2022 03:14:04 - INFO - __main__ - Global step 800 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 03:14:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.58 on epoch=404
05/17/2022 03:14:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.56 on epoch=409
05/17/2022 03:14:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.55 on epoch=414
05/17/2022 03:14:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=419
05/17/2022 03:14:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=424
05/17/2022 03:14:14 - INFO - __main__ - Global step 850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 03:14:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=429
05/17/2022 03:14:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=434
05/17/2022 03:14:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=439
05/17/2022 03:14:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
05/17/2022 03:14:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=449
05/17/2022 03:14:24 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 03:14:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=454
05/17/2022 03:14:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.53 on epoch=459
05/17/2022 03:14:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=464
05/17/2022 03:14:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=469
05/17/2022 03:14:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=474
05/17/2022 03:14:34 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 03:14:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.51 on epoch=479
05/17/2022 03:14:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=484
05/17/2022 03:14:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=489
05/17/2022 03:14:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=494
05/17/2022 03:14:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=499
05/17/2022 03:14:45 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 03:14:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=504
05/17/2022 03:14:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=509
05/17/2022 03:14:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=514
05/17/2022 03:14:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=519
05/17/2022 03:14:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=524
05/17/2022 03:14:55 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 03:14:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=529
05/17/2022 03:14:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=534
05/17/2022 03:15:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.51 on epoch=539
05/17/2022 03:15:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=544
05/17/2022 03:15:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=549
05/17/2022 03:15:05 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 03:15:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=554
05/17/2022 03:15:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=559
05/17/2022 03:15:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=564
05/17/2022 03:15:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=569
05/17/2022 03:15:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=574
05/17/2022 03:15:15 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 03:15:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=579
05/17/2022 03:15:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=584
05/17/2022 03:15:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=589
05/17/2022 03:15:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=594
05/17/2022 03:15:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=599
05/17/2022 03:15:25 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=599
05/17/2022 03:15:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=599, global_step=1200
05/17/2022 03:15:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=604
05/17/2022 03:15:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=609
05/17/2022 03:15:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=614
05/17/2022 03:15:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=619
05/17/2022 03:15:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=624
05/17/2022 03:15:36 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 03:15:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=629
05/17/2022 03:15:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=634
05/17/2022 03:15:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=639
05/17/2022 03:15:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=644
05/17/2022 03:15:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=649
05/17/2022 03:15:46 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.3191489361702127 on epoch=649
05/17/2022 03:15:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=654
05/17/2022 03:15:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=659
05/17/2022 03:15:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=664
05/17/2022 03:15:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=669
05/17/2022 03:15:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=674
05/17/2022 03:15:56 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 03:15:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=679
05/17/2022 03:16:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=684
05/17/2022 03:16:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=689
05/17/2022 03:16:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=694
05/17/2022 03:16:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=699
05/17/2022 03:16:06 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 03:16:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=704
05/17/2022 03:16:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=709
05/17/2022 03:16:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=714
05/17/2022 03:16:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=719
05/17/2022 03:16:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=724
05/17/2022 03:16:16 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 03:16:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=729
05/17/2022 03:16:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=734
05/17/2022 03:16:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=739
05/17/2022 03:16:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=744
05/17/2022 03:16:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=749
05/17/2022 03:16:27 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 03:16:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=754
05/17/2022 03:16:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=759
05/17/2022 03:16:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=764
05/17/2022 03:16:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=769
05/17/2022 03:16:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=774
05/17/2022 03:16:37 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 03:16:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=779
05/17/2022 03:16:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
05/17/2022 03:16:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
05/17/2022 03:16:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=794
05/17/2022 03:16:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=799
05/17/2022 03:16:47 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 03:16:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=804
05/17/2022 03:16:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.33 on epoch=809
05/17/2022 03:16:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=814
05/17/2022 03:16:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=819
05/17/2022 03:16:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=824
05/17/2022 03:16:57 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 03:16:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=829
05/17/2022 03:17:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=834
05/17/2022 03:17:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
05/17/2022 03:17:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=844
05/17/2022 03:17:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=849
05/17/2022 03:17:07 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 03:17:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=854
05/17/2022 03:17:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.31 on epoch=859
05/17/2022 03:17:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/17/2022 03:17:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=869
05/17/2022 03:17:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=874
05/17/2022 03:17:17 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 03:17:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=879
05/17/2022 03:17:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=884
05/17/2022 03:17:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=889
05/17/2022 03:17:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=894
05/17/2022 03:17:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
05/17/2022 03:17:28 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 03:17:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=904
05/17/2022 03:17:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/17/2022 03:17:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=914
05/17/2022 03:17:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=919
05/17/2022 03:17:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
05/17/2022 03:17:38 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 03:17:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
05/17/2022 03:17:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
05/17/2022 03:17:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.32 on epoch=939
05/17/2022 03:17:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=944
05/17/2022 03:17:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
05/17/2022 03:17:48 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 03:17:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
05/17/2022 03:17:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=959
05/17/2022 03:17:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=964
05/17/2022 03:17:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
05/17/2022 03:17:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
05/17/2022 03:17:58 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 03:18:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
05/17/2022 03:18:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=984
05/17/2022 03:18:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.27 on epoch=989
05/17/2022 03:18:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/17/2022 03:18:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=999
05/17/2022 03:18:08 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 03:18:08 - INFO - __main__ - save last model!
05/17/2022 03:18:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 03:18:08 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 03:18:08 - INFO - __main__ - Printing 3 examples
05/17/2022 03:18:08 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:18:08 - INFO - __main__ - ['entailed']
05/17/2022 03:18:08 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:18:08 - INFO - __main__ - ['entailed']
05/17/2022 03:18:08 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:18:08 - INFO - __main__ - ['entailed']
05/17/2022 03:18:08 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:18:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:18:09 - INFO - __main__ - Printing 3 examples
05/17/2022 03:18:09 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/17/2022 03:18:09 - INFO - __main__ - ['entailed']
05/17/2022 03:18:09 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/17/2022 03:18:09 - INFO - __main__ - ['entailed']
05/17/2022 03:18:09 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/17/2022 03:18:09 - INFO - __main__ - ['entailed']
05/17/2022 03:18:09 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:18:09 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:18:09 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 03:18:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:18:09 - INFO - __main__ - Printing 3 examples
05/17/2022 03:18:09 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/17/2022 03:18:09 - INFO - __main__ - ['entailed']
05/17/2022 03:18:09 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/17/2022 03:18:09 - INFO - __main__ - ['entailed']
05/17/2022 03:18:09 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/17/2022 03:18:09 - INFO - __main__ - ['entailed']
05/17/2022 03:18:09 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:18:09 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:18:09 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 03:18:15 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 03:18:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 03:18:15 - INFO - __main__ - Starting training!
05/17/2022 03:18:32 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:18:45 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 03:22:52 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.4_8_predictions.txt
05/17/2022 03:22:52 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 03:22:52 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.4, bsz=8, dev_performance=0.3992490613266583, test_performance=0.33047210300429186
05/17/2022 03:22:52 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.3, bsz=8 ...
05/17/2022 03:22:53 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:22:53 - INFO - __main__ - Printing 3 examples
05/17/2022 03:22:53 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/17/2022 03:22:53 - INFO - __main__ - ['entailed']
05/17/2022 03:22:53 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/17/2022 03:22:53 - INFO - __main__ - ['entailed']
05/17/2022 03:22:53 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/17/2022 03:22:53 - INFO - __main__ - ['entailed']
05/17/2022 03:22:53 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:22:53 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:22:53 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 03:22:53 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:22:53 - INFO - __main__ - Printing 3 examples
05/17/2022 03:22:53 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/17/2022 03:22:53 - INFO - __main__ - ['entailed']
05/17/2022 03:22:53 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/17/2022 03:22:53 - INFO - __main__ - ['entailed']
05/17/2022 03:22:53 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/17/2022 03:22:53 - INFO - __main__ - ['entailed']
05/17/2022 03:22:53 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:22:53 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:22:53 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 03:22:59 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 03:22:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 03:22:59 - INFO - __main__ - Starting training!
05/17/2022 03:23:01 - INFO - __main__ - Step 10 Global step 10 Train loss 5.11 on epoch=4
05/17/2022 03:23:03 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/17/2022 03:23:05 - INFO - __main__ - Step 30 Global step 30 Train loss 4.91 on epoch=14
05/17/2022 03:23:07 - INFO - __main__ - Step 40 Global step 40 Train loss 4.90 on epoch=19
05/17/2022 03:23:09 - INFO - __main__ - Step 50 Global step 50 Train loss 4.86 on epoch=24
05/17/2022 03:23:09 - INFO - __main__ - Global step 50 Train loss 4.95 Classification-F1 0.0 on epoch=24
05/17/2022 03:23:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 03:23:11 - INFO - __main__ - Step 60 Global step 60 Train loss 4.75 on epoch=29
05/17/2022 03:23:13 - INFO - __main__ - Step 70 Global step 70 Train loss 4.66 on epoch=34
05/17/2022 03:23:15 - INFO - __main__ - Step 80 Global step 80 Train loss 4.62 on epoch=39
05/17/2022 03:23:17 - INFO - __main__ - Step 90 Global step 90 Train loss 4.59 on epoch=44
05/17/2022 03:23:19 - INFO - __main__ - Step 100 Global step 100 Train loss 4.54 on epoch=49
05/17/2022 03:23:30 - INFO - __main__ - Global step 100 Train loss 4.63 Classification-F1 0.0 on epoch=49
05/17/2022 03:23:32 - INFO - __main__ - Step 110 Global step 110 Train loss 4.44 on epoch=54
05/17/2022 03:23:34 - INFO - __main__ - Step 120 Global step 120 Train loss 4.30 on epoch=59
05/17/2022 03:23:36 - INFO - __main__ - Step 130 Global step 130 Train loss 4.27 on epoch=64
05/17/2022 03:23:38 - INFO - __main__ - Step 140 Global step 140 Train loss 4.18 on epoch=69
05/17/2022 03:23:40 - INFO - __main__ - Step 150 Global step 150 Train loss 4.14 on epoch=74
05/17/2022 03:23:46 - INFO - __main__ - Global step 150 Train loss 4.27 Classification-F1 0.0 on epoch=74
05/17/2022 03:23:48 - INFO - __main__ - Step 160 Global step 160 Train loss 4.08 on epoch=79
05/17/2022 03:23:50 - INFO - __main__ - Step 170 Global step 170 Train loss 4.13 on epoch=84
05/17/2022 03:23:52 - INFO - __main__ - Step 180 Global step 180 Train loss 3.95 on epoch=89
05/17/2022 03:23:53 - INFO - __main__ - Step 190 Global step 190 Train loss 3.97 on epoch=94
05/17/2022 03:23:55 - INFO - __main__ - Step 200 Global step 200 Train loss 3.73 on epoch=99
05/17/2022 03:24:02 - INFO - __main__ - Global step 200 Train loss 3.97 Classification-F1 0.041666666666666664 on epoch=99
05/17/2022 03:24:02 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.041666666666666664 on epoch=99, global_step=200
05/17/2022 03:24:04 - INFO - __main__ - Step 210 Global step 210 Train loss 3.72 on epoch=104
05/17/2022 03:24:06 - INFO - __main__ - Step 220 Global step 220 Train loss 3.63 on epoch=109
05/17/2022 03:24:08 - INFO - __main__ - Step 230 Global step 230 Train loss 3.66 on epoch=114
05/17/2022 03:24:10 - INFO - __main__ - Step 240 Global step 240 Train loss 3.57 on epoch=119
05/17/2022 03:24:11 - INFO - __main__ - Step 250 Global step 250 Train loss 3.38 on epoch=124
05/17/2022 03:24:14 - INFO - __main__ - Global step 250 Train loss 3.59 Classification-F1 0.17045454545454544 on epoch=124
05/17/2022 03:24:14 - INFO - __main__ - Saving model with best Classification-F1: 0.041666666666666664 -> 0.17045454545454544 on epoch=124, global_step=250
05/17/2022 03:24:16 - INFO - __main__ - Step 260 Global step 260 Train loss 3.43 on epoch=129
05/17/2022 03:24:18 - INFO - __main__ - Step 270 Global step 270 Train loss 3.31 on epoch=134
05/17/2022 03:24:20 - INFO - __main__ - Step 280 Global step 280 Train loss 3.24 on epoch=139
05/17/2022 03:24:22 - INFO - __main__ - Step 290 Global step 290 Train loss 3.25 on epoch=144
05/17/2022 03:24:24 - INFO - __main__ - Step 300 Global step 300 Train loss 3.07 on epoch=149
05/17/2022 03:24:27 - INFO - __main__ - Global step 300 Train loss 3.26 Classification-F1 0.12727272727272726 on epoch=149
05/17/2022 03:24:29 - INFO - __main__ - Step 310 Global step 310 Train loss 3.09 on epoch=154
05/17/2022 03:24:31 - INFO - __main__ - Step 320 Global step 320 Train loss 3.08 on epoch=159
05/17/2022 03:24:33 - INFO - __main__ - Step 330 Global step 330 Train loss 2.98 on epoch=164
05/17/2022 03:24:34 - INFO - __main__ - Step 340 Global step 340 Train loss 2.89 on epoch=169
05/17/2022 03:24:36 - INFO - __main__ - Step 350 Global step 350 Train loss 2.79 on epoch=174
05/17/2022 03:24:40 - INFO - __main__ - Global step 350 Train loss 2.96 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 03:24:40 - INFO - __main__ - Saving model with best Classification-F1: 0.17045454545454544 -> 0.3333333333333333 on epoch=174, global_step=350
05/17/2022 03:24:42 - INFO - __main__ - Step 360 Global step 360 Train loss 2.87 on epoch=179
05/17/2022 03:24:44 - INFO - __main__ - Step 370 Global step 370 Train loss 2.70 on epoch=184
05/17/2022 03:24:46 - INFO - __main__ - Step 380 Global step 380 Train loss 2.64 on epoch=189
05/17/2022 03:24:47 - INFO - __main__ - Step 390 Global step 390 Train loss 2.52 on epoch=194
05/17/2022 03:24:49 - INFO - __main__ - Step 400 Global step 400 Train loss 2.54 on epoch=199
05/17/2022 03:24:55 - INFO - __main__ - Global step 400 Train loss 2.66 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 03:24:57 - INFO - __main__ - Step 410 Global step 410 Train loss 2.58 on epoch=204
05/17/2022 03:24:58 - INFO - __main__ - Step 420 Global step 420 Train loss 2.52 on epoch=209
05/17/2022 03:25:00 - INFO - __main__ - Step 430 Global step 430 Train loss 2.52 on epoch=214
05/17/2022 03:25:02 - INFO - __main__ - Step 440 Global step 440 Train loss 2.37 on epoch=219
05/17/2022 03:25:04 - INFO - __main__ - Step 450 Global step 450 Train loss 2.34 on epoch=224
05/17/2022 03:25:07 - INFO - __main__ - Global step 450 Train loss 2.47 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 03:25:09 - INFO - __main__ - Step 460 Global step 460 Train loss 2.21 on epoch=229
05/17/2022 03:25:11 - INFO - __main__ - Step 470 Global step 470 Train loss 2.20 on epoch=234
05/17/2022 03:25:13 - INFO - __main__ - Step 480 Global step 480 Train loss 2.15 on epoch=239
05/17/2022 03:25:15 - INFO - __main__ - Step 490 Global step 490 Train loss 2.12 on epoch=244
05/17/2022 03:25:17 - INFO - __main__ - Step 500 Global step 500 Train loss 2.07 on epoch=249
05/17/2022 03:25:19 - INFO - __main__ - Global step 500 Train loss 2.15 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 03:25:21 - INFO - __main__ - Step 510 Global step 510 Train loss 2.11 on epoch=254
05/17/2022 03:25:23 - INFO - __main__ - Step 520 Global step 520 Train loss 1.92 on epoch=259
05/17/2022 03:25:25 - INFO - __main__ - Step 530 Global step 530 Train loss 1.90 on epoch=264
05/17/2022 03:25:27 - INFO - __main__ - Step 540 Global step 540 Train loss 1.90 on epoch=269
05/17/2022 03:25:29 - INFO - __main__ - Step 550 Global step 550 Train loss 1.80 on epoch=274
05/17/2022 03:25:31 - INFO - __main__ - Global step 550 Train loss 1.93 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 03:25:32 - INFO - __main__ - Step 560 Global step 560 Train loss 1.68 on epoch=279
05/17/2022 03:25:34 - INFO - __main__ - Step 570 Global step 570 Train loss 1.80 on epoch=284
05/17/2022 03:25:36 - INFO - __main__ - Step 580 Global step 580 Train loss 1.83 on epoch=289
05/17/2022 03:25:38 - INFO - __main__ - Step 590 Global step 590 Train loss 1.61 on epoch=294
05/17/2022 03:25:40 - INFO - __main__ - Step 600 Global step 600 Train loss 1.72 on epoch=299
05/17/2022 03:25:41 - INFO - __main__ - Global step 600 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 03:25:43 - INFO - __main__ - Step 610 Global step 610 Train loss 1.61 on epoch=304
05/17/2022 03:25:45 - INFO - __main__ - Step 620 Global step 620 Train loss 1.70 on epoch=309
05/17/2022 03:25:46 - INFO - __main__ - Step 630 Global step 630 Train loss 1.57 on epoch=314
05/17/2022 03:25:48 - INFO - __main__ - Step 640 Global step 640 Train loss 1.45 on epoch=319
05/17/2022 03:25:50 - INFO - __main__ - Step 650 Global step 650 Train loss 1.45 on epoch=324
05/17/2022 03:25:51 - INFO - __main__ - Global step 650 Train loss 1.56 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 03:25:53 - INFO - __main__ - Step 660 Global step 660 Train loss 1.59 on epoch=329
05/17/2022 03:25:55 - INFO - __main__ - Step 670 Global step 670 Train loss 1.38 on epoch=334
05/17/2022 03:25:57 - INFO - __main__ - Step 680 Global step 680 Train loss 1.45 on epoch=339
05/17/2022 03:25:59 - INFO - __main__ - Step 690 Global step 690 Train loss 1.30 on epoch=344
05/17/2022 03:26:01 - INFO - __main__ - Step 700 Global step 700 Train loss 1.25 on epoch=349
05/17/2022 03:26:01 - INFO - __main__ - Global step 700 Train loss 1.39 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 03:26:03 - INFO - __main__ - Step 710 Global step 710 Train loss 1.25 on epoch=354
05/17/2022 03:26:05 - INFO - __main__ - Step 720 Global step 720 Train loss 1.28 on epoch=359
05/17/2022 03:26:07 - INFO - __main__ - Step 730 Global step 730 Train loss 1.19 on epoch=364
05/17/2022 03:26:09 - INFO - __main__ - Step 740 Global step 740 Train loss 1.16 on epoch=369
05/17/2022 03:26:11 - INFO - __main__ - Step 750 Global step 750 Train loss 1.25 on epoch=374
05/17/2022 03:26:11 - INFO - __main__ - Global step 750 Train loss 1.23 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 03:26:13 - INFO - __main__ - Step 760 Global step 760 Train loss 1.29 on epoch=379
05/17/2022 03:26:15 - INFO - __main__ - Step 770 Global step 770 Train loss 1.14 on epoch=384
05/17/2022 03:26:17 - INFO - __main__ - Step 780 Global step 780 Train loss 1.11 on epoch=389
05/17/2022 03:26:19 - INFO - __main__ - Step 790 Global step 790 Train loss 1.13 on epoch=394
05/17/2022 03:26:21 - INFO - __main__ - Step 800 Global step 800 Train loss 1.07 on epoch=399
05/17/2022 03:26:22 - INFO - __main__ - Global step 800 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 03:26:23 - INFO - __main__ - Step 810 Global step 810 Train loss 1.11 on epoch=404
05/17/2022 03:26:25 - INFO - __main__ - Step 820 Global step 820 Train loss 1.11 on epoch=409
05/17/2022 03:26:27 - INFO - __main__ - Step 830 Global step 830 Train loss 1.04 on epoch=414
05/17/2022 03:26:29 - INFO - __main__ - Step 840 Global step 840 Train loss 1.05 on epoch=419
05/17/2022 03:26:31 - INFO - __main__ - Step 850 Global step 850 Train loss 1.06 on epoch=424
05/17/2022 03:26:32 - INFO - __main__ - Global step 850 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 03:26:34 - INFO - __main__ - Step 860 Global step 860 Train loss 1.04 on epoch=429
05/17/2022 03:26:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.95 on epoch=434
05/17/2022 03:26:37 - INFO - __main__ - Step 880 Global step 880 Train loss 1.02 on epoch=439
05/17/2022 03:26:39 - INFO - __main__ - Step 890 Global step 890 Train loss 1.05 on epoch=444
05/17/2022 03:26:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.89 on epoch=449
05/17/2022 03:26:42 - INFO - __main__ - Global step 900 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 03:26:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.97 on epoch=454
05/17/2022 03:26:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.91 on epoch=459
05/17/2022 03:26:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.79 on epoch=464
05/17/2022 03:26:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.95 on epoch=469
05/17/2022 03:26:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.89 on epoch=474
05/17/2022 03:26:52 - INFO - __main__ - Global step 950 Train loss 0.90 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 03:26:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.86 on epoch=479
05/17/2022 03:26:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.86 on epoch=484
05/17/2022 03:26:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.78 on epoch=489
05/17/2022 03:27:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.93 on epoch=494
05/17/2022 03:27:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.84 on epoch=499
05/17/2022 03:27:03 - INFO - __main__ - Global step 1000 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 03:27:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.80 on epoch=504
05/17/2022 03:27:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.79 on epoch=509
05/17/2022 03:27:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.83 on epoch=514
05/17/2022 03:27:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.83 on epoch=519
05/17/2022 03:27:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.71 on epoch=524
05/17/2022 03:27:14 - INFO - __main__ - Global step 1050 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 03:27:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.79 on epoch=529
05/17/2022 03:27:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.75 on epoch=534
05/17/2022 03:27:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.73 on epoch=539
05/17/2022 03:27:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.75 on epoch=544
05/17/2022 03:27:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.72 on epoch=549
05/17/2022 03:27:25 - INFO - __main__ - Global step 1100 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 03:27:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.67 on epoch=554
05/17/2022 03:27:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.74 on epoch=559
05/17/2022 03:27:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.72 on epoch=564
05/17/2022 03:27:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.70 on epoch=569
05/17/2022 03:27:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.67 on epoch=574
05/17/2022 03:27:36 - INFO - __main__ - Global step 1150 Train loss 0.70 Classification-F1 0.3191489361702127 on epoch=574
05/17/2022 03:27:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.76 on epoch=579
05/17/2022 03:27:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.79 on epoch=584
05/17/2022 03:27:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.74 on epoch=589
05/17/2022 03:27:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.69 on epoch=594
05/17/2022 03:27:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.70 on epoch=599
05/17/2022 03:27:47 - INFO - __main__ - Global step 1200 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 03:27:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.67 on epoch=604
05/17/2022 03:27:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.71 on epoch=609
05/17/2022 03:27:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.66 on epoch=614
05/17/2022 03:27:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.68 on epoch=619
05/17/2022 03:27:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=624
05/17/2022 03:27:58 - INFO - __main__ - Global step 1250 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 03:28:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.67 on epoch=629
05/17/2022 03:28:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.64 on epoch=634
05/17/2022 03:28:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.68 on epoch=639
05/17/2022 03:28:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.61 on epoch=644
05/17/2022 03:28:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.60 on epoch=649
05/17/2022 03:28:10 - INFO - __main__ - Global step 1300 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 03:28:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.66 on epoch=654
05/17/2022 03:28:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.56 on epoch=659
05/17/2022 03:28:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.53 on epoch=664
05/17/2022 03:28:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.59 on epoch=669
05/17/2022 03:28:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.62 on epoch=674
05/17/2022 03:28:21 - INFO - __main__ - Global step 1350 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 03:28:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.57 on epoch=679
05/17/2022 03:28:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.55 on epoch=684
05/17/2022 03:28:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
05/17/2022 03:28:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.55 on epoch=694
05/17/2022 03:28:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.51 on epoch=699
05/17/2022 03:28:32 - INFO - __main__ - Global step 1400 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 03:28:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.62 on epoch=704
05/17/2022 03:28:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.59 on epoch=709
05/17/2022 03:28:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.63 on epoch=714
05/17/2022 03:28:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.56 on epoch=719
05/17/2022 03:28:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.55 on epoch=724
05/17/2022 03:28:43 - INFO - __main__ - Global step 1450 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 03:28:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
05/17/2022 03:28:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
05/17/2022 03:28:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=739
05/17/2022 03:28:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.53 on epoch=744
05/17/2022 03:28:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.58 on epoch=749
05/17/2022 03:28:54 - INFO - __main__ - Global step 1500 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 03:28:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.55 on epoch=754
05/17/2022 03:28:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.50 on epoch=759
05/17/2022 03:29:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.60 on epoch=764
05/17/2022 03:29:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.51 on epoch=769
05/17/2022 03:29:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.53 on epoch=774
05/17/2022 03:29:05 - INFO - __main__ - Global step 1550 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 03:29:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.50 on epoch=779
05/17/2022 03:29:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
05/17/2022 03:29:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.63 on epoch=789
05/17/2022 03:29:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.54 on epoch=794
05/17/2022 03:29:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=799
05/17/2022 03:29:15 - INFO - __main__ - Global step 1600 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 03:29:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=804
05/17/2022 03:29:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.58 on epoch=809
05/17/2022 03:29:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
05/17/2022 03:29:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=819
05/17/2022 03:29:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.48 on epoch=824
05/17/2022 03:29:25 - INFO - __main__ - Global step 1650 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 03:29:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=829
05/17/2022 03:29:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.48 on epoch=834
05/17/2022 03:29:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.53 on epoch=839
05/17/2022 03:29:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=844
05/17/2022 03:29:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=849
05/17/2022 03:29:35 - INFO - __main__ - Global step 1700 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 03:29:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=854
05/17/2022 03:29:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=859
05/17/2022 03:29:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
05/17/2022 03:29:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=869
05/17/2022 03:29:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.48 on epoch=874
05/17/2022 03:29:45 - INFO - __main__ - Global step 1750 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 03:29:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=879
05/17/2022 03:29:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.49 on epoch=884
05/17/2022 03:29:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=889
05/17/2022 03:29:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
05/17/2022 03:29:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=899
05/17/2022 03:29:55 - INFO - __main__ - Global step 1800 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 03:29:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.56 on epoch=904
05/17/2022 03:29:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
05/17/2022 03:30:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=914
05/17/2022 03:30:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=919
05/17/2022 03:30:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=924
05/17/2022 03:30:05 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 03:30:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
05/17/2022 03:30:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.48 on epoch=934
05/17/2022 03:30:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
05/17/2022 03:30:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
05/17/2022 03:30:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.48 on epoch=949
05/17/2022 03:30:15 - INFO - __main__ - Global step 1900 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 03:30:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
05/17/2022 03:30:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
05/17/2022 03:30:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=964
05/17/2022 03:30:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=969
05/17/2022 03:30:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
05/17/2022 03:30:25 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 03:30:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
05/17/2022 03:30:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
05/17/2022 03:30:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=989
05/17/2022 03:30:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=994
05/17/2022 03:30:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=999
05/17/2022 03:30:35 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 03:30:35 - INFO - __main__ - save last model!
05/17/2022 03:30:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 03:30:35 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 03:30:35 - INFO - __main__ - Printing 3 examples
05/17/2022 03:30:35 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:30:35 - INFO - __main__ - ['entailed']
05/17/2022 03:30:35 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:30:35 - INFO - __main__ - ['entailed']
05/17/2022 03:30:35 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:30:35 - INFO - __main__ - ['entailed']
05/17/2022 03:30:35 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:30:36 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:30:36 - INFO - __main__ - Printing 3 examples
05/17/2022 03:30:36 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/17/2022 03:30:36 - INFO - __main__ - ['entailed']
05/17/2022 03:30:36 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/17/2022 03:30:36 - INFO - __main__ - ['entailed']
05/17/2022 03:30:36 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/17/2022 03:30:36 - INFO - __main__ - ['entailed']
05/17/2022 03:30:36 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:30:36 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:30:36 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 03:30:36 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:30:36 - INFO - __main__ - Printing 3 examples
05/17/2022 03:30:36 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/17/2022 03:30:36 - INFO - __main__ - ['entailed']
05/17/2022 03:30:36 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/17/2022 03:30:36 - INFO - __main__ - ['entailed']
05/17/2022 03:30:36 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/17/2022 03:30:36 - INFO - __main__ - ['entailed']
05/17/2022 03:30:36 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:30:36 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:30:36 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 03:30:42 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 03:30:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 03:30:42 - INFO - __main__ - Starting training!
05/17/2022 03:31:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:31:13 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 03:35:19 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.3_8_predictions.txt
05/17/2022 03:35:19 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 03:35:20 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 03:35:20 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.2, bsz=8 ...
05/17/2022 03:35:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:35:21 - INFO - __main__ - Printing 3 examples
05/17/2022 03:35:21 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/17/2022 03:35:21 - INFO - __main__ - ['entailed']
05/17/2022 03:35:21 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/17/2022 03:35:21 - INFO - __main__ - ['entailed']
05/17/2022 03:35:21 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/17/2022 03:35:21 - INFO - __main__ - ['entailed']
05/17/2022 03:35:21 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:35:21 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:35:21 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 03:35:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:35:21 - INFO - __main__ - Printing 3 examples
05/17/2022 03:35:21 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/17/2022 03:35:21 - INFO - __main__ - ['entailed']
05/17/2022 03:35:21 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/17/2022 03:35:21 - INFO - __main__ - ['entailed']
05/17/2022 03:35:21 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/17/2022 03:35:21 - INFO - __main__ - ['entailed']
05/17/2022 03:35:21 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:35:21 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:35:21 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 03:35:28 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 03:35:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 03:35:28 - INFO - __main__ - Starting training!
05/17/2022 03:35:30 - INFO - __main__ - Step 10 Global step 10 Train loss 5.05 on epoch=4
05/17/2022 03:35:32 - INFO - __main__ - Step 20 Global step 20 Train loss 5.04 on epoch=9
05/17/2022 03:35:34 - INFO - __main__ - Step 30 Global step 30 Train loss 4.99 on epoch=14
05/17/2022 03:35:36 - INFO - __main__ - Step 40 Global step 40 Train loss 4.92 on epoch=19
05/17/2022 03:35:38 - INFO - __main__ - Step 50 Global step 50 Train loss 4.83 on epoch=24
05/17/2022 03:35:39 - INFO - __main__ - Global step 50 Train loss 4.97 Classification-F1 0.0 on epoch=24
05/17/2022 03:35:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 03:35:41 - INFO - __main__ - Step 60 Global step 60 Train loss 4.92 on epoch=29
05/17/2022 03:35:43 - INFO - __main__ - Step 70 Global step 70 Train loss 4.83 on epoch=34
05/17/2022 03:35:45 - INFO - __main__ - Step 80 Global step 80 Train loss 4.73 on epoch=39
05/17/2022 03:35:46 - INFO - __main__ - Step 90 Global step 90 Train loss 4.64 on epoch=44
05/17/2022 03:35:48 - INFO - __main__ - Step 100 Global step 100 Train loss 4.64 on epoch=49
05/17/2022 03:35:50 - INFO - __main__ - Global step 100 Train loss 4.75 Classification-F1 0.0 on epoch=49
05/17/2022 03:35:52 - INFO - __main__ - Step 110 Global step 110 Train loss 4.56 on epoch=54
05/17/2022 03:35:54 - INFO - __main__ - Step 120 Global step 120 Train loss 4.47 on epoch=59
05/17/2022 03:35:55 - INFO - __main__ - Step 130 Global step 130 Train loss 4.45 on epoch=64
05/17/2022 03:35:57 - INFO - __main__ - Step 140 Global step 140 Train loss 4.42 on epoch=69
05/17/2022 03:35:59 - INFO - __main__ - Step 150 Global step 150 Train loss 4.32 on epoch=74
05/17/2022 03:36:01 - INFO - __main__ - Global step 150 Train loss 4.45 Classification-F1 0.0 on epoch=74
05/17/2022 03:36:03 - INFO - __main__ - Step 160 Global step 160 Train loss 4.29 on epoch=79
05/17/2022 03:36:05 - INFO - __main__ - Step 170 Global step 170 Train loss 4.24 on epoch=84
05/17/2022 03:36:06 - INFO - __main__ - Step 180 Global step 180 Train loss 4.11 on epoch=89
05/17/2022 03:36:08 - INFO - __main__ - Step 190 Global step 190 Train loss 4.08 on epoch=94
05/17/2022 03:36:10 - INFO - __main__ - Step 200 Global step 200 Train loss 3.98 on epoch=99
05/17/2022 03:36:11 - INFO - __main__ - Global step 200 Train loss 4.14 Classification-F1 0.0 on epoch=99
05/17/2022 03:36:13 - INFO - __main__ - Step 210 Global step 210 Train loss 3.86 on epoch=104
05/17/2022 03:36:15 - INFO - __main__ - Step 220 Global step 220 Train loss 3.86 on epoch=109
05/17/2022 03:36:17 - INFO - __main__ - Step 230 Global step 230 Train loss 3.75 on epoch=114
05/17/2022 03:36:19 - INFO - __main__ - Step 240 Global step 240 Train loss 3.74 on epoch=119
05/17/2022 03:36:21 - INFO - __main__ - Step 250 Global step 250 Train loss 3.72 on epoch=124
05/17/2022 03:36:23 - INFO - __main__ - Global step 250 Train loss 3.78 Classification-F1 0.0 on epoch=124
05/17/2022 03:36:25 - INFO - __main__ - Step 260 Global step 260 Train loss 3.59 on epoch=129
05/17/2022 03:36:27 - INFO - __main__ - Step 270 Global step 270 Train loss 3.54 on epoch=134
05/17/2022 03:36:29 - INFO - __main__ - Step 280 Global step 280 Train loss 3.55 on epoch=139
05/17/2022 03:36:31 - INFO - __main__ - Step 290 Global step 290 Train loss 3.45 on epoch=144
05/17/2022 03:36:33 - INFO - __main__ - Step 300 Global step 300 Train loss 3.45 on epoch=149
05/17/2022 03:36:34 - INFO - __main__ - Global step 300 Train loss 3.51 Classification-F1 0.11666666666666668 on epoch=149
05/17/2022 03:36:34 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.11666666666666668 on epoch=149, global_step=300
05/17/2022 03:36:36 - INFO - __main__ - Step 310 Global step 310 Train loss 3.39 on epoch=154
05/17/2022 03:36:38 - INFO - __main__ - Step 320 Global step 320 Train loss 3.32 on epoch=159
05/17/2022 03:36:40 - INFO - __main__ - Step 330 Global step 330 Train loss 3.37 on epoch=164
05/17/2022 03:36:42 - INFO - __main__ - Step 340 Global step 340 Train loss 3.34 on epoch=169
05/17/2022 03:36:44 - INFO - __main__ - Step 350 Global step 350 Train loss 3.16 on epoch=174
05/17/2022 03:36:47 - INFO - __main__ - Global step 350 Train loss 3.32 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 03:36:47 - INFO - __main__ - Saving model with best Classification-F1: 0.11666666666666668 -> 0.3333333333333333 on epoch=174, global_step=350
05/17/2022 03:36:49 - INFO - __main__ - Step 360 Global step 360 Train loss 3.21 on epoch=179
05/17/2022 03:36:50 - INFO - __main__ - Step 370 Global step 370 Train loss 3.11 on epoch=184
05/17/2022 03:36:52 - INFO - __main__ - Step 380 Global step 380 Train loss 3.11 on epoch=189
05/17/2022 03:36:54 - INFO - __main__ - Step 390 Global step 390 Train loss 3.14 on epoch=194
05/17/2022 03:36:56 - INFO - __main__ - Step 400 Global step 400 Train loss 2.96 on epoch=199
05/17/2022 03:36:58 - INFO - __main__ - Global step 400 Train loss 3.11 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 03:37:00 - INFO - __main__ - Step 410 Global step 410 Train loss 2.97 on epoch=204
05/17/2022 03:37:02 - INFO - __main__ - Step 420 Global step 420 Train loss 2.98 on epoch=209
05/17/2022 03:37:03 - INFO - __main__ - Step 430 Global step 430 Train loss 2.88 on epoch=214
05/17/2022 03:37:05 - INFO - __main__ - Step 440 Global step 440 Train loss 2.76 on epoch=219
05/17/2022 03:37:07 - INFO - __main__ - Step 450 Global step 450 Train loss 2.77 on epoch=224
05/17/2022 03:37:10 - INFO - __main__ - Global step 450 Train loss 2.88 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 03:37:12 - INFO - __main__ - Step 460 Global step 460 Train loss 2.71 on epoch=229
05/17/2022 03:37:14 - INFO - __main__ - Step 470 Global step 470 Train loss 2.64 on epoch=234
05/17/2022 03:37:15 - INFO - __main__ - Step 480 Global step 480 Train loss 2.58 on epoch=239
05/17/2022 03:37:17 - INFO - __main__ - Step 490 Global step 490 Train loss 2.51 on epoch=244
05/17/2022 03:37:19 - INFO - __main__ - Step 500 Global step 500 Train loss 2.51 on epoch=249
05/17/2022 03:37:21 - INFO - __main__ - Global step 500 Train loss 2.59 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 03:37:23 - INFO - __main__ - Step 510 Global step 510 Train loss 2.37 on epoch=254
05/17/2022 03:37:25 - INFO - __main__ - Step 520 Global step 520 Train loss 2.36 on epoch=259
05/17/2022 03:37:27 - INFO - __main__ - Step 530 Global step 530 Train loss 2.40 on epoch=264
05/17/2022 03:37:29 - INFO - __main__ - Step 540 Global step 540 Train loss 2.21 on epoch=269
05/17/2022 03:37:31 - INFO - __main__ - Step 550 Global step 550 Train loss 2.17 on epoch=274
05/17/2022 03:37:33 - INFO - __main__ - Global step 550 Train loss 2.30 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 03:37:35 - INFO - __main__ - Step 560 Global step 560 Train loss 2.21 on epoch=279
05/17/2022 03:37:37 - INFO - __main__ - Step 570 Global step 570 Train loss 2.24 on epoch=284
05/17/2022 03:37:39 - INFO - __main__ - Step 580 Global step 580 Train loss 2.23 on epoch=289
05/17/2022 03:37:41 - INFO - __main__ - Step 590 Global step 590 Train loss 2.12 on epoch=294
05/17/2022 03:37:42 - INFO - __main__ - Step 600 Global step 600 Train loss 2.04 on epoch=299
05/17/2022 03:37:43 - INFO - __main__ - Global step 600 Train loss 2.17 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 03:37:45 - INFO - __main__ - Step 610 Global step 610 Train loss 2.15 on epoch=304
05/17/2022 03:37:47 - INFO - __main__ - Step 620 Global step 620 Train loss 2.10 on epoch=309
05/17/2022 03:37:49 - INFO - __main__ - Step 630 Global step 630 Train loss 2.05 on epoch=314
05/17/2022 03:37:51 - INFO - __main__ - Step 640 Global step 640 Train loss 1.92 on epoch=319
05/17/2022 03:37:53 - INFO - __main__ - Step 650 Global step 650 Train loss 1.89 on epoch=324
05/17/2022 03:37:54 - INFO - __main__ - Global step 650 Train loss 2.02 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 03:37:56 - INFO - __main__ - Step 660 Global step 660 Train loss 1.94 on epoch=329
05/17/2022 03:37:58 - INFO - __main__ - Step 670 Global step 670 Train loss 1.96 on epoch=334
05/17/2022 03:37:59 - INFO - __main__ - Step 680 Global step 680 Train loss 1.95 on epoch=339
05/17/2022 03:38:01 - INFO - __main__ - Step 690 Global step 690 Train loss 1.83 on epoch=344
05/17/2022 03:38:03 - INFO - __main__ - Step 700 Global step 700 Train loss 1.90 on epoch=349
05/17/2022 03:38:04 - INFO - __main__ - Global step 700 Train loss 1.92 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 03:38:06 - INFO - __main__ - Step 710 Global step 710 Train loss 1.81 on epoch=354
05/17/2022 03:38:08 - INFO - __main__ - Step 720 Global step 720 Train loss 1.69 on epoch=359
05/17/2022 03:38:10 - INFO - __main__ - Step 730 Global step 730 Train loss 1.81 on epoch=364
05/17/2022 03:38:12 - INFO - __main__ - Step 740 Global step 740 Train loss 1.58 on epoch=369
05/17/2022 03:38:14 - INFO - __main__ - Step 750 Global step 750 Train loss 1.62 on epoch=374
05/17/2022 03:38:16 - INFO - __main__ - Global step 750 Train loss 1.70 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 03:38:18 - INFO - __main__ - Step 760 Global step 760 Train loss 1.72 on epoch=379
05/17/2022 03:38:20 - INFO - __main__ - Step 770 Global step 770 Train loss 1.56 on epoch=384
05/17/2022 03:38:21 - INFO - __main__ - Step 780 Global step 780 Train loss 1.70 on epoch=389
05/17/2022 03:38:23 - INFO - __main__ - Step 790 Global step 790 Train loss 1.53 on epoch=394
05/17/2022 03:38:25 - INFO - __main__ - Step 800 Global step 800 Train loss 1.63 on epoch=399
05/17/2022 03:38:28 - INFO - __main__ - Global step 800 Train loss 1.63 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 03:38:29 - INFO - __main__ - Step 810 Global step 810 Train loss 1.70 on epoch=404
05/17/2022 03:38:31 - INFO - __main__ - Step 820 Global step 820 Train loss 1.58 on epoch=409
05/17/2022 03:38:33 - INFO - __main__ - Step 830 Global step 830 Train loss 1.50 on epoch=414
05/17/2022 03:38:35 - INFO - __main__ - Step 840 Global step 840 Train loss 1.55 on epoch=419
05/17/2022 03:38:37 - INFO - __main__ - Step 850 Global step 850 Train loss 1.60 on epoch=424
05/17/2022 03:38:39 - INFO - __main__ - Global step 850 Train loss 1.59 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 03:38:41 - INFO - __main__ - Step 860 Global step 860 Train loss 1.42 on epoch=429
05/17/2022 03:38:43 - INFO - __main__ - Step 870 Global step 870 Train loss 1.46 on epoch=434
05/17/2022 03:38:45 - INFO - __main__ - Step 880 Global step 880 Train loss 1.43 on epoch=439
05/17/2022 03:38:47 - INFO - __main__ - Step 890 Global step 890 Train loss 1.40 on epoch=444
05/17/2022 03:38:49 - INFO - __main__ - Step 900 Global step 900 Train loss 1.42 on epoch=449
05/17/2022 03:38:51 - INFO - __main__ - Global step 900 Train loss 1.43 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 03:38:53 - INFO - __main__ - Step 910 Global step 910 Train loss 1.36 on epoch=454
05/17/2022 03:38:55 - INFO - __main__ - Step 920 Global step 920 Train loss 1.39 on epoch=459
05/17/2022 03:38:57 - INFO - __main__ - Step 930 Global step 930 Train loss 1.49 on epoch=464
05/17/2022 03:38:59 - INFO - __main__ - Step 940 Global step 940 Train loss 1.42 on epoch=469
05/17/2022 03:39:01 - INFO - __main__ - Step 950 Global step 950 Train loss 1.41 on epoch=474
05/17/2022 03:39:04 - INFO - __main__ - Global step 950 Train loss 1.41 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 03:39:05 - INFO - __main__ - Step 960 Global step 960 Train loss 1.25 on epoch=479
05/17/2022 03:39:07 - INFO - __main__ - Step 970 Global step 970 Train loss 1.30 on epoch=484
05/17/2022 03:39:09 - INFO - __main__ - Step 980 Global step 980 Train loss 1.24 on epoch=489
05/17/2022 03:39:11 - INFO - __main__ - Step 990 Global step 990 Train loss 1.25 on epoch=494
05/17/2022 03:39:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.20 on epoch=499
05/17/2022 03:39:14 - INFO - __main__ - Global step 1000 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 03:39:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.18 on epoch=504
05/17/2022 03:39:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.18 on epoch=509
05/17/2022 03:39:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.22 on epoch=514
05/17/2022 03:39:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.27 on epoch=519
05/17/2022 03:39:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.21 on epoch=524
05/17/2022 03:39:24 - INFO - __main__ - Global step 1050 Train loss 1.21 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 03:39:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.17 on epoch=529
05/17/2022 03:39:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.16 on epoch=534
05/17/2022 03:39:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.19 on epoch=539
05/17/2022 03:39:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.30 on epoch=544
05/17/2022 03:39:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.07 on epoch=549
05/17/2022 03:39:35 - INFO - __main__ - Global step 1100 Train loss 1.18 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 03:39:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.05 on epoch=554
05/17/2022 03:39:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.15 on epoch=559
05/17/2022 03:39:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.98 on epoch=564
05/17/2022 03:39:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.08 on epoch=569
05/17/2022 03:39:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.02 on epoch=574
05/17/2022 03:39:45 - INFO - __main__ - Global step 1150 Train loss 1.06 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 03:39:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.98 on epoch=579
05/17/2022 03:39:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.04 on epoch=584
05/17/2022 03:39:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.07 on epoch=589
05/17/2022 03:39:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.04 on epoch=594
05/17/2022 03:39:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.05 on epoch=599
05/17/2022 03:39:55 - INFO - __main__ - Global step 1200 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 03:39:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.94 on epoch=604
05/17/2022 03:39:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.95 on epoch=609
05/17/2022 03:40:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.13 on epoch=614
05/17/2022 03:40:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.01 on epoch=619
05/17/2022 03:40:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.93 on epoch=624
05/17/2022 03:40:06 - INFO - __main__ - Global step 1250 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 03:40:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.88 on epoch=629
05/17/2022 03:40:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.91 on epoch=634
05/17/2022 03:40:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.87 on epoch=639
05/17/2022 03:40:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.90 on epoch=644
05/17/2022 03:40:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.89 on epoch=649
05/17/2022 03:40:16 - INFO - __main__ - Global step 1300 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 03:40:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.91 on epoch=654
05/17/2022 03:40:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.97 on epoch=659
05/17/2022 03:40:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.95 on epoch=664
05/17/2022 03:40:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.93 on epoch=669
05/17/2022 03:40:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=674
05/17/2022 03:40:26 - INFO - __main__ - Global step 1350 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 03:40:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.88 on epoch=679
05/17/2022 03:40:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.86 on epoch=684
05/17/2022 03:40:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.84 on epoch=689
05/17/2022 03:40:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.75 on epoch=694
05/17/2022 03:40:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.81 on epoch=699
05/17/2022 03:40:37 - INFO - __main__ - Global step 1400 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 03:40:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.79 on epoch=704
05/17/2022 03:40:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.80 on epoch=709
05/17/2022 03:40:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.81 on epoch=714
05/17/2022 03:40:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.84 on epoch=719
05/17/2022 03:40:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=724
05/17/2022 03:40:47 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 03:40:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.83 on epoch=729
05/17/2022 03:40:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.79 on epoch=734
05/17/2022 03:40:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.83 on epoch=739
05/17/2022 03:40:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.71 on epoch=744
05/17/2022 03:40:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.73 on epoch=749
05/17/2022 03:40:57 - INFO - __main__ - Global step 1500 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 03:40:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.71 on epoch=754
05/17/2022 03:41:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.79 on epoch=759
05/17/2022 03:41:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.69 on epoch=764
05/17/2022 03:41:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.64 on epoch=769
05/17/2022 03:41:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.65 on epoch=774
05/17/2022 03:41:08 - INFO - __main__ - Global step 1550 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 03:41:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.59 on epoch=779
05/17/2022 03:41:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.67 on epoch=784
05/17/2022 03:41:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.75 on epoch=789
05/17/2022 03:41:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.69 on epoch=794
05/17/2022 03:41:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.64 on epoch=799
05/17/2022 03:41:18 - INFO - __main__ - Global step 1600 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 03:41:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.70 on epoch=804
05/17/2022 03:41:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.68 on epoch=809
05/17/2022 03:41:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.66 on epoch=814
05/17/2022 03:41:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.56 on epoch=819
05/17/2022 03:41:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.60 on epoch=824
05/17/2022 03:41:29 - INFO - __main__ - Global step 1650 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 03:41:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.60 on epoch=829
05/17/2022 03:41:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.64 on epoch=834
05/17/2022 03:41:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.68 on epoch=839
05/17/2022 03:41:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.60 on epoch=844
05/17/2022 03:41:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.65 on epoch=849
05/17/2022 03:41:39 - INFO - __main__ - Global step 1700 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 03:41:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.53 on epoch=854
05/17/2022 03:41:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.69 on epoch=859
05/17/2022 03:41:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.65 on epoch=864
05/17/2022 03:41:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.55 on epoch=869
05/17/2022 03:41:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.57 on epoch=874
05/17/2022 03:41:49 - INFO - __main__ - Global step 1750 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 03:41:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.59 on epoch=879
05/17/2022 03:41:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.58 on epoch=884
05/17/2022 03:41:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
05/17/2022 03:41:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.56 on epoch=894
05/17/2022 03:41:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.64 on epoch=899
05/17/2022 03:41:59 - INFO - __main__ - Global step 1800 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 03:42:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.56 on epoch=904
05/17/2022 03:42:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.58 on epoch=909
05/17/2022 03:42:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.55 on epoch=914
05/17/2022 03:42:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.52 on epoch=919
05/17/2022 03:42:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.51 on epoch=924
05/17/2022 03:42:10 - INFO - __main__ - Global step 1850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 03:42:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.61 on epoch=929
05/17/2022 03:42:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.59 on epoch=934
05/17/2022 03:42:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.50 on epoch=939
05/17/2022 03:42:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.62 on epoch=944
05/17/2022 03:42:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=949
05/17/2022 03:42:20 - INFO - __main__ - Global step 1900 Train loss 0.55 Classification-F1 0.3191489361702127 on epoch=949
05/17/2022 03:42:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.55 on epoch=954
05/17/2022 03:42:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.49 on epoch=959
05/17/2022 03:42:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.50 on epoch=964
05/17/2022 03:42:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.51 on epoch=969
05/17/2022 03:42:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=974
05/17/2022 03:42:30 - INFO - __main__ - Global step 1950 Train loss 0.51 Classification-F1 0.5151515151515151 on epoch=974
05/17/2022 03:42:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5151515151515151 on epoch=974, global_step=1950
05/17/2022 03:42:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
05/17/2022 03:42:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=984
05/17/2022 03:42:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=989
05/17/2022 03:42:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.50 on epoch=994
05/17/2022 03:42:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=999
05/17/2022 03:42:40 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 03:42:40 - INFO - __main__ - save last model!
05/17/2022 03:42:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 03:42:40 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 03:42:40 - INFO - __main__ - Printing 3 examples
05/17/2022 03:42:40 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:42:40 - INFO - __main__ - ['entailed']
05/17/2022 03:42:40 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:42:40 - INFO - __main__ - ['entailed']
05/17/2022 03:42:40 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:42:40 - INFO - __main__ - ['entailed']
05/17/2022 03:42:40 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:42:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:42:41 - INFO - __main__ - Printing 3 examples
05/17/2022 03:42:41 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/17/2022 03:42:41 - INFO - __main__ - ['refuted']
05/17/2022 03:42:41 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/17/2022 03:42:41 - INFO - __main__ - ['refuted']
05/17/2022 03:42:41 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/17/2022 03:42:41 - INFO - __main__ - ['refuted']
05/17/2022 03:42:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:42:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:42:41 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 03:42:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:42:41 - INFO - __main__ - Printing 3 examples
05/17/2022 03:42:41 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/17/2022 03:42:41 - INFO - __main__ - ['refuted']
05/17/2022 03:42:41 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/17/2022 03:42:41 - INFO - __main__ - ['refuted']
05/17/2022 03:42:41 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/17/2022 03:42:41 - INFO - __main__ - ['refuted']
05/17/2022 03:42:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:42:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:42:41 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 03:42:46 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 03:42:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 03:42:47 - INFO - __main__ - Starting training!
05/17/2022 03:43:04 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:43:17 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 03:49:08 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.2_8_predictions.txt
05/17/2022 03:49:08 - INFO - __main__ - Classification-F1 on test data: 0.3350
05/17/2022 03:49:08 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.2, bsz=8, dev_performance=0.5151515151515151, test_performance=0.3349734394132839
05/17/2022 03:49:08 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.5, bsz=8 ...
05/17/2022 03:49:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:49:09 - INFO - __main__ - Printing 3 examples
05/17/2022 03:49:09 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/17/2022 03:49:09 - INFO - __main__ - ['refuted']
05/17/2022 03:49:09 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/17/2022 03:49:09 - INFO - __main__ - ['refuted']
05/17/2022 03:49:09 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/17/2022 03:49:09 - INFO - __main__ - ['refuted']
05/17/2022 03:49:09 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:49:09 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:49:09 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 03:49:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:49:09 - INFO - __main__ - Printing 3 examples
05/17/2022 03:49:09 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/17/2022 03:49:09 - INFO - __main__ - ['refuted']
05/17/2022 03:49:09 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/17/2022 03:49:09 - INFO - __main__ - ['refuted']
05/17/2022 03:49:09 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/17/2022 03:49:09 - INFO - __main__ - ['refuted']
05/17/2022 03:49:09 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:49:09 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:49:09 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 03:49:14 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 03:49:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 03:49:15 - INFO - __main__ - Starting training!
05/17/2022 03:49:17 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/17/2022 03:49:19 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/17/2022 03:49:21 - INFO - __main__ - Step 30 Global step 30 Train loss 4.83 on epoch=14
05/17/2022 03:49:22 - INFO - __main__ - Step 40 Global step 40 Train loss 4.83 on epoch=19
05/17/2022 03:49:24 - INFO - __main__ - Step 50 Global step 50 Train loss 4.62 on epoch=24
05/17/2022 03:49:25 - INFO - __main__ - Global step 50 Train loss 4.86 Classification-F1 0.0 on epoch=24
05/17/2022 03:49:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 03:49:27 - INFO - __main__ - Step 60 Global step 60 Train loss 4.56 on epoch=29
05/17/2022 03:49:29 - INFO - __main__ - Step 70 Global step 70 Train loss 4.38 on epoch=34
05/17/2022 03:49:31 - INFO - __main__ - Step 80 Global step 80 Train loss 4.20 on epoch=39
05/17/2022 03:49:33 - INFO - __main__ - Step 90 Global step 90 Train loss 4.13 on epoch=44
05/17/2022 03:49:35 - INFO - __main__ - Step 100 Global step 100 Train loss 4.00 on epoch=49
05/17/2022 03:49:36 - INFO - __main__ - Global step 100 Train loss 4.25 Classification-F1 0.0 on epoch=49
05/17/2022 03:49:38 - INFO - __main__ - Step 110 Global step 110 Train loss 3.94 on epoch=54
05/17/2022 03:49:40 - INFO - __main__ - Step 120 Global step 120 Train loss 3.78 on epoch=59
05/17/2022 03:49:42 - INFO - __main__ - Step 130 Global step 130 Train loss 3.62 on epoch=64
05/17/2022 03:49:44 - INFO - __main__ - Step 140 Global step 140 Train loss 3.48 on epoch=69
05/17/2022 03:49:46 - INFO - __main__ - Step 150 Global step 150 Train loss 3.34 on epoch=74
05/17/2022 03:49:49 - INFO - __main__ - Global step 150 Train loss 3.63 Classification-F1 0.10526315789473684 on epoch=74
05/17/2022 03:49:49 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.10526315789473684 on epoch=74, global_step=150
05/17/2022 03:49:51 - INFO - __main__ - Step 160 Global step 160 Train loss 3.24 on epoch=79
05/17/2022 03:49:53 - INFO - __main__ - Step 170 Global step 170 Train loss 3.04 on epoch=84
05/17/2022 03:49:54 - INFO - __main__ - Step 180 Global step 180 Train loss 2.89 on epoch=89
05/17/2022 03:49:56 - INFO - __main__ - Step 190 Global step 190 Train loss 2.63 on epoch=94
05/17/2022 03:49:58 - INFO - __main__ - Step 200 Global step 200 Train loss 2.74 on epoch=99
05/17/2022 03:49:59 - INFO - __main__ - Global step 200 Train loss 2.91 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 03:49:59 - INFO - __main__ - Saving model with best Classification-F1: 0.10526315789473684 -> 0.3333333333333333 on epoch=99, global_step=200
05/17/2022 03:50:01 - INFO - __main__ - Step 210 Global step 210 Train loss 2.49 on epoch=104
05/17/2022 03:50:03 - INFO - __main__ - Step 220 Global step 220 Train loss 2.25 on epoch=109
05/17/2022 03:50:05 - INFO - __main__ - Step 230 Global step 230 Train loss 2.25 on epoch=114
05/17/2022 03:50:07 - INFO - __main__ - Step 240 Global step 240 Train loss 2.11 on epoch=119
05/17/2022 03:50:09 - INFO - __main__ - Step 250 Global step 250 Train loss 1.89 on epoch=124
05/17/2022 03:50:11 - INFO - __main__ - Global step 250 Train loss 2.20 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 03:50:13 - INFO - __main__ - Step 260 Global step 260 Train loss 1.77 on epoch=129
05/17/2022 03:50:15 - INFO - __main__ - Step 270 Global step 270 Train loss 1.76 on epoch=134
05/17/2022 03:50:17 - INFO - __main__ - Step 280 Global step 280 Train loss 1.68 on epoch=139
05/17/2022 03:50:19 - INFO - __main__ - Step 290 Global step 290 Train loss 1.60 on epoch=144
05/17/2022 03:50:21 - INFO - __main__ - Step 300 Global step 300 Train loss 1.60 on epoch=149
05/17/2022 03:50:22 - INFO - __main__ - Global step 300 Train loss 1.68 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 03:50:24 - INFO - __main__ - Step 310 Global step 310 Train loss 1.46 on epoch=154
05/17/2022 03:50:25 - INFO - __main__ - Step 320 Global step 320 Train loss 1.54 on epoch=159
05/17/2022 03:50:27 - INFO - __main__ - Step 330 Global step 330 Train loss 1.45 on epoch=164
05/17/2022 03:50:29 - INFO - __main__ - Step 340 Global step 340 Train loss 1.50 on epoch=169
05/17/2022 03:50:31 - INFO - __main__ - Step 350 Global step 350 Train loss 1.38 on epoch=174
05/17/2022 03:50:34 - INFO - __main__ - Global step 350 Train loss 1.47 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 03:50:36 - INFO - __main__ - Step 360 Global step 360 Train loss 1.30 on epoch=179
05/17/2022 03:50:38 - INFO - __main__ - Step 370 Global step 370 Train loss 1.31 on epoch=184
05/17/2022 03:50:40 - INFO - __main__ - Step 380 Global step 380 Train loss 1.16 on epoch=189
05/17/2022 03:50:41 - INFO - __main__ - Step 390 Global step 390 Train loss 1.28 on epoch=194
05/17/2022 03:50:43 - INFO - __main__ - Step 400 Global step 400 Train loss 1.20 on epoch=199
05/17/2022 03:50:46 - INFO - __main__ - Global step 400 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 03:50:47 - INFO - __main__ - Step 410 Global step 410 Train loss 1.14 on epoch=204
05/17/2022 03:50:49 - INFO - __main__ - Step 420 Global step 420 Train loss 1.20 on epoch=209
05/17/2022 03:50:51 - INFO - __main__ - Step 430 Global step 430 Train loss 1.10 on epoch=214
05/17/2022 03:50:53 - INFO - __main__ - Step 440 Global step 440 Train loss 1.02 on epoch=219
05/17/2022 03:50:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.99 on epoch=224
05/17/2022 03:50:56 - INFO - __main__ - Global step 450 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 03:50:58 - INFO - __main__ - Step 460 Global step 460 Train loss 1.07 on epoch=229
05/17/2022 03:51:00 - INFO - __main__ - Step 470 Global step 470 Train loss 1.01 on epoch=234
05/17/2022 03:51:02 - INFO - __main__ - Step 480 Global step 480 Train loss 1.03 on epoch=239
05/17/2022 03:51:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.93 on epoch=244
05/17/2022 03:51:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.92 on epoch=249
05/17/2022 03:51:06 - INFO - __main__ - Global step 500 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 03:51:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.86 on epoch=254
05/17/2022 03:51:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=259
05/17/2022 03:51:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.85 on epoch=264
05/17/2022 03:51:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.82 on epoch=269
05/17/2022 03:51:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=274
05/17/2022 03:51:16 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 03:51:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.91 on epoch=279
05/17/2022 03:51:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.73 on epoch=284
05/17/2022 03:51:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.71 on epoch=289
05/17/2022 03:51:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.68 on epoch=294
05/17/2022 03:51:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.75 on epoch=299
05/17/2022 03:51:27 - INFO - __main__ - Global step 600 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 03:51:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.81 on epoch=304
05/17/2022 03:51:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.75 on epoch=309
05/17/2022 03:51:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.77 on epoch=314
05/17/2022 03:51:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.66 on epoch=319
05/17/2022 03:51:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.63 on epoch=324
05/17/2022 03:51:37 - INFO - __main__ - Global step 650 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 03:51:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.70 on epoch=329
05/17/2022 03:51:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.65 on epoch=334
05/17/2022 03:51:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=339
05/17/2022 03:51:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.65 on epoch=344
05/17/2022 03:51:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.75 on epoch=349
05/17/2022 03:51:47 - INFO - __main__ - Global step 700 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 03:51:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.61 on epoch=354
05/17/2022 03:51:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
05/17/2022 03:51:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.61 on epoch=364
05/17/2022 03:51:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.59 on epoch=369
05/17/2022 03:51:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=374
05/17/2022 03:51:58 - INFO - __main__ - Global step 750 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 03:52:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.58 on epoch=379
05/17/2022 03:52:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.67 on epoch=384
05/17/2022 03:52:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.56 on epoch=389
05/17/2022 03:52:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.59 on epoch=394
05/17/2022 03:52:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.62 on epoch=399
05/17/2022 03:52:08 - INFO - __main__ - Global step 800 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 03:52:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.54 on epoch=404
05/17/2022 03:52:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.59 on epoch=409
05/17/2022 03:52:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.61 on epoch=414
05/17/2022 03:52:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.53 on epoch=419
05/17/2022 03:52:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=424
05/17/2022 03:52:19 - INFO - __main__ - Global step 850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 03:52:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=429
05/17/2022 03:52:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=434
05/17/2022 03:52:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=439
05/17/2022 03:52:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=444
05/17/2022 03:52:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=449
05/17/2022 03:52:29 - INFO - __main__ - Global step 900 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 03:52:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=454
05/17/2022 03:52:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.55 on epoch=459
05/17/2022 03:52:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.49 on epoch=464
05/17/2022 03:52:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=469
05/17/2022 03:52:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=474
05/17/2022 03:52:39 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 03:52:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.52 on epoch=479
05/17/2022 03:52:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=484
05/17/2022 03:52:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=489
05/17/2022 03:52:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=494
05/17/2022 03:52:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=499
05/17/2022 03:52:50 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 03:52:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=504
05/17/2022 03:52:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=509
05/17/2022 03:52:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.55 on epoch=514
05/17/2022 03:52:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=519
05/17/2022 03:52:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=524
05/17/2022 03:53:00 - INFO - __main__ - Global step 1050 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 03:53:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=529
05/17/2022 03:53:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=534
05/17/2022 03:53:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=539
05/17/2022 03:53:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
05/17/2022 03:53:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=549
05/17/2022 03:53:10 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 03:53:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=554
05/17/2022 03:53:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=559
05/17/2022 03:53:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=564
05/17/2022 03:53:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.49 on epoch=569
05/17/2022 03:53:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=574
05/17/2022 03:53:21 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 03:53:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=579
05/17/2022 03:53:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=584
05/17/2022 03:53:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=589
05/17/2022 03:53:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=594
05/17/2022 03:53:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=599
05/17/2022 03:53:31 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 03:53:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=604
05/17/2022 03:53:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.46 on epoch=609
05/17/2022 03:53:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=614
05/17/2022 03:53:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=619
05/17/2022 03:53:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=624
05/17/2022 03:53:41 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 03:53:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=629
05/17/2022 03:53:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=634
05/17/2022 03:53:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=639
05/17/2022 03:53:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=644
05/17/2022 03:53:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=649
05/17/2022 03:53:52 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.3191489361702127 on epoch=649
05/17/2022 03:53:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=654
05/17/2022 03:53:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=659
05/17/2022 03:53:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=664
05/17/2022 03:54:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=669
05/17/2022 03:54:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=674
05/17/2022 03:54:03 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 03:54:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=679
05/17/2022 03:54:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=684
05/17/2022 03:54:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=689
05/17/2022 03:54:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=694
05/17/2022 03:54:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=699
05/17/2022 03:54:13 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 03:54:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=704
05/17/2022 03:54:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=709
05/17/2022 03:54:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=714
05/17/2022 03:54:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=719
05/17/2022 03:54:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=724
05/17/2022 03:54:23 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 03:54:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=729
05/17/2022 03:54:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=734
05/17/2022 03:54:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=739
05/17/2022 03:54:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=744
05/17/2022 03:54:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=749
05/17/2022 03:54:34 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 03:54:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=754
05/17/2022 03:54:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=759
05/17/2022 03:54:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=764
05/17/2022 03:54:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=769
05/17/2022 03:54:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=774
05/17/2022 03:54:44 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 03:54:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=779
05/17/2022 03:54:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=784
05/17/2022 03:54:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
05/17/2022 03:54:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
05/17/2022 03:54:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=799
05/17/2022 03:54:54 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 03:54:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=804
05/17/2022 03:54:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=809
05/17/2022 03:55:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=814
05/17/2022 03:55:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=819
05/17/2022 03:55:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=824
05/17/2022 03:55:05 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 03:55:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=829
05/17/2022 03:55:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=834
05/17/2022 03:55:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.36 on epoch=839
05/17/2022 03:55:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=844
05/17/2022 03:55:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=849
05/17/2022 03:55:15 - INFO - __main__ - Global step 1700 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 03:55:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/17/2022 03:55:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=859
05/17/2022 03:55:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=864
05/17/2022 03:55:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=869
05/17/2022 03:55:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
05/17/2022 03:55:26 - INFO - __main__ - Global step 1750 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 03:55:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=879
05/17/2022 03:55:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
05/17/2022 03:55:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.31 on epoch=889
05/17/2022 03:55:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=894
05/17/2022 03:55:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
05/17/2022 03:55:36 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 03:55:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=904
05/17/2022 03:55:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/17/2022 03:55:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=914
05/17/2022 03:55:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.29 on epoch=919
05/17/2022 03:55:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
05/17/2022 03:55:46 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 03:55:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=929
05/17/2022 03:55:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=934
05/17/2022 03:55:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=939
05/17/2022 03:55:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=944
05/17/2022 03:55:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=949
05/17/2022 03:55:57 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 03:55:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.30 on epoch=954
05/17/2022 03:56:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.34 on epoch=959
05/17/2022 03:56:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/17/2022 03:56:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
05/17/2022 03:56:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=974
05/17/2022 03:56:07 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 03:56:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.33 on epoch=979
05/17/2022 03:56:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=984
05/17/2022 03:56:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.25 on epoch=989
05/17/2022 03:56:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/17/2022 03:56:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=999
05/17/2022 03:56:17 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 03:56:17 - INFO - __main__ - save last model!
05/17/2022 03:56:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 03:56:17 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 03:56:17 - INFO - __main__ - Printing 3 examples
05/17/2022 03:56:17 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:56:17 - INFO - __main__ - ['entailed']
05/17/2022 03:56:17 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:56:17 - INFO - __main__ - ['entailed']
05/17/2022 03:56:17 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 03:56:17 - INFO - __main__ - ['entailed']
05/17/2022 03:56:17 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:56:18 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:56:18 - INFO - __main__ - Printing 3 examples
05/17/2022 03:56:18 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/17/2022 03:56:18 - INFO - __main__ - ['refuted']
05/17/2022 03:56:18 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/17/2022 03:56:18 - INFO - __main__ - ['refuted']
05/17/2022 03:56:18 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/17/2022 03:56:18 - INFO - __main__ - ['refuted']
05/17/2022 03:56:18 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:56:18 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:56:18 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 03:56:18 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 03:56:18 - INFO - __main__ - Printing 3 examples
05/17/2022 03:56:18 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/17/2022 03:56:18 - INFO - __main__ - ['refuted']
05/17/2022 03:56:18 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/17/2022 03:56:18 - INFO - __main__ - ['refuted']
05/17/2022 03:56:18 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/17/2022 03:56:18 - INFO - __main__ - ['refuted']
05/17/2022 03:56:18 - INFO - __main__ - Tokenizing Input ...
05/17/2022 03:56:18 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:56:18 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 03:56:23 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 03:56:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 03:56:24 - INFO - __main__ - Starting training!
05/17/2022 03:56:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 03:56:53 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 04:01:23 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.5_8_predictions.txt
05/17/2022 04:01:23 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 04:01:24 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 04:01:24 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.4, bsz=8 ...
05/17/2022 04:01:25 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:01:25 - INFO - __main__ - Printing 3 examples
05/17/2022 04:01:25 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/17/2022 04:01:25 - INFO - __main__ - ['refuted']
05/17/2022 04:01:25 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/17/2022 04:01:25 - INFO - __main__ - ['refuted']
05/17/2022 04:01:25 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/17/2022 04:01:25 - INFO - __main__ - ['refuted']
05/17/2022 04:01:25 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:01:25 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:01:25 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 04:01:25 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:01:25 - INFO - __main__ - Printing 3 examples
05/17/2022 04:01:25 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/17/2022 04:01:25 - INFO - __main__ - ['refuted']
05/17/2022 04:01:25 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/17/2022 04:01:25 - INFO - __main__ - ['refuted']
05/17/2022 04:01:25 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/17/2022 04:01:25 - INFO - __main__ - ['refuted']
05/17/2022 04:01:25 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:01:25 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:01:25 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 04:01:30 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 04:01:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 04:01:30 - INFO - __main__ - Starting training!
05/17/2022 04:01:33 - INFO - __main__ - Step 10 Global step 10 Train loss 4.97 on epoch=4
05/17/2022 04:01:34 - INFO - __main__ - Step 20 Global step 20 Train loss 5.00 on epoch=9
05/17/2022 04:01:36 - INFO - __main__ - Step 30 Global step 30 Train loss 4.73 on epoch=14
05/17/2022 04:01:38 - INFO - __main__ - Step 40 Global step 40 Train loss 4.75 on epoch=19
05/17/2022 04:01:40 - INFO - __main__ - Step 50 Global step 50 Train loss 4.65 on epoch=24
05/17/2022 04:01:46 - INFO - __main__ - Global step 50 Train loss 4.82 Classification-F1 0.0 on epoch=24
05/17/2022 04:01:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 04:01:48 - INFO - __main__ - Step 60 Global step 60 Train loss 4.51 on epoch=29
05/17/2022 04:01:50 - INFO - __main__ - Step 70 Global step 70 Train loss 4.44 on epoch=34
05/17/2022 04:01:52 - INFO - __main__ - Step 80 Global step 80 Train loss 4.27 on epoch=39
05/17/2022 04:01:54 - INFO - __main__ - Step 90 Global step 90 Train loss 4.15 on epoch=44
05/17/2022 04:01:56 - INFO - __main__ - Step 100 Global step 100 Train loss 4.08 on epoch=49
05/17/2022 04:02:02 - INFO - __main__ - Global step 100 Train loss 4.29 Classification-F1 0.0 on epoch=49
05/17/2022 04:02:04 - INFO - __main__ - Step 110 Global step 110 Train loss 3.96 on epoch=54
05/17/2022 04:02:06 - INFO - __main__ - Step 120 Global step 120 Train loss 3.76 on epoch=59
05/17/2022 04:02:08 - INFO - __main__ - Step 130 Global step 130 Train loss 3.70 on epoch=64
05/17/2022 04:02:10 - INFO - __main__ - Step 140 Global step 140 Train loss 3.53 on epoch=69
05/17/2022 04:02:12 - INFO - __main__ - Step 150 Global step 150 Train loss 3.63 on epoch=74
05/17/2022 04:02:16 - INFO - __main__ - Global step 150 Train loss 3.72 Classification-F1 0.016304347826086953 on epoch=74
05/17/2022 04:02:16 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.016304347826086953 on epoch=74, global_step=150
05/17/2022 04:02:18 - INFO - __main__ - Step 160 Global step 160 Train loss 3.35 on epoch=79
05/17/2022 04:02:20 - INFO - __main__ - Step 170 Global step 170 Train loss 3.28 on epoch=84
05/17/2022 04:02:22 - INFO - __main__ - Step 180 Global step 180 Train loss 3.14 on epoch=89
05/17/2022 04:02:24 - INFO - __main__ - Step 190 Global step 190 Train loss 3.07 on epoch=94
05/17/2022 04:02:26 - INFO - __main__ - Step 200 Global step 200 Train loss 3.01 on epoch=99
05/17/2022 04:02:30 - INFO - __main__ - Global step 200 Train loss 3.17 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 04:02:30 - INFO - __main__ - Saving model with best Classification-F1: 0.016304347826086953 -> 0.3333333333333333 on epoch=99, global_step=200
05/17/2022 04:02:32 - INFO - __main__ - Step 210 Global step 210 Train loss 2.93 on epoch=104
05/17/2022 04:02:34 - INFO - __main__ - Step 220 Global step 220 Train loss 2.89 on epoch=109
05/17/2022 04:02:35 - INFO - __main__ - Step 230 Global step 230 Train loss 2.64 on epoch=114
05/17/2022 04:02:37 - INFO - __main__ - Step 240 Global step 240 Train loss 2.50 on epoch=119
05/17/2022 04:02:39 - INFO - __main__ - Step 250 Global step 250 Train loss 2.54 on epoch=124
05/17/2022 04:02:42 - INFO - __main__ - Global step 250 Train loss 2.70 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 04:02:44 - INFO - __main__ - Step 260 Global step 260 Train loss 2.38 on epoch=129
05/17/2022 04:02:45 - INFO - __main__ - Step 270 Global step 270 Train loss 2.28 on epoch=134
05/17/2022 04:02:47 - INFO - __main__ - Step 280 Global step 280 Train loss 2.24 on epoch=139
05/17/2022 04:02:49 - INFO - __main__ - Step 290 Global step 290 Train loss 2.22 on epoch=144
05/17/2022 04:02:51 - INFO - __main__ - Step 300 Global step 300 Train loss 2.20 on epoch=149
05/17/2022 04:03:03 - INFO - __main__ - Global step 300 Train loss 2.26 Classification-F1 0.10852713178294572 on epoch=149
05/17/2022 04:03:05 - INFO - __main__ - Step 310 Global step 310 Train loss 2.17 on epoch=154
05/17/2022 04:03:07 - INFO - __main__ - Step 320 Global step 320 Train loss 2.00 on epoch=159
05/17/2022 04:03:08 - INFO - __main__ - Step 330 Global step 330 Train loss 2.00 on epoch=164
05/17/2022 04:03:10 - INFO - __main__ - Step 340 Global step 340 Train loss 1.97 on epoch=169
05/17/2022 04:03:12 - INFO - __main__ - Step 350 Global step 350 Train loss 1.86 on epoch=174
05/17/2022 04:03:23 - INFO - __main__ - Global step 350 Train loss 2.00 Classification-F1 0.22695035460992907 on epoch=174
05/17/2022 04:03:25 - INFO - __main__ - Step 360 Global step 360 Train loss 1.90 on epoch=179
05/17/2022 04:03:27 - INFO - __main__ - Step 370 Global step 370 Train loss 1.89 on epoch=184
05/17/2022 04:03:29 - INFO - __main__ - Step 380 Global step 380 Train loss 1.74 on epoch=189
05/17/2022 04:03:31 - INFO - __main__ - Step 390 Global step 390 Train loss 1.78 on epoch=194
05/17/2022 04:03:33 - INFO - __main__ - Step 400 Global step 400 Train loss 1.75 on epoch=199
05/17/2022 04:03:43 - INFO - __main__ - Global step 400 Train loss 1.81 Classification-F1 0.21276595744680848 on epoch=199
05/17/2022 04:03:45 - INFO - __main__ - Step 410 Global step 410 Train loss 1.72 on epoch=204
05/17/2022 04:03:47 - INFO - __main__ - Step 420 Global step 420 Train loss 1.73 on epoch=209
05/17/2022 04:03:49 - INFO - __main__ - Step 430 Global step 430 Train loss 1.65 on epoch=214
05/17/2022 04:03:51 - INFO - __main__ - Step 440 Global step 440 Train loss 1.60 on epoch=219
05/17/2022 04:03:53 - INFO - __main__ - Step 450 Global step 450 Train loss 1.50 on epoch=224
05/17/2022 04:03:55 - INFO - __main__ - Global step 450 Train loss 1.64 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 04:03:57 - INFO - __main__ - Step 460 Global step 460 Train loss 1.40 on epoch=229
05/17/2022 04:03:59 - INFO - __main__ - Step 470 Global step 470 Train loss 1.41 on epoch=234
05/17/2022 04:04:00 - INFO - __main__ - Step 480 Global step 480 Train loss 1.41 on epoch=239
05/17/2022 04:04:02 - INFO - __main__ - Step 490 Global step 490 Train loss 1.25 on epoch=244
05/17/2022 04:04:04 - INFO - __main__ - Step 500 Global step 500 Train loss 1.29 on epoch=249
05/17/2022 04:04:05 - INFO - __main__ - Global step 500 Train loss 1.35 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 04:04:07 - INFO - __main__ - Step 510 Global step 510 Train loss 1.32 on epoch=254
05/17/2022 04:04:09 - INFO - __main__ - Step 520 Global step 520 Train loss 1.34 on epoch=259
05/17/2022 04:04:11 - INFO - __main__ - Step 530 Global step 530 Train loss 1.21 on epoch=264
05/17/2022 04:04:13 - INFO - __main__ - Step 540 Global step 540 Train loss 1.16 on epoch=269
05/17/2022 04:04:15 - INFO - __main__ - Step 550 Global step 550 Train loss 1.08 on epoch=274
05/17/2022 04:04:16 - INFO - __main__ - Global step 550 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 04:04:17 - INFO - __main__ - Step 560 Global step 560 Train loss 1.09 on epoch=279
05/17/2022 04:04:19 - INFO - __main__ - Step 570 Global step 570 Train loss 1.17 on epoch=284
05/17/2022 04:04:21 - INFO - __main__ - Step 580 Global step 580 Train loss 1.16 on epoch=289
05/17/2022 04:04:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.97 on epoch=294
05/17/2022 04:04:25 - INFO - __main__ - Step 600 Global step 600 Train loss 1.04 on epoch=299
05/17/2022 04:04:26 - INFO - __main__ - Global step 600 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 04:04:28 - INFO - __main__ - Step 610 Global step 610 Train loss 1.00 on epoch=304
05/17/2022 04:04:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.98 on epoch=309
05/17/2022 04:04:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.95 on epoch=314
05/17/2022 04:04:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.90 on epoch=319
05/17/2022 04:04:35 - INFO - __main__ - Step 650 Global step 650 Train loss 1.05 on epoch=324
05/17/2022 04:04:36 - INFO - __main__ - Global step 650 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 04:04:38 - INFO - __main__ - Step 660 Global step 660 Train loss 1.03 on epoch=329
05/17/2022 04:04:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.87 on epoch=334
05/17/2022 04:04:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.98 on epoch=339
05/17/2022 04:04:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.91 on epoch=344
05/17/2022 04:04:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.96 on epoch=349
05/17/2022 04:04:47 - INFO - __main__ - Global step 700 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 04:04:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.90 on epoch=354
05/17/2022 04:04:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=359
05/17/2022 04:04:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.81 on epoch=364
05/17/2022 04:04:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.81 on epoch=369
05/17/2022 04:04:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.77 on epoch=374
05/17/2022 04:04:57 - INFO - __main__ - Global step 750 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 04:04:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.73 on epoch=379
05/17/2022 04:05:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=384
05/17/2022 04:05:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.72 on epoch=389
05/17/2022 04:05:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.66 on epoch=394
05/17/2022 04:05:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.75 on epoch=399
05/17/2022 04:05:08 - INFO - __main__ - Global step 800 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 04:05:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.66 on epoch=404
05/17/2022 04:05:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.64 on epoch=409
05/17/2022 04:05:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.76 on epoch=414
05/17/2022 04:05:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.68 on epoch=419
05/17/2022 04:05:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.68 on epoch=424
05/17/2022 04:05:18 - INFO - __main__ - Global step 850 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 04:05:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.77 on epoch=429
05/17/2022 04:05:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.70 on epoch=434
05/17/2022 04:05:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.58 on epoch=439
05/17/2022 04:05:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.60 on epoch=444
05/17/2022 04:05:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.58 on epoch=449
05/17/2022 04:05:28 - INFO - __main__ - Global step 900 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 04:05:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.59 on epoch=454
05/17/2022 04:05:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.64 on epoch=459
05/17/2022 04:05:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.61 on epoch=464
05/17/2022 04:05:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.66 on epoch=469
05/17/2022 04:05:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.62 on epoch=474
05/17/2022 04:05:39 - INFO - __main__ - Global step 950 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 04:05:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.64 on epoch=479
05/17/2022 04:05:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.67 on epoch=484
05/17/2022 04:05:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.55 on epoch=489
05/17/2022 04:05:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.60 on epoch=494
05/17/2022 04:05:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.58 on epoch=499
05/17/2022 04:05:50 - INFO - __main__ - Global step 1000 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 04:05:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=504
05/17/2022 04:05:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=509
05/17/2022 04:05:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=514
05/17/2022 04:05:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=519
05/17/2022 04:05:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=524
05/17/2022 04:06:00 - INFO - __main__ - Global step 1050 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 04:06:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.53 on epoch=529
05/17/2022 04:06:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=534
05/17/2022 04:06:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.55 on epoch=539
05/17/2022 04:06:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.60 on epoch=544
05/17/2022 04:06:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=549
05/17/2022 04:06:11 - INFO - __main__ - Global step 1100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 04:06:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=554
05/17/2022 04:06:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.53 on epoch=559
05/17/2022 04:06:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.55 on epoch=564
05/17/2022 04:06:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=569
05/17/2022 04:06:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=574
05/17/2022 04:06:21 - INFO - __main__ - Global step 1150 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 04:06:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=579
05/17/2022 04:06:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=584
05/17/2022 04:06:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=589
05/17/2022 04:06:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=594
05/17/2022 04:06:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=599
05/17/2022 04:06:31 - INFO - __main__ - Global step 1200 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 04:06:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=604
05/17/2022 04:06:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
05/17/2022 04:06:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=614
05/17/2022 04:06:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=619
05/17/2022 04:06:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=624
05/17/2022 04:06:42 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 04:06:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=629
05/17/2022 04:06:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=634
05/17/2022 04:06:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=639
05/17/2022 04:06:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.51 on epoch=644
05/17/2022 04:06:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=649
05/17/2022 04:06:52 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 04:06:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=654
05/17/2022 04:06:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
05/17/2022 04:06:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=664
05/17/2022 04:07:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=669
05/17/2022 04:07:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=674
05/17/2022 04:07:03 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 04:07:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=679
05/17/2022 04:07:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=684
05/17/2022 04:07:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=689
05/17/2022 04:07:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.54 on epoch=694
05/17/2022 04:07:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.55 on epoch=699
05/17/2022 04:07:13 - INFO - __main__ - Global step 1400 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 04:07:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=704
05/17/2022 04:07:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=709
05/17/2022 04:07:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=714
05/17/2022 04:07:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=719
05/17/2022 04:07:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
05/17/2022 04:07:23 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 04:07:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=729
05/17/2022 04:07:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=734
05/17/2022 04:07:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/17/2022 04:07:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=744
05/17/2022 04:07:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=749
05/17/2022 04:07:34 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 04:07:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.45 on epoch=754
05/17/2022 04:07:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=759
05/17/2022 04:07:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=764
05/17/2022 04:07:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=769
05/17/2022 04:07:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=774
05/17/2022 04:07:44 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 04:07:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=779
05/17/2022 04:07:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=784
05/17/2022 04:07:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=789
05/17/2022 04:07:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=794
05/17/2022 04:07:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=799
05/17/2022 04:07:55 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 04:07:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=804
05/17/2022 04:07:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=809
05/17/2022 04:08:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=814
05/17/2022 04:08:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
05/17/2022 04:08:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=824
05/17/2022 04:08:05 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 04:08:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=829
05/17/2022 04:08:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=834
05/17/2022 04:08:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=839
05/17/2022 04:08:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=844
05/17/2022 04:08:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.52 on epoch=849
05/17/2022 04:08:15 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 04:08:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/17/2022 04:08:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=859
05/17/2022 04:08:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=864
05/17/2022 04:08:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=869
05/17/2022 04:08:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
05/17/2022 04:08:26 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 04:08:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=879
05/17/2022 04:08:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
05/17/2022 04:08:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=889
05/17/2022 04:08:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
05/17/2022 04:08:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
05/17/2022 04:08:36 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 04:08:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=904
05/17/2022 04:08:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
05/17/2022 04:08:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=914
05/17/2022 04:08:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=919
05/17/2022 04:08:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
05/17/2022 04:08:46 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 04:08:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=929
05/17/2022 04:08:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=934
05/17/2022 04:08:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.52 on epoch=939
05/17/2022 04:08:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=944
05/17/2022 04:08:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=949
05/17/2022 04:08:57 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 04:08:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=954
05/17/2022 04:09:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
05/17/2022 04:09:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=964
05/17/2022 04:09:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.37 on epoch=969
05/17/2022 04:09:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=974
05/17/2022 04:09:07 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 04:09:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
05/17/2022 04:09:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/17/2022 04:09:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
05/17/2022 04:09:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/17/2022 04:09:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=999
05/17/2022 04:09:17 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 04:09:17 - INFO - __main__ - save last model!
05/17/2022 04:09:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 04:09:17 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 04:09:17 - INFO - __main__ - Printing 3 examples
05/17/2022 04:09:17 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:09:17 - INFO - __main__ - ['entailed']
05/17/2022 04:09:17 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:09:17 - INFO - __main__ - ['entailed']
05/17/2022 04:09:17 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:09:17 - INFO - __main__ - ['entailed']
05/17/2022 04:09:17 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:09:19 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:09:19 - INFO - __main__ - Printing 3 examples
05/17/2022 04:09:19 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/17/2022 04:09:19 - INFO - __main__ - ['refuted']
05/17/2022 04:09:19 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/17/2022 04:09:19 - INFO - __main__ - ['refuted']
05/17/2022 04:09:19 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/17/2022 04:09:19 - INFO - __main__ - ['refuted']
05/17/2022 04:09:19 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:09:19 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:09:19 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 04:09:19 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:09:19 - INFO - __main__ - Printing 3 examples
05/17/2022 04:09:19 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/17/2022 04:09:19 - INFO - __main__ - ['refuted']
05/17/2022 04:09:19 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/17/2022 04:09:19 - INFO - __main__ - ['refuted']
05/17/2022 04:09:19 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/17/2022 04:09:19 - INFO - __main__ - ['refuted']
05/17/2022 04:09:19 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:09:19 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:09:19 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 04:09:24 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 04:09:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 04:09:25 - INFO - __main__ - Starting training!
05/17/2022 04:09:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:09:54 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 04:14:01 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.4_8_predictions.txt
05/17/2022 04:14:01 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 04:14:01 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 04:14:01 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.3, bsz=8 ...
05/17/2022 04:14:02 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:14:02 - INFO - __main__ - Printing 3 examples
05/17/2022 04:14:02 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/17/2022 04:14:02 - INFO - __main__ - ['refuted']
05/17/2022 04:14:02 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/17/2022 04:14:02 - INFO - __main__ - ['refuted']
05/17/2022 04:14:02 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/17/2022 04:14:02 - INFO - __main__ - ['refuted']
05/17/2022 04:14:02 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:14:02 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:14:02 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 04:14:02 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:14:02 - INFO - __main__ - Printing 3 examples
05/17/2022 04:14:02 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/17/2022 04:14:02 - INFO - __main__ - ['refuted']
05/17/2022 04:14:02 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/17/2022 04:14:02 - INFO - __main__ - ['refuted']
05/17/2022 04:14:02 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/17/2022 04:14:02 - INFO - __main__ - ['refuted']
05/17/2022 04:14:02 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:14:02 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:14:02 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 04:14:07 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 04:14:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 04:14:08 - INFO - __main__ - Starting training!
05/17/2022 04:14:10 - INFO - __main__ - Step 10 Global step 10 Train loss 4.93 on epoch=4
05/17/2022 04:14:12 - INFO - __main__ - Step 20 Global step 20 Train loss 4.96 on epoch=9
05/17/2022 04:14:13 - INFO - __main__ - Step 30 Global step 30 Train loss 4.83 on epoch=14
05/17/2022 04:14:15 - INFO - __main__ - Step 40 Global step 40 Train loss 4.79 on epoch=19
05/17/2022 04:14:17 - INFO - __main__ - Step 50 Global step 50 Train loss 4.76 on epoch=24
05/17/2022 04:14:24 - INFO - __main__ - Global step 50 Train loss 4.85 Classification-F1 0.0 on epoch=24
05/17/2022 04:14:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 04:14:26 - INFO - __main__ - Step 60 Global step 60 Train loss 4.55 on epoch=29
05/17/2022 04:14:28 - INFO - __main__ - Step 70 Global step 70 Train loss 4.49 on epoch=34
05/17/2022 04:14:30 - INFO - __main__ - Step 80 Global step 80 Train loss 4.46 on epoch=39
05/17/2022 04:14:32 - INFO - __main__ - Step 90 Global step 90 Train loss 4.41 on epoch=44
05/17/2022 04:14:34 - INFO - __main__ - Step 100 Global step 100 Train loss 4.27 on epoch=49
05/17/2022 04:14:35 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/17/2022 04:14:37 - INFO - __main__ - Step 110 Global step 110 Train loss 4.04 on epoch=54
05/17/2022 04:14:39 - INFO - __main__ - Step 120 Global step 120 Train loss 3.99 on epoch=59
05/17/2022 04:14:41 - INFO - __main__ - Step 130 Global step 130 Train loss 3.90 on epoch=64
05/17/2022 04:14:43 - INFO - __main__ - Step 140 Global step 140 Train loss 3.86 on epoch=69
05/17/2022 04:14:45 - INFO - __main__ - Step 150 Global step 150 Train loss 3.85 on epoch=74
05/17/2022 04:14:46 - INFO - __main__ - Global step 150 Train loss 3.93 Classification-F1 0.0 on epoch=74
05/17/2022 04:14:48 - INFO - __main__ - Step 160 Global step 160 Train loss 3.69 on epoch=79
05/17/2022 04:14:50 - INFO - __main__ - Step 170 Global step 170 Train loss 3.57 on epoch=84
05/17/2022 04:14:52 - INFO - __main__ - Step 180 Global step 180 Train loss 3.63 on epoch=89
05/17/2022 04:14:54 - INFO - __main__ - Step 190 Global step 190 Train loss 3.55 on epoch=94
05/17/2022 04:14:55 - INFO - __main__ - Step 200 Global step 200 Train loss 3.35 on epoch=99
05/17/2022 04:14:57 - INFO - __main__ - Global step 200 Train loss 3.56 Classification-F1 0.0 on epoch=99
05/17/2022 04:14:59 - INFO - __main__ - Step 210 Global step 210 Train loss 3.37 on epoch=104
05/17/2022 04:15:00 - INFO - __main__ - Step 220 Global step 220 Train loss 3.39 on epoch=109
05/17/2022 04:15:02 - INFO - __main__ - Step 230 Global step 230 Train loss 3.29 on epoch=114
05/17/2022 04:15:04 - INFO - __main__ - Step 240 Global step 240 Train loss 3.10 on epoch=119
05/17/2022 04:15:06 - INFO - __main__ - Step 250 Global step 250 Train loss 3.17 on epoch=124
05/17/2022 04:15:07 - INFO - __main__ - Global step 250 Train loss 3.26 Classification-F1 0.14285714285714285 on epoch=124
05/17/2022 04:15:07 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.14285714285714285 on epoch=124, global_step=250
05/17/2022 04:15:09 - INFO - __main__ - Step 260 Global step 260 Train loss 2.88 on epoch=129
05/17/2022 04:15:11 - INFO - __main__ - Step 270 Global step 270 Train loss 2.99 on epoch=134
05/17/2022 04:15:13 - INFO - __main__ - Step 280 Global step 280 Train loss 2.91 on epoch=139
05/17/2022 04:15:15 - INFO - __main__ - Step 290 Global step 290 Train loss 2.79 on epoch=144
05/17/2022 04:15:17 - INFO - __main__ - Step 300 Global step 300 Train loss 2.69 on epoch=149
05/17/2022 04:15:19 - INFO - __main__ - Global step 300 Train loss 2.85 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 04:15:19 - INFO - __main__ - Saving model with best Classification-F1: 0.14285714285714285 -> 0.3333333333333333 on epoch=149, global_step=300
05/17/2022 04:15:21 - INFO - __main__ - Step 310 Global step 310 Train loss 2.65 on epoch=154
05/17/2022 04:15:23 - INFO - __main__ - Step 320 Global step 320 Train loss 2.49 on epoch=159
05/17/2022 04:15:25 - INFO - __main__ - Step 330 Global step 330 Train loss 2.46 on epoch=164
05/17/2022 04:15:27 - INFO - __main__ - Step 340 Global step 340 Train loss 2.35 on epoch=169
05/17/2022 04:15:28 - INFO - __main__ - Step 350 Global step 350 Train loss 2.37 on epoch=174
05/17/2022 04:15:30 - INFO - __main__ - Global step 350 Train loss 2.46 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 04:15:32 - INFO - __main__ - Step 360 Global step 360 Train loss 2.20 on epoch=179
05/17/2022 04:15:34 - INFO - __main__ - Step 370 Global step 370 Train loss 2.00 on epoch=184
05/17/2022 04:15:36 - INFO - __main__ - Step 380 Global step 380 Train loss 1.99 on epoch=189
05/17/2022 04:15:38 - INFO - __main__ - Step 390 Global step 390 Train loss 1.93 on epoch=194
05/17/2022 04:15:40 - INFO - __main__ - Step 400 Global step 400 Train loss 1.79 on epoch=199
05/17/2022 04:15:42 - INFO - __main__ - Global step 400 Train loss 1.98 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 04:15:44 - INFO - __main__ - Step 410 Global step 410 Train loss 1.78 on epoch=204
05/17/2022 04:15:46 - INFO - __main__ - Step 420 Global step 420 Train loss 1.79 on epoch=209
05/17/2022 04:15:48 - INFO - __main__ - Step 430 Global step 430 Train loss 1.69 on epoch=214
05/17/2022 04:15:50 - INFO - __main__ - Step 440 Global step 440 Train loss 1.56 on epoch=219
05/17/2022 04:15:52 - INFO - __main__ - Step 450 Global step 450 Train loss 1.54 on epoch=224
05/17/2022 04:15:55 - INFO - __main__ - Global step 450 Train loss 1.67 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 04:15:57 - INFO - __main__ - Step 460 Global step 460 Train loss 1.67 on epoch=229
05/17/2022 04:15:59 - INFO - __main__ - Step 470 Global step 470 Train loss 1.39 on epoch=234
05/17/2022 04:16:01 - INFO - __main__ - Step 480 Global step 480 Train loss 1.46 on epoch=239
05/17/2022 04:16:03 - INFO - __main__ - Step 490 Global step 490 Train loss 1.44 on epoch=244
05/17/2022 04:16:05 - INFO - __main__ - Step 500 Global step 500 Train loss 1.30 on epoch=249
05/17/2022 04:16:11 - INFO - __main__ - Global step 500 Train loss 1.45 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 04:16:13 - INFO - __main__ - Step 510 Global step 510 Train loss 1.29 on epoch=254
05/17/2022 04:16:15 - INFO - __main__ - Step 520 Global step 520 Train loss 1.30 on epoch=259
05/17/2022 04:16:17 - INFO - __main__ - Step 530 Global step 530 Train loss 1.34 on epoch=264
05/17/2022 04:16:19 - INFO - __main__ - Step 540 Global step 540 Train loss 1.18 on epoch=269
05/17/2022 04:16:20 - INFO - __main__ - Step 550 Global step 550 Train loss 1.17 on epoch=274
05/17/2022 04:16:27 - INFO - __main__ - Global step 550 Train loss 1.26 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 04:16:29 - INFO - __main__ - Step 560 Global step 560 Train loss 1.27 on epoch=279
05/17/2022 04:16:31 - INFO - __main__ - Step 570 Global step 570 Train loss 1.20 on epoch=284
05/17/2022 04:16:33 - INFO - __main__ - Step 580 Global step 580 Train loss 1.12 on epoch=289
05/17/2022 04:16:35 - INFO - __main__ - Step 590 Global step 590 Train loss 1.11 on epoch=294
05/17/2022 04:16:36 - INFO - __main__ - Step 600 Global step 600 Train loss 1.02 on epoch=299
05/17/2022 04:16:39 - INFO - __main__ - Global step 600 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 04:16:41 - INFO - __main__ - Step 610 Global step 610 Train loss 1.07 on epoch=304
05/17/2022 04:16:43 - INFO - __main__ - Step 620 Global step 620 Train loss 1.07 on epoch=309
05/17/2022 04:16:45 - INFO - __main__ - Step 630 Global step 630 Train loss 1.07 on epoch=314
05/17/2022 04:16:47 - INFO - __main__ - Step 640 Global step 640 Train loss 1.04 on epoch=319
05/17/2022 04:16:49 - INFO - __main__ - Step 650 Global step 650 Train loss 1.07 on epoch=324
05/17/2022 04:16:50 - INFO - __main__ - Global step 650 Train loss 1.06 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 04:16:52 - INFO - __main__ - Step 660 Global step 660 Train loss 1.06 on epoch=329
05/17/2022 04:16:53 - INFO - __main__ - Step 670 Global step 670 Train loss 1.01 on epoch=334
05/17/2022 04:16:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.98 on epoch=339
05/17/2022 04:16:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.98 on epoch=344
05/17/2022 04:16:59 - INFO - __main__ - Step 700 Global step 700 Train loss 1.04 on epoch=349
05/17/2022 04:17:00 - INFO - __main__ - Global step 700 Train loss 1.01 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 04:17:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.89 on epoch=354
05/17/2022 04:17:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.95 on epoch=359
05/17/2022 04:17:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.95 on epoch=364
05/17/2022 04:17:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.92 on epoch=369
05/17/2022 04:17:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.92 on epoch=374
05/17/2022 04:17:10 - INFO - __main__ - Global step 750 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 04:17:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=379
05/17/2022 04:17:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.92 on epoch=384
05/17/2022 04:17:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.81 on epoch=389
05/17/2022 04:17:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=394
05/17/2022 04:17:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.87 on epoch=399
05/17/2022 04:17:21 - INFO - __main__ - Global step 800 Train loss 0.86 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 04:17:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=404
05/17/2022 04:17:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.75 on epoch=409
05/17/2022 04:17:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=414
05/17/2022 04:17:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.78 on epoch=419
05/17/2022 04:17:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.85 on epoch=424
05/17/2022 04:17:31 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 04:17:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.75 on epoch=429
05/17/2022 04:17:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.81 on epoch=434
05/17/2022 04:17:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.68 on epoch=439
05/17/2022 04:17:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.68 on epoch=444
05/17/2022 04:17:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.68 on epoch=449
05/17/2022 04:17:42 - INFO - __main__ - Global step 900 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 04:17:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.68 on epoch=454
05/17/2022 04:17:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.71 on epoch=459
05/17/2022 04:17:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.63 on epoch=464
05/17/2022 04:17:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.70 on epoch=469
05/17/2022 04:17:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.75 on epoch=474
05/17/2022 04:17:52 - INFO - __main__ - Global step 950 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 04:17:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.63 on epoch=479
05/17/2022 04:17:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.64 on epoch=484
05/17/2022 04:17:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.62 on epoch=489
05/17/2022 04:18:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.66 on epoch=494
05/17/2022 04:18:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.76 on epoch=499
05/17/2022 04:18:03 - INFO - __main__ - Global step 1000 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 04:18:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.63 on epoch=504
05/17/2022 04:18:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.71 on epoch=509
05/17/2022 04:18:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=514
05/17/2022 04:18:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.68 on epoch=519
05/17/2022 04:18:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=524
05/17/2022 04:18:13 - INFO - __main__ - Global step 1050 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 04:18:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.65 on epoch=529
05/17/2022 04:18:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.65 on epoch=534
05/17/2022 04:18:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.65 on epoch=539
05/17/2022 04:18:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.57 on epoch=544
05/17/2022 04:18:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=549
05/17/2022 04:18:23 - INFO - __main__ - Global step 1100 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 04:18:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.61 on epoch=554
05/17/2022 04:18:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.55 on epoch=559
05/17/2022 04:18:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.56 on epoch=564
05/17/2022 04:18:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.56 on epoch=569
05/17/2022 04:18:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.57 on epoch=574
05/17/2022 04:18:34 - INFO - __main__ - Global step 1150 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 04:18:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.53 on epoch=579
05/17/2022 04:18:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=584
05/17/2022 04:18:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=589
05/17/2022 04:18:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.51 on epoch=594
05/17/2022 04:18:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.50 on epoch=599
05/17/2022 04:18:44 - INFO - __main__ - Global step 1200 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 04:18:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.48 on epoch=604
05/17/2022 04:18:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.52 on epoch=609
05/17/2022 04:18:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=614
05/17/2022 04:18:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.53 on epoch=619
05/17/2022 04:18:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=624
05/17/2022 04:18:55 - INFO - __main__ - Global step 1250 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 04:18:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=629
05/17/2022 04:18:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=634
05/17/2022 04:19:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=639
05/17/2022 04:19:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=644
05/17/2022 04:19:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=649
05/17/2022 04:19:05 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 04:19:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=654
05/17/2022 04:19:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.52 on epoch=659
05/17/2022 04:19:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=664
05/17/2022 04:19:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=669
05/17/2022 04:19:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=674
05/17/2022 04:19:15 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 04:19:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=679
05/17/2022 04:19:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
05/17/2022 04:19:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=689
05/17/2022 04:19:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=694
05/17/2022 04:19:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=699
05/17/2022 04:19:25 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 04:19:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.47 on epoch=704
05/17/2022 04:19:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
05/17/2022 04:19:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
05/17/2022 04:19:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
05/17/2022 04:19:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=724
05/17/2022 04:19:36 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 04:19:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=729
05/17/2022 04:19:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=734
05/17/2022 04:19:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/17/2022 04:19:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=744
05/17/2022 04:19:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=749
05/17/2022 04:19:46 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 04:19:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=754
05/17/2022 04:19:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=759
05/17/2022 04:19:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=764
05/17/2022 04:19:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=769
05/17/2022 04:19:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=774
05/17/2022 04:19:56 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 04:19:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=779
05/17/2022 04:20:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
05/17/2022 04:20:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
05/17/2022 04:20:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=794
05/17/2022 04:20:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
05/17/2022 04:20:06 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 04:20:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=804
05/17/2022 04:20:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=809
05/17/2022 04:20:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=814
05/17/2022 04:20:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=819
05/17/2022 04:20:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=824
05/17/2022 04:20:16 - INFO - __main__ - Global step 1650 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 04:20:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=829
05/17/2022 04:20:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
05/17/2022 04:20:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
05/17/2022 04:20:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=844
05/17/2022 04:20:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=849
05/17/2022 04:20:27 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 04:20:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/17/2022 04:20:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=859
05/17/2022 04:20:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=864
05/17/2022 04:20:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
05/17/2022 04:20:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/17/2022 04:20:37 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 04:20:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=879
05/17/2022 04:20:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=884
05/17/2022 04:20:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=889
05/17/2022 04:20:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=894
05/17/2022 04:20:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=899
05/17/2022 04:20:47 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 04:20:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=904
05/17/2022 04:20:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/17/2022 04:20:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=914
05/17/2022 04:20:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=919
05/17/2022 04:20:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
05/17/2022 04:20:57 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 04:20:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=929
05/17/2022 04:21:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
05/17/2022 04:21:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=939
05/17/2022 04:21:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=944
05/17/2022 04:21:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
05/17/2022 04:21:07 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 04:21:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=954
05/17/2022 04:21:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=959
05/17/2022 04:21:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/17/2022 04:21:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=969
05/17/2022 04:21:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/17/2022 04:21:18 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 04:21:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
05/17/2022 04:21:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=984
05/17/2022 04:21:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
05/17/2022 04:21:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/17/2022 04:21:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=999
05/17/2022 04:21:28 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 04:21:28 - INFO - __main__ - save last model!
05/17/2022 04:21:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 04:21:28 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 04:21:28 - INFO - __main__ - Printing 3 examples
05/17/2022 04:21:28 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:21:28 - INFO - __main__ - ['entailed']
05/17/2022 04:21:28 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:21:28 - INFO - __main__ - ['entailed']
05/17/2022 04:21:28 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:21:28 - INFO - __main__ - ['entailed']
05/17/2022 04:21:28 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:21:28 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:21:28 - INFO - __main__ - Printing 3 examples
05/17/2022 04:21:28 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/17/2022 04:21:28 - INFO - __main__ - ['refuted']
05/17/2022 04:21:28 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/17/2022 04:21:28 - INFO - __main__ - ['refuted']
05/17/2022 04:21:28 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/17/2022 04:21:28 - INFO - __main__ - ['refuted']
05/17/2022 04:21:28 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:21:28 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:21:28 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 04:21:28 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:21:28 - INFO - __main__ - Printing 3 examples
05/17/2022 04:21:28 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/17/2022 04:21:28 - INFO - __main__ - ['refuted']
05/17/2022 04:21:28 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/17/2022 04:21:28 - INFO - __main__ - ['refuted']
05/17/2022 04:21:28 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/17/2022 04:21:28 - INFO - __main__ - ['refuted']
05/17/2022 04:21:28 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:21:29 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:21:29 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 04:21:34 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 04:21:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 04:21:34 - INFO - __main__ - Starting training!
05/17/2022 04:21:52 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:22:04 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 04:26:12 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.3_8_predictions.txt
05/17/2022 04:26:12 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 04:26:12 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 04:26:12 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.2, bsz=8 ...
05/17/2022 04:26:13 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:26:13 - INFO - __main__ - Printing 3 examples
05/17/2022 04:26:13 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/17/2022 04:26:13 - INFO - __main__ - ['refuted']
05/17/2022 04:26:13 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/17/2022 04:26:13 - INFO - __main__ - ['refuted']
05/17/2022 04:26:13 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/17/2022 04:26:13 - INFO - __main__ - ['refuted']
05/17/2022 04:26:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:26:13 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:26:13 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 04:26:13 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:26:13 - INFO - __main__ - Printing 3 examples
05/17/2022 04:26:13 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/17/2022 04:26:13 - INFO - __main__ - ['refuted']
05/17/2022 04:26:13 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/17/2022 04:26:13 - INFO - __main__ - ['refuted']
05/17/2022 04:26:13 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/17/2022 04:26:13 - INFO - __main__ - ['refuted']
05/17/2022 04:26:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:26:13 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:26:13 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 04:26:19 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 04:26:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 04:26:19 - INFO - __main__ - Starting training!
05/17/2022 04:26:23 - INFO - __main__ - Step 10 Global step 10 Train loss 4.93 on epoch=4
05/17/2022 04:26:24 - INFO - __main__ - Step 20 Global step 20 Train loss 4.93 on epoch=9
05/17/2022 04:26:26 - INFO - __main__ - Step 30 Global step 30 Train loss 4.94 on epoch=14
05/17/2022 04:26:28 - INFO - __main__ - Step 40 Global step 40 Train loss 4.89 on epoch=19
05/17/2022 04:26:30 - INFO - __main__ - Step 50 Global step 50 Train loss 4.86 on epoch=24
05/17/2022 04:26:32 - INFO - __main__ - Global step 50 Train loss 4.91 Classification-F1 0.0 on epoch=24
05/17/2022 04:26:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 04:26:34 - INFO - __main__ - Step 60 Global step 60 Train loss 4.71 on epoch=29
05/17/2022 04:26:35 - INFO - __main__ - Step 70 Global step 70 Train loss 4.81 on epoch=34
05/17/2022 04:26:37 - INFO - __main__ - Step 80 Global step 80 Train loss 4.81 on epoch=39
05/17/2022 04:26:39 - INFO - __main__ - Step 90 Global step 90 Train loss 4.82 on epoch=44
05/17/2022 04:26:41 - INFO - __main__ - Step 100 Global step 100 Train loss 4.63 on epoch=49
05/17/2022 04:26:42 - INFO - __main__ - Global step 100 Train loss 4.76 Classification-F1 0.0 on epoch=49
05/17/2022 04:26:44 - INFO - __main__ - Step 110 Global step 110 Train loss 4.68 on epoch=54
05/17/2022 04:26:46 - INFO - __main__ - Step 120 Global step 120 Train loss 4.58 on epoch=59
05/17/2022 04:26:48 - INFO - __main__ - Step 130 Global step 130 Train loss 4.53 on epoch=64
05/17/2022 04:26:50 - INFO - __main__ - Step 140 Global step 140 Train loss 4.56 on epoch=69
05/17/2022 04:26:51 - INFO - __main__ - Step 150 Global step 150 Train loss 4.47 on epoch=74
05/17/2022 04:26:54 - INFO - __main__ - Global step 150 Train loss 4.57 Classification-F1 0.0 on epoch=74
05/17/2022 04:26:55 - INFO - __main__ - Step 160 Global step 160 Train loss 4.39 on epoch=79
05/17/2022 04:26:57 - INFO - __main__ - Step 170 Global step 170 Train loss 4.40 on epoch=84
05/17/2022 04:26:59 - INFO - __main__ - Step 180 Global step 180 Train loss 4.32 on epoch=89
05/17/2022 04:27:01 - INFO - __main__ - Step 190 Global step 190 Train loss 4.13 on epoch=94
05/17/2022 04:27:03 - INFO - __main__ - Step 200 Global step 200 Train loss 4.18 on epoch=99
05/17/2022 04:27:05 - INFO - __main__ - Global step 200 Train loss 4.28 Classification-F1 0.0 on epoch=99
05/17/2022 04:27:07 - INFO - __main__ - Step 210 Global step 210 Train loss 4.16 on epoch=104
05/17/2022 04:27:09 - INFO - __main__ - Step 220 Global step 220 Train loss 4.06 on epoch=109
05/17/2022 04:27:11 - INFO - __main__ - Step 230 Global step 230 Train loss 3.93 on epoch=114
05/17/2022 04:27:13 - INFO - __main__ - Step 240 Global step 240 Train loss 4.01 on epoch=119
05/17/2022 04:27:15 - INFO - __main__ - Step 250 Global step 250 Train loss 3.95 on epoch=124
05/17/2022 04:27:17 - INFO - __main__ - Global step 250 Train loss 4.02 Classification-F1 0.0 on epoch=124
05/17/2022 04:27:19 - INFO - __main__ - Step 260 Global step 260 Train loss 3.93 on epoch=129
05/17/2022 04:27:21 - INFO - __main__ - Step 270 Global step 270 Train loss 3.83 on epoch=134
05/17/2022 04:27:23 - INFO - __main__ - Step 280 Global step 280 Train loss 3.80 on epoch=139
05/17/2022 04:27:25 - INFO - __main__ - Step 290 Global step 290 Train loss 3.77 on epoch=144
05/17/2022 04:27:27 - INFO - __main__ - Step 300 Global step 300 Train loss 3.70 on epoch=149
05/17/2022 04:27:28 - INFO - __main__ - Global step 300 Train loss 3.81 Classification-F1 0.0 on epoch=149
05/17/2022 04:27:30 - INFO - __main__ - Step 310 Global step 310 Train loss 3.56 on epoch=154
05/17/2022 04:27:32 - INFO - __main__ - Step 320 Global step 320 Train loss 3.63 on epoch=159
05/17/2022 04:27:34 - INFO - __main__ - Step 330 Global step 330 Train loss 3.55 on epoch=164
05/17/2022 04:27:36 - INFO - __main__ - Step 340 Global step 340 Train loss 3.53 on epoch=169
05/17/2022 04:27:38 - INFO - __main__ - Step 350 Global step 350 Train loss 3.49 on epoch=174
05/17/2022 04:27:39 - INFO - __main__ - Global step 350 Train loss 3.55 Classification-F1 0.08637873754152824 on epoch=174
05/17/2022 04:27:39 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.08637873754152824 on epoch=174, global_step=350
05/17/2022 04:27:41 - INFO - __main__ - Step 360 Global step 360 Train loss 3.48 on epoch=179
05/17/2022 04:27:43 - INFO - __main__ - Step 370 Global step 370 Train loss 3.44 on epoch=184
05/17/2022 04:27:45 - INFO - __main__ - Step 380 Global step 380 Train loss 3.40 on epoch=189
05/17/2022 04:27:46 - INFO - __main__ - Step 390 Global step 390 Train loss 3.21 on epoch=194
05/17/2022 04:27:48 - INFO - __main__ - Step 400 Global step 400 Train loss 3.29 on epoch=199
05/17/2022 04:27:50 - INFO - __main__ - Global step 400 Train loss 3.36 Classification-F1 0.21276595744680848 on epoch=199
05/17/2022 04:27:50 - INFO - __main__ - Saving model with best Classification-F1: 0.08637873754152824 -> 0.21276595744680848 on epoch=199, global_step=400
05/17/2022 04:27:51 - INFO - __main__ - Step 410 Global step 410 Train loss 3.18 on epoch=204
05/17/2022 04:27:53 - INFO - __main__ - Step 420 Global step 420 Train loss 3.16 on epoch=209
05/17/2022 04:27:55 - INFO - __main__ - Step 430 Global step 430 Train loss 3.07 on epoch=214
05/17/2022 04:27:57 - INFO - __main__ - Step 440 Global step 440 Train loss 3.14 on epoch=219
05/17/2022 04:27:59 - INFO - __main__ - Step 450 Global step 450 Train loss 2.96 on epoch=224
05/17/2022 04:28:02 - INFO - __main__ - Global step 450 Train loss 3.10 Classification-F1 0.1590909090909091 on epoch=224
05/17/2022 04:28:04 - INFO - __main__ - Step 460 Global step 460 Train loss 2.87 on epoch=229
05/17/2022 04:28:06 - INFO - __main__ - Step 470 Global step 470 Train loss 3.00 on epoch=234
05/17/2022 04:28:08 - INFO - __main__ - Step 480 Global step 480 Train loss 2.81 on epoch=239
05/17/2022 04:28:10 - INFO - __main__ - Step 490 Global step 490 Train loss 2.82 on epoch=244
05/17/2022 04:28:12 - INFO - __main__ - Step 500 Global step 500 Train loss 2.80 on epoch=249
05/17/2022 04:28:15 - INFO - __main__ - Global step 500 Train loss 2.86 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 04:28:15 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.3333333333333333 on epoch=249, global_step=500
05/17/2022 04:28:17 - INFO - __main__ - Step 510 Global step 510 Train loss 2.76 on epoch=254
05/17/2022 04:28:18 - INFO - __main__ - Step 520 Global step 520 Train loss 2.67 on epoch=259
05/17/2022 04:28:20 - INFO - __main__ - Step 530 Global step 530 Train loss 2.52 on epoch=264
05/17/2022 04:28:22 - INFO - __main__ - Step 540 Global step 540 Train loss 2.53 on epoch=269
05/17/2022 04:28:24 - INFO - __main__ - Step 550 Global step 550 Train loss 2.52 on epoch=274
05/17/2022 04:28:28 - INFO - __main__ - Global step 550 Train loss 2.60 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 04:28:29 - INFO - __main__ - Step 560 Global step 560 Train loss 2.54 on epoch=279
05/17/2022 04:28:31 - INFO - __main__ - Step 570 Global step 570 Train loss 2.45 on epoch=284
05/17/2022 04:28:33 - INFO - __main__ - Step 580 Global step 580 Train loss 2.34 on epoch=289
05/17/2022 04:28:35 - INFO - __main__ - Step 590 Global step 590 Train loss 2.43 on epoch=294
05/17/2022 04:28:37 - INFO - __main__ - Step 600 Global step 600 Train loss 2.28 on epoch=299
05/17/2022 04:28:41 - INFO - __main__ - Global step 600 Train loss 2.41 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 04:28:42 - INFO - __main__ - Step 610 Global step 610 Train loss 2.26 on epoch=304
05/17/2022 04:28:44 - INFO - __main__ - Step 620 Global step 620 Train loss 2.27 on epoch=309
05/17/2022 04:28:46 - INFO - __main__ - Step 630 Global step 630 Train loss 2.15 on epoch=314
05/17/2022 04:28:48 - INFO - __main__ - Step 640 Global step 640 Train loss 2.18 on epoch=319
05/17/2022 04:28:50 - INFO - __main__ - Step 650 Global step 650 Train loss 2.09 on epoch=324
05/17/2022 04:28:54 - INFO - __main__ - Global step 650 Train loss 2.19 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 04:28:56 - INFO - __main__ - Step 660 Global step 660 Train loss 2.01 on epoch=329
05/17/2022 04:28:58 - INFO - __main__ - Step 670 Global step 670 Train loss 2.02 on epoch=334
05/17/2022 04:29:00 - INFO - __main__ - Step 680 Global step 680 Train loss 2.06 on epoch=339
05/17/2022 04:29:02 - INFO - __main__ - Step 690 Global step 690 Train loss 2.03 on epoch=344
05/17/2022 04:29:03 - INFO - __main__ - Step 700 Global step 700 Train loss 1.95 on epoch=349
05/17/2022 04:29:07 - INFO - __main__ - Global step 700 Train loss 2.02 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 04:29:09 - INFO - __main__ - Step 710 Global step 710 Train loss 1.86 on epoch=354
05/17/2022 04:29:11 - INFO - __main__ - Step 720 Global step 720 Train loss 1.92 on epoch=359
05/17/2022 04:29:13 - INFO - __main__ - Step 730 Global step 730 Train loss 1.83 on epoch=364
05/17/2022 04:29:15 - INFO - __main__ - Step 740 Global step 740 Train loss 1.80 on epoch=369
05/17/2022 04:29:17 - INFO - __main__ - Step 750 Global step 750 Train loss 1.82 on epoch=374
05/17/2022 04:29:20 - INFO - __main__ - Global step 750 Train loss 1.85 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 04:29:22 - INFO - __main__ - Step 760 Global step 760 Train loss 1.65 on epoch=379
05/17/2022 04:29:24 - INFO - __main__ - Step 770 Global step 770 Train loss 1.83 on epoch=384
05/17/2022 04:29:26 - INFO - __main__ - Step 780 Global step 780 Train loss 1.74 on epoch=389
05/17/2022 04:29:28 - INFO - __main__ - Step 790 Global step 790 Train loss 1.65 on epoch=394
05/17/2022 04:29:30 - INFO - __main__ - Step 800 Global step 800 Train loss 1.57 on epoch=399
05/17/2022 04:29:33 - INFO - __main__ - Global step 800 Train loss 1.69 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 04:29:35 - INFO - __main__ - Step 810 Global step 810 Train loss 1.61 on epoch=404
05/17/2022 04:29:37 - INFO - __main__ - Step 820 Global step 820 Train loss 1.57 on epoch=409
05/17/2022 04:29:39 - INFO - __main__ - Step 830 Global step 830 Train loss 1.54 on epoch=414
05/17/2022 04:29:41 - INFO - __main__ - Step 840 Global step 840 Train loss 1.48 on epoch=419
05/17/2022 04:29:42 - INFO - __main__ - Step 850 Global step 850 Train loss 1.46 on epoch=424
05/17/2022 04:29:45 - INFO - __main__ - Global step 850 Train loss 1.53 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 04:29:48 - INFO - __main__ - Step 860 Global step 860 Train loss 1.38 on epoch=429
05/17/2022 04:29:49 - INFO - __main__ - Step 870 Global step 870 Train loss 1.30 on epoch=434
05/17/2022 04:29:51 - INFO - __main__ - Step 880 Global step 880 Train loss 1.33 on epoch=439
05/17/2022 04:29:53 - INFO - __main__ - Step 890 Global step 890 Train loss 1.44 on epoch=444
05/17/2022 04:29:55 - INFO - __main__ - Step 900 Global step 900 Train loss 1.25 on epoch=449
05/17/2022 04:29:58 - INFO - __main__ - Global step 900 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 04:30:00 - INFO - __main__ - Step 910 Global step 910 Train loss 1.29 on epoch=454
05/17/2022 04:30:02 - INFO - __main__ - Step 920 Global step 920 Train loss 1.28 on epoch=459
05/17/2022 04:30:04 - INFO - __main__ - Step 930 Global step 930 Train loss 1.16 on epoch=464
05/17/2022 04:30:06 - INFO - __main__ - Step 940 Global step 940 Train loss 1.19 on epoch=469
05/17/2022 04:30:08 - INFO - __main__ - Step 950 Global step 950 Train loss 1.10 on epoch=474
05/17/2022 04:30:10 - INFO - __main__ - Global step 950 Train loss 1.20 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 04:30:12 - INFO - __main__ - Step 960 Global step 960 Train loss 1.16 on epoch=479
05/17/2022 04:30:14 - INFO - __main__ - Step 970 Global step 970 Train loss 1.20 on epoch=484
05/17/2022 04:30:16 - INFO - __main__ - Step 980 Global step 980 Train loss 1.08 on epoch=489
05/17/2022 04:30:18 - INFO - __main__ - Step 990 Global step 990 Train loss 1.10 on epoch=494
05/17/2022 04:30:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.14 on epoch=499
05/17/2022 04:30:21 - INFO - __main__ - Global step 1000 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 04:30:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.09 on epoch=504
05/17/2022 04:30:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.15 on epoch=509
05/17/2022 04:30:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.12 on epoch=514
05/17/2022 04:30:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.03 on epoch=519
05/17/2022 04:30:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.01 on epoch=524
05/17/2022 04:30:31 - INFO - __main__ - Global step 1050 Train loss 1.08 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 04:30:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.99 on epoch=529
05/17/2022 04:30:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.09 on epoch=534
05/17/2022 04:30:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.92 on epoch=539
05/17/2022 04:30:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.11 on epoch=544
05/17/2022 04:30:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.02 on epoch=549
05/17/2022 04:30:41 - INFO - __main__ - Global step 1100 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 04:30:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.88 on epoch=554
05/17/2022 04:30:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.02 on epoch=559
05/17/2022 04:30:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.96 on epoch=564
05/17/2022 04:30:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.90 on epoch=569
05/17/2022 04:30:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.86 on epoch=574
05/17/2022 04:30:52 - INFO - __main__ - Global step 1150 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 04:30:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.92 on epoch=579
05/17/2022 04:30:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=584
05/17/2022 04:30:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.94 on epoch=589
05/17/2022 04:30:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.89 on epoch=594
05/17/2022 04:31:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.85 on epoch=599
05/17/2022 04:31:02 - INFO - __main__ - Global step 1200 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 04:31:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.88 on epoch=604
05/17/2022 04:31:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.76 on epoch=609
05/17/2022 04:31:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.77 on epoch=614
05/17/2022 04:31:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=619
05/17/2022 04:31:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.84 on epoch=624
05/17/2022 04:31:12 - INFO - __main__ - Global step 1250 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 04:31:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.81 on epoch=629
05/17/2022 04:31:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.80 on epoch=634
05/17/2022 04:31:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.71 on epoch=639
05/17/2022 04:31:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.78 on epoch=644
05/17/2022 04:31:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.85 on epoch=649
05/17/2022 04:31:23 - INFO - __main__ - Global step 1300 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 04:31:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=654
05/17/2022 04:31:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.73 on epoch=659
05/17/2022 04:31:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.71 on epoch=664
05/17/2022 04:31:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=669
05/17/2022 04:31:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.73 on epoch=674
05/17/2022 04:31:33 - INFO - __main__ - Global step 1350 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 04:31:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.76 on epoch=679
05/17/2022 04:31:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.77 on epoch=684
05/17/2022 04:31:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.72 on epoch=689
05/17/2022 04:31:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.67 on epoch=694
05/17/2022 04:31:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.69 on epoch=699
05/17/2022 04:31:43 - INFO - __main__ - Global step 1400 Train loss 0.72 Classification-F1 0.3191489361702127 on epoch=699
05/17/2022 04:31:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.68 on epoch=704
05/17/2022 04:31:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.72 on epoch=709
05/17/2022 04:31:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.76 on epoch=714
05/17/2022 04:31:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.63 on epoch=719
05/17/2022 04:31:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.59 on epoch=724
05/17/2022 04:31:54 - INFO - __main__ - Global step 1450 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 04:31:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.73 on epoch=729
05/17/2022 04:31:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.63 on epoch=734
05/17/2022 04:31:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.69 on epoch=739
05/17/2022 04:32:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.65 on epoch=744
05/17/2022 04:32:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.63 on epoch=749
05/17/2022 04:32:04 - INFO - __main__ - Global step 1500 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 04:32:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.72 on epoch=754
05/17/2022 04:32:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.64 on epoch=759
05/17/2022 04:32:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.62 on epoch=764
05/17/2022 04:32:12 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=769
05/17/2022 04:32:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.53 on epoch=774
05/17/2022 04:32:14 - INFO - __main__ - Global step 1550 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 04:32:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=779
05/17/2022 04:32:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
05/17/2022 04:32:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.69 on epoch=789
05/17/2022 04:32:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.58 on epoch=794
05/17/2022 04:32:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.54 on epoch=799
05/17/2022 04:32:25 - INFO - __main__ - Global step 1600 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 04:32:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.59 on epoch=804
05/17/2022 04:32:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.55 on epoch=809
05/17/2022 04:32:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=814
05/17/2022 04:32:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.59 on epoch=819
05/17/2022 04:32:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.57 on epoch=824
05/17/2022 04:32:35 - INFO - __main__ - Global step 1650 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 04:32:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.53 on epoch=829
05/17/2022 04:32:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.61 on epoch=834
05/17/2022 04:32:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.59 on epoch=839
05/17/2022 04:32:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
05/17/2022 04:32:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.59 on epoch=849
05/17/2022 04:32:45 - INFO - __main__ - Global step 1700 Train loss 0.56 Classification-F1 0.3043478260869565 on epoch=849
05/17/2022 04:32:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.58 on epoch=854
05/17/2022 04:32:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.53 on epoch=859
05/17/2022 04:32:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.52 on epoch=864
05/17/2022 04:32:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=869
05/17/2022 04:32:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.53 on epoch=874
05/17/2022 04:32:56 - INFO - __main__ - Global step 1750 Train loss 0.52 Classification-F1 0.39139139139139134 on epoch=874
05/17/2022 04:32:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39139139139139134 on epoch=874, global_step=1750
05/17/2022 04:32:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.51 on epoch=879
05/17/2022 04:33:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.51 on epoch=884
05/17/2022 04:33:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.57 on epoch=889
05/17/2022 04:33:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.55 on epoch=894
05/17/2022 04:33:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
05/17/2022 04:33:06 - INFO - __main__ - Global step 1800 Train loss 0.51 Classification-F1 0.3043478260869565 on epoch=899
05/17/2022 04:33:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=904
05/17/2022 04:33:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.47 on epoch=909
05/17/2022 04:33:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.55 on epoch=914
05/17/2022 04:33:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=919
05/17/2022 04:33:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.62 on epoch=924
05/17/2022 04:33:17 - INFO - __main__ - Global step 1850 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 04:33:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
05/17/2022 04:33:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.53 on epoch=934
05/17/2022 04:33:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.53 on epoch=939
05/17/2022 04:33:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.48 on epoch=944
05/17/2022 04:33:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.58 on epoch=949
05/17/2022 04:33:27 - INFO - __main__ - Global step 1900 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 04:33:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=954
05/17/2022 04:33:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=959
05/17/2022 04:33:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=964
05/17/2022 04:33:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=969
05/17/2022 04:33:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=974
05/17/2022 04:33:37 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 04:33:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=979
05/17/2022 04:33:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.56 on epoch=984
05/17/2022 04:33:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.49 on epoch=989
05/17/2022 04:33:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=994
05/17/2022 04:33:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.50 on epoch=999
05/17/2022 04:33:47 - INFO - __main__ - Global step 2000 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 04:33:47 - INFO - __main__ - save last model!
05/17/2022 04:33:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 04:33:48 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 04:33:48 - INFO - __main__ - Printing 3 examples
05/17/2022 04:33:48 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:33:48 - INFO - __main__ - ['entailed']
05/17/2022 04:33:48 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:33:48 - INFO - __main__ - ['entailed']
05/17/2022 04:33:48 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:33:48 - INFO - __main__ - ['entailed']
05/17/2022 04:33:48 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:33:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:33:49 - INFO - __main__ - Printing 3 examples
05/17/2022 04:33:49 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/17/2022 04:33:49 - INFO - __main__ - ['entailed']
05/17/2022 04:33:49 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/17/2022 04:33:49 - INFO - __main__ - ['entailed']
05/17/2022 04:33:49 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/17/2022 04:33:49 - INFO - __main__ - ['entailed']
05/17/2022 04:33:49 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:33:49 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:33:49 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 04:33:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:33:49 - INFO - __main__ - Printing 3 examples
05/17/2022 04:33:49 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/17/2022 04:33:49 - INFO - __main__ - ['entailed']
05/17/2022 04:33:49 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/17/2022 04:33:49 - INFO - __main__ - ['entailed']
05/17/2022 04:33:49 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/17/2022 04:33:49 - INFO - __main__ - ['entailed']
05/17/2022 04:33:49 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:33:49 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:33:49 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 04:33:55 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 04:33:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 04:33:55 - INFO - __main__ - Starting training!
05/17/2022 04:34:11 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:34:24 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 04:38:42 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.2_8_predictions.txt
05/17/2022 04:38:42 - INFO - __main__ - Classification-F1 on test data: 0.3373
05/17/2022 04:38:42 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.2, bsz=8, dev_performance=0.39139139139139134, test_performance=0.3373174086208301
05/17/2022 04:38:42 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.5, bsz=8 ...
05/17/2022 04:38:43 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:38:43 - INFO - __main__ - Printing 3 examples
05/17/2022 04:38:43 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/17/2022 04:38:43 - INFO - __main__ - ['entailed']
05/17/2022 04:38:43 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/17/2022 04:38:43 - INFO - __main__ - ['entailed']
05/17/2022 04:38:43 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/17/2022 04:38:43 - INFO - __main__ - ['entailed']
05/17/2022 04:38:43 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:38:43 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:38:43 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 04:38:43 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:38:43 - INFO - __main__ - Printing 3 examples
05/17/2022 04:38:43 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/17/2022 04:38:43 - INFO - __main__ - ['entailed']
05/17/2022 04:38:43 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/17/2022 04:38:43 - INFO - __main__ - ['entailed']
05/17/2022 04:38:43 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/17/2022 04:38:43 - INFO - __main__ - ['entailed']
05/17/2022 04:38:43 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:38:43 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:38:43 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 04:38:49 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 04:38:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 04:38:50 - INFO - __main__ - Starting training!
05/17/2022 04:38:52 - INFO - __main__ - Step 10 Global step 10 Train loss 5.10 on epoch=4
05/17/2022 04:38:54 - INFO - __main__ - Step 20 Global step 20 Train loss 4.90 on epoch=9
05/17/2022 04:38:56 - INFO - __main__ - Step 30 Global step 30 Train loss 4.87 on epoch=14
05/17/2022 04:38:58 - INFO - __main__ - Step 40 Global step 40 Train loss 4.81 on epoch=19
05/17/2022 04:39:00 - INFO - __main__ - Step 50 Global step 50 Train loss 4.67 on epoch=24
05/17/2022 04:39:07 - INFO - __main__ - Global step 50 Train loss 4.87 Classification-F1 0.0 on epoch=24
05/17/2022 04:39:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 04:39:09 - INFO - __main__ - Step 60 Global step 60 Train loss 4.56 on epoch=29
05/17/2022 04:39:11 - INFO - __main__ - Step 70 Global step 70 Train loss 4.45 on epoch=34
05/17/2022 04:39:13 - INFO - __main__ - Step 80 Global step 80 Train loss 4.29 on epoch=39
05/17/2022 04:39:15 - INFO - __main__ - Step 90 Global step 90 Train loss 4.23 on epoch=44
05/17/2022 04:39:17 - INFO - __main__ - Step 100 Global step 100 Train loss 4.07 on epoch=49
05/17/2022 04:39:25 - INFO - __main__ - Global step 100 Train loss 4.32 Classification-F1 0.0 on epoch=49
05/17/2022 04:39:27 - INFO - __main__ - Step 110 Global step 110 Train loss 3.94 on epoch=54
05/17/2022 04:39:29 - INFO - __main__ - Step 120 Global step 120 Train loss 3.75 on epoch=59
05/17/2022 04:39:31 - INFO - __main__ - Step 130 Global step 130 Train loss 3.72 on epoch=64
05/17/2022 04:39:33 - INFO - __main__ - Step 140 Global step 140 Train loss 3.47 on epoch=69
05/17/2022 04:39:35 - INFO - __main__ - Step 150 Global step 150 Train loss 3.33 on epoch=74
05/17/2022 04:39:39 - INFO - __main__ - Global step 150 Train loss 3.64 Classification-F1 0.3333333333333333 on epoch=74
05/17/2022 04:39:40 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=74, global_step=150
05/17/2022 04:39:41 - INFO - __main__ - Step 160 Global step 160 Train loss 3.34 on epoch=79
05/17/2022 04:39:43 - INFO - __main__ - Step 170 Global step 170 Train loss 3.11 on epoch=84
05/17/2022 04:39:45 - INFO - __main__ - Step 180 Global step 180 Train loss 2.85 on epoch=89
05/17/2022 04:39:47 - INFO - __main__ - Step 190 Global step 190 Train loss 2.89 on epoch=94
05/17/2022 04:39:49 - INFO - __main__ - Step 200 Global step 200 Train loss 2.64 on epoch=99
05/17/2022 04:39:53 - INFO - __main__ - Global step 200 Train loss 2.97 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 04:39:55 - INFO - __main__ - Step 210 Global step 210 Train loss 2.48 on epoch=104
05/17/2022 04:39:57 - INFO - __main__ - Step 220 Global step 220 Train loss 2.30 on epoch=109
05/17/2022 04:39:59 - INFO - __main__ - Step 230 Global step 230 Train loss 2.37 on epoch=114
05/17/2022 04:40:01 - INFO - __main__ - Step 240 Global step 240 Train loss 2.18 on epoch=119
05/17/2022 04:40:03 - INFO - __main__ - Step 250 Global step 250 Train loss 2.12 on epoch=124
05/17/2022 04:40:07 - INFO - __main__ - Global step 250 Train loss 2.29 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 04:40:09 - INFO - __main__ - Step 260 Global step 260 Train loss 2.00 on epoch=129
05/17/2022 04:40:11 - INFO - __main__ - Step 270 Global step 270 Train loss 1.89 on epoch=134
05/17/2022 04:40:13 - INFO - __main__ - Step 280 Global step 280 Train loss 1.66 on epoch=139
05/17/2022 04:40:15 - INFO - __main__ - Step 290 Global step 290 Train loss 1.62 on epoch=144
05/17/2022 04:40:17 - INFO - __main__ - Step 300 Global step 300 Train loss 1.49 on epoch=149
05/17/2022 04:40:23 - INFO - __main__ - Global step 300 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 04:40:25 - INFO - __main__ - Step 310 Global step 310 Train loss 1.41 on epoch=154
05/17/2022 04:40:27 - INFO - __main__ - Step 320 Global step 320 Train loss 1.34 on epoch=159
05/17/2022 04:40:29 - INFO - __main__ - Step 330 Global step 330 Train loss 1.17 on epoch=164
05/17/2022 04:40:31 - INFO - __main__ - Step 340 Global step 340 Train loss 1.19 on epoch=169
05/17/2022 04:40:33 - INFO - __main__ - Step 350 Global step 350 Train loss 1.17 on epoch=174
05/17/2022 04:40:34 - INFO - __main__ - Global step 350 Train loss 1.26 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 04:40:36 - INFO - __main__ - Step 360 Global step 360 Train loss 1.23 on epoch=179
05/17/2022 04:40:38 - INFO - __main__ - Step 370 Global step 370 Train loss 1.04 on epoch=184
05/17/2022 04:40:40 - INFO - __main__ - Step 380 Global step 380 Train loss 1.10 on epoch=189
05/17/2022 04:40:42 - INFO - __main__ - Step 390 Global step 390 Train loss 1.01 on epoch=194
05/17/2022 04:40:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.95 on epoch=199
05/17/2022 04:40:45 - INFO - __main__ - Global step 400 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 04:40:47 - INFO - __main__ - Step 410 Global step 410 Train loss 1.10 on epoch=204
05/17/2022 04:40:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.89 on epoch=209
05/17/2022 04:40:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.91 on epoch=214
05/17/2022 04:40:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.87 on epoch=219
05/17/2022 04:40:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=224
05/17/2022 04:40:56 - INFO - __main__ - Global step 450 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 04:40:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.92 on epoch=229
05/17/2022 04:41:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.89 on epoch=234
05/17/2022 04:41:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=239
05/17/2022 04:41:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.68 on epoch=244
05/17/2022 04:41:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.74 on epoch=249
05/17/2022 04:41:07 - INFO - __main__ - Global step 500 Train loss 0.79 Classification-F1 0.3191489361702127 on epoch=249
05/17/2022 04:41:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.64 on epoch=254
05/17/2022 04:41:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.71 on epoch=259
05/17/2022 04:41:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.74 on epoch=264
05/17/2022 04:41:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.61 on epoch=269
05/17/2022 04:41:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.61 on epoch=274
05/17/2022 04:41:19 - INFO - __main__ - Global step 550 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 04:41:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=279
05/17/2022 04:41:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=284
05/17/2022 04:41:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=289
05/17/2022 04:41:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=294
05/17/2022 04:41:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=299
05/17/2022 04:41:30 - INFO - __main__ - Global step 600 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 04:41:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.62 on epoch=304
05/17/2022 04:41:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.55 on epoch=309
05/17/2022 04:41:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=314
05/17/2022 04:41:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=319
05/17/2022 04:41:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=324
05/17/2022 04:41:41 - INFO - __main__ - Global step 650 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 04:41:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=329
05/17/2022 04:41:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.56 on epoch=334
05/17/2022 04:41:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=339
05/17/2022 04:41:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=344
05/17/2022 04:41:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=349
05/17/2022 04:41:51 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.4458874458874459 on epoch=349
05/17/2022 04:41:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4458874458874459 on epoch=349, global_step=700
05/17/2022 04:41:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=354
05/17/2022 04:41:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=359
05/17/2022 04:41:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=364
05/17/2022 04:42:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=369
05/17/2022 04:42:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.51 on epoch=374
05/17/2022 04:42:03 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.4181818181818182 on epoch=374
05/17/2022 04:42:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=379
05/17/2022 04:42:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=384
05/17/2022 04:42:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=389
05/17/2022 04:42:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=394
05/17/2022 04:42:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=399
05/17/2022 04:42:14 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.3816425120772947 on epoch=399
05/17/2022 04:42:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=404
05/17/2022 04:42:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=409
05/17/2022 04:42:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=414
05/17/2022 04:42:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=419
05/17/2022 04:42:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=424
05/17/2022 04:42:25 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 04:42:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=429
05/17/2022 04:42:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=434
05/17/2022 04:42:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=439
05/17/2022 04:42:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=444
05/17/2022 04:42:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=449
05/17/2022 04:42:36 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 04:42:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=454
05/17/2022 04:42:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=459
05/17/2022 04:42:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=464
05/17/2022 04:42:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=469
05/17/2022 04:42:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=474
05/17/2022 04:42:47 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 04:42:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=479
05/17/2022 04:42:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=484
05/17/2022 04:42:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=489
05/17/2022 04:42:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=494
05/17/2022 04:42:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
05/17/2022 04:42:57 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 04:42:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=504
05/17/2022 04:43:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=509
05/17/2022 04:43:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=514
05/17/2022 04:43:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=519
05/17/2022 04:43:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=524
05/17/2022 04:43:08 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.3191489361702127 on epoch=524
05/17/2022 04:43:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=529
05/17/2022 04:43:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=534
05/17/2022 04:43:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=539
05/17/2022 04:43:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=544
05/17/2022 04:43:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=549
05/17/2022 04:43:19 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.3992490613266583 on epoch=549
05/17/2022 04:43:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=554
05/17/2022 04:43:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=559
05/17/2022 04:43:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=564
05/17/2022 04:43:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=569
05/17/2022 04:43:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=574
05/17/2022 04:43:29 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 04:43:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=579
05/17/2022 04:43:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=584
05/17/2022 04:43:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
05/17/2022 04:43:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=594
05/17/2022 04:43:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=599
05/17/2022 04:43:40 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 04:43:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
05/17/2022 04:43:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=609
05/17/2022 04:43:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=614
05/17/2022 04:43:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=619
05/17/2022 04:43:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=624
05/17/2022 04:43:50 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 04:43:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=629
05/17/2022 04:43:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=634
05/17/2022 04:43:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=639
05/17/2022 04:43:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=644
05/17/2022 04:44:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=649
05/17/2022 04:44:01 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 04:44:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=654
05/17/2022 04:44:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=659
05/17/2022 04:44:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=664
05/17/2022 04:44:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.28 on epoch=669
05/17/2022 04:44:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=674
05/17/2022 04:44:12 - INFO - __main__ - Global step 1350 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 04:44:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=679
05/17/2022 04:44:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=684
05/17/2022 04:44:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=689
05/17/2022 04:44:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.32 on epoch=694
05/17/2022 04:44:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=699
05/17/2022 04:44:23 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 04:44:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=704
05/17/2022 04:44:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=709
05/17/2022 04:44:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=714
05/17/2022 04:44:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.26 on epoch=719
05/17/2022 04:44:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=724
05/17/2022 04:44:34 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 04:44:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
05/17/2022 04:44:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=734
05/17/2022 04:44:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=739
05/17/2022 04:44:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=744
05/17/2022 04:44:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=749
05/17/2022 04:44:44 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 04:44:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=754
05/17/2022 04:44:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=759
05/17/2022 04:44:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=764
05/17/2022 04:44:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=769
05/17/2022 04:44:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=774
05/17/2022 04:44:55 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 04:44:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=779
05/17/2022 04:44:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=784
05/17/2022 04:45:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=789
05/17/2022 04:45:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=794
05/17/2022 04:45:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=799
05/17/2022 04:45:06 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 04:45:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=804
05/17/2022 04:45:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=809
05/17/2022 04:45:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=814
05/17/2022 04:45:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=819
05/17/2022 04:45:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=824
05/17/2022 04:45:17 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 04:45:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=829
05/17/2022 04:45:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=834
05/17/2022 04:45:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.29 on epoch=839
05/17/2022 04:45:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=844
05/17/2022 04:45:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.27 on epoch=849
05/17/2022 04:45:28 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 04:45:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=854
05/17/2022 04:45:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=859
05/17/2022 04:45:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/17/2022 04:45:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=869
05/17/2022 04:45:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
05/17/2022 04:45:39 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 04:45:41 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.31 on epoch=879
05/17/2022 04:45:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=884
05/17/2022 04:45:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
05/17/2022 04:45:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.30 on epoch=894
05/17/2022 04:45:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=899
05/17/2022 04:45:49 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 04:45:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=904
05/17/2022 04:45:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=909
05/17/2022 04:45:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=914
05/17/2022 04:45:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.29 on epoch=919
05/17/2022 04:46:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
05/17/2022 04:46:00 - INFO - __main__ - Global step 1850 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 04:46:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.31 on epoch=929
05/17/2022 04:46:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=934
05/17/2022 04:46:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=939
05/17/2022 04:46:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=944
05/17/2022 04:46:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.29 on epoch=949
05/17/2022 04:46:11 - INFO - __main__ - Global step 1900 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 04:46:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=954
05/17/2022 04:46:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=959
05/17/2022 04:46:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=964
05/17/2022 04:46:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=969
05/17/2022 04:46:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=974
05/17/2022 04:46:22 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 04:46:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=979
05/17/2022 04:46:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=984
05/17/2022 04:46:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/17/2022 04:46:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=994
05/17/2022 04:46:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=999
05/17/2022 04:46:33 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 04:46:33 - INFO - __main__ - save last model!
05/17/2022 04:46:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 04:46:33 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 04:46:33 - INFO - __main__ - Printing 3 examples
05/17/2022 04:46:33 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:46:33 - INFO - __main__ - ['entailed']
05/17/2022 04:46:33 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:46:33 - INFO - __main__ - ['entailed']
05/17/2022 04:46:33 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:46:33 - INFO - __main__ - ['entailed']
05/17/2022 04:46:33 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:46:33 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:46:33 - INFO - __main__ - Printing 3 examples
05/17/2022 04:46:33 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/17/2022 04:46:33 - INFO - __main__ - ['entailed']
05/17/2022 04:46:33 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/17/2022 04:46:33 - INFO - __main__ - ['entailed']
05/17/2022 04:46:33 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/17/2022 04:46:33 - INFO - __main__ - ['entailed']
05/17/2022 04:46:33 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:46:33 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:46:33 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 04:46:33 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:46:33 - INFO - __main__ - Printing 3 examples
05/17/2022 04:46:33 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/17/2022 04:46:33 - INFO - __main__ - ['entailed']
05/17/2022 04:46:33 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/17/2022 04:46:33 - INFO - __main__ - ['entailed']
05/17/2022 04:46:33 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/17/2022 04:46:33 - INFO - __main__ - ['entailed']
05/17/2022 04:46:33 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:46:33 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:46:33 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 04:46:40 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 04:46:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 04:46:40 - INFO - __main__ - Starting training!
05/17/2022 04:46:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:47:11 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 04:51:26 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.5_8_predictions.txt
05/17/2022 04:51:26 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 04:51:26 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.5, bsz=8, dev_performance=0.4458874458874459, test_performance=0.33047210300429186
05/17/2022 04:51:26 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.4, bsz=8 ...
05/17/2022 04:51:27 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:51:27 - INFO - __main__ - Printing 3 examples
05/17/2022 04:51:27 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/17/2022 04:51:27 - INFO - __main__ - ['entailed']
05/17/2022 04:51:27 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/17/2022 04:51:27 - INFO - __main__ - ['entailed']
05/17/2022 04:51:27 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/17/2022 04:51:27 - INFO - __main__ - ['entailed']
05/17/2022 04:51:27 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:51:27 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:51:27 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 04:51:27 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:51:27 - INFO - __main__ - Printing 3 examples
05/17/2022 04:51:27 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/17/2022 04:51:27 - INFO - __main__ - ['entailed']
05/17/2022 04:51:27 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/17/2022 04:51:27 - INFO - __main__ - ['entailed']
05/17/2022 04:51:27 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/17/2022 04:51:27 - INFO - __main__ - ['entailed']
05/17/2022 04:51:27 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:51:27 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:51:27 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 04:51:33 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 04:51:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 04:51:33 - INFO - __main__ - Starting training!
05/17/2022 04:51:35 - INFO - __main__ - Step 10 Global step 10 Train loss 4.97 on epoch=4
05/17/2022 04:51:37 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/17/2022 04:51:39 - INFO - __main__ - Step 30 Global step 30 Train loss 4.80 on epoch=14
05/17/2022 04:51:41 - INFO - __main__ - Step 40 Global step 40 Train loss 4.77 on epoch=19
05/17/2022 04:51:43 - INFO - __main__ - Step 50 Global step 50 Train loss 4.72 on epoch=24
05/17/2022 04:51:44 - INFO - __main__ - Global step 50 Train loss 4.86 Classification-F1 0.0 on epoch=24
05/17/2022 04:51:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 04:51:46 - INFO - __main__ - Step 60 Global step 60 Train loss 4.71 on epoch=29
05/17/2022 04:51:48 - INFO - __main__ - Step 70 Global step 70 Train loss 4.70 on epoch=34
05/17/2022 04:51:50 - INFO - __main__ - Step 80 Global step 80 Train loss 4.60 on epoch=39
05/17/2022 04:51:52 - INFO - __main__ - Step 90 Global step 90 Train loss 4.56 on epoch=44
05/17/2022 04:51:54 - INFO - __main__ - Step 100 Global step 100 Train loss 4.38 on epoch=49
05/17/2022 04:51:55 - INFO - __main__ - Global step 100 Train loss 4.59 Classification-F1 0.0 on epoch=49
05/17/2022 04:51:57 - INFO - __main__ - Step 110 Global step 110 Train loss 4.34 on epoch=54
05/17/2022 04:51:59 - INFO - __main__ - Step 120 Global step 120 Train loss 4.35 on epoch=59
05/17/2022 04:52:01 - INFO - __main__ - Step 130 Global step 130 Train loss 4.22 on epoch=64
05/17/2022 04:52:03 - INFO - __main__ - Step 140 Global step 140 Train loss 4.29 on epoch=69
05/17/2022 04:52:05 - INFO - __main__ - Step 150 Global step 150 Train loss 4.15 on epoch=74
05/17/2022 04:52:07 - INFO - __main__ - Global step 150 Train loss 4.27 Classification-F1 0.0 on epoch=74
05/17/2022 04:52:09 - INFO - __main__ - Step 160 Global step 160 Train loss 4.11 on epoch=79
05/17/2022 04:52:11 - INFO - __main__ - Step 170 Global step 170 Train loss 3.98 on epoch=84
05/17/2022 04:52:13 - INFO - __main__ - Step 180 Global step 180 Train loss 3.89 on epoch=89
05/17/2022 04:52:15 - INFO - __main__ - Step 190 Global step 190 Train loss 3.76 on epoch=94
05/17/2022 04:52:17 - INFO - __main__ - Step 200 Global step 200 Train loss 3.77 on epoch=99
05/17/2022 04:52:25 - INFO - __main__ - Global step 200 Train loss 3.90 Classification-F1 0.0058823529411764705 on epoch=99
05/17/2022 04:52:25 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0058823529411764705 on epoch=99, global_step=200
05/17/2022 04:52:27 - INFO - __main__ - Step 210 Global step 210 Train loss 3.70 on epoch=104
05/17/2022 04:52:29 - INFO - __main__ - Step 220 Global step 220 Train loss 3.61 on epoch=109
05/17/2022 04:52:31 - INFO - __main__ - Step 230 Global step 230 Train loss 3.59 on epoch=114
05/17/2022 04:52:33 - INFO - __main__ - Step 240 Global step 240 Train loss 3.50 on epoch=119
05/17/2022 04:52:35 - INFO - __main__ - Step 250 Global step 250 Train loss 3.44 on epoch=124
05/17/2022 04:52:42 - INFO - __main__ - Global step 250 Train loss 3.57 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 04:52:42 - INFO - __main__ - Saving model with best Classification-F1: 0.0058823529411764705 -> 0.3333333333333333 on epoch=124, global_step=250
05/17/2022 04:52:44 - INFO - __main__ - Step 260 Global step 260 Train loss 3.33 on epoch=129
05/17/2022 04:52:46 - INFO - __main__ - Step 270 Global step 270 Train loss 3.18 on epoch=134
05/17/2022 04:52:48 - INFO - __main__ - Step 280 Global step 280 Train loss 3.07 on epoch=139
05/17/2022 04:52:50 - INFO - __main__ - Step 290 Global step 290 Train loss 2.91 on epoch=144
05/17/2022 04:52:51 - INFO - __main__ - Step 300 Global step 300 Train loss 2.78 on epoch=149
05/17/2022 04:52:55 - INFO - __main__ - Global step 300 Train loss 3.05 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 04:52:57 - INFO - __main__ - Step 310 Global step 310 Train loss 2.87 on epoch=154
05/17/2022 04:52:59 - INFO - __main__ - Step 320 Global step 320 Train loss 2.75 on epoch=159
05/17/2022 04:53:01 - INFO - __main__ - Step 330 Global step 330 Train loss 2.62 on epoch=164
05/17/2022 04:53:03 - INFO - __main__ - Step 340 Global step 340 Train loss 2.57 on epoch=169
05/17/2022 04:53:05 - INFO - __main__ - Step 350 Global step 350 Train loss 2.50 on epoch=174
05/17/2022 04:53:08 - INFO - __main__ - Global step 350 Train loss 2.66 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 04:53:10 - INFO - __main__ - Step 360 Global step 360 Train loss 2.47 on epoch=179
05/17/2022 04:53:12 - INFO - __main__ - Step 370 Global step 370 Train loss 2.33 on epoch=184
05/17/2022 04:53:14 - INFO - __main__ - Step 380 Global step 380 Train loss 2.28 on epoch=189
05/17/2022 04:53:16 - INFO - __main__ - Step 390 Global step 390 Train loss 2.15 on epoch=194
05/17/2022 04:53:18 - INFO - __main__ - Step 400 Global step 400 Train loss 2.11 on epoch=199
05/17/2022 04:53:21 - INFO - __main__ - Global step 400 Train loss 2.27 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 04:53:23 - INFO - __main__ - Step 410 Global step 410 Train loss 2.04 on epoch=204
05/17/2022 04:53:25 - INFO - __main__ - Step 420 Global step 420 Train loss 2.01 on epoch=209
05/17/2022 04:53:27 - INFO - __main__ - Step 430 Global step 430 Train loss 1.99 on epoch=214
05/17/2022 04:53:29 - INFO - __main__ - Step 440 Global step 440 Train loss 1.84 on epoch=219
05/17/2022 04:53:31 - INFO - __main__ - Step 450 Global step 450 Train loss 1.71 on epoch=224
05/17/2022 04:53:34 - INFO - __main__ - Global step 450 Train loss 1.91 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 04:53:36 - INFO - __main__ - Step 460 Global step 460 Train loss 1.78 on epoch=229
05/17/2022 04:53:38 - INFO - __main__ - Step 470 Global step 470 Train loss 1.74 on epoch=234
05/17/2022 04:53:40 - INFO - __main__ - Step 480 Global step 480 Train loss 1.74 on epoch=239
05/17/2022 04:53:42 - INFO - __main__ - Step 490 Global step 490 Train loss 1.69 on epoch=244
05/17/2022 04:53:44 - INFO - __main__ - Step 500 Global step 500 Train loss 1.71 on epoch=249
05/17/2022 04:53:50 - INFO - __main__ - Global step 500 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 04:53:52 - INFO - __main__ - Step 510 Global step 510 Train loss 1.66 on epoch=254
05/17/2022 04:53:54 - INFO - __main__ - Step 520 Global step 520 Train loss 1.72 on epoch=259
05/17/2022 04:53:56 - INFO - __main__ - Step 530 Global step 530 Train loss 1.55 on epoch=264
05/17/2022 04:53:58 - INFO - __main__ - Step 540 Global step 540 Train loss 1.54 on epoch=269
05/17/2022 04:54:00 - INFO - __main__ - Step 550 Global step 550 Train loss 1.51 on epoch=274
05/17/2022 04:54:03 - INFO - __main__ - Global step 550 Train loss 1.59 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 04:54:05 - INFO - __main__ - Step 560 Global step 560 Train loss 1.41 on epoch=279
05/17/2022 04:54:07 - INFO - __main__ - Step 570 Global step 570 Train loss 1.49 on epoch=284
05/17/2022 04:54:09 - INFO - __main__ - Step 580 Global step 580 Train loss 1.39 on epoch=289
05/17/2022 04:54:11 - INFO - __main__ - Step 590 Global step 590 Train loss 1.35 on epoch=294
05/17/2022 04:54:13 - INFO - __main__ - Step 600 Global step 600 Train loss 1.29 on epoch=299
05/17/2022 04:54:16 - INFO - __main__ - Global step 600 Train loss 1.39 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 04:54:18 - INFO - __main__ - Step 610 Global step 610 Train loss 1.25 on epoch=304
05/17/2022 04:54:20 - INFO - __main__ - Step 620 Global step 620 Train loss 1.27 on epoch=309
05/17/2022 04:54:22 - INFO - __main__ - Step 630 Global step 630 Train loss 1.24 on epoch=314
05/17/2022 04:54:24 - INFO - __main__ - Step 640 Global step 640 Train loss 1.31 on epoch=319
05/17/2022 04:54:26 - INFO - __main__ - Step 650 Global step 650 Train loss 1.17 on epoch=324
05/17/2022 04:54:26 - INFO - __main__ - Global step 650 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 04:54:28 - INFO - __main__ - Step 660 Global step 660 Train loss 1.34 on epoch=329
05/17/2022 04:54:30 - INFO - __main__ - Step 670 Global step 670 Train loss 1.17 on epoch=334
05/17/2022 04:54:32 - INFO - __main__ - Step 680 Global step 680 Train loss 1.12 on epoch=339
05/17/2022 04:54:34 - INFO - __main__ - Step 690 Global step 690 Train loss 1.13 on epoch=344
05/17/2022 04:54:36 - INFO - __main__ - Step 700 Global step 700 Train loss 1.01 on epoch=349
05/17/2022 04:54:37 - INFO - __main__ - Global step 700 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 04:54:39 - INFO - __main__ - Step 710 Global step 710 Train loss 1.22 on epoch=354
05/17/2022 04:54:41 - INFO - __main__ - Step 720 Global step 720 Train loss 1.05 on epoch=359
05/17/2022 04:54:43 - INFO - __main__ - Step 730 Global step 730 Train loss 1.01 on epoch=364
05/17/2022 04:54:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.91 on epoch=369
05/17/2022 04:54:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.92 on epoch=374
05/17/2022 04:54:48 - INFO - __main__ - Global step 750 Train loss 1.02 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 04:54:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.93 on epoch=379
05/17/2022 04:54:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.96 on epoch=384
05/17/2022 04:54:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.87 on epoch=389
05/17/2022 04:54:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.86 on epoch=394
05/17/2022 04:54:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.80 on epoch=399
05/17/2022 04:54:59 - INFO - __main__ - Global step 800 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 04:55:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.86 on epoch=404
05/17/2022 04:55:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.84 on epoch=409
05/17/2022 04:55:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=414
05/17/2022 04:55:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.83 on epoch=419
05/17/2022 04:55:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.77 on epoch=424
05/17/2022 04:55:10 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 04:55:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.71 on epoch=429
05/17/2022 04:55:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.69 on epoch=434
05/17/2022 04:55:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.77 on epoch=439
05/17/2022 04:55:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.77 on epoch=444
05/17/2022 04:55:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.67 on epoch=449
05/17/2022 04:55:21 - INFO - __main__ - Global step 900 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 04:55:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.73 on epoch=454
05/17/2022 04:55:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.72 on epoch=459
05/17/2022 04:55:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.71 on epoch=464
05/17/2022 04:55:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.65 on epoch=469
05/17/2022 04:55:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.65 on epoch=474
05/17/2022 04:55:32 - INFO - __main__ - Global step 950 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 04:55:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.58 on epoch=479
05/17/2022 04:55:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.62 on epoch=484
05/17/2022 04:55:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.65 on epoch=489
05/17/2022 04:55:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.65 on epoch=494
05/17/2022 04:55:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.66 on epoch=499
05/17/2022 04:55:43 - INFO - __main__ - Global step 1000 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 04:55:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.57 on epoch=504
05/17/2022 04:55:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.68 on epoch=509
05/17/2022 04:55:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.56 on epoch=514
05/17/2022 04:55:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.59 on epoch=519
05/17/2022 04:55:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.58 on epoch=524
05/17/2022 04:55:54 - INFO - __main__ - Global step 1050 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 04:55:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.50 on epoch=529
05/17/2022 04:55:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.55 on epoch=534
05/17/2022 04:56:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.55 on epoch=539
05/17/2022 04:56:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.60 on epoch=544
05/17/2022 04:56:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=549
05/17/2022 04:56:05 - INFO - __main__ - Global step 1100 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 04:56:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.53 on epoch=554
05/17/2022 04:56:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.52 on epoch=559
05/17/2022 04:56:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.54 on epoch=564
05/17/2022 04:56:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.55 on epoch=569
05/17/2022 04:56:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.58 on epoch=574
05/17/2022 04:56:16 - INFO - __main__ - Global step 1150 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 04:56:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.53 on epoch=579
05/17/2022 04:56:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.54 on epoch=584
05/17/2022 04:56:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=589
05/17/2022 04:56:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.53 on epoch=594
05/17/2022 04:56:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=599
05/17/2022 04:56:26 - INFO - __main__ - Global step 1200 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 04:56:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=604
05/17/2022 04:56:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=609
05/17/2022 04:56:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=614
05/17/2022 04:56:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.51 on epoch=619
05/17/2022 04:56:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.54 on epoch=624
05/17/2022 04:56:37 - INFO - __main__ - Global step 1250 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 04:56:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=629
05/17/2022 04:56:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=634
05/17/2022 04:56:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=639
05/17/2022 04:56:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=644
05/17/2022 04:56:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=649
05/17/2022 04:56:48 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 04:56:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=654
05/17/2022 04:56:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
05/17/2022 04:56:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=664
05/17/2022 04:56:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=669
05/17/2022 04:56:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.47 on epoch=674
05/17/2022 04:56:59 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 04:57:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=679
05/17/2022 04:57:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
05/17/2022 04:57:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=689
05/17/2022 04:57:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=694
05/17/2022 04:57:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=699
05/17/2022 04:57:10 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 04:57:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=704
05/17/2022 04:57:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
05/17/2022 04:57:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
05/17/2022 04:57:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=719
05/17/2022 04:57:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=724
05/17/2022 04:57:20 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 04:57:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=729
05/17/2022 04:57:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
05/17/2022 04:57:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=739
05/17/2022 04:57:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
05/17/2022 04:57:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.50 on epoch=749
05/17/2022 04:57:31 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 04:57:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=754
05/17/2022 04:57:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=759
05/17/2022 04:57:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=764
05/17/2022 04:57:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=769
05/17/2022 04:57:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=774
05/17/2022 04:57:42 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 04:57:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=779
05/17/2022 04:57:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=784
05/17/2022 04:57:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=789
05/17/2022 04:57:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=794
05/17/2022 04:57:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
05/17/2022 04:57:52 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 04:57:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=804
05/17/2022 04:57:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=809
05/17/2022 04:57:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=814
05/17/2022 04:58:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
05/17/2022 04:58:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=824
05/17/2022 04:58:03 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 04:58:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=829
05/17/2022 04:58:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=834
05/17/2022 04:58:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
05/17/2022 04:58:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
05/17/2022 04:58:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=849
05/17/2022 04:58:14 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 04:58:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=854
05/17/2022 04:58:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=859
05/17/2022 04:58:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/17/2022 04:58:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=869
05/17/2022 04:58:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=874
05/17/2022 04:58:25 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 04:58:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=879
05/17/2022 04:58:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
05/17/2022 04:58:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=889
05/17/2022 04:58:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
05/17/2022 04:58:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=899
05/17/2022 04:58:35 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 04:58:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=904
05/17/2022 04:58:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=909
05/17/2022 04:58:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=914
05/17/2022 04:58:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=919
05/17/2022 04:58:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=924
05/17/2022 04:58:46 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 04:58:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=929
05/17/2022 04:58:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
05/17/2022 04:58:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=939
05/17/2022 04:58:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
05/17/2022 04:58:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/17/2022 04:58:57 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 04:58:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=954
05/17/2022 04:59:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
05/17/2022 04:59:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=964
05/17/2022 04:59:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=969
05/17/2022 04:59:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/17/2022 04:59:07 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 04:59:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=979
05/17/2022 04:59:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/17/2022 04:59:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/17/2022 04:59:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.34 on epoch=994
05/17/2022 04:59:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=999
05/17/2022 04:59:18 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 04:59:18 - INFO - __main__ - save last model!
05/17/2022 04:59:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 04:59:18 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 04:59:18 - INFO - __main__ - Printing 3 examples
05/17/2022 04:59:18 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:59:18 - INFO - __main__ - ['entailed']
05/17/2022 04:59:18 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:59:18 - INFO - __main__ - ['entailed']
05/17/2022 04:59:18 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 04:59:18 - INFO - __main__ - ['entailed']
05/17/2022 04:59:18 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:59:19 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:59:19 - INFO - __main__ - Printing 3 examples
05/17/2022 04:59:19 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/17/2022 04:59:19 - INFO - __main__ - ['entailed']
05/17/2022 04:59:19 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/17/2022 04:59:19 - INFO - __main__ - ['entailed']
05/17/2022 04:59:19 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/17/2022 04:59:19 - INFO - __main__ - ['entailed']
05/17/2022 04:59:19 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:59:19 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:59:19 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 04:59:19 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 04:59:19 - INFO - __main__ - Printing 3 examples
05/17/2022 04:59:19 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/17/2022 04:59:19 - INFO - __main__ - ['entailed']
05/17/2022 04:59:19 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/17/2022 04:59:19 - INFO - __main__ - ['entailed']
05/17/2022 04:59:19 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/17/2022 04:59:19 - INFO - __main__ - ['entailed']
05/17/2022 04:59:19 - INFO - __main__ - Tokenizing Input ...
05/17/2022 04:59:19 - INFO - __main__ - Tokenizing Output ...
05/17/2022 04:59:19 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 04:59:25 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 04:59:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 04:59:25 - INFO - __main__ - Starting training!
05/17/2022 04:59:47 - INFO - __main__ - Tokenizing Output ...
05/17/2022 05:00:03 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 05:04:12 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.4_8_predictions.txt
05/17/2022 05:04:13 - INFO - __main__ - Classification-F1 on test data: 0.3306
05/17/2022 05:04:13 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.3306437454867474
05/17/2022 05:04:13 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.3, bsz=8 ...
05/17/2022 05:04:14 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 05:04:14 - INFO - __main__ - Printing 3 examples
05/17/2022 05:04:14 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/17/2022 05:04:14 - INFO - __main__ - ['entailed']
05/17/2022 05:04:14 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/17/2022 05:04:14 - INFO - __main__ - ['entailed']
05/17/2022 05:04:14 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/17/2022 05:04:14 - INFO - __main__ - ['entailed']
05/17/2022 05:04:14 - INFO - __main__ - Tokenizing Input ...
05/17/2022 05:04:14 - INFO - __main__ - Tokenizing Output ...
05/17/2022 05:04:14 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 05:04:14 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 05:04:14 - INFO - __main__ - Printing 3 examples
05/17/2022 05:04:14 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/17/2022 05:04:14 - INFO - __main__ - ['entailed']
05/17/2022 05:04:14 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/17/2022 05:04:14 - INFO - __main__ - ['entailed']
05/17/2022 05:04:14 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/17/2022 05:04:14 - INFO - __main__ - ['entailed']
05/17/2022 05:04:14 - INFO - __main__ - Tokenizing Input ...
05/17/2022 05:04:14 - INFO - __main__ - Tokenizing Output ...
05/17/2022 05:04:14 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 05:04:19 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 05:04:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 05:04:19 - INFO - __main__ - Starting training!
05/17/2022 05:04:23 - INFO - __main__ - Step 10 Global step 10 Train loss 5.01 on epoch=4
05/17/2022 05:04:25 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/17/2022 05:04:27 - INFO - __main__ - Step 30 Global step 30 Train loss 4.96 on epoch=14
05/17/2022 05:04:29 - INFO - __main__ - Step 40 Global step 40 Train loss 4.83 on epoch=19
05/17/2022 05:04:31 - INFO - __main__ - Step 50 Global step 50 Train loss 4.86 on epoch=24
05/17/2022 05:04:32 - INFO - __main__ - Global step 50 Train loss 4.93 Classification-F1 0.0 on epoch=24
05/17/2022 05:04:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 05:04:34 - INFO - __main__ - Step 60 Global step 60 Train loss 4.69 on epoch=29
05/17/2022 05:04:36 - INFO - __main__ - Step 70 Global step 70 Train loss 4.51 on epoch=34
05/17/2022 05:04:38 - INFO - __main__ - Step 80 Global step 80 Train loss 4.46 on epoch=39
05/17/2022 05:04:40 - INFO - __main__ - Step 90 Global step 90 Train loss 4.33 on epoch=44
05/17/2022 05:04:42 - INFO - __main__ - Step 100 Global step 100 Train loss 4.19 on epoch=49
05/17/2022 05:04:43 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/17/2022 05:04:45 - INFO - __main__ - Step 110 Global step 110 Train loss 4.12 on epoch=54
05/17/2022 05:04:47 - INFO - __main__ - Step 120 Global step 120 Train loss 3.96 on epoch=59
05/17/2022 05:04:49 - INFO - __main__ - Step 130 Global step 130 Train loss 3.95 on epoch=64
05/17/2022 05:04:51 - INFO - __main__ - Step 140 Global step 140 Train loss 3.91 on epoch=69
05/17/2022 05:04:53 - INFO - __main__ - Step 150 Global step 150 Train loss 3.81 on epoch=74
05/17/2022 05:04:54 - INFO - __main__ - Global step 150 Train loss 3.95 Classification-F1 0.0 on epoch=74
05/17/2022 05:04:56 - INFO - __main__ - Step 160 Global step 160 Train loss 3.79 on epoch=79
05/17/2022 05:04:58 - INFO - __main__ - Step 170 Global step 170 Train loss 3.65 on epoch=84
05/17/2022 05:05:00 - INFO - __main__ - Step 180 Global step 180 Train loss 3.62 on epoch=89
05/17/2022 05:05:02 - INFO - __main__ - Step 190 Global step 190 Train loss 3.61 on epoch=94
05/17/2022 05:05:04 - INFO - __main__ - Step 200 Global step 200 Train loss 3.52 on epoch=99
05/17/2022 05:05:07 - INFO - __main__ - Global step 200 Train loss 3.64 Classification-F1 0.13 on epoch=99
05/17/2022 05:05:07 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.13 on epoch=99, global_step=200
05/17/2022 05:05:09 - INFO - __main__ - Step 210 Global step 210 Train loss 3.45 on epoch=104
05/17/2022 05:05:11 - INFO - __main__ - Step 220 Global step 220 Train loss 3.31 on epoch=109
05/17/2022 05:05:13 - INFO - __main__ - Step 230 Global step 230 Train loss 3.34 on epoch=114
05/17/2022 05:05:15 - INFO - __main__ - Step 240 Global step 240 Train loss 3.22 on epoch=119
05/17/2022 05:05:17 - INFO - __main__ - Step 250 Global step 250 Train loss 3.17 on epoch=124
05/17/2022 05:05:19 - INFO - __main__ - Global step 250 Train loss 3.30 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 05:05:19 - INFO - __main__ - Saving model with best Classification-F1: 0.13 -> 0.3333333333333333 on epoch=124, global_step=250
05/17/2022 05:05:21 - INFO - __main__ - Step 260 Global step 260 Train loss 3.02 on epoch=129
05/17/2022 05:05:23 - INFO - __main__ - Step 270 Global step 270 Train loss 3.06 on epoch=134
05/17/2022 05:05:25 - INFO - __main__ - Step 280 Global step 280 Train loss 2.93 on epoch=139
05/17/2022 05:05:27 - INFO - __main__ - Step 290 Global step 290 Train loss 2.82 on epoch=144
05/17/2022 05:05:29 - INFO - __main__ - Step 300 Global step 300 Train loss 2.70 on epoch=149
05/17/2022 05:05:32 - INFO - __main__ - Global step 300 Train loss 2.91 Classification-F1 0.3333333333333333 on epoch=149
05/17/2022 05:05:34 - INFO - __main__ - Step 310 Global step 310 Train loss 2.70 on epoch=154
05/17/2022 05:05:36 - INFO - __main__ - Step 320 Global step 320 Train loss 2.72 on epoch=159
05/17/2022 05:05:38 - INFO - __main__ - Step 330 Global step 330 Train loss 2.53 on epoch=164
05/17/2022 05:05:40 - INFO - __main__ - Step 340 Global step 340 Train loss 2.60 on epoch=169
05/17/2022 05:05:42 - INFO - __main__ - Step 350 Global step 350 Train loss 2.45 on epoch=174
05/17/2022 05:05:45 - INFO - __main__ - Global step 350 Train loss 2.60 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 05:05:47 - INFO - __main__ - Step 360 Global step 360 Train loss 2.36 on epoch=179
05/17/2022 05:05:49 - INFO - __main__ - Step 370 Global step 370 Train loss 2.42 on epoch=184
05/17/2022 05:05:51 - INFO - __main__ - Step 380 Global step 380 Train loss 2.39 on epoch=189
05/17/2022 05:05:52 - INFO - __main__ - Step 390 Global step 390 Train loss 2.32 on epoch=194
05/17/2022 05:05:55 - INFO - __main__ - Step 400 Global step 400 Train loss 2.23 on epoch=199
05/17/2022 05:05:57 - INFO - __main__ - Global step 400 Train loss 2.34 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 05:05:59 - INFO - __main__ - Step 410 Global step 410 Train loss 2.27 on epoch=204
05/17/2022 05:06:01 - INFO - __main__ - Step 420 Global step 420 Train loss 2.29 on epoch=209
05/17/2022 05:06:03 - INFO - __main__ - Step 430 Global step 430 Train loss 2.24 on epoch=214
05/17/2022 05:06:05 - INFO - __main__ - Step 440 Global step 440 Train loss 2.06 on epoch=219
05/17/2022 05:06:07 - INFO - __main__ - Step 450 Global step 450 Train loss 1.93 on epoch=224
05/17/2022 05:06:08 - INFO - __main__ - Global step 450 Train loss 2.16 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 05:06:10 - INFO - __main__ - Step 460 Global step 460 Train loss 2.06 on epoch=229
05/17/2022 05:06:12 - INFO - __main__ - Step 470 Global step 470 Train loss 1.86 on epoch=234
05/17/2022 05:06:14 - INFO - __main__ - Step 480 Global step 480 Train loss 1.87 on epoch=239
05/17/2022 05:06:16 - INFO - __main__ - Step 490 Global step 490 Train loss 1.88 on epoch=244
05/17/2022 05:06:18 - INFO - __main__ - Step 500 Global step 500 Train loss 1.90 on epoch=249
05/17/2022 05:06:19 - INFO - __main__ - Global step 500 Train loss 1.91 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 05:06:21 - INFO - __main__ - Step 510 Global step 510 Train loss 1.76 on epoch=254
05/17/2022 05:06:23 - INFO - __main__ - Step 520 Global step 520 Train loss 1.78 on epoch=259
05/17/2022 05:06:25 - INFO - __main__ - Step 530 Global step 530 Train loss 1.80 on epoch=264
05/17/2022 05:06:27 - INFO - __main__ - Step 540 Global step 540 Train loss 1.58 on epoch=269
05/17/2022 05:06:29 - INFO - __main__ - Step 550 Global step 550 Train loss 1.61 on epoch=274
05/17/2022 05:06:30 - INFO - __main__ - Global step 550 Train loss 1.71 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 05:06:32 - INFO - __main__ - Step 560 Global step 560 Train loss 1.59 on epoch=279
05/17/2022 05:06:34 - INFO - __main__ - Step 570 Global step 570 Train loss 1.56 on epoch=284
05/17/2022 05:06:36 - INFO - __main__ - Step 580 Global step 580 Train loss 1.54 on epoch=289
05/17/2022 05:06:38 - INFO - __main__ - Step 590 Global step 590 Train loss 1.56 on epoch=294
05/17/2022 05:06:40 - INFO - __main__ - Step 600 Global step 600 Train loss 1.59 on epoch=299
05/17/2022 05:06:41 - INFO - __main__ - Global step 600 Train loss 1.57 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 05:06:43 - INFO - __main__ - Step 610 Global step 610 Train loss 1.50 on epoch=304
05/17/2022 05:06:45 - INFO - __main__ - Step 620 Global step 620 Train loss 1.54 on epoch=309
05/17/2022 05:06:47 - INFO - __main__ - Step 630 Global step 630 Train loss 1.44 on epoch=314
05/17/2022 05:06:49 - INFO - __main__ - Step 640 Global step 640 Train loss 1.54 on epoch=319
05/17/2022 05:06:51 - INFO - __main__ - Step 650 Global step 650 Train loss 1.44 on epoch=324
05/17/2022 05:06:53 - INFO - __main__ - Global step 650 Train loss 1.49 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 05:06:55 - INFO - __main__ - Step 660 Global step 660 Train loss 1.31 on epoch=329
05/17/2022 05:06:57 - INFO - __main__ - Step 670 Global step 670 Train loss 1.28 on epoch=334
05/17/2022 05:06:59 - INFO - __main__ - Step 680 Global step 680 Train loss 1.25 on epoch=339
05/17/2022 05:07:01 - INFO - __main__ - Step 690 Global step 690 Train loss 1.20 on epoch=344
05/17/2022 05:07:03 - INFO - __main__ - Step 700 Global step 700 Train loss 1.30 on epoch=349
05/17/2022 05:07:05 - INFO - __main__ - Global step 700 Train loss 1.27 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 05:07:07 - INFO - __main__ - Step 710 Global step 710 Train loss 1.10 on epoch=354
05/17/2022 05:07:09 - INFO - __main__ - Step 720 Global step 720 Train loss 1.16 on epoch=359
05/17/2022 05:07:11 - INFO - __main__ - Step 730 Global step 730 Train loss 1.15 on epoch=364
05/17/2022 05:07:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.99 on epoch=369
05/17/2022 05:07:15 - INFO - __main__ - Step 750 Global step 750 Train loss 1.04 on epoch=374
05/17/2022 05:07:17 - INFO - __main__ - Global step 750 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 05:07:20 - INFO - __main__ - Step 760 Global step 760 Train loss 1.12 on epoch=379
05/17/2022 05:07:22 - INFO - __main__ - Step 770 Global step 770 Train loss 1.14 on epoch=384
05/17/2022 05:07:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.93 on epoch=389
05/17/2022 05:07:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.97 on epoch=394
05/17/2022 05:07:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.98 on epoch=399
05/17/2022 05:07:29 - INFO - __main__ - Global step 800 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 05:07:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.95 on epoch=404
05/17/2022 05:07:33 - INFO - __main__ - Step 820 Global step 820 Train loss 1.02 on epoch=409
05/17/2022 05:07:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.90 on epoch=414
05/17/2022 05:07:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.92 on epoch=419
05/17/2022 05:07:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.89 on epoch=424
05/17/2022 05:07:40 - INFO - __main__ - Global step 850 Train loss 0.93 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 05:07:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.93 on epoch=429
05/17/2022 05:07:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.90 on epoch=434
05/17/2022 05:07:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=439
05/17/2022 05:07:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.75 on epoch=444
05/17/2022 05:07:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.80 on epoch=449
05/17/2022 05:07:51 - INFO - __main__ - Global step 900 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 05:07:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.73 on epoch=454
05/17/2022 05:07:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.75 on epoch=459
05/17/2022 05:07:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.76 on epoch=464
05/17/2022 05:07:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.75 on epoch=469
05/17/2022 05:08:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.69 on epoch=474
05/17/2022 05:08:02 - INFO - __main__ - Global step 950 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 05:08:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.80 on epoch=479
05/17/2022 05:08:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.66 on epoch=484
05/17/2022 05:08:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.82 on epoch=489
05/17/2022 05:08:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.71 on epoch=494
05/17/2022 05:08:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.69 on epoch=499
05/17/2022 05:08:13 - INFO - __main__ - Global step 1000 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 05:08:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.76 on epoch=504
05/17/2022 05:08:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.73 on epoch=509
05/17/2022 05:08:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.68 on epoch=514
05/17/2022 05:08:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.70 on epoch=519
05/17/2022 05:08:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.69 on epoch=524
05/17/2022 05:08:24 - INFO - __main__ - Global step 1050 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 05:08:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.70 on epoch=529
05/17/2022 05:08:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.69 on epoch=534
05/17/2022 05:08:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.67 on epoch=539
05/17/2022 05:08:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.59 on epoch=544
05/17/2022 05:08:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.59 on epoch=549
05/17/2022 05:08:35 - INFO - __main__ - Global step 1100 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 05:08:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.56 on epoch=554
05/17/2022 05:08:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.62 on epoch=559
05/17/2022 05:08:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.58 on epoch=564
05/17/2022 05:08:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.55 on epoch=569
05/17/2022 05:08:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.61 on epoch=574
05/17/2022 05:08:46 - INFO - __main__ - Global step 1150 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 05:08:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.55 on epoch=579
05/17/2022 05:08:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.61 on epoch=584
05/17/2022 05:08:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.59 on epoch=589
05/17/2022 05:08:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.65 on epoch=594
05/17/2022 05:08:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.54 on epoch=599
05/17/2022 05:08:57 - INFO - __main__ - Global step 1200 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 05:08:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=604
05/17/2022 05:09:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.54 on epoch=609
05/17/2022 05:09:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=614
05/17/2022 05:09:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.60 on epoch=619
05/17/2022 05:09:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=624
05/17/2022 05:09:08 - INFO - __main__ - Global step 1250 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 05:09:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=629
05/17/2022 05:09:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.51 on epoch=634
05/17/2022 05:09:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
05/17/2022 05:09:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.60 on epoch=644
05/17/2022 05:09:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.57 on epoch=649
05/17/2022 05:09:19 - INFO - __main__ - Global step 1300 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 05:09:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.51 on epoch=654
05/17/2022 05:09:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=659
05/17/2022 05:09:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.54 on epoch=664
05/17/2022 05:09:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=669
05/17/2022 05:09:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=674
05/17/2022 05:09:30 - INFO - __main__ - Global step 1350 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 05:09:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.52 on epoch=679
05/17/2022 05:09:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.51 on epoch=684
05/17/2022 05:09:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=689
05/17/2022 05:09:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=694
05/17/2022 05:09:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=699
05/17/2022 05:09:41 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 05:09:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=704
05/17/2022 05:09:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=709
05/17/2022 05:09:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
05/17/2022 05:09:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=719
05/17/2022 05:09:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=724
05/17/2022 05:09:52 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=724
05/17/2022 05:09:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=729
05/17/2022 05:09:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.49 on epoch=734
05/17/2022 05:09:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.50 on epoch=739
05/17/2022 05:10:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=744
05/17/2022 05:10:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=749
05/17/2022 05:10:03 - INFO - __main__ - Global step 1500 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 05:10:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
05/17/2022 05:10:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=759
05/17/2022 05:10:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=764
05/17/2022 05:10:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=769
05/17/2022 05:10:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=774
05/17/2022 05:10:13 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=774
05/17/2022 05:10:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=779
05/17/2022 05:10:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.42 on epoch=784
05/17/2022 05:10:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=789
05/17/2022 05:10:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=794
05/17/2022 05:10:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.41 on epoch=799
05/17/2022 05:10:24 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 05:10:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=804
05/17/2022 05:10:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=809
05/17/2022 05:10:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=814
05/17/2022 05:10:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
05/17/2022 05:10:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=824
05/17/2022 05:10:34 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 05:10:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=829
05/17/2022 05:10:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=834
05/17/2022 05:10:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
05/17/2022 05:10:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=844
05/17/2022 05:10:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=849
05/17/2022 05:10:45 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 05:10:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=854
05/17/2022 05:10:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.18 on epoch=859
05/17/2022 05:10:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=864
05/17/2022 05:10:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=869
05/17/2022 05:10:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
05/17/2022 05:10:55 - INFO - __main__ - Global step 1750 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 05:10:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=879
05/17/2022 05:10:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
05/17/2022 05:11:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=889
05/17/2022 05:11:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=894
05/17/2022 05:11:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=899
05/17/2022 05:11:05 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 05:11:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=904
05/17/2022 05:11:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=909
05/17/2022 05:11:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=914
05/17/2022 05:11:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=919
05/17/2022 05:11:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=924
05/17/2022 05:11:16 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 05:11:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=929
05/17/2022 05:11:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
05/17/2022 05:11:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=939
05/17/2022 05:11:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=944
05/17/2022 05:11:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
05/17/2022 05:11:26 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 05:11:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=954
05/17/2022 05:11:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
05/17/2022 05:11:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=964
05/17/2022 05:11:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=969
05/17/2022 05:11:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=974
05/17/2022 05:11:37 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 05:11:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=979
05/17/2022 05:11:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=984
05/17/2022 05:11:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=989
05/17/2022 05:11:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=994
05/17/2022 05:11:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=999
05/17/2022 05:11:48 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=999
05/17/2022 05:11:48 - INFO - __main__ - save last model!
05/17/2022 05:11:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 05:11:48 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 05:11:48 - INFO - __main__ - Printing 3 examples
05/17/2022 05:11:48 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 05:11:48 - INFO - __main__ - ['entailed']
05/17/2022 05:11:48 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 05:11:48 - INFO - __main__ - ['entailed']
05/17/2022 05:11:48 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 05:11:48 - INFO - __main__ - ['entailed']
05/17/2022 05:11:48 - INFO - __main__ - Tokenizing Input ...
05/17/2022 05:11:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 05:11:49 - INFO - __main__ - Printing 3 examples
05/17/2022 05:11:49 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/17/2022 05:11:49 - INFO - __main__ - ['entailed']
05/17/2022 05:11:49 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/17/2022 05:11:49 - INFO - __main__ - ['entailed']
05/17/2022 05:11:49 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/17/2022 05:11:49 - INFO - __main__ - ['entailed']
05/17/2022 05:11:49 - INFO - __main__ - Tokenizing Input ...
05/17/2022 05:11:49 - INFO - __main__ - Tokenizing Output ...
05/17/2022 05:11:49 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 05:11:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 05:11:49 - INFO - __main__ - Printing 3 examples
05/17/2022 05:11:49 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/17/2022 05:11:49 - INFO - __main__ - ['entailed']
05/17/2022 05:11:49 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/17/2022 05:11:49 - INFO - __main__ - ['entailed']
05/17/2022 05:11:49 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/17/2022 05:11:49 - INFO - __main__ - ['entailed']
05/17/2022 05:11:49 - INFO - __main__ - Tokenizing Input ...
05/17/2022 05:11:49 - INFO - __main__ - Tokenizing Output ...
05/17/2022 05:11:49 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 05:11:55 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 05:11:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 05:11:56 - INFO - __main__ - Starting training!
05/17/2022 05:12:14 - INFO - __main__ - Tokenizing Output ...
05/17/2022 05:12:27 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 05:16:33 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.3_8_predictions.txt
05/17/2022 05:16:33 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/17/2022 05:16:33 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/17/2022 05:16:33 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.2, bsz=8 ...
05/17/2022 05:16:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 05:16:34 - INFO - __main__ - Printing 3 examples
05/17/2022 05:16:34 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/17/2022 05:16:34 - INFO - __main__ - ['entailed']
05/17/2022 05:16:34 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/17/2022 05:16:34 - INFO - __main__ - ['entailed']
05/17/2022 05:16:34 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/17/2022 05:16:34 - INFO - __main__ - ['entailed']
05/17/2022 05:16:34 - INFO - __main__ - Tokenizing Input ...
05/17/2022 05:16:34 - INFO - __main__ - Tokenizing Output ...
05/17/2022 05:16:34 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 05:16:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 05:16:34 - INFO - __main__ - Printing 3 examples
05/17/2022 05:16:34 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/17/2022 05:16:34 - INFO - __main__ - ['entailed']
05/17/2022 05:16:34 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/17/2022 05:16:34 - INFO - __main__ - ['entailed']
05/17/2022 05:16:34 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/17/2022 05:16:34 - INFO - __main__ - ['entailed']
05/17/2022 05:16:34 - INFO - __main__ - Tokenizing Input ...
05/17/2022 05:16:34 - INFO - __main__ - Tokenizing Output ...
05/17/2022 05:16:34 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 05:16:41 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 05:16:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 05:16:41 - INFO - __main__ - Starting training!
05/17/2022 05:16:44 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/17/2022 05:16:46 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/17/2022 05:16:48 - INFO - __main__ - Step 30 Global step 30 Train loss 4.96 on epoch=14
05/17/2022 05:16:50 - INFO - __main__ - Step 40 Global step 40 Train loss 4.86 on epoch=19
05/17/2022 05:16:52 - INFO - __main__ - Step 50 Global step 50 Train loss 4.80 on epoch=24
05/17/2022 05:16:54 - INFO - __main__ - Global step 50 Train loss 4.93 Classification-F1 0.0 on epoch=24
05/17/2022 05:16:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/17/2022 05:16:56 - INFO - __main__ - Step 60 Global step 60 Train loss 4.86 on epoch=29
05/17/2022 05:16:58 - INFO - __main__ - Step 70 Global step 70 Train loss 4.82 on epoch=34
05/17/2022 05:17:00 - INFO - __main__ - Step 80 Global step 80 Train loss 4.67 on epoch=39
05/17/2022 05:17:02 - INFO - __main__ - Step 90 Global step 90 Train loss 4.71 on epoch=44
05/17/2022 05:17:04 - INFO - __main__ - Step 100 Global step 100 Train loss 4.65 on epoch=49
05/17/2022 05:17:05 - INFO - __main__ - Global step 100 Train loss 4.74 Classification-F1 0.0 on epoch=49
05/17/2022 05:17:07 - INFO - __main__ - Step 110 Global step 110 Train loss 4.58 on epoch=54
05/17/2022 05:17:09 - INFO - __main__ - Step 120 Global step 120 Train loss 4.59 on epoch=59
05/17/2022 05:17:11 - INFO - __main__ - Step 130 Global step 130 Train loss 4.59 on epoch=64
05/17/2022 05:17:13 - INFO - __main__ - Step 140 Global step 140 Train loss 4.45 on epoch=69
05/17/2022 05:17:15 - INFO - __main__ - Step 150 Global step 150 Train loss 4.52 on epoch=74
05/17/2022 05:17:17 - INFO - __main__ - Global step 150 Train loss 4.55 Classification-F1 0.0 on epoch=74
05/17/2022 05:17:20 - INFO - __main__ - Step 160 Global step 160 Train loss 4.51 on epoch=79
05/17/2022 05:17:22 - INFO - __main__ - Step 170 Global step 170 Train loss 4.40 on epoch=84
05/17/2022 05:17:24 - INFO - __main__ - Step 180 Global step 180 Train loss 4.29 on epoch=89
05/17/2022 05:17:26 - INFO - __main__ - Step 190 Global step 190 Train loss 4.24 on epoch=94
05/17/2022 05:17:28 - INFO - __main__ - Step 200 Global step 200 Train loss 4.27 on epoch=99
05/17/2022 05:17:30 - INFO - __main__ - Global step 200 Train loss 4.34 Classification-F1 0.0 on epoch=99
05/17/2022 05:17:32 - INFO - __main__ - Step 210 Global step 210 Train loss 4.27 on epoch=104
05/17/2022 05:17:34 - INFO - __main__ - Step 220 Global step 220 Train loss 4.16 on epoch=109
05/17/2022 05:17:36 - INFO - __main__ - Step 230 Global step 230 Train loss 4.01 on epoch=114
05/17/2022 05:17:38 - INFO - __main__ - Step 240 Global step 240 Train loss 4.02 on epoch=119
05/17/2022 05:17:40 - INFO - __main__ - Step 250 Global step 250 Train loss 3.99 on epoch=124
05/17/2022 05:17:43 - INFO - __main__ - Global step 250 Train loss 4.09 Classification-F1 0.0 on epoch=124
05/17/2022 05:17:45 - INFO - __main__ - Step 260 Global step 260 Train loss 3.88 on epoch=129
05/17/2022 05:17:47 - INFO - __main__ - Step 270 Global step 270 Train loss 3.93 on epoch=134
05/17/2022 05:17:49 - INFO - __main__ - Step 280 Global step 280 Train loss 3.83 on epoch=139
05/17/2022 05:17:51 - INFO - __main__ - Step 290 Global step 290 Train loss 3.84 on epoch=144
05/17/2022 05:17:53 - INFO - __main__ - Step 300 Global step 300 Train loss 3.73 on epoch=149
05/17/2022 05:17:54 - INFO - __main__ - Global step 300 Train loss 3.84 Classification-F1 0.0 on epoch=149
05/17/2022 05:17:56 - INFO - __main__ - Step 310 Global step 310 Train loss 3.75 on epoch=154
05/17/2022 05:17:58 - INFO - __main__ - Step 320 Global step 320 Train loss 3.62 on epoch=159
05/17/2022 05:18:00 - INFO - __main__ - Step 330 Global step 330 Train loss 3.63 on epoch=164
05/17/2022 05:18:02 - INFO - __main__ - Step 340 Global step 340 Train loss 3.62 on epoch=169
05/17/2022 05:18:04 - INFO - __main__ - Step 350 Global step 350 Train loss 3.54 on epoch=174
05/17/2022 05:18:07 - INFO - __main__ - Global step 350 Train loss 3.63 Classification-F1 0.0 on epoch=174
05/17/2022 05:18:09 - INFO - __main__ - Step 360 Global step 360 Train loss 3.50 on epoch=179
05/17/2022 05:18:11 - INFO - __main__ - Step 370 Global step 370 Train loss 3.48 on epoch=184
05/17/2022 05:18:13 - INFO - __main__ - Step 380 Global step 380 Train loss 3.51 on epoch=189
05/17/2022 05:18:15 - INFO - __main__ - Step 390 Global step 390 Train loss 3.39 on epoch=194
05/17/2022 05:18:17 - INFO - __main__ - Step 400 Global step 400 Train loss 3.29 on epoch=199
05/17/2022 05:18:20 - INFO - __main__ - Global step 400 Train loss 3.43 Classification-F1 0.0625 on epoch=199
05/17/2022 05:18:20 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0625 on epoch=199, global_step=400
05/17/2022 05:18:22 - INFO - __main__ - Step 410 Global step 410 Train loss 3.35 on epoch=204
05/17/2022 05:18:24 - INFO - __main__ - Step 420 Global step 420 Train loss 3.26 on epoch=209
05/17/2022 05:18:26 - INFO - __main__ - Step 430 Global step 430 Train loss 3.23 on epoch=214
05/17/2022 05:18:28 - INFO - __main__ - Step 440 Global step 440 Train loss 3.30 on epoch=219
05/17/2022 05:18:29 - INFO - __main__ - Step 450 Global step 450 Train loss 3.18 on epoch=224
05/17/2022 05:18:33 - INFO - __main__ - Global step 450 Train loss 3.26 Classification-F1 0.10317460317460318 on epoch=224
05/17/2022 05:18:33 - INFO - __main__ - Saving model with best Classification-F1: 0.0625 -> 0.10317460317460318 on epoch=224, global_step=450
05/17/2022 05:18:35 - INFO - __main__ - Step 460 Global step 460 Train loss 3.17 on epoch=229
05/17/2022 05:18:36 - INFO - __main__ - Step 470 Global step 470 Train loss 3.11 on epoch=234
05/17/2022 05:18:38 - INFO - __main__ - Step 480 Global step 480 Train loss 3.01 on epoch=239
05/17/2022 05:18:41 - INFO - __main__ - Step 490 Global step 490 Train loss 3.09 on epoch=244
05/17/2022 05:18:43 - INFO - __main__ - Step 500 Global step 500 Train loss 2.92 on epoch=249
05/17/2022 05:18:45 - INFO - __main__ - Global step 500 Train loss 3.06 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 05:18:45 - INFO - __main__ - Saving model with best Classification-F1: 0.10317460317460318 -> 0.3333333333333333 on epoch=249, global_step=500
05/17/2022 05:18:47 - INFO - __main__ - Step 510 Global step 510 Train loss 3.00 on epoch=254
05/17/2022 05:18:49 - INFO - __main__ - Step 520 Global step 520 Train loss 2.88 on epoch=259
05/17/2022 05:18:51 - INFO - __main__ - Step 530 Global step 530 Train loss 2.94 on epoch=264
05/17/2022 05:18:53 - INFO - __main__ - Step 540 Global step 540 Train loss 2.75 on epoch=269
05/17/2022 05:18:55 - INFO - __main__ - Step 550 Global step 550 Train loss 2.89 on epoch=274
05/17/2022 05:18:58 - INFO - __main__ - Global step 550 Train loss 2.89 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 05:19:00 - INFO - __main__ - Step 560 Global step 560 Train loss 2.72 on epoch=279
05/17/2022 05:19:02 - INFO - __main__ - Step 570 Global step 570 Train loss 2.73 on epoch=284
05/17/2022 05:19:04 - INFO - __main__ - Step 580 Global step 580 Train loss 2.69 on epoch=289
05/17/2022 05:19:06 - INFO - __main__ - Step 590 Global step 590 Train loss 2.71 on epoch=294
05/17/2022 05:19:08 - INFO - __main__ - Step 600 Global step 600 Train loss 2.70 on epoch=299
05/17/2022 05:19:11 - INFO - __main__ - Global step 600 Train loss 2.71 Classification-F1 0.3333333333333333 on epoch=299
05/17/2022 05:19:13 - INFO - __main__ - Step 610 Global step 610 Train loss 2.58 on epoch=304
05/17/2022 05:19:15 - INFO - __main__ - Step 620 Global step 620 Train loss 2.63 on epoch=309
05/17/2022 05:19:17 - INFO - __main__ - Step 630 Global step 630 Train loss 2.50 on epoch=314
05/17/2022 05:19:19 - INFO - __main__ - Step 640 Global step 640 Train loss 2.43 on epoch=319
05/17/2022 05:19:21 - INFO - __main__ - Step 650 Global step 650 Train loss 2.49 on epoch=324
05/17/2022 05:19:24 - INFO - __main__ - Global step 650 Train loss 2.53 Classification-F1 0.3333333333333333 on epoch=324
05/17/2022 05:19:26 - INFO - __main__ - Step 660 Global step 660 Train loss 2.48 on epoch=329
05/17/2022 05:19:28 - INFO - __main__ - Step 670 Global step 670 Train loss 2.36 on epoch=334
05/17/2022 05:19:30 - INFO - __main__ - Step 680 Global step 680 Train loss 2.28 on epoch=339
05/17/2022 05:19:32 - INFO - __main__ - Step 690 Global step 690 Train loss 2.31 on epoch=344
05/17/2022 05:19:34 - INFO - __main__ - Step 700 Global step 700 Train loss 2.34 on epoch=349
05/17/2022 05:19:38 - INFO - __main__ - Global step 700 Train loss 2.35 Classification-F1 0.3333333333333333 on epoch=349
05/17/2022 05:19:40 - INFO - __main__ - Step 710 Global step 710 Train loss 2.20 on epoch=354
05/17/2022 05:19:42 - INFO - __main__ - Step 720 Global step 720 Train loss 2.09 on epoch=359
05/17/2022 05:19:44 - INFO - __main__ - Step 730 Global step 730 Train loss 2.13 on epoch=364
05/17/2022 05:19:46 - INFO - __main__ - Step 740 Global step 740 Train loss 2.08 on epoch=369
05/17/2022 05:19:48 - INFO - __main__ - Step 750 Global step 750 Train loss 2.09 on epoch=374
05/17/2022 05:19:51 - INFO - __main__ - Global step 750 Train loss 2.12 Classification-F1 0.3333333333333333 on epoch=374
05/17/2022 05:19:53 - INFO - __main__ - Step 760 Global step 760 Train loss 2.04 on epoch=379
05/17/2022 05:19:56 - INFO - __main__ - Step 770 Global step 770 Train loss 1.91 on epoch=384
05/17/2022 05:19:57 - INFO - __main__ - Step 780 Global step 780 Train loss 1.88 on epoch=389
05/17/2022 05:20:00 - INFO - __main__ - Step 790 Global step 790 Train loss 1.89 on epoch=394
05/17/2022 05:20:02 - INFO - __main__ - Step 800 Global step 800 Train loss 1.88 on epoch=399
05/17/2022 05:20:05 - INFO - __main__ - Global step 800 Train loss 1.92 Classification-F1 0.3333333333333333 on epoch=399
05/17/2022 05:20:07 - INFO - __main__ - Step 810 Global step 810 Train loss 1.82 on epoch=404
05/17/2022 05:20:09 - INFO - __main__ - Step 820 Global step 820 Train loss 1.90 on epoch=409
05/17/2022 05:20:11 - INFO - __main__ - Step 830 Global step 830 Train loss 1.75 on epoch=414
05/17/2022 05:20:13 - INFO - __main__ - Step 840 Global step 840 Train loss 1.61 on epoch=419
05/17/2022 05:20:15 - INFO - __main__ - Step 850 Global step 850 Train loss 1.66 on epoch=424
05/17/2022 05:20:19 - INFO - __main__ - Global step 850 Train loss 1.75 Classification-F1 0.3333333333333333 on epoch=424
05/17/2022 05:20:21 - INFO - __main__ - Step 860 Global step 860 Train loss 1.55 on epoch=429
05/17/2022 05:20:23 - INFO - __main__ - Step 870 Global step 870 Train loss 1.59 on epoch=434
05/17/2022 05:20:25 - INFO - __main__ - Step 880 Global step 880 Train loss 1.49 on epoch=439
05/17/2022 05:20:27 - INFO - __main__ - Step 890 Global step 890 Train loss 1.53 on epoch=444
05/17/2022 05:20:29 - INFO - __main__ - Step 900 Global step 900 Train loss 1.55 on epoch=449
05/17/2022 05:20:32 - INFO - __main__ - Global step 900 Train loss 1.54 Classification-F1 0.3333333333333333 on epoch=449
05/17/2022 05:20:34 - INFO - __main__ - Step 910 Global step 910 Train loss 1.46 on epoch=454
05/17/2022 05:20:36 - INFO - __main__ - Step 920 Global step 920 Train loss 1.56 on epoch=459
05/17/2022 05:20:38 - INFO - __main__ - Step 930 Global step 930 Train loss 1.42 on epoch=464
05/17/2022 05:20:40 - INFO - __main__ - Step 940 Global step 940 Train loss 1.33 on epoch=469
05/17/2022 05:20:42 - INFO - __main__ - Step 950 Global step 950 Train loss 1.35 on epoch=474
05/17/2022 05:20:43 - INFO - __main__ - Global step 950 Train loss 1.42 Classification-F1 0.3333333333333333 on epoch=474
05/17/2022 05:20:45 - INFO - __main__ - Step 960 Global step 960 Train loss 1.26 on epoch=479
05/17/2022 05:20:47 - INFO - __main__ - Step 970 Global step 970 Train loss 1.34 on epoch=484
05/17/2022 05:20:49 - INFO - __main__ - Step 980 Global step 980 Train loss 1.37 on epoch=489
05/17/2022 05:20:51 - INFO - __main__ - Step 990 Global step 990 Train loss 1.42 on epoch=494
05/17/2022 05:20:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.24 on epoch=499
05/17/2022 05:20:54 - INFO - __main__ - Global step 1000 Train loss 1.33 Classification-F1 0.3333333333333333 on epoch=499
05/17/2022 05:20:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.31 on epoch=504
05/17/2022 05:20:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.19 on epoch=509
05/17/2022 05:21:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.23 on epoch=514
05/17/2022 05:21:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.11 on epoch=519
05/17/2022 05:21:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.16 on epoch=524
05/17/2022 05:21:05 - INFO - __main__ - Global step 1050 Train loss 1.20 Classification-F1 0.3333333333333333 on epoch=524
05/17/2022 05:21:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.14 on epoch=529
05/17/2022 05:21:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.05 on epoch=534
05/17/2022 05:21:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.03 on epoch=539
05/17/2022 05:21:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.03 on epoch=544
05/17/2022 05:21:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.96 on epoch=549
05/17/2022 05:21:16 - INFO - __main__ - Global step 1100 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=549
05/17/2022 05:21:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.05 on epoch=554
05/17/2022 05:21:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.87 on epoch=559
05/17/2022 05:21:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.02 on epoch=564
05/17/2022 05:21:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.94 on epoch=569
05/17/2022 05:21:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.88 on epoch=574
05/17/2022 05:21:27 - INFO - __main__ - Global step 1150 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=574
05/17/2022 05:21:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.87 on epoch=579
05/17/2022 05:21:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.94 on epoch=584
05/17/2022 05:21:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.93 on epoch=589
05/17/2022 05:21:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.94 on epoch=594
05/17/2022 05:21:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=599
05/17/2022 05:21:38 - INFO - __main__ - Global step 1200 Train loss 0.90 Classification-F1 0.3333333333333333 on epoch=599
05/17/2022 05:21:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.86 on epoch=604
05/17/2022 05:21:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.90 on epoch=609
05/17/2022 05:21:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.85 on epoch=614
05/17/2022 05:21:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.85 on epoch=619
05/17/2022 05:21:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.71 on epoch=624
05/17/2022 05:21:49 - INFO - __main__ - Global step 1250 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=624
05/17/2022 05:21:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.82 on epoch=629
05/17/2022 05:21:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=634
05/17/2022 05:21:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.78 on epoch=639
05/17/2022 05:21:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.79 on epoch=644
05/17/2022 05:21:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.75 on epoch=649
05/17/2022 05:22:00 - INFO - __main__ - Global step 1300 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=649
05/17/2022 05:22:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.67 on epoch=654
05/17/2022 05:22:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.74 on epoch=659
05/17/2022 05:22:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.79 on epoch=664
05/17/2022 05:22:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.67 on epoch=669
05/17/2022 05:22:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.74 on epoch=674
05/17/2022 05:22:11 - INFO - __main__ - Global step 1350 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=674
05/17/2022 05:22:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.72 on epoch=679
05/17/2022 05:22:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.59 on epoch=684
05/17/2022 05:22:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.66 on epoch=689
05/17/2022 05:22:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.63 on epoch=694
05/17/2022 05:22:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.64 on epoch=699
05/17/2022 05:22:22 - INFO - __main__ - Global step 1400 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=699
05/17/2022 05:22:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.72 on epoch=704
05/17/2022 05:22:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.56 on epoch=709
05/17/2022 05:22:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.61 on epoch=714
05/17/2022 05:22:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=719
05/17/2022 05:22:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=724
05/17/2022 05:22:33 - INFO - __main__ - Global step 1450 Train loss 0.59 Classification-F1 0.3992490613266583 on epoch=724
05/17/2022 05:22:33 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=724, global_step=1450
05/17/2022 05:22:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.61 on epoch=729
05/17/2022 05:22:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=734
05/17/2022 05:22:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=739
05/17/2022 05:22:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.60 on epoch=744
05/17/2022 05:22:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.57 on epoch=749
05/17/2022 05:22:45 - INFO - __main__ - Global step 1500 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=749
05/17/2022 05:22:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.56 on epoch=754
05/17/2022 05:22:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.55 on epoch=759
05/17/2022 05:22:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.50 on epoch=764
05/17/2022 05:22:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.52 on epoch=769
05/17/2022 05:22:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.55 on epoch=774
05/17/2022 05:22:56 - INFO - __main__ - Global step 1550 Train loss 0.54 Classification-F1 0.3992490613266583 on epoch=774
05/17/2022 05:22:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=779
05/17/2022 05:23:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.46 on epoch=784
05/17/2022 05:23:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.51 on epoch=789
05/17/2022 05:23:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.61 on epoch=794
05/17/2022 05:23:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=799
05/17/2022 05:23:07 - INFO - __main__ - Global step 1600 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=799
05/17/2022 05:23:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=804
05/17/2022 05:23:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.56 on epoch=809
05/17/2022 05:23:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
05/17/2022 05:23:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.50 on epoch=819
05/17/2022 05:23:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.56 on epoch=824
05/17/2022 05:23:19 - INFO - __main__ - Global step 1650 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=824
05/17/2022 05:23:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=829
05/17/2022 05:23:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=834
05/17/2022 05:23:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=839
05/17/2022 05:23:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=844
05/17/2022 05:23:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=849
05/17/2022 05:23:30 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=849
05/17/2022 05:23:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=854
05/17/2022 05:23:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.46 on epoch=859
05/17/2022 05:23:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=864
05/17/2022 05:23:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=869
05/17/2022 05:23:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.47 on epoch=874
05/17/2022 05:23:41 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=874
05/17/2022 05:23:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=879
05/17/2022 05:23:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
05/17/2022 05:23:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=889
05/17/2022 05:23:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=894
05/17/2022 05:23:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=899
05/17/2022 05:23:51 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=899
05/17/2022 05:23:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=904
05/17/2022 05:23:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=909
05/17/2022 05:23:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=914
05/17/2022 05:23:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=919
05/17/2022 05:24:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=924
05/17/2022 05:24:02 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=924
05/17/2022 05:24:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=929
05/17/2022 05:24:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=934
05/17/2022 05:24:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
05/17/2022 05:24:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.50 on epoch=944
05/17/2022 05:24:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=949
05/17/2022 05:24:13 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=949
05/17/2022 05:24:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
05/17/2022 05:24:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
05/17/2022 05:24:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
05/17/2022 05:24:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=969
05/17/2022 05:24:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=974
05/17/2022 05:24:23 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=974
05/17/2022 05:24:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=979
05/17/2022 05:24:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/17/2022 05:24:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=989
05/17/2022 05:24:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=994
05/17/2022 05:24:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=999
05/17/2022 05:24:34 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.3191489361702127 on epoch=999
05/17/2022 05:24:34 - INFO - __main__ - save last model!
05/17/2022 05:24:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 05:24:34 - INFO - __main__ - Start tokenizing ... 12792 instances
05/17/2022 05:24:34 - INFO - __main__ - Printing 3 examples
05/17/2022 05:24:34 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 05:24:34 - INFO - __main__ - ['entailed']
05/17/2022 05:24:34 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 05:24:34 - INFO - __main__ - ['entailed']
05/17/2022 05:24:34 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/17/2022 05:24:34 - INFO - __main__ - ['entailed']
05/17/2022 05:24:34 - INFO - __main__ - Tokenizing Input ...
05/17/2022 05:25:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 05:25:12 - INFO - __main__ - Loaded 12792 examples from test data
05/17/2022 05:29:21 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.2_8_predictions.txt
05/17/2022 05:29:21 - INFO - __main__ - Classification-F1 on test data: 0.3372
05/17/2022 05:29:21 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.2, bsz=8, dev_performance=0.3992490613266583, test_performance=0.33724740154282734
05/20/2022 22:58:21 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-base-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
05/20/2022 22:58:21 - INFO - __main__ - models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact
05/20/2022 22:58:21 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-base-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
05/20/2022 22:58:21 - INFO - __main__ - models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact
05/20/2022 22:58:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/20/2022 22:58:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/20/2022 22:58:23 - INFO - __main__ - args.device: cuda:0
05/20/2022 22:58:23 - INFO - __main__ - Using 2 gpus
05/20/2022 22:58:23 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
05/20/2022 22:58:23 - INFO - __main__ - args.device: cuda:1
05/20/2022 22:58:23 - INFO - __main__ - Using 2 gpus
05/20/2022 22:58:23 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
05/20/2022 22:58:28 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.5, bsz=8 ...
05/20/2022 22:58:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 22:58:34 - INFO - __main__ - Printing 3 examples
05/20/2022 22:58:34 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/20/2022 22:58:34 - INFO - __main__ - ['refuted']
05/20/2022 22:58:34 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/20/2022 22:58:34 - INFO - __main__ - ['refuted']
05/20/2022 22:58:34 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/20/2022 22:58:34 - INFO - __main__ - ['refuted']
05/20/2022 22:58:34 - INFO - __main__ - Tokenizing Input ...
05/20/2022 22:58:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 22:58:34 - INFO - __main__ - Printing 3 examples
05/20/2022 22:58:34 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/20/2022 22:58:34 - INFO - __main__ - ['refuted']
05/20/2022 22:58:34 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/20/2022 22:58:34 - INFO - __main__ - ['refuted']
05/20/2022 22:58:34 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/20/2022 22:58:34 - INFO - __main__ - ['refuted']
05/20/2022 22:58:34 - INFO - __main__ - Tokenizing Input ...
05/20/2022 22:58:34 - INFO - __main__ - Tokenizing Output ...
05/20/2022 22:58:34 - INFO - __main__ - Loaded 32 examples from train data
05/20/2022 22:58:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 22:58:34 - INFO - __main__ - Printing 3 examples
05/20/2022 22:58:34 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/20/2022 22:58:34 - INFO - __main__ - ['refuted']
05/20/2022 22:58:34 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/20/2022 22:58:34 - INFO - __main__ - ['refuted']
05/20/2022 22:58:34 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/20/2022 22:58:34 - INFO - __main__ - ['refuted']
05/20/2022 22:58:34 - INFO - __main__ - Tokenizing Input ...
05/20/2022 22:58:35 - INFO - __main__ - Tokenizing Output ...
05/20/2022 22:58:35 - INFO - __main__ - Loaded 32 examples from train data
05/20/2022 22:58:35 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 22:58:35 - INFO - __main__ - Printing 3 examples
05/20/2022 22:58:35 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/20/2022 22:58:35 - INFO - __main__ - ['refuted']
05/20/2022 22:58:35 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/20/2022 22:58:35 - INFO - __main__ - ['refuted']
05/20/2022 22:58:35 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/20/2022 22:58:35 - INFO - __main__ - ['refuted']
05/20/2022 22:58:35 - INFO - __main__ - Tokenizing Input ...
05/20/2022 22:58:35 - INFO - __main__ - Tokenizing Output ...
05/20/2022 22:58:35 - INFO - __main__ - Tokenizing Output ...
05/20/2022 22:58:35 - INFO - __main__ - Loaded 32 examples from dev data
05/20/2022 22:58:35 - INFO - __main__ - Loaded 32 examples from dev data
05/20/2022 22:58:40 - INFO - __main__ - load prompt embedding from ckpt
05/20/2022 22:58:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/20/2022 22:58:41 - INFO - __main__ - Starting training!
05/20/2022 22:58:41 - INFO - __main__ - load prompt embedding from ckpt
05/20/2022 22:58:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/20/2022 22:58:51 - INFO - __main__ - Starting training!
05/20/2022 22:58:54 - INFO - __main__ - Step 10 Global step 10 Train loss 5.01 on epoch=4
05/20/2022 22:58:56 - INFO - __main__ - Step 20 Global step 20 Train loss 4.89 on epoch=9
05/20/2022 22:58:58 - INFO - __main__ - Step 30 Global step 30 Train loss 4.82 on epoch=14
05/20/2022 22:59:00 - INFO - __main__ - Step 40 Global step 40 Train loss 4.63 on epoch=19
05/20/2022 22:59:02 - INFO - __main__ - Step 50 Global step 50 Train loss 4.47 on epoch=24
05/20/2022 22:59:03 - INFO - __main__ - Global step 50 Train loss 4.76 Classification-F1 0.0 on epoch=24
05/20/2022 22:59:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/20/2022 22:59:05 - INFO - __main__ - Step 60 Global step 60 Train loss 4.52 on epoch=29
05/20/2022 22:59:07 - INFO - __main__ - Step 70 Global step 70 Train loss 4.22 on epoch=34
05/20/2022 22:59:08 - INFO - __main__ - Step 80 Global step 80 Train loss 4.14 on epoch=39
05/20/2022 22:59:10 - INFO - __main__ - Step 90 Global step 90 Train loss 4.04 on epoch=44
05/20/2022 22:59:12 - INFO - __main__ - Step 100 Global step 100 Train loss 3.88 on epoch=49
05/20/2022 22:59:15 - INFO - __main__ - Global step 100 Train loss 4.16 Classification-F1 0.0 on epoch=49
05/20/2022 22:59:17 - INFO - __main__ - Step 110 Global step 110 Train loss 3.72 on epoch=54
05/20/2022 22:59:19 - INFO - __main__ - Step 120 Global step 120 Train loss 3.64 on epoch=59
05/20/2022 22:59:21 - INFO - __main__ - Step 130 Global step 130 Train loss 3.54 on epoch=64
05/20/2022 22:59:23 - INFO - __main__ - Step 140 Global step 140 Train loss 3.41 on epoch=69
05/20/2022 22:59:25 - INFO - __main__ - Step 150 Global step 150 Train loss 3.34 on epoch=74
05/20/2022 22:59:28 - INFO - __main__ - Global step 150 Train loss 3.53 Classification-F1 0.21276595744680848 on epoch=74
05/20/2022 22:59:28 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.21276595744680848 on epoch=74, global_step=150
05/20/2022 22:59:30 - INFO - __main__ - Step 160 Global step 160 Train loss 3.17 on epoch=79
05/20/2022 22:59:32 - INFO - __main__ - Step 170 Global step 170 Train loss 2.98 on epoch=84
05/20/2022 22:59:34 - INFO - __main__ - Step 180 Global step 180 Train loss 2.84 on epoch=89
05/20/2022 22:59:36 - INFO - __main__ - Step 190 Global step 190 Train loss 2.77 on epoch=94
05/20/2022 22:59:38 - INFO - __main__ - Step 200 Global step 200 Train loss 2.64 on epoch=99
05/20/2022 22:59:42 - INFO - __main__ - Global step 200 Train loss 2.88 Classification-F1 0.3333333333333333 on epoch=99
05/20/2022 22:59:42 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.3333333333333333 on epoch=99, global_step=200
05/20/2022 22:59:44 - INFO - __main__ - Step 210 Global step 210 Train loss 2.52 on epoch=104
05/20/2022 22:59:46 - INFO - __main__ - Step 220 Global step 220 Train loss 2.41 on epoch=109
05/20/2022 22:59:48 - INFO - __main__ - Step 230 Global step 230 Train loss 2.36 on epoch=114
05/20/2022 22:59:50 - INFO - __main__ - Step 240 Global step 240 Train loss 2.26 on epoch=119
05/20/2022 22:59:52 - INFO - __main__ - Step 250 Global step 250 Train loss 2.11 on epoch=124
05/20/2022 22:59:56 - INFO - __main__ - Global step 250 Train loss 2.33 Classification-F1 0.3333333333333333 on epoch=124
05/20/2022 22:59:58 - INFO - __main__ - Step 260 Global step 260 Train loss 2.13 on epoch=129
05/20/2022 23:00:00 - INFO - __main__ - Step 270 Global step 270 Train loss 1.91 on epoch=134
05/20/2022 23:00:02 - INFO - __main__ - Step 280 Global step 280 Train loss 1.90 on epoch=139
05/20/2022 23:00:04 - INFO - __main__ - Step 290 Global step 290 Train loss 1.84 on epoch=144
05/20/2022 23:00:06 - INFO - __main__ - Step 300 Global step 300 Train loss 1.67 on epoch=149
05/20/2022 23:00:09 - INFO - __main__ - Global step 300 Train loss 1.89 Classification-F1 0.3333333333333333 on epoch=149
05/20/2022 23:00:11 - INFO - __main__ - Step 310 Global step 310 Train loss 1.73 on epoch=154
05/20/2022 23:00:13 - INFO - __main__ - Step 320 Global step 320 Train loss 1.55 on epoch=159
05/20/2022 23:00:15 - INFO - __main__ - Step 330 Global step 330 Train loss 1.60 on epoch=164
05/20/2022 23:00:17 - INFO - __main__ - Step 340 Global step 340 Train loss 1.55 on epoch=169
05/20/2022 23:00:19 - INFO - __main__ - Step 350 Global step 350 Train loss 1.38 on epoch=174
05/20/2022 23:00:23 - INFO - __main__ - Global step 350 Train loss 1.56 Classification-F1 0.3333333333333333 on epoch=174
05/20/2022 23:00:25 - INFO - __main__ - Step 360 Global step 360 Train loss 1.35 on epoch=179
05/20/2022 23:00:27 - INFO - __main__ - Step 370 Global step 370 Train loss 1.26 on epoch=184
05/20/2022 23:00:29 - INFO - __main__ - Step 380 Global step 380 Train loss 1.30 on epoch=189
05/20/2022 23:00:31 - INFO - __main__ - Step 390 Global step 390 Train loss 1.13 on epoch=194
05/20/2022 23:00:33 - INFO - __main__ - Step 400 Global step 400 Train loss 1.07 on epoch=199
05/20/2022 23:00:33 - INFO - __main__ - Global step 400 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=199
05/20/2022 23:00:35 - INFO - __main__ - Step 410 Global step 410 Train loss 1.04 on epoch=204
05/20/2022 23:00:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.98 on epoch=209
05/20/2022 23:00:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.95 on epoch=214
05/20/2022 23:00:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=219
05/20/2022 23:00:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.83 on epoch=224
05/20/2022 23:00:45 - INFO - __main__ - Global step 450 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=224
05/20/2022 23:00:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.88 on epoch=229
05/20/2022 23:00:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.87 on epoch=234
05/20/2022 23:00:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.79 on epoch=239
05/20/2022 23:00:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.73 on epoch=244
05/20/2022 23:00:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=249
05/20/2022 23:00:56 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=249
05/20/2022 23:00:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.62 on epoch=254
05/20/2022 23:01:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=259
05/20/2022 23:01:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.68 on epoch=264
05/20/2022 23:01:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.67 on epoch=269
05/20/2022 23:01:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.63 on epoch=274
05/20/2022 23:01:07 - INFO - __main__ - Global step 550 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=274
05/20/2022 23:01:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=279
05/20/2022 23:01:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.58 on epoch=284
05/20/2022 23:01:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=289
05/20/2022 23:01:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.60 on epoch=294
05/20/2022 23:01:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=299
05/20/2022 23:01:18 - INFO - __main__ - Global step 600 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=299
05/20/2022 23:01:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.54 on epoch=304
05/20/2022 23:01:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=309
05/20/2022 23:01:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=314
05/20/2022 23:01:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=319
05/20/2022 23:01:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=324
05/20/2022 23:01:28 - INFO - __main__ - Global step 650 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=324
05/20/2022 23:01:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=329
05/20/2022 23:01:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=334
05/20/2022 23:01:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=339
05/20/2022 23:01:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=344
05/20/2022 23:01:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=349
05/20/2022 23:01:38 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=349
05/20/2022 23:01:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=354
05/20/2022 23:01:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=359
05/20/2022 23:01:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=364
05/20/2022 23:01:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=369
05/20/2022 23:01:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=374
05/20/2022 23:01:49 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=374
05/20/2022 23:01:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=379
05/20/2022 23:01:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=384
05/20/2022 23:01:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=389
05/20/2022 23:01:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=394
05/20/2022 23:01:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=399
05/20/2022 23:02:00 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=399
05/20/2022 23:02:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=404
05/20/2022 23:02:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=409
05/20/2022 23:02:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
05/20/2022 23:02:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=419
05/20/2022 23:02:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=424
05/20/2022 23:02:11 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=424
05/20/2022 23:02:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=429
05/20/2022 23:02:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=434
05/20/2022 23:02:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=439
05/20/2022 23:02:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
05/20/2022 23:02:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=449
05/20/2022 23:02:21 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=449
05/20/2022 23:02:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=454
05/20/2022 23:02:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=459
05/20/2022 23:02:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=464
05/20/2022 23:02:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=469
05/20/2022 23:02:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=474
05/20/2022 23:02:32 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=474
05/20/2022 23:02:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=479
05/20/2022 23:02:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=484
05/20/2022 23:02:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=489
05/20/2022 23:02:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=494
05/20/2022 23:02:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
05/20/2022 23:02:43 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=499
05/20/2022 23:02:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=504
05/20/2022 23:02:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=509
05/20/2022 23:02:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=514
05/20/2022 23:02:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=519
05/20/2022 23:02:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=524
05/20/2022 23:02:53 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=524
05/20/2022 23:02:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=529
05/20/2022 23:02:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=534
05/20/2022 23:02:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=539
05/20/2022 23:03:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=544
05/20/2022 23:03:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=549
05/20/2022 23:03:04 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=549
05/20/2022 23:03:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=554
05/20/2022 23:03:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=559
05/20/2022 23:03:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=564
05/20/2022 23:03:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=569
05/20/2022 23:03:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=574
05/20/2022 23:03:14 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=574
05/20/2022 23:03:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=579
05/20/2022 23:03:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=584
05/20/2022 23:03:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
05/20/2022 23:03:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=594
05/20/2022 23:03:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=599
05/20/2022 23:03:25 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=599
05/20/2022 23:03:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
05/20/2022 23:03:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.31 on epoch=609
05/20/2022 23:03:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=614
05/20/2022 23:03:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.31 on epoch=619
05/20/2022 23:03:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=624
05/20/2022 23:03:36 - INFO - __main__ - Global step 1250 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=624
05/20/2022 23:03:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=629
05/20/2022 23:03:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=634
05/20/2022 23:03:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=639
05/20/2022 23:03:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=644
05/20/2022 23:03:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=649
05/20/2022 23:03:46 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=649
05/20/2022 23:03:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=654
05/20/2022 23:03:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=659
05/20/2022 23:03:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=664
05/20/2022 23:03:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=669
05/20/2022 23:03:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=674
05/20/2022 23:03:57 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=674
05/20/2022 23:03:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=679
05/20/2022 23:04:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=684
05/20/2022 23:04:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=689
05/20/2022 23:04:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=694
05/20/2022 23:04:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=699
05/20/2022 23:04:08 - INFO - __main__ - Global step 1400 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=699
05/20/2022 23:04:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=704
05/20/2022 23:04:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=709
05/20/2022 23:04:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=714
05/20/2022 23:04:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.28 on epoch=719
05/20/2022 23:04:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.27 on epoch=724
05/20/2022 23:04:18 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=724
05/20/2022 23:04:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=729
05/20/2022 23:04:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=734
05/20/2022 23:04:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=739
05/20/2022 23:04:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=744
05/20/2022 23:04:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=749
05/20/2022 23:04:29 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=749
05/20/2022 23:04:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=754
05/20/2022 23:04:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=759
05/20/2022 23:04:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=764
05/20/2022 23:04:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=769
05/20/2022 23:04:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=774
05/20/2022 23:04:40 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=774
05/20/2022 23:04:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=779
05/20/2022 23:04:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=784
05/20/2022 23:04:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.30 on epoch=789
05/20/2022 23:04:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=794
05/20/2022 23:04:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=799
05/20/2022 23:04:50 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=799
05/20/2022 23:04:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=804
05/20/2022 23:04:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=809
05/20/2022 23:04:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=814
05/20/2022 23:04:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=819
05/20/2022 23:05:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=824
05/20/2022 23:05:01 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=824
05/20/2022 23:05:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=829
05/20/2022 23:05:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=834
05/20/2022 23:05:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=839
05/20/2022 23:05:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=844
05/20/2022 23:05:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=849
05/20/2022 23:05:11 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=849
05/20/2022 23:05:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=854
05/20/2022 23:05:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=859
05/20/2022 23:05:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=864
05/20/2022 23:05:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=869
05/20/2022 23:05:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.30 on epoch=874
05/20/2022 23:05:22 - INFO - __main__ - Global step 1750 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=874
05/20/2022 23:05:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=879
05/20/2022 23:05:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=884
05/20/2022 23:05:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=889
05/20/2022 23:05:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=894
05/20/2022 23:05:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=899
05/20/2022 23:05:33 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=899
05/20/2022 23:05:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=904
05/20/2022 23:05:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=909
05/20/2022 23:05:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=914
05/20/2022 23:05:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.31 on epoch=919
05/20/2022 23:05:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=924
05/20/2022 23:05:43 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=924
05/20/2022 23:05:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=929
05/20/2022 23:05:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=934
05/20/2022 23:05:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=939
05/20/2022 23:05:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.28 on epoch=944
05/20/2022 23:05:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=949
05/20/2022 23:05:54 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=949
05/20/2022 23:05:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=954
05/20/2022 23:05:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=959
05/20/2022 23:06:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=964
05/20/2022 23:06:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=969
05/20/2022 23:06:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=974
05/20/2022 23:06:05 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=974
05/20/2022 23:06:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=979
05/20/2022 23:06:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=984
05/20/2022 23:06:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
05/20/2022 23:06:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=994
05/20/2022 23:06:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=999
05/20/2022 23:06:15 - INFO - __main__ - Global step 2000 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=999
05/20/2022 23:06:15 - INFO - __main__ - save last model!
05/20/2022 23:06:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/20/2022 23:06:15 - INFO - __main__ - Start tokenizing ... 12792 instances
05/20/2022 23:06:15 - INFO - __main__ - Printing 3 examples
05/20/2022 23:06:15 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:06:15 - INFO - __main__ - ['entailed']
05/20/2022 23:06:15 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:06:15 - INFO - __main__ - ['entailed']
05/20/2022 23:06:15 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:06:15 - INFO - __main__ - ['entailed']
05/20/2022 23:06:15 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:06:16 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:06:16 - INFO - __main__ - Printing 3 examples
05/20/2022 23:06:16 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/20/2022 23:06:16 - INFO - __main__ - ['refuted']
05/20/2022 23:06:16 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/20/2022 23:06:16 - INFO - __main__ - ['refuted']
05/20/2022 23:06:16 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/20/2022 23:06:16 - INFO - __main__ - ['refuted']
05/20/2022 23:06:16 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:06:16 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:06:16 - INFO - __main__ - Loaded 32 examples from train data
05/20/2022 23:06:16 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:06:16 - INFO - __main__ - Printing 3 examples
05/20/2022 23:06:16 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/20/2022 23:06:16 - INFO - __main__ - ['refuted']
05/20/2022 23:06:16 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/20/2022 23:06:16 - INFO - __main__ - ['refuted']
05/20/2022 23:06:16 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/20/2022 23:06:16 - INFO - __main__ - ['refuted']
05/20/2022 23:06:16 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:06:16 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:06:16 - INFO - __main__ - Loaded 32 examples from dev data
05/20/2022 23:06:22 - INFO - __main__ - load prompt embedding from ckpt
05/20/2022 23:06:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/20/2022 23:06:22 - INFO - __main__ - Starting training!
05/20/2022 23:06:40 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:06:52 - INFO - __main__ - Loaded 12792 examples from test data
05/20/2022 23:10:59 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.5_8_predictions.txt
05/20/2022 23:10:59 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/20/2022 23:10:59 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/20/2022 23:10:59 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.4, bsz=8 ...
05/20/2022 23:11:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:11:00 - INFO - __main__ - Printing 3 examples
05/20/2022 23:11:00 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/20/2022 23:11:00 - INFO - __main__ - ['refuted']
05/20/2022 23:11:00 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/20/2022 23:11:00 - INFO - __main__ - ['refuted']
05/20/2022 23:11:00 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/20/2022 23:11:00 - INFO - __main__ - ['refuted']
05/20/2022 23:11:00 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:11:00 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:11:00 - INFO - __main__ - Loaded 32 examples from train data
05/20/2022 23:11:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:11:00 - INFO - __main__ - Printing 3 examples
05/20/2022 23:11:00 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/20/2022 23:11:00 - INFO - __main__ - ['refuted']
05/20/2022 23:11:00 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/20/2022 23:11:00 - INFO - __main__ - ['refuted']
05/20/2022 23:11:00 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/20/2022 23:11:00 - INFO - __main__ - ['refuted']
05/20/2022 23:11:00 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:11:00 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:11:00 - INFO - __main__ - Loaded 32 examples from dev data
05/20/2022 23:11:07 - INFO - __main__ - load prompt embedding from ckpt
05/20/2022 23:11:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/20/2022 23:11:07 - INFO - __main__ - Starting training!
05/20/2022 23:11:09 - INFO - __main__ - Step 10 Global step 10 Train loss 5.06 on epoch=4
05/20/2022 23:11:11 - INFO - __main__ - Step 20 Global step 20 Train loss 4.99 on epoch=9
05/20/2022 23:11:13 - INFO - __main__ - Step 30 Global step 30 Train loss 4.88 on epoch=14
05/20/2022 23:11:15 - INFO - __main__ - Step 40 Global step 40 Train loss 4.85 on epoch=19
05/20/2022 23:11:17 - INFO - __main__ - Step 50 Global step 50 Train loss 4.59 on epoch=24
05/20/2022 23:11:19 - INFO - __main__ - Global step 50 Train loss 4.88 Classification-F1 0.0 on epoch=24
05/20/2022 23:11:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/20/2022 23:11:21 - INFO - __main__ - Step 60 Global step 60 Train loss 4.69 on epoch=29
05/20/2022 23:11:23 - INFO - __main__ - Step 70 Global step 70 Train loss 4.53 on epoch=34
05/20/2022 23:11:25 - INFO - __main__ - Step 80 Global step 80 Train loss 4.48 on epoch=39
05/20/2022 23:11:27 - INFO - __main__ - Step 90 Global step 90 Train loss 4.24 on epoch=44
05/20/2022 23:11:29 - INFO - __main__ - Step 100 Global step 100 Train loss 4.29 on epoch=49
05/20/2022 23:11:33 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/20/2022 23:11:35 - INFO - __main__ - Step 110 Global step 110 Train loss 4.08 on epoch=54
05/20/2022 23:11:37 - INFO - __main__ - Step 120 Global step 120 Train loss 4.21 on epoch=59
05/20/2022 23:11:39 - INFO - __main__ - Step 130 Global step 130 Train loss 3.97 on epoch=64
05/20/2022 23:11:41 - INFO - __main__ - Step 140 Global step 140 Train loss 3.87 on epoch=69
05/20/2022 23:11:43 - INFO - __main__ - Step 150 Global step 150 Train loss 3.72 on epoch=74
05/20/2022 23:11:47 - INFO - __main__ - Global step 150 Train loss 3.97 Classification-F1 0.0 on epoch=74
05/20/2022 23:11:49 - INFO - __main__ - Step 160 Global step 160 Train loss 3.66 on epoch=79
05/20/2022 23:11:51 - INFO - __main__ - Step 170 Global step 170 Train loss 3.57 on epoch=84
05/20/2022 23:11:53 - INFO - __main__ - Step 180 Global step 180 Train loss 3.59 on epoch=89
05/20/2022 23:11:55 - INFO - __main__ - Step 190 Global step 190 Train loss 3.51 on epoch=94
05/20/2022 23:11:57 - INFO - __main__ - Step 200 Global step 200 Train loss 3.35 on epoch=99
05/20/2022 23:12:02 - INFO - __main__ - Global step 200 Train loss 3.53 Classification-F1 0.07973421926910298 on epoch=99
05/20/2022 23:12:02 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.07973421926910298 on epoch=99, global_step=200
05/20/2022 23:12:04 - INFO - __main__ - Step 210 Global step 210 Train loss 3.34 on epoch=104
05/20/2022 23:12:06 - INFO - __main__ - Step 220 Global step 220 Train loss 3.29 on epoch=109
05/20/2022 23:12:08 - INFO - __main__ - Step 230 Global step 230 Train loss 3.04 on epoch=114
05/20/2022 23:12:10 - INFO - __main__ - Step 240 Global step 240 Train loss 3.09 on epoch=119
05/20/2022 23:12:13 - INFO - __main__ - Step 250 Global step 250 Train loss 2.88 on epoch=124
05/20/2022 23:12:16 - INFO - __main__ - Global step 250 Train loss 3.13 Classification-F1 0.1739130434782609 on epoch=124
05/20/2022 23:12:16 - INFO - __main__ - Saving model with best Classification-F1: 0.07973421926910298 -> 0.1739130434782609 on epoch=124, global_step=250
05/20/2022 23:12:18 - INFO - __main__ - Step 260 Global step 260 Train loss 2.89 on epoch=129
05/20/2022 23:12:20 - INFO - __main__ - Step 270 Global step 270 Train loss 2.84 on epoch=134
05/20/2022 23:12:23 - INFO - __main__ - Step 280 Global step 280 Train loss 2.83 on epoch=139
05/20/2022 23:12:25 - INFO - __main__ - Step 290 Global step 290 Train loss 2.72 on epoch=144
05/20/2022 23:12:27 - INFO - __main__ - Step 300 Global step 300 Train loss 2.50 on epoch=149
05/20/2022 23:12:30 - INFO - __main__ - Global step 300 Train loss 2.75 Classification-F1 0.3333333333333333 on epoch=149
05/20/2022 23:12:30 - INFO - __main__ - Saving model with best Classification-F1: 0.1739130434782609 -> 0.3333333333333333 on epoch=149, global_step=300
05/20/2022 23:12:32 - INFO - __main__ - Step 310 Global step 310 Train loss 2.59 on epoch=154
05/20/2022 23:12:34 - INFO - __main__ - Step 320 Global step 320 Train loss 2.30 on epoch=159
05/20/2022 23:12:36 - INFO - __main__ - Step 330 Global step 330 Train loss 2.41 on epoch=164
05/20/2022 23:12:38 - INFO - __main__ - Step 340 Global step 340 Train loss 2.39 on epoch=169
05/20/2022 23:12:40 - INFO - __main__ - Step 350 Global step 350 Train loss 2.35 on epoch=174
05/20/2022 23:12:44 - INFO - __main__ - Global step 350 Train loss 2.41 Classification-F1 0.3333333333333333 on epoch=174
05/20/2022 23:12:46 - INFO - __main__ - Step 360 Global step 360 Train loss 2.29 on epoch=179
05/20/2022 23:12:48 - INFO - __main__ - Step 370 Global step 370 Train loss 2.17 on epoch=184
05/20/2022 23:12:50 - INFO - __main__ - Step 380 Global step 380 Train loss 2.15 on epoch=189
05/20/2022 23:12:52 - INFO - __main__ - Step 390 Global step 390 Train loss 2.05 on epoch=194
05/20/2022 23:12:54 - INFO - __main__ - Step 400 Global step 400 Train loss 1.97 on epoch=199
05/20/2022 23:13:01 - INFO - __main__ - Global step 400 Train loss 2.13 Classification-F1 0.3333333333333333 on epoch=199
05/20/2022 23:13:03 - INFO - __main__ - Step 410 Global step 410 Train loss 2.00 on epoch=204
05/20/2022 23:13:05 - INFO - __main__ - Step 420 Global step 420 Train loss 1.99 on epoch=209
05/20/2022 23:13:07 - INFO - __main__ - Step 430 Global step 430 Train loss 2.04 on epoch=214
05/20/2022 23:13:09 - INFO - __main__ - Step 440 Global step 440 Train loss 1.99 on epoch=219
05/20/2022 23:13:11 - INFO - __main__ - Step 450 Global step 450 Train loss 1.96 on epoch=224
05/20/2022 23:13:18 - INFO - __main__ - Global step 450 Train loss 2.00 Classification-F1 0.3333333333333333 on epoch=224
05/20/2022 23:13:20 - INFO - __main__ - Step 460 Global step 460 Train loss 1.81 on epoch=229
05/20/2022 23:13:22 - INFO - __main__ - Step 470 Global step 470 Train loss 1.71 on epoch=234
05/20/2022 23:13:24 - INFO - __main__ - Step 480 Global step 480 Train loss 1.64 on epoch=239
05/20/2022 23:13:26 - INFO - __main__ - Step 490 Global step 490 Train loss 1.71 on epoch=244
05/20/2022 23:13:28 - INFO - __main__ - Step 500 Global step 500 Train loss 1.60 on epoch=249
05/20/2022 23:13:31 - INFO - __main__ - Global step 500 Train loss 1.69 Classification-F1 0.3333333333333333 on epoch=249
05/20/2022 23:13:33 - INFO - __main__ - Step 510 Global step 510 Train loss 1.59 on epoch=254
05/20/2022 23:13:36 - INFO - __main__ - Step 520 Global step 520 Train loss 1.53 on epoch=259
05/20/2022 23:13:38 - INFO - __main__ - Step 530 Global step 530 Train loss 1.49 on epoch=264
05/20/2022 23:13:40 - INFO - __main__ - Step 540 Global step 540 Train loss 1.47 on epoch=269
05/20/2022 23:13:42 - INFO - __main__ - Step 550 Global step 550 Train loss 1.48 on epoch=274
05/20/2022 23:13:45 - INFO - __main__ - Global step 550 Train loss 1.51 Classification-F1 0.3333333333333333 on epoch=274
05/20/2022 23:13:47 - INFO - __main__ - Step 560 Global step 560 Train loss 1.47 on epoch=279
05/20/2022 23:13:49 - INFO - __main__ - Step 570 Global step 570 Train loss 1.39 on epoch=284
05/20/2022 23:13:51 - INFO - __main__ - Step 580 Global step 580 Train loss 1.33 on epoch=289
05/20/2022 23:13:53 - INFO - __main__ - Step 590 Global step 590 Train loss 1.24 on epoch=294
05/20/2022 23:13:55 - INFO - __main__ - Step 600 Global step 600 Train loss 1.30 on epoch=299
05/20/2022 23:13:56 - INFO - __main__ - Global step 600 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=299
05/20/2022 23:13:58 - INFO - __main__ - Step 610 Global step 610 Train loss 1.25 on epoch=304
05/20/2022 23:14:00 - INFO - __main__ - Step 620 Global step 620 Train loss 1.20 on epoch=309
05/20/2022 23:14:03 - INFO - __main__ - Step 630 Global step 630 Train loss 1.18 on epoch=314
05/20/2022 23:14:05 - INFO - __main__ - Step 640 Global step 640 Train loss 1.29 on epoch=319
05/20/2022 23:14:07 - INFO - __main__ - Step 650 Global step 650 Train loss 1.13 on epoch=324
05/20/2022 23:14:08 - INFO - __main__ - Global step 650 Train loss 1.21 Classification-F1 0.3333333333333333 on epoch=324
05/20/2022 23:14:10 - INFO - __main__ - Step 660 Global step 660 Train loss 1.14 on epoch=329
05/20/2022 23:14:12 - INFO - __main__ - Step 670 Global step 670 Train loss 1.08 on epoch=334
05/20/2022 23:14:14 - INFO - __main__ - Step 680 Global step 680 Train loss 1.15 on epoch=339
05/20/2022 23:14:16 - INFO - __main__ - Step 690 Global step 690 Train loss 1.15 on epoch=344
05/20/2022 23:14:18 - INFO - __main__ - Step 700 Global step 700 Train loss 1.08 on epoch=349
05/20/2022 23:14:19 - INFO - __main__ - Global step 700 Train loss 1.12 Classification-F1 0.3333333333333333 on epoch=349
05/20/2022 23:14:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.95 on epoch=354
05/20/2022 23:14:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.98 on epoch=359
05/20/2022 23:14:25 - INFO - __main__ - Step 730 Global step 730 Train loss 1.05 on epoch=364
05/20/2022 23:14:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.99 on epoch=369
05/20/2022 23:14:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.97 on epoch=374
05/20/2022 23:14:31 - INFO - __main__ - Global step 750 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=374
05/20/2022 23:14:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.88 on epoch=379
05/20/2022 23:14:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=384
05/20/2022 23:14:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.91 on epoch=389
05/20/2022 23:14:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.93 on epoch=394
05/20/2022 23:14:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.92 on epoch=399
05/20/2022 23:14:42 - INFO - __main__ - Global step 800 Train loss 0.91 Classification-F1 0.3333333333333333 on epoch=399
05/20/2022 23:14:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=404
05/20/2022 23:14:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.88 on epoch=409
05/20/2022 23:14:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.85 on epoch=414
05/20/2022 23:14:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.81 on epoch=419
05/20/2022 23:14:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.85 on epoch=424
05/20/2022 23:14:53 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=424
05/20/2022 23:14:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.74 on epoch=429
05/20/2022 23:14:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=434
05/20/2022 23:15:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.82 on epoch=439
05/20/2022 23:15:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.73 on epoch=444
05/20/2022 23:15:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.76 on epoch=449
05/20/2022 23:15:05 - INFO - __main__ - Global step 900 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=449
05/20/2022 23:15:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.73 on epoch=454
05/20/2022 23:15:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.83 on epoch=459
05/20/2022 23:15:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.69 on epoch=464
05/20/2022 23:15:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.74 on epoch=469
05/20/2022 23:15:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.72 on epoch=474
05/20/2022 23:15:16 - INFO - __main__ - Global step 950 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=474
05/20/2022 23:15:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.72 on epoch=479
05/20/2022 23:15:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.74 on epoch=484
05/20/2022 23:15:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.69 on epoch=489
05/20/2022 23:15:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.68 on epoch=494
05/20/2022 23:15:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.78 on epoch=499
05/20/2022 23:15:27 - INFO - __main__ - Global step 1000 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=499
05/20/2022 23:15:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.62 on epoch=504
05/20/2022 23:15:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.78 on epoch=509
05/20/2022 23:15:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.77 on epoch=514
05/20/2022 23:15:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.62 on epoch=519
05/20/2022 23:15:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.66 on epoch=524
05/20/2022 23:15:38 - INFO - __main__ - Global step 1050 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=524
05/20/2022 23:15:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.64 on epoch=529
05/20/2022 23:15:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.65 on epoch=534
05/20/2022 23:15:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.62 on epoch=539
05/20/2022 23:15:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.57 on epoch=544
05/20/2022 23:15:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.63 on epoch=549
05/20/2022 23:15:49 - INFO - __main__ - Global step 1100 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=549
05/20/2022 23:15:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=554
05/20/2022 23:15:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.65 on epoch=559
05/20/2022 23:15:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.56 on epoch=564
05/20/2022 23:15:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.53 on epoch=569
05/20/2022 23:15:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.60 on epoch=574
05/20/2022 23:15:59 - INFO - __main__ - Global step 1150 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=574
05/20/2022 23:16:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=579
05/20/2022 23:16:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.58 on epoch=584
05/20/2022 23:16:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=589
05/20/2022 23:16:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.63 on epoch=594
05/20/2022 23:16:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.53 on epoch=599
05/20/2022 23:16:10 - INFO - __main__ - Global step 1200 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=599
05/20/2022 23:16:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.52 on epoch=604
05/20/2022 23:16:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=609
05/20/2022 23:16:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.53 on epoch=614
05/20/2022 23:16:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.58 on epoch=619
05/20/2022 23:16:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=624
05/20/2022 23:16:21 - INFO - __main__ - Global step 1250 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=624
05/20/2022 23:16:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=629
05/20/2022 23:16:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.58 on epoch=634
05/20/2022 23:16:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=639
05/20/2022 23:16:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.57 on epoch=644
05/20/2022 23:16:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.54 on epoch=649
05/20/2022 23:16:32 - INFO - __main__ - Global step 1300 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=649
05/20/2022 23:16:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=654
05/20/2022 23:16:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=659
05/20/2022 23:16:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.50 on epoch=664
05/20/2022 23:16:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.53 on epoch=669
05/20/2022 23:16:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=674
05/20/2022 23:16:43 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=674
05/20/2022 23:16:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=679
05/20/2022 23:16:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.53 on epoch=684
05/20/2022 23:16:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=689
05/20/2022 23:16:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=694
05/20/2022 23:16:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=699
05/20/2022 23:16:54 - INFO - __main__ - Global step 1400 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=699
05/20/2022 23:16:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=704
05/20/2022 23:16:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
05/20/2022 23:17:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=714
05/20/2022 23:17:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.53 on epoch=719
05/20/2022 23:17:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
05/20/2022 23:17:05 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=724
05/20/2022 23:17:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=729
05/20/2022 23:17:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=734
05/20/2022 23:17:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/20/2022 23:17:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=744
05/20/2022 23:17:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=749
05/20/2022 23:17:16 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=749
05/20/2022 23:17:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.47 on epoch=754
05/20/2022 23:17:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=759
05/20/2022 23:17:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=764
05/20/2022 23:17:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=769
05/20/2022 23:17:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=774
05/20/2022 23:17:26 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=774
05/20/2022 23:17:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.47 on epoch=779
05/20/2022 23:17:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
05/20/2022 23:17:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=789
05/20/2022 23:17:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=794
05/20/2022 23:17:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=799
05/20/2022 23:17:37 - INFO - __main__ - Global step 1600 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=799
05/20/2022 23:17:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=804
05/20/2022 23:17:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=809
05/20/2022 23:17:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=814
05/20/2022 23:17:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
05/20/2022 23:17:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=824
05/20/2022 23:17:48 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=824
05/20/2022 23:17:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=829
05/20/2022 23:17:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=834
05/20/2022 23:17:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
05/20/2022 23:17:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=844
05/20/2022 23:17:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
05/20/2022 23:17:59 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=849
05/20/2022 23:18:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=854
05/20/2022 23:18:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=859
05/20/2022 23:18:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/20/2022 23:18:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=869
05/20/2022 23:18:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=874
05/20/2022 23:18:10 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=874
05/20/2022 23:18:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=879
05/20/2022 23:18:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=884
05/20/2022 23:18:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
05/20/2022 23:18:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=894
05/20/2022 23:18:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
05/20/2022 23:18:20 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=899
05/20/2022 23:18:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=904
05/20/2022 23:18:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=909
05/20/2022 23:18:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=914
05/20/2022 23:18:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=919
05/20/2022 23:18:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=924
05/20/2022 23:18:31 - INFO - __main__ - Global step 1850 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=924
05/20/2022 23:18:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
05/20/2022 23:18:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=934
05/20/2022 23:18:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=939
05/20/2022 23:18:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=944
05/20/2022 23:18:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/20/2022 23:18:41 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=949
05/20/2022 23:18:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
05/20/2022 23:18:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
05/20/2022 23:18:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.34 on epoch=964
05/20/2022 23:18:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=969
05/20/2022 23:18:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=974
05/20/2022 23:18:52 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=974
05/20/2022 23:18:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
05/20/2022 23:18:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/20/2022 23:18:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/20/2022 23:19:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=994
05/20/2022 23:19:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=999
05/20/2022 23:19:03 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=999
05/20/2022 23:19:03 - INFO - __main__ - save last model!
05/20/2022 23:19:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/20/2022 23:19:03 - INFO - __main__ - Start tokenizing ... 12792 instances
05/20/2022 23:19:03 - INFO - __main__ - Printing 3 examples
05/20/2022 23:19:03 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:19:03 - INFO - __main__ - ['entailed']
05/20/2022 23:19:03 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:19:03 - INFO - __main__ - ['entailed']
05/20/2022 23:19:03 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:19:03 - INFO - __main__ - ['entailed']
05/20/2022 23:19:03 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:19:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:19:03 - INFO - __main__ - Printing 3 examples
05/20/2022 23:19:03 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/20/2022 23:19:03 - INFO - __main__ - ['refuted']
05/20/2022 23:19:03 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/20/2022 23:19:03 - INFO - __main__ - ['refuted']
05/20/2022 23:19:03 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/20/2022 23:19:03 - INFO - __main__ - ['refuted']
05/20/2022 23:19:03 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:19:03 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:19:03 - INFO - __main__ - Loaded 32 examples from train data
05/20/2022 23:19:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:19:03 - INFO - __main__ - Printing 3 examples
05/20/2022 23:19:03 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/20/2022 23:19:03 - INFO - __main__ - ['refuted']
05/20/2022 23:19:03 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/20/2022 23:19:03 - INFO - __main__ - ['refuted']
05/20/2022 23:19:03 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/20/2022 23:19:03 - INFO - __main__ - ['refuted']
05/20/2022 23:19:03 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:19:03 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:19:03 - INFO - __main__ - Loaded 32 examples from dev data
05/20/2022 23:19:09 - INFO - __main__ - load prompt embedding from ckpt
05/20/2022 23:19:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/20/2022 23:19:09 - INFO - __main__ - Starting training!
05/20/2022 23:19:28 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:19:44 - INFO - __main__ - Loaded 12792 examples from test data
05/20/2022 23:23:55 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.4_8_predictions.txt
05/20/2022 23:23:55 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/20/2022 23:23:55 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/20/2022 23:23:55 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.3, bsz=8 ...
05/20/2022 23:23:56 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:23:56 - INFO - __main__ - Printing 3 examples
05/20/2022 23:23:56 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/20/2022 23:23:56 - INFO - __main__ - ['refuted']
05/20/2022 23:23:56 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/20/2022 23:23:56 - INFO - __main__ - ['refuted']
05/20/2022 23:23:56 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/20/2022 23:23:56 - INFO - __main__ - ['refuted']
05/20/2022 23:23:56 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:23:56 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:23:56 - INFO - __main__ - Loaded 32 examples from train data
05/20/2022 23:23:56 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:23:56 - INFO - __main__ - Printing 3 examples
05/20/2022 23:23:56 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/20/2022 23:23:56 - INFO - __main__ - ['refuted']
05/20/2022 23:23:56 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/20/2022 23:23:56 - INFO - __main__ - ['refuted']
05/20/2022 23:23:56 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/20/2022 23:23:56 - INFO - __main__ - ['refuted']
05/20/2022 23:23:56 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:23:56 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:23:56 - INFO - __main__ - Loaded 32 examples from dev data
05/20/2022 23:24:02 - INFO - __main__ - load prompt embedding from ckpt
05/20/2022 23:24:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/20/2022 23:24:03 - INFO - __main__ - Starting training!
05/20/2022 23:24:05 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/20/2022 23:24:07 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/20/2022 23:24:10 - INFO - __main__ - Step 30 Global step 30 Train loss 4.98 on epoch=14
05/20/2022 23:24:11 - INFO - __main__ - Step 40 Global step 40 Train loss 4.81 on epoch=19
05/20/2022 23:24:13 - INFO - __main__ - Step 50 Global step 50 Train loss 4.73 on epoch=24
05/20/2022 23:24:15 - INFO - __main__ - Global step 50 Train loss 4.90 Classification-F1 0.0 on epoch=24
05/20/2022 23:24:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/20/2022 23:24:17 - INFO - __main__ - Step 60 Global step 60 Train loss 4.69 on epoch=29
05/20/2022 23:24:19 - INFO - __main__ - Step 70 Global step 70 Train loss 4.64 on epoch=34
05/20/2022 23:24:21 - INFO - __main__ - Step 80 Global step 80 Train loss 4.54 on epoch=39
05/20/2022 23:24:23 - INFO - __main__ - Step 90 Global step 90 Train loss 4.52 on epoch=44
05/20/2022 23:24:25 - INFO - __main__ - Step 100 Global step 100 Train loss 4.46 on epoch=49
05/20/2022 23:24:26 - INFO - __main__ - Global step 100 Train loss 4.57 Classification-F1 0.0 on epoch=49
05/20/2022 23:24:28 - INFO - __main__ - Step 110 Global step 110 Train loss 4.43 on epoch=54
05/20/2022 23:24:30 - INFO - __main__ - Step 120 Global step 120 Train loss 4.40 on epoch=59
05/20/2022 23:24:32 - INFO - __main__ - Step 130 Global step 130 Train loss 4.23 on epoch=64
05/20/2022 23:24:34 - INFO - __main__ - Step 140 Global step 140 Train loss 4.11 on epoch=69
05/20/2022 23:24:36 - INFO - __main__ - Step 150 Global step 150 Train loss 4.04 on epoch=74
05/20/2022 23:24:38 - INFO - __main__ - Global step 150 Train loss 4.24 Classification-F1 0.0 on epoch=74
05/20/2022 23:24:40 - INFO - __main__ - Step 160 Global step 160 Train loss 4.00 on epoch=79
05/20/2022 23:24:42 - INFO - __main__ - Step 170 Global step 170 Train loss 3.86 on epoch=84
05/20/2022 23:24:44 - INFO - __main__ - Step 180 Global step 180 Train loss 3.70 on epoch=89
05/20/2022 23:24:46 - INFO - __main__ - Step 190 Global step 190 Train loss 3.73 on epoch=94
05/20/2022 23:24:48 - INFO - __main__ - Step 200 Global step 200 Train loss 3.67 on epoch=99
05/20/2022 23:24:49 - INFO - __main__ - Global step 200 Train loss 3.79 Classification-F1 0.0 on epoch=99
05/20/2022 23:24:51 - INFO - __main__ - Step 210 Global step 210 Train loss 3.64 on epoch=104
05/20/2022 23:24:53 - INFO - __main__ - Step 220 Global step 220 Train loss 3.49 on epoch=109
05/20/2022 23:24:55 - INFO - __main__ - Step 230 Global step 230 Train loss 3.41 on epoch=114
05/20/2022 23:24:57 - INFO - __main__ - Step 240 Global step 240 Train loss 3.38 on epoch=119
05/20/2022 23:24:59 - INFO - __main__ - Step 250 Global step 250 Train loss 3.28 on epoch=124
05/20/2022 23:25:01 - INFO - __main__ - Global step 250 Train loss 3.44 Classification-F1 0.16666666666666669 on epoch=124
05/20/2022 23:25:01 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.16666666666666669 on epoch=124, global_step=250
05/20/2022 23:25:03 - INFO - __main__ - Step 260 Global step 260 Train loss 3.10 on epoch=129
05/20/2022 23:25:05 - INFO - __main__ - Step 270 Global step 270 Train loss 3.12 on epoch=134
05/20/2022 23:25:07 - INFO - __main__ - Step 280 Global step 280 Train loss 3.04 on epoch=139
05/20/2022 23:25:09 - INFO - __main__ - Step 290 Global step 290 Train loss 2.97 on epoch=144
05/20/2022 23:25:11 - INFO - __main__ - Step 300 Global step 300 Train loss 2.84 on epoch=149
05/20/2022 23:25:14 - INFO - __main__ - Global step 300 Train loss 3.01 Classification-F1 0.3333333333333333 on epoch=149
05/20/2022 23:25:14 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666669 -> 0.3333333333333333 on epoch=149, global_step=300
05/20/2022 23:25:16 - INFO - __main__ - Step 310 Global step 310 Train loss 2.76 on epoch=154
05/20/2022 23:25:18 - INFO - __main__ - Step 320 Global step 320 Train loss 2.74 on epoch=159
05/20/2022 23:25:20 - INFO - __main__ - Step 330 Global step 330 Train loss 2.53 on epoch=164
05/20/2022 23:25:22 - INFO - __main__ - Step 340 Global step 340 Train loss 2.35 on epoch=169
05/20/2022 23:25:24 - INFO - __main__ - Step 350 Global step 350 Train loss 2.26 on epoch=174
05/20/2022 23:25:27 - INFO - __main__ - Global step 350 Train loss 2.53 Classification-F1 0.3333333333333333 on epoch=174
05/20/2022 23:25:29 - INFO - __main__ - Step 360 Global step 360 Train loss 2.25 on epoch=179
05/20/2022 23:25:31 - INFO - __main__ - Step 370 Global step 370 Train loss 2.33 on epoch=184
05/20/2022 23:25:33 - INFO - __main__ - Step 380 Global step 380 Train loss 2.10 on epoch=189
05/20/2022 23:25:35 - INFO - __main__ - Step 390 Global step 390 Train loss 2.02 on epoch=194
05/20/2022 23:25:37 - INFO - __main__ - Step 400 Global step 400 Train loss 1.97 on epoch=199
05/20/2022 23:25:40 - INFO - __main__ - Global step 400 Train loss 2.14 Classification-F1 0.3333333333333333 on epoch=199
05/20/2022 23:25:42 - INFO - __main__ - Step 410 Global step 410 Train loss 1.92 on epoch=204
05/20/2022 23:25:44 - INFO - __main__ - Step 420 Global step 420 Train loss 1.88 on epoch=209
05/20/2022 23:25:46 - INFO - __main__ - Step 430 Global step 430 Train loss 1.84 on epoch=214
05/20/2022 23:25:48 - INFO - __main__ - Step 440 Global step 440 Train loss 1.75 on epoch=219
05/20/2022 23:25:50 - INFO - __main__ - Step 450 Global step 450 Train loss 1.66 on epoch=224
05/20/2022 23:25:52 - INFO - __main__ - Global step 450 Train loss 1.81 Classification-F1 0.3333333333333333 on epoch=224
05/20/2022 23:25:54 - INFO - __main__ - Step 460 Global step 460 Train loss 1.61 on epoch=229
05/20/2022 23:25:56 - INFO - __main__ - Step 470 Global step 470 Train loss 1.55 on epoch=234
05/20/2022 23:25:58 - INFO - __main__ - Step 480 Global step 480 Train loss 1.51 on epoch=239
05/20/2022 23:26:00 - INFO - __main__ - Step 490 Global step 490 Train loss 1.53 on epoch=244
05/20/2022 23:26:02 - INFO - __main__ - Step 500 Global step 500 Train loss 1.37 on epoch=249
05/20/2022 23:26:04 - INFO - __main__ - Global step 500 Train loss 1.51 Classification-F1 0.3333333333333333 on epoch=249
05/20/2022 23:26:06 - INFO - __main__ - Step 510 Global step 510 Train loss 1.41 on epoch=254
05/20/2022 23:26:08 - INFO - __main__ - Step 520 Global step 520 Train loss 1.32 on epoch=259
05/20/2022 23:26:10 - INFO - __main__ - Step 530 Global step 530 Train loss 1.32 on epoch=264
05/20/2022 23:26:12 - INFO - __main__ - Step 540 Global step 540 Train loss 1.21 on epoch=269
05/20/2022 23:26:14 - INFO - __main__ - Step 550 Global step 550 Train loss 1.13 on epoch=274
05/20/2022 23:26:15 - INFO - __main__ - Global step 550 Train loss 1.28 Classification-F1 0.3333333333333333 on epoch=274
05/20/2022 23:26:17 - INFO - __main__ - Step 560 Global step 560 Train loss 1.17 on epoch=279
05/20/2022 23:26:20 - INFO - __main__ - Step 570 Global step 570 Train loss 1.10 on epoch=284
05/20/2022 23:26:22 - INFO - __main__ - Step 580 Global step 580 Train loss 1.06 on epoch=289
05/20/2022 23:26:24 - INFO - __main__ - Step 590 Global step 590 Train loss 1.13 on epoch=294
05/20/2022 23:26:26 - INFO - __main__ - Step 600 Global step 600 Train loss 1.13 on epoch=299
05/20/2022 23:26:27 - INFO - __main__ - Global step 600 Train loss 1.12 Classification-F1 0.3333333333333333 on epoch=299
05/20/2022 23:26:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.99 on epoch=304
05/20/2022 23:26:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.92 on epoch=309
05/20/2022 23:26:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.96 on epoch=314
05/20/2022 23:26:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.87 on epoch=319
05/20/2022 23:26:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.94 on epoch=324
05/20/2022 23:26:39 - INFO - __main__ - Global step 650 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=324
05/20/2022 23:26:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.95 on epoch=329
05/20/2022 23:26:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.84 on epoch=334
05/20/2022 23:26:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.87 on epoch=339
05/20/2022 23:26:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.79 on epoch=344
05/20/2022 23:26:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.79 on epoch=349
05/20/2022 23:26:50 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=349
05/20/2022 23:26:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.78 on epoch=354
05/20/2022 23:26:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.76 on epoch=359
05/20/2022 23:26:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.76 on epoch=364
05/20/2022 23:26:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.75 on epoch=369
05/20/2022 23:27:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.72 on epoch=374
05/20/2022 23:27:02 - INFO - __main__ - Global step 750 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=374
05/20/2022 23:27:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.73 on epoch=379
05/20/2022 23:27:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=384
05/20/2022 23:27:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.71 on epoch=389
05/20/2022 23:27:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.58 on epoch=394
05/20/2022 23:27:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.63 on epoch=399
05/20/2022 23:27:13 - INFO - __main__ - Global step 800 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=399
05/20/2022 23:27:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.68 on epoch=404
05/20/2022 23:27:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.69 on epoch=409
05/20/2022 23:27:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.71 on epoch=414
05/20/2022 23:27:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.65 on epoch=419
05/20/2022 23:27:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.69 on epoch=424
05/20/2022 23:27:25 - INFO - __main__ - Global step 850 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=424
05/20/2022 23:27:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.55 on epoch=429
05/20/2022 23:27:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.57 on epoch=434
05/20/2022 23:27:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.56 on epoch=439
05/20/2022 23:27:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.59 on epoch=444
05/20/2022 23:27:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=449
05/20/2022 23:27:37 - INFO - __main__ - Global step 900 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=449
05/20/2022 23:27:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.60 on epoch=454
05/20/2022 23:27:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.60 on epoch=459
05/20/2022 23:27:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.56 on epoch=464
05/20/2022 23:27:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.53 on epoch=469
05/20/2022 23:27:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.57 on epoch=474
05/20/2022 23:27:48 - INFO - __main__ - Global step 950 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=474
05/20/2022 23:27:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.53 on epoch=479
05/20/2022 23:27:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=484
05/20/2022 23:27:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=489
05/20/2022 23:27:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=494
05/20/2022 23:27:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.55 on epoch=499
05/20/2022 23:27:59 - INFO - __main__ - Global step 1000 Train loss 0.51 Classification-F1 0.3992490613266583 on epoch=499
05/20/2022 23:27:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=499, global_step=1000
05/20/2022 23:28:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=504
05/20/2022 23:28:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.53 on epoch=509
05/20/2022 23:28:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=514
05/20/2022 23:28:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=519
05/20/2022 23:28:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=524
05/20/2022 23:28:10 - INFO - __main__ - Global step 1050 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=524
05/20/2022 23:28:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.52 on epoch=529
05/20/2022 23:28:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.50 on epoch=534
05/20/2022 23:28:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.56 on epoch=539
05/20/2022 23:28:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=544
05/20/2022 23:28:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=549
05/20/2022 23:28:21 - INFO - __main__ - Global step 1100 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=549
05/20/2022 23:28:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=554
05/20/2022 23:28:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=559
05/20/2022 23:28:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.49 on epoch=564
05/20/2022 23:28:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=569
05/20/2022 23:28:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=574
05/20/2022 23:28:32 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=574
05/20/2022 23:28:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=579
05/20/2022 23:28:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=584
05/20/2022 23:28:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
05/20/2022 23:28:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.50 on epoch=594
05/20/2022 23:28:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=599
05/20/2022 23:28:43 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=599
05/20/2022 23:28:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.48 on epoch=604
05/20/2022 23:28:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
05/20/2022 23:28:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=614
05/20/2022 23:28:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=619
05/20/2022 23:28:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=624
05/20/2022 23:28:54 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=624
05/20/2022 23:28:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=629
05/20/2022 23:28:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.52 on epoch=634
05/20/2022 23:29:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=639
05/20/2022 23:29:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=644
05/20/2022 23:29:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.49 on epoch=649
05/20/2022 23:29:05 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=649
05/20/2022 23:29:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=654
05/20/2022 23:29:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=659
05/20/2022 23:29:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
05/20/2022 23:29:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=669
05/20/2022 23:29:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=674
05/20/2022 23:29:16 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=674
05/20/2022 23:29:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=679
05/20/2022 23:29:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
05/20/2022 23:29:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=689
05/20/2022 23:29:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=694
05/20/2022 23:29:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=699
05/20/2022 23:29:27 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=699
05/20/2022 23:29:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=704
05/20/2022 23:29:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=709
05/20/2022 23:29:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
05/20/2022 23:29:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=719
05/20/2022 23:29:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
05/20/2022 23:29:38 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=724
05/20/2022 23:29:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=729
05/20/2022 23:29:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=734
05/20/2022 23:29:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/20/2022 23:29:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
05/20/2022 23:29:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=749
05/20/2022 23:29:49 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=749
05/20/2022 23:29:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=754
05/20/2022 23:29:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=759
05/20/2022 23:29:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=764
05/20/2022 23:29:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=769
05/20/2022 23:29:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=774
05/20/2022 23:30:00 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=774
05/20/2022 23:30:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=779
05/20/2022 23:30:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=784
05/20/2022 23:30:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
05/20/2022 23:30:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=794
05/20/2022 23:30:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=799
05/20/2022 23:30:11 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=799
05/20/2022 23:30:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=804
05/20/2022 23:30:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=809
05/20/2022 23:30:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=814
05/20/2022 23:30:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
05/20/2022 23:30:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=824
05/20/2022 23:30:21 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=824
05/20/2022 23:30:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
05/20/2022 23:30:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=834
05/20/2022 23:30:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
05/20/2022 23:30:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
05/20/2022 23:30:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=849
05/20/2022 23:30:32 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.4589371980676329 on epoch=849
05/20/2022 23:30:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4589371980676329 on epoch=849, global_step=1700
05/20/2022 23:30:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/20/2022 23:30:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=859
05/20/2022 23:30:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=864
05/20/2022 23:30:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=869
05/20/2022 23:30:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
05/20/2022 23:30:43 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.4181818181818182 on epoch=874
05/20/2022 23:30:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=879
05/20/2022 23:30:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=884
05/20/2022 23:30:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=889
05/20/2022 23:30:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
05/20/2022 23:30:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=899
05/20/2022 23:30:53 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.3816425120772947 on epoch=899
05/20/2022 23:30:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=904
05/20/2022 23:30:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=909
05/20/2022 23:30:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=914
05/20/2022 23:31:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=919
05/20/2022 23:31:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
05/20/2022 23:31:04 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.4385964912280702 on epoch=924
05/20/2022 23:31:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=929
05/20/2022 23:31:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=934
05/20/2022 23:31:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=939
05/20/2022 23:31:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
05/20/2022 23:31:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=949
05/20/2022 23:31:15 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.46843853820598 on epoch=949
05/20/2022 23:31:15 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.46843853820598 on epoch=949, global_step=1900
05/20/2022 23:31:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
05/20/2022 23:31:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=959
05/20/2022 23:31:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
05/20/2022 23:31:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
05/20/2022 23:31:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/20/2022 23:31:26 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=974
05/20/2022 23:31:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=979
05/20/2022 23:31:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=984
05/20/2022 23:31:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=989
05/20/2022 23:31:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.34 on epoch=994
05/20/2022 23:31:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=999
05/20/2022 23:31:36 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=999
05/20/2022 23:31:36 - INFO - __main__ - save last model!
05/20/2022 23:31:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/20/2022 23:31:36 - INFO - __main__ - Start tokenizing ... 12792 instances
05/20/2022 23:31:36 - INFO - __main__ - Printing 3 examples
05/20/2022 23:31:36 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:31:36 - INFO - __main__ - ['entailed']
05/20/2022 23:31:36 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:31:36 - INFO - __main__ - ['entailed']
05/20/2022 23:31:36 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:31:36 - INFO - __main__ - ['entailed']
05/20/2022 23:31:36 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:31:37 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:31:37 - INFO - __main__ - Printing 3 examples
05/20/2022 23:31:37 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/20/2022 23:31:37 - INFO - __main__ - ['refuted']
05/20/2022 23:31:37 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/20/2022 23:31:37 - INFO - __main__ - ['refuted']
05/20/2022 23:31:37 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/20/2022 23:31:37 - INFO - __main__ - ['refuted']
05/20/2022 23:31:37 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:31:37 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:31:37 - INFO - __main__ - Loaded 32 examples from train data
05/20/2022 23:31:37 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:31:37 - INFO - __main__ - Printing 3 examples
05/20/2022 23:31:37 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/20/2022 23:31:37 - INFO - __main__ - ['refuted']
05/20/2022 23:31:37 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/20/2022 23:31:37 - INFO - __main__ - ['refuted']
05/20/2022 23:31:37 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/20/2022 23:31:37 - INFO - __main__ - ['refuted']
05/20/2022 23:31:37 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:31:37 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:31:37 - INFO - __main__ - Loaded 32 examples from dev data
05/20/2022 23:31:42 - INFO - __main__ - load prompt embedding from ckpt
05/20/2022 23:31:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/20/2022 23:31:42 - INFO - __main__ - Starting training!
05/20/2022 23:32:00 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:32:15 - INFO - __main__ - Loaded 12792 examples from test data
05/20/2022 23:36:28 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.3_8_predictions.txt
05/20/2022 23:36:28 - INFO - __main__ - Classification-F1 on test data: 0.3312
05/20/2022 23:36:28 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.3, bsz=8, dev_performance=0.46843853820598, test_performance=0.3311539798652483
05/20/2022 23:36:28 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.2, bsz=8 ...
05/20/2022 23:36:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:36:29 - INFO - __main__ - Printing 3 examples
05/20/2022 23:36:29 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/20/2022 23:36:29 - INFO - __main__ - ['refuted']
05/20/2022 23:36:29 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/20/2022 23:36:29 - INFO - __main__ - ['refuted']
05/20/2022 23:36:29 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/20/2022 23:36:29 - INFO - __main__ - ['refuted']
05/20/2022 23:36:29 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:36:29 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:36:29 - INFO - __main__ - Loaded 32 examples from train data
05/20/2022 23:36:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:36:29 - INFO - __main__ - Printing 3 examples
05/20/2022 23:36:29 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/20/2022 23:36:29 - INFO - __main__ - ['refuted']
05/20/2022 23:36:29 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/20/2022 23:36:29 - INFO - __main__ - ['refuted']
05/20/2022 23:36:29 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/20/2022 23:36:29 - INFO - __main__ - ['refuted']
05/20/2022 23:36:29 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:36:29 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:36:29 - INFO - __main__ - Loaded 32 examples from dev data
05/20/2022 23:36:36 - INFO - __main__ - load prompt embedding from ckpt
05/20/2022 23:36:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/20/2022 23:36:36 - INFO - __main__ - Starting training!
05/20/2022 23:36:38 - INFO - __main__ - Step 10 Global step 10 Train loss 5.08 on epoch=4
05/20/2022 23:36:40 - INFO - __main__ - Step 20 Global step 20 Train loss 4.89 on epoch=9
05/20/2022 23:36:42 - INFO - __main__ - Step 30 Global step 30 Train loss 4.90 on epoch=14
05/20/2022 23:36:44 - INFO - __main__ - Step 40 Global step 40 Train loss 4.80 on epoch=19
05/20/2022 23:36:46 - INFO - __main__ - Step 50 Global step 50 Train loss 4.85 on epoch=24
05/20/2022 23:36:48 - INFO - __main__ - Global step 50 Train loss 4.90 Classification-F1 0.0 on epoch=24
05/20/2022 23:36:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/20/2022 23:36:50 - INFO - __main__ - Step 60 Global step 60 Train loss 4.85 on epoch=29
05/20/2022 23:36:53 - INFO - __main__ - Step 70 Global step 70 Train loss 4.71 on epoch=34
05/20/2022 23:36:55 - INFO - __main__ - Step 80 Global step 80 Train loss 4.64 on epoch=39
05/20/2022 23:36:57 - INFO - __main__ - Step 90 Global step 90 Train loss 4.51 on epoch=44
05/20/2022 23:36:59 - INFO - __main__ - Step 100 Global step 100 Train loss 4.61 on epoch=49
05/20/2022 23:37:00 - INFO - __main__ - Global step 100 Train loss 4.66 Classification-F1 0.0 on epoch=49
05/20/2022 23:37:02 - INFO - __main__ - Step 110 Global step 110 Train loss 4.45 on epoch=54
05/20/2022 23:37:04 - INFO - __main__ - Step 120 Global step 120 Train loss 4.38 on epoch=59
05/20/2022 23:37:06 - INFO - __main__ - Step 130 Global step 130 Train loss 4.29 on epoch=64
05/20/2022 23:37:08 - INFO - __main__ - Step 140 Global step 140 Train loss 4.30 on epoch=69
05/20/2022 23:37:10 - INFO - __main__ - Step 150 Global step 150 Train loss 4.19 on epoch=74
05/20/2022 23:37:13 - INFO - __main__ - Global step 150 Train loss 4.32 Classification-F1 0.0 on epoch=74
05/20/2022 23:37:15 - INFO - __main__ - Step 160 Global step 160 Train loss 4.07 on epoch=79
05/20/2022 23:37:18 - INFO - __main__ - Step 170 Global step 170 Train loss 4.13 on epoch=84
05/20/2022 23:37:20 - INFO - __main__ - Step 180 Global step 180 Train loss 4.10 on epoch=89
05/20/2022 23:37:22 - INFO - __main__ - Step 190 Global step 190 Train loss 4.02 on epoch=94
05/20/2022 23:37:24 - INFO - __main__ - Step 200 Global step 200 Train loss 4.03 on epoch=99
05/20/2022 23:37:26 - INFO - __main__ - Global step 200 Train loss 4.07 Classification-F1 0.0 on epoch=99
05/20/2022 23:37:28 - INFO - __main__ - Step 210 Global step 210 Train loss 3.87 on epoch=104
05/20/2022 23:37:29 - INFO - __main__ - Step 220 Global step 220 Train loss 3.78 on epoch=109
05/20/2022 23:37:31 - INFO - __main__ - Step 230 Global step 230 Train loss 3.77 on epoch=114
05/20/2022 23:37:33 - INFO - __main__ - Step 240 Global step 240 Train loss 3.81 on epoch=119
05/20/2022 23:37:35 - INFO - __main__ - Step 250 Global step 250 Train loss 3.79 on epoch=124
05/20/2022 23:37:37 - INFO - __main__ - Global step 250 Train loss 3.80 Classification-F1 0.0 on epoch=124
05/20/2022 23:37:39 - INFO - __main__ - Step 260 Global step 260 Train loss 3.74 on epoch=129
05/20/2022 23:37:41 - INFO - __main__ - Step 270 Global step 270 Train loss 3.69 on epoch=134
05/20/2022 23:37:43 - INFO - __main__ - Step 280 Global step 280 Train loss 3.61 on epoch=139
05/20/2022 23:37:45 - INFO - __main__ - Step 290 Global step 290 Train loss 3.55 on epoch=144
05/20/2022 23:37:47 - INFO - __main__ - Step 300 Global step 300 Train loss 3.56 on epoch=149
05/20/2022 23:37:48 - INFO - __main__ - Global step 300 Train loss 3.63 Classification-F1 0.0 on epoch=149
05/20/2022 23:37:50 - INFO - __main__ - Step 310 Global step 310 Train loss 3.53 on epoch=154
05/20/2022 23:37:52 - INFO - __main__ - Step 320 Global step 320 Train loss 3.50 on epoch=159
05/20/2022 23:37:54 - INFO - __main__ - Step 330 Global step 330 Train loss 3.40 on epoch=164
05/20/2022 23:37:56 - INFO - __main__ - Step 340 Global step 340 Train loss 3.39 on epoch=169
05/20/2022 23:37:58 - INFO - __main__ - Step 350 Global step 350 Train loss 3.28 on epoch=174
05/20/2022 23:38:09 - INFO - __main__ - Global step 350 Train loss 3.42 Classification-F1 0.019047619047619046 on epoch=174
05/20/2022 23:38:09 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.019047619047619046 on epoch=174, global_step=350
05/20/2022 23:38:11 - INFO - __main__ - Step 360 Global step 360 Train loss 3.39 on epoch=179
05/20/2022 23:38:13 - INFO - __main__ - Step 370 Global step 370 Train loss 3.30 on epoch=184
05/20/2022 23:38:15 - INFO - __main__ - Step 380 Global step 380 Train loss 3.25 on epoch=189
05/20/2022 23:38:17 - INFO - __main__ - Step 390 Global step 390 Train loss 3.12 on epoch=194
05/20/2022 23:38:19 - INFO - __main__ - Step 400 Global step 400 Train loss 3.18 on epoch=199
05/20/2022 23:38:26 - INFO - __main__ - Global step 400 Train loss 3.25 Classification-F1 0.10077519379844961 on epoch=199
05/20/2022 23:38:26 - INFO - __main__ - Saving model with best Classification-F1: 0.019047619047619046 -> 0.10077519379844961 on epoch=199, global_step=400
05/20/2022 23:38:28 - INFO - __main__ - Step 410 Global step 410 Train loss 3.02 on epoch=204
05/20/2022 23:38:30 - INFO - __main__ - Step 420 Global step 420 Train loss 3.11 on epoch=209
05/20/2022 23:38:32 - INFO - __main__ - Step 430 Global step 430 Train loss 2.97 on epoch=214
05/20/2022 23:38:34 - INFO - __main__ - Step 440 Global step 440 Train loss 2.92 on epoch=219
05/20/2022 23:38:36 - INFO - __main__ - Step 450 Global step 450 Train loss 2.92 on epoch=224
05/20/2022 23:38:39 - INFO - __main__ - Global step 450 Train loss 2.99 Classification-F1 0.09677419354838708 on epoch=224
05/20/2022 23:38:41 - INFO - __main__ - Step 460 Global step 460 Train loss 2.84 on epoch=229
05/20/2022 23:38:43 - INFO - __main__ - Step 470 Global step 470 Train loss 2.84 on epoch=234
05/20/2022 23:38:45 - INFO - __main__ - Step 480 Global step 480 Train loss 2.77 on epoch=239
05/20/2022 23:38:47 - INFO - __main__ - Step 490 Global step 490 Train loss 2.70 on epoch=244
05/20/2022 23:38:49 - INFO - __main__ - Step 500 Global step 500 Train loss 2.73 on epoch=249
05/20/2022 23:38:52 - INFO - __main__ - Global step 500 Train loss 2.78 Classification-F1 0.1627906976744186 on epoch=249
05/20/2022 23:38:52 - INFO - __main__ - Saving model with best Classification-F1: 0.10077519379844961 -> 0.1627906976744186 on epoch=249, global_step=500
05/20/2022 23:38:54 - INFO - __main__ - Step 510 Global step 510 Train loss 2.57 on epoch=254
05/20/2022 23:38:56 - INFO - __main__ - Step 520 Global step 520 Train loss 2.52 on epoch=259
05/20/2022 23:38:58 - INFO - __main__ - Step 530 Global step 530 Train loss 2.47 on epoch=264
05/20/2022 23:39:00 - INFO - __main__ - Step 540 Global step 540 Train loss 2.53 on epoch=269
05/20/2022 23:39:02 - INFO - __main__ - Step 550 Global step 550 Train loss 2.37 on epoch=274
05/20/2022 23:39:05 - INFO - __main__ - Global step 550 Train loss 2.49 Classification-F1 0.3333333333333333 on epoch=274
05/20/2022 23:39:05 - INFO - __main__ - Saving model with best Classification-F1: 0.1627906976744186 -> 0.3333333333333333 on epoch=274, global_step=550
05/20/2022 23:39:07 - INFO - __main__ - Step 560 Global step 560 Train loss 2.39 on epoch=279
05/20/2022 23:39:09 - INFO - __main__ - Step 570 Global step 570 Train loss 2.23 on epoch=284
05/20/2022 23:39:10 - INFO - __main__ - Step 580 Global step 580 Train loss 2.35 on epoch=289
05/20/2022 23:39:12 - INFO - __main__ - Step 590 Global step 590 Train loss 2.20 on epoch=294
05/20/2022 23:39:14 - INFO - __main__ - Step 600 Global step 600 Train loss 2.25 on epoch=299
05/20/2022 23:39:17 - INFO - __main__ - Global step 600 Train loss 2.28 Classification-F1 0.3333333333333333 on epoch=299
05/20/2022 23:39:19 - INFO - __main__ - Step 610 Global step 610 Train loss 2.21 on epoch=304
05/20/2022 23:39:21 - INFO - __main__ - Step 620 Global step 620 Train loss 2.17 on epoch=309
05/20/2022 23:39:23 - INFO - __main__ - Step 630 Global step 630 Train loss 2.05 on epoch=314
05/20/2022 23:39:25 - INFO - __main__ - Step 640 Global step 640 Train loss 2.15 on epoch=319
05/20/2022 23:39:27 - INFO - __main__ - Step 650 Global step 650 Train loss 1.94 on epoch=324
05/20/2022 23:39:29 - INFO - __main__ - Global step 650 Train loss 2.10 Classification-F1 0.3333333333333333 on epoch=324
05/20/2022 23:39:31 - INFO - __main__ - Step 660 Global step 660 Train loss 1.86 on epoch=329
05/20/2022 23:39:33 - INFO - __main__ - Step 670 Global step 670 Train loss 1.91 on epoch=334
05/20/2022 23:39:35 - INFO - __main__ - Step 680 Global step 680 Train loss 1.90 on epoch=339
05/20/2022 23:39:37 - INFO - __main__ - Step 690 Global step 690 Train loss 1.85 on epoch=344
05/20/2022 23:39:39 - INFO - __main__ - Step 700 Global step 700 Train loss 1.86 on epoch=349
05/20/2022 23:39:41 - INFO - __main__ - Global step 700 Train loss 1.88 Classification-F1 0.3333333333333333 on epoch=349
05/20/2022 23:39:43 - INFO - __main__ - Step 710 Global step 710 Train loss 1.78 on epoch=354
05/20/2022 23:39:45 - INFO - __main__ - Step 720 Global step 720 Train loss 1.74 on epoch=359
05/20/2022 23:39:47 - INFO - __main__ - Step 730 Global step 730 Train loss 1.64 on epoch=364
05/20/2022 23:39:49 - INFO - __main__ - Step 740 Global step 740 Train loss 1.68 on epoch=369
05/20/2022 23:39:51 - INFO - __main__ - Step 750 Global step 750 Train loss 1.70 on epoch=374
05/20/2022 23:39:52 - INFO - __main__ - Global step 750 Train loss 1.71 Classification-F1 0.3333333333333333 on epoch=374
05/20/2022 23:39:54 - INFO - __main__ - Step 760 Global step 760 Train loss 1.58 on epoch=379
05/20/2022 23:39:56 - INFO - __main__ - Step 770 Global step 770 Train loss 1.58 on epoch=384
05/20/2022 23:39:58 - INFO - __main__ - Step 780 Global step 780 Train loss 1.53 on epoch=389
05/20/2022 23:40:00 - INFO - __main__ - Step 790 Global step 790 Train loss 1.54 on epoch=394
05/20/2022 23:40:02 - INFO - __main__ - Step 800 Global step 800 Train loss 1.53 on epoch=399
05/20/2022 23:40:03 - INFO - __main__ - Global step 800 Train loss 1.55 Classification-F1 0.3333333333333333 on epoch=399
05/20/2022 23:40:05 - INFO - __main__ - Step 810 Global step 810 Train loss 1.34 on epoch=404
05/20/2022 23:40:07 - INFO - __main__ - Step 820 Global step 820 Train loss 1.51 on epoch=409
05/20/2022 23:40:09 - INFO - __main__ - Step 830 Global step 830 Train loss 1.44 on epoch=414
05/20/2022 23:40:11 - INFO - __main__ - Step 840 Global step 840 Train loss 1.47 on epoch=419
05/20/2022 23:40:13 - INFO - __main__ - Step 850 Global step 850 Train loss 1.53 on epoch=424
05/20/2022 23:40:14 - INFO - __main__ - Global step 850 Train loss 1.46 Classification-F1 0.3333333333333333 on epoch=424
05/20/2022 23:40:16 - INFO - __main__ - Step 860 Global step 860 Train loss 1.37 on epoch=429
05/20/2022 23:40:18 - INFO - __main__ - Step 870 Global step 870 Train loss 1.39 on epoch=434
05/20/2022 23:40:20 - INFO - __main__ - Step 880 Global step 880 Train loss 1.44 on epoch=439
05/20/2022 23:40:22 - INFO - __main__ - Step 890 Global step 890 Train loss 1.29 on epoch=444
05/20/2022 23:40:24 - INFO - __main__ - Step 900 Global step 900 Train loss 1.30 on epoch=449
05/20/2022 23:40:25 - INFO - __main__ - Global step 900 Train loss 1.36 Classification-F1 0.3333333333333333 on epoch=449
05/20/2022 23:40:27 - INFO - __main__ - Step 910 Global step 910 Train loss 1.47 on epoch=454
05/20/2022 23:40:29 - INFO - __main__ - Step 920 Global step 920 Train loss 1.41 on epoch=459
05/20/2022 23:40:31 - INFO - __main__ - Step 930 Global step 930 Train loss 1.41 on epoch=464
05/20/2022 23:40:33 - INFO - __main__ - Step 940 Global step 940 Train loss 1.38 on epoch=469
05/20/2022 23:40:35 - INFO - __main__ - Step 950 Global step 950 Train loss 1.31 on epoch=474
05/20/2022 23:40:36 - INFO - __main__ - Global step 950 Train loss 1.40 Classification-F1 0.3333333333333333 on epoch=474
05/20/2022 23:40:38 - INFO - __main__ - Step 960 Global step 960 Train loss 1.34 on epoch=479
05/20/2022 23:40:40 - INFO - __main__ - Step 970 Global step 970 Train loss 1.22 on epoch=484
05/20/2022 23:40:42 - INFO - __main__ - Step 980 Global step 980 Train loss 1.37 on epoch=489
05/20/2022 23:40:44 - INFO - __main__ - Step 990 Global step 990 Train loss 1.30 on epoch=494
05/20/2022 23:40:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.21 on epoch=499
05/20/2022 23:40:48 - INFO - __main__ - Global step 1000 Train loss 1.29 Classification-F1 0.3333333333333333 on epoch=499
05/20/2022 23:40:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.26 on epoch=504
05/20/2022 23:40:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.11 on epoch=509
05/20/2022 23:40:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.13 on epoch=514
05/20/2022 23:40:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.15 on epoch=519
05/20/2022 23:40:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.04 on epoch=524
05/20/2022 23:41:00 - INFO - __main__ - Global step 1050 Train loss 1.14 Classification-F1 0.3333333333333333 on epoch=524
05/20/2022 23:41:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.10 on epoch=529
05/20/2022 23:41:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.01 on epoch=534
05/20/2022 23:41:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.10 on epoch=539
05/20/2022 23:41:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.09 on epoch=544
05/20/2022 23:41:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.05 on epoch=549
05/20/2022 23:41:11 - INFO - __main__ - Global step 1100 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=549
05/20/2022 23:41:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.17 on epoch=554
05/20/2022 23:41:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.16 on epoch=559
05/20/2022 23:41:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.12 on epoch=564
05/20/2022 23:41:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.98 on epoch=569
05/20/2022 23:41:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.09 on epoch=574
05/20/2022 23:41:22 - INFO - __main__ - Global step 1150 Train loss 1.10 Classification-F1 0.3333333333333333 on epoch=574
05/20/2022 23:41:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.02 on epoch=579
05/20/2022 23:41:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.16 on epoch=584
05/20/2022 23:41:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.99 on epoch=589
05/20/2022 23:41:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.92 on epoch=594
05/20/2022 23:41:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.94 on epoch=599
05/20/2022 23:41:33 - INFO - __main__ - Global step 1200 Train loss 1.01 Classification-F1 0.3333333333333333 on epoch=599
05/20/2022 23:41:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.97 on epoch=604
05/20/2022 23:41:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.92 on epoch=609
05/20/2022 23:41:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.98 on epoch=614
05/20/2022 23:41:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.92 on epoch=619
05/20/2022 23:41:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.01 on epoch=624
05/20/2022 23:41:44 - INFO - __main__ - Global step 1250 Train loss 0.96 Classification-F1 0.3333333333333333 on epoch=624
05/20/2022 23:41:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.93 on epoch=629
05/20/2022 23:41:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.96 on epoch=634
05/20/2022 23:41:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.88 on epoch=639
05/20/2022 23:41:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.87 on epoch=644
05/20/2022 23:41:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.81 on epoch=649
05/20/2022 23:41:55 - INFO - __main__ - Global step 1300 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=649
05/20/2022 23:41:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.84 on epoch=654
05/20/2022 23:41:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.79 on epoch=659
05/20/2022 23:42:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.77 on epoch=664
05/20/2022 23:42:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.82 on epoch=669
05/20/2022 23:42:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.90 on epoch=674
05/20/2022 23:42:06 - INFO - __main__ - Global step 1350 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=674
05/20/2022 23:42:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.83 on epoch=679
05/20/2022 23:42:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.81 on epoch=684
05/20/2022 23:42:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.81 on epoch=689
05/20/2022 23:42:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.78 on epoch=694
05/20/2022 23:42:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.78 on epoch=699
05/20/2022 23:42:17 - INFO - __main__ - Global step 1400 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=699
05/20/2022 23:42:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.73 on epoch=704
05/20/2022 23:42:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.74 on epoch=709
05/20/2022 23:42:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.84 on epoch=714
05/20/2022 23:42:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.83 on epoch=719
05/20/2022 23:42:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.82 on epoch=724
05/20/2022 23:42:28 - INFO - __main__ - Global step 1450 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=724
05/20/2022 23:42:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.73 on epoch=729
05/20/2022 23:42:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.71 on epoch=734
05/20/2022 23:42:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.69 on epoch=739
05/20/2022 23:42:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.77 on epoch=744
05/20/2022 23:42:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.68 on epoch=749
05/20/2022 23:42:39 - INFO - __main__ - Global step 1500 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=749
05/20/2022 23:42:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.69 on epoch=754
05/20/2022 23:42:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.71 on epoch=759
05/20/2022 23:42:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.65 on epoch=764
05/20/2022 23:42:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.72 on epoch=769
05/20/2022 23:42:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.62 on epoch=774
05/20/2022 23:42:50 - INFO - __main__ - Global step 1550 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=774
05/20/2022 23:42:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.73 on epoch=779
05/20/2022 23:42:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.63 on epoch=784
05/20/2022 23:42:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.63 on epoch=789
05/20/2022 23:42:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.61 on epoch=794
05/20/2022 23:43:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.60 on epoch=799
05/20/2022 23:43:00 - INFO - __main__ - Global step 1600 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=799
05/20/2022 23:43:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.65 on epoch=804
05/20/2022 23:43:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.64 on epoch=809
05/20/2022 23:43:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.58 on epoch=814
05/20/2022 23:43:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.65 on epoch=819
05/20/2022 23:43:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.63 on epoch=824
05/20/2022 23:43:11 - INFO - __main__ - Global step 1650 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=824
05/20/2022 23:43:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.61 on epoch=829
05/20/2022 23:43:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.57 on epoch=834
05/20/2022 23:43:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.62 on epoch=839
05/20/2022 23:43:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.62 on epoch=844
05/20/2022 23:43:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.59 on epoch=849
05/20/2022 23:43:22 - INFO - __main__ - Global step 1700 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=849
05/20/2022 23:43:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.54 on epoch=854
05/20/2022 23:43:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.62 on epoch=859
05/20/2022 23:43:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.57 on epoch=864
05/20/2022 23:43:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=869
05/20/2022 23:43:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.54 on epoch=874
05/20/2022 23:43:34 - INFO - __main__ - Global step 1750 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=874
05/20/2022 23:43:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.51 on epoch=879
05/20/2022 23:43:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.59 on epoch=884
05/20/2022 23:43:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.54 on epoch=889
05/20/2022 23:43:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.51 on epoch=894
05/20/2022 23:43:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.50 on epoch=899
05/20/2022 23:43:44 - INFO - __main__ - Global step 1800 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=899
05/20/2022 23:43:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.47 on epoch=904
05/20/2022 23:43:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.53 on epoch=909
05/20/2022 23:43:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.49 on epoch=914
05/20/2022 23:43:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.48 on epoch=919
05/20/2022 23:43:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.56 on epoch=924
05/20/2022 23:43:55 - INFO - __main__ - Global step 1850 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=924
05/20/2022 23:43:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.51 on epoch=929
05/20/2022 23:43:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.52 on epoch=934
05/20/2022 23:44:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=939
05/20/2022 23:44:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
05/20/2022 23:44:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=949
05/20/2022 23:44:06 - INFO - __main__ - Global step 1900 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=949
05/20/2022 23:44:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.57 on epoch=954
05/20/2022 23:44:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=959
05/20/2022 23:44:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.48 on epoch=964
05/20/2022 23:44:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.54 on epoch=969
05/20/2022 23:44:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=974
05/20/2022 23:44:17 - INFO - __main__ - Global step 1950 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=974
05/20/2022 23:44:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=979
05/20/2022 23:44:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
05/20/2022 23:44:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=989
05/20/2022 23:44:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=994
05/20/2022 23:44:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=999
05/20/2022 23:44:28 - INFO - __main__ - Global step 2000 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=999
05/20/2022 23:44:28 - INFO - __main__ - save last model!
05/20/2022 23:44:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/20/2022 23:44:28 - INFO - __main__ - Start tokenizing ... 12792 instances
05/20/2022 23:44:28 - INFO - __main__ - Printing 3 examples
05/20/2022 23:44:28 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:44:28 - INFO - __main__ - ['entailed']
05/20/2022 23:44:28 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:44:28 - INFO - __main__ - ['entailed']
05/20/2022 23:44:28 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:44:28 - INFO - __main__ - ['entailed']
05/20/2022 23:44:28 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:44:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:44:29 - INFO - __main__ - Printing 3 examples
05/20/2022 23:44:29 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/20/2022 23:44:29 - INFO - __main__ - ['refuted']
05/20/2022 23:44:29 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/20/2022 23:44:29 - INFO - __main__ - ['refuted']
05/20/2022 23:44:29 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/20/2022 23:44:29 - INFO - __main__ - ['refuted']
05/20/2022 23:44:29 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:44:29 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:44:29 - INFO - __main__ - Loaded 32 examples from train data
05/20/2022 23:44:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:44:29 - INFO - __main__ - Printing 3 examples
05/20/2022 23:44:29 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/20/2022 23:44:29 - INFO - __main__ - ['refuted']
05/20/2022 23:44:29 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/20/2022 23:44:29 - INFO - __main__ - ['refuted']
05/20/2022 23:44:29 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/20/2022 23:44:29 - INFO - __main__ - ['refuted']
05/20/2022 23:44:29 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:44:29 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:44:29 - INFO - __main__ - Loaded 32 examples from dev data
05/20/2022 23:44:35 - INFO - __main__ - load prompt embedding from ckpt
05/20/2022 23:44:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/20/2022 23:44:35 - INFO - __main__ - Starting training!
05/20/2022 23:44:57 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:45:10 - INFO - __main__ - Loaded 12792 examples from test data
05/20/2022 23:50:28 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.2_8_predictions.txt
05/20/2022 23:50:28 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/20/2022 23:50:28 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.2, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/20/2022 23:50:28 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.5, bsz=8 ...
05/20/2022 23:50:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:50:29 - INFO - __main__ - Printing 3 examples
05/20/2022 23:50:29 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/20/2022 23:50:29 - INFO - __main__ - ['refuted']
05/20/2022 23:50:29 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/20/2022 23:50:29 - INFO - __main__ - ['refuted']
05/20/2022 23:50:29 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/20/2022 23:50:29 - INFO - __main__ - ['refuted']
05/20/2022 23:50:29 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:50:29 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:50:29 - INFO - __main__ - Loaded 32 examples from train data
05/20/2022 23:50:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:50:29 - INFO - __main__ - Printing 3 examples
05/20/2022 23:50:29 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/20/2022 23:50:29 - INFO - __main__ - ['refuted']
05/20/2022 23:50:29 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/20/2022 23:50:29 - INFO - __main__ - ['refuted']
05/20/2022 23:50:29 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/20/2022 23:50:29 - INFO - __main__ - ['refuted']
05/20/2022 23:50:29 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:50:29 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:50:29 - INFO - __main__ - Loaded 32 examples from dev data
05/20/2022 23:50:35 - INFO - __main__ - load prompt embedding from ckpt
05/20/2022 23:50:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/20/2022 23:50:35 - INFO - __main__ - Starting training!
05/20/2022 23:50:38 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/20/2022 23:50:40 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/20/2022 23:50:42 - INFO - __main__ - Step 30 Global step 30 Train loss 4.85 on epoch=14
05/20/2022 23:50:44 - INFO - __main__ - Step 40 Global step 40 Train loss 4.77 on epoch=19
05/20/2022 23:50:46 - INFO - __main__ - Step 50 Global step 50 Train loss 4.65 on epoch=24
05/20/2022 23:50:47 - INFO - __main__ - Global step 50 Train loss 4.85 Classification-F1 0.0 on epoch=24
05/20/2022 23:50:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/20/2022 23:50:49 - INFO - __main__ - Step 60 Global step 60 Train loss 4.53 on epoch=29
05/20/2022 23:50:51 - INFO - __main__ - Step 70 Global step 70 Train loss 4.49 on epoch=34
05/20/2022 23:50:53 - INFO - __main__ - Step 80 Global step 80 Train loss 4.27 on epoch=39
05/20/2022 23:50:55 - INFO - __main__ - Step 90 Global step 90 Train loss 4.13 on epoch=44
05/20/2022 23:50:57 - INFO - __main__ - Step 100 Global step 100 Train loss 3.96 on epoch=49
05/20/2022 23:51:00 - INFO - __main__ - Global step 100 Train loss 4.28 Classification-F1 0.0 on epoch=49
05/20/2022 23:51:02 - INFO - __main__ - Step 110 Global step 110 Train loss 3.87 on epoch=54
05/20/2022 23:51:04 - INFO - __main__ - Step 120 Global step 120 Train loss 3.84 on epoch=59
05/20/2022 23:51:06 - INFO - __main__ - Step 130 Global step 130 Train loss 3.74 on epoch=64
05/20/2022 23:51:08 - INFO - __main__ - Step 140 Global step 140 Train loss 3.65 on epoch=69
05/20/2022 23:51:10 - INFO - __main__ - Step 150 Global step 150 Train loss 3.45 on epoch=74
05/20/2022 23:51:19 - INFO - __main__ - Global step 150 Train loss 3.71 Classification-F1 0.0 on epoch=74
05/20/2022 23:51:21 - INFO - __main__ - Step 160 Global step 160 Train loss 3.42 on epoch=79
05/20/2022 23:51:23 - INFO - __main__ - Step 170 Global step 170 Train loss 3.31 on epoch=84
05/20/2022 23:51:25 - INFO - __main__ - Step 180 Global step 180 Train loss 3.06 on epoch=89
05/20/2022 23:51:27 - INFO - __main__ - Step 190 Global step 190 Train loss 2.98 on epoch=94
05/20/2022 23:51:29 - INFO - __main__ - Step 200 Global step 200 Train loss 2.67 on epoch=99
05/20/2022 23:51:32 - INFO - __main__ - Global step 200 Train loss 3.09 Classification-F1 0.3333333333333333 on epoch=99
05/20/2022 23:51:32 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=99, global_step=200
05/20/2022 23:51:34 - INFO - __main__ - Step 210 Global step 210 Train loss 2.55 on epoch=104
05/20/2022 23:51:36 - INFO - __main__ - Step 220 Global step 220 Train loss 2.43 on epoch=109
05/20/2022 23:51:38 - INFO - __main__ - Step 230 Global step 230 Train loss 2.14 on epoch=114
05/20/2022 23:51:40 - INFO - __main__ - Step 240 Global step 240 Train loss 2.14 on epoch=119
05/20/2022 23:51:42 - INFO - __main__ - Step 250 Global step 250 Train loss 1.96 on epoch=124
05/20/2022 23:51:44 - INFO - __main__ - Global step 250 Train loss 2.24 Classification-F1 0.3333333333333333 on epoch=124
05/20/2022 23:51:46 - INFO - __main__ - Step 260 Global step 260 Train loss 1.86 on epoch=129
05/20/2022 23:51:48 - INFO - __main__ - Step 270 Global step 270 Train loss 1.60 on epoch=134
05/20/2022 23:51:50 - INFO - __main__ - Step 280 Global step 280 Train loss 1.55 on epoch=139
05/20/2022 23:51:51 - INFO - __main__ - Step 290 Global step 290 Train loss 1.55 on epoch=144
05/20/2022 23:51:53 - INFO - __main__ - Step 300 Global step 300 Train loss 1.52 on epoch=149
05/20/2022 23:51:56 - INFO - __main__ - Global step 300 Train loss 1.62 Classification-F1 0.3333333333333333 on epoch=149
05/20/2022 23:51:58 - INFO - __main__ - Step 310 Global step 310 Train loss 1.33 on epoch=154
05/20/2022 23:52:00 - INFO - __main__ - Step 320 Global step 320 Train loss 1.41 on epoch=159
05/20/2022 23:52:02 - INFO - __main__ - Step 330 Global step 330 Train loss 1.30 on epoch=164
05/20/2022 23:52:04 - INFO - __main__ - Step 340 Global step 340 Train loss 1.25 on epoch=169
05/20/2022 23:52:06 - INFO - __main__ - Step 350 Global step 350 Train loss 1.22 on epoch=174
05/20/2022 23:52:07 - INFO - __main__ - Global step 350 Train loss 1.30 Classification-F1 0.3333333333333333 on epoch=174
05/20/2022 23:52:09 - INFO - __main__ - Step 360 Global step 360 Train loss 1.21 on epoch=179
05/20/2022 23:52:11 - INFO - __main__ - Step 370 Global step 370 Train loss 1.16 on epoch=184
05/20/2022 23:52:13 - INFO - __main__ - Step 380 Global step 380 Train loss 1.19 on epoch=189
05/20/2022 23:52:15 - INFO - __main__ - Step 390 Global step 390 Train loss 1.19 on epoch=194
05/20/2022 23:52:17 - INFO - __main__ - Step 400 Global step 400 Train loss 1.01 on epoch=199
05/20/2022 23:52:18 - INFO - __main__ - Global step 400 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=199
05/20/2022 23:52:20 - INFO - __main__ - Step 410 Global step 410 Train loss 1.00 on epoch=204
05/20/2022 23:52:22 - INFO - __main__ - Step 420 Global step 420 Train loss 1.01 on epoch=209
05/20/2022 23:52:24 - INFO - __main__ - Step 430 Global step 430 Train loss 1.01 on epoch=214
05/20/2022 23:52:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.85 on epoch=219
05/20/2022 23:52:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.86 on epoch=224
05/20/2022 23:52:29 - INFO - __main__ - Global step 450 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=224
05/20/2022 23:52:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=229
05/20/2022 23:52:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.85 on epoch=234
05/20/2022 23:52:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.78 on epoch=239
05/20/2022 23:52:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.73 on epoch=244
05/20/2022 23:52:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.79 on epoch=249
05/20/2022 23:52:39 - INFO - __main__ - Global step 500 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=249
05/20/2022 23:52:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.67 on epoch=254
05/20/2022 23:52:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.69 on epoch=259
05/20/2022 23:52:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.71 on epoch=264
05/20/2022 23:52:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=269
05/20/2022 23:52:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.68 on epoch=274
05/20/2022 23:52:50 - INFO - __main__ - Global step 550 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=274
05/20/2022 23:52:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=279
05/20/2022 23:52:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.71 on epoch=284
05/20/2022 23:52:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.63 on epoch=289
05/20/2022 23:52:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.65 on epoch=294
05/20/2022 23:53:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.62 on epoch=299
05/20/2022 23:53:01 - INFO - __main__ - Global step 600 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=299
05/20/2022 23:53:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.64 on epoch=304
05/20/2022 23:53:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.63 on epoch=309
05/20/2022 23:53:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.61 on epoch=314
05/20/2022 23:53:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.55 on epoch=319
05/20/2022 23:53:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.52 on epoch=324
05/20/2022 23:53:12 - INFO - __main__ - Global step 650 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=324
05/20/2022 23:53:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=329
05/20/2022 23:53:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=334
05/20/2022 23:53:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.57 on epoch=339
05/20/2022 23:53:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.54 on epoch=344
05/20/2022 23:53:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=349
05/20/2022 23:53:23 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=349
05/20/2022 23:53:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=354
05/20/2022 23:53:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.50 on epoch=359
05/20/2022 23:53:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=364
05/20/2022 23:53:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=369
05/20/2022 23:53:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=374
05/20/2022 23:53:34 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=374
05/20/2022 23:53:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=379
05/20/2022 23:53:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.51 on epoch=384
05/20/2022 23:53:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.54 on epoch=389
05/20/2022 23:53:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=394
05/20/2022 23:53:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.53 on epoch=399
05/20/2022 23:53:44 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=399
05/20/2022 23:53:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=404
05/20/2022 23:53:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=409
05/20/2022 23:53:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
05/20/2022 23:53:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=419
05/20/2022 23:53:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=424
05/20/2022 23:53:55 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=424
05/20/2022 23:53:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=429
05/20/2022 23:53:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
05/20/2022 23:54:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=439
05/20/2022 23:54:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
05/20/2022 23:54:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=449
05/20/2022 23:54:06 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=449
05/20/2022 23:54:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.49 on epoch=454
05/20/2022 23:54:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=459
05/20/2022 23:54:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=464
05/20/2022 23:54:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=469
05/20/2022 23:54:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
05/20/2022 23:54:17 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=474
05/20/2022 23:54:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=479
05/20/2022 23:54:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=484
05/20/2022 23:54:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=489
05/20/2022 23:54:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=494
05/20/2022 23:54:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=499
05/20/2022 23:54:27 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=499
05/20/2022 23:54:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
05/20/2022 23:54:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=509
05/20/2022 23:54:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=514
05/20/2022 23:54:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=519
05/20/2022 23:54:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=524
05/20/2022 23:54:38 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=524
05/20/2022 23:54:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=529
05/20/2022 23:54:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=534
05/20/2022 23:54:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=539
05/20/2022 23:54:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=544
05/20/2022 23:54:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=549
05/20/2022 23:54:49 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=549
05/20/2022 23:54:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=554
05/20/2022 23:54:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=559
05/20/2022 23:54:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=564
05/20/2022 23:54:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=569
05/20/2022 23:54:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=574
05/20/2022 23:55:00 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=574
05/20/2022 23:55:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=579
05/20/2022 23:55:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=584
05/20/2022 23:55:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=589
05/20/2022 23:55:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=594
05/20/2022 23:55:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=599
05/20/2022 23:55:10 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=599
05/20/2022 23:55:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=604
05/20/2022 23:55:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=609
05/20/2022 23:55:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=614
05/20/2022 23:55:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=619
05/20/2022 23:55:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=624
05/20/2022 23:55:23 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=624
05/20/2022 23:55:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=629
05/20/2022 23:55:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=634
05/20/2022 23:55:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=639
05/20/2022 23:55:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=644
05/20/2022 23:55:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=649
05/20/2022 23:55:34 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=649
05/20/2022 23:55:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=654
05/20/2022 23:55:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=659
05/20/2022 23:55:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
05/20/2022 23:55:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=669
05/20/2022 23:55:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=674
05/20/2022 23:55:44 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=674
05/20/2022 23:55:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=679
05/20/2022 23:55:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=684
05/20/2022 23:55:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=689
05/20/2022 23:55:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=694
05/20/2022 23:55:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=699
05/20/2022 23:55:55 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=699
05/20/2022 23:55:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=704
05/20/2022 23:55:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=709
05/20/2022 23:56:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=714
05/20/2022 23:56:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=719
05/20/2022 23:56:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=724
05/20/2022 23:56:06 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=724
05/20/2022 23:56:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=729
05/20/2022 23:56:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=734
05/20/2022 23:56:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=739
05/20/2022 23:56:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=744
05/20/2022 23:56:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=749
05/20/2022 23:56:17 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=749
05/20/2022 23:56:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=754
05/20/2022 23:56:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=759
05/20/2022 23:56:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=764
05/20/2022 23:56:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=769
05/20/2022 23:56:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=774
05/20/2022 23:56:28 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=774
05/20/2022 23:56:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=779
05/20/2022 23:56:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=784
05/20/2022 23:56:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
05/20/2022 23:56:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=794
05/20/2022 23:56:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=799
05/20/2022 23:56:39 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=799
05/20/2022 23:56:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=804
05/20/2022 23:56:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=809
05/20/2022 23:56:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=814
05/20/2022 23:56:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=819
05/20/2022 23:56:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=824
05/20/2022 23:56:50 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=824
05/20/2022 23:56:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=829
05/20/2022 23:56:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=834
05/20/2022 23:56:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=839
05/20/2022 23:56:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=844
05/20/2022 23:57:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=849
05/20/2022 23:57:01 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=849
05/20/2022 23:57:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/20/2022 23:57:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=859
05/20/2022 23:57:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/20/2022 23:57:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=869
05/20/2022 23:57:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/20/2022 23:57:11 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=874
05/20/2022 23:57:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=879
05/20/2022 23:57:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
05/20/2022 23:57:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.31 on epoch=889
05/20/2022 23:57:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=894
05/20/2022 23:57:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=899
05/20/2022 23:57:22 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=899
05/20/2022 23:57:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=904
05/20/2022 23:57:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/20/2022 23:57:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=914
05/20/2022 23:57:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=919
05/20/2022 23:57:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=924
05/20/2022 23:57:32 - INFO - __main__ - Global step 1850 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=924
05/20/2022 23:57:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=929
05/20/2022 23:57:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=934
05/20/2022 23:57:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=939
05/20/2022 23:57:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=944
05/20/2022 23:57:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/20/2022 23:57:43 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=949
05/20/2022 23:57:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
05/20/2022 23:57:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=959
05/20/2022 23:57:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/20/2022 23:57:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=969
05/20/2022 23:57:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=974
05/20/2022 23:57:54 - INFO - __main__ - Global step 1950 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=974
05/20/2022 23:57:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.33 on epoch=979
05/20/2022 23:57:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=984
05/20/2022 23:58:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/20/2022 23:58:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=994
05/20/2022 23:58:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
05/20/2022 23:58:04 - INFO - __main__ - Global step 2000 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=999
05/20/2022 23:58:04 - INFO - __main__ - save last model!
05/20/2022 23:58:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/20/2022 23:58:04 - INFO - __main__ - Start tokenizing ... 12792 instances
05/20/2022 23:58:04 - INFO - __main__ - Printing 3 examples
05/20/2022 23:58:04 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:58:04 - INFO - __main__ - ['entailed']
05/20/2022 23:58:04 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:58:04 - INFO - __main__ - ['entailed']
05/20/2022 23:58:04 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/20/2022 23:58:04 - INFO - __main__ - ['entailed']
05/20/2022 23:58:04 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:58:05 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:58:05 - INFO - __main__ - Printing 3 examples
05/20/2022 23:58:05 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/20/2022 23:58:05 - INFO - __main__ - ['refuted']
05/20/2022 23:58:05 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/20/2022 23:58:05 - INFO - __main__ - ['refuted']
05/20/2022 23:58:05 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/20/2022 23:58:05 - INFO - __main__ - ['refuted']
05/20/2022 23:58:05 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:58:05 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:58:05 - INFO - __main__ - Loaded 32 examples from train data
05/20/2022 23:58:05 - INFO - __main__ - Start tokenizing ... 32 instances
05/20/2022 23:58:05 - INFO - __main__ - Printing 3 examples
05/20/2022 23:58:05 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/20/2022 23:58:05 - INFO - __main__ - ['refuted']
05/20/2022 23:58:05 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/20/2022 23:58:05 - INFO - __main__ - ['refuted']
05/20/2022 23:58:05 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/20/2022 23:58:05 - INFO - __main__ - ['refuted']
05/20/2022 23:58:05 - INFO - __main__ - Tokenizing Input ...
05/20/2022 23:58:05 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:58:05 - INFO - __main__ - Loaded 32 examples from dev data
05/20/2022 23:58:12 - INFO - __main__ - load prompt embedding from ckpt
05/20/2022 23:58:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/20/2022 23:58:12 - INFO - __main__ - Starting training!
05/20/2022 23:58:28 - INFO - __main__ - Tokenizing Output ...
05/20/2022 23:58:41 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 00:02:53 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.5_8_predictions.txt
05/21/2022 00:02:53 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 00:02:54 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/21/2022 00:02:54 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.4, bsz=8 ...
05/21/2022 00:02:55 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:02:55 - INFO - __main__ - Printing 3 examples
05/21/2022 00:02:55 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/21/2022 00:02:55 - INFO - __main__ - ['refuted']
05/21/2022 00:02:55 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/21/2022 00:02:55 - INFO - __main__ - ['refuted']
05/21/2022 00:02:55 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/21/2022 00:02:55 - INFO - __main__ - ['refuted']
05/21/2022 00:02:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:02:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:02:55 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 00:02:55 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:02:55 - INFO - __main__ - Printing 3 examples
05/21/2022 00:02:55 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/21/2022 00:02:55 - INFO - __main__ - ['refuted']
05/21/2022 00:02:55 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/21/2022 00:02:55 - INFO - __main__ - ['refuted']
05/21/2022 00:02:55 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/21/2022 00:02:55 - INFO - __main__ - ['refuted']
05/21/2022 00:02:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:02:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:02:55 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 00:03:01 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 00:03:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 00:03:01 - INFO - __main__ - Starting training!
05/21/2022 00:03:03 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/21/2022 00:03:05 - INFO - __main__ - Step 20 Global step 20 Train loss 4.91 on epoch=9
05/21/2022 00:03:07 - INFO - __main__ - Step 30 Global step 30 Train loss 4.75 on epoch=14
05/21/2022 00:03:09 - INFO - __main__ - Step 40 Global step 40 Train loss 4.66 on epoch=19
05/21/2022 00:03:12 - INFO - __main__ - Step 50 Global step 50 Train loss 4.53 on epoch=24
05/21/2022 00:03:13 - INFO - __main__ - Global step 50 Train loss 4.78 Classification-F1 0.0 on epoch=24
05/21/2022 00:03:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 00:03:15 - INFO - __main__ - Step 60 Global step 60 Train loss 4.33 on epoch=29
05/21/2022 00:03:17 - INFO - __main__ - Step 70 Global step 70 Train loss 4.24 on epoch=34
05/21/2022 00:03:19 - INFO - __main__ - Step 80 Global step 80 Train loss 4.09 on epoch=39
05/21/2022 00:03:21 - INFO - __main__ - Step 90 Global step 90 Train loss 3.92 on epoch=44
05/21/2022 00:03:23 - INFO - __main__ - Step 100 Global step 100 Train loss 3.90 on epoch=49
05/21/2022 00:03:25 - INFO - __main__ - Global step 100 Train loss 4.10 Classification-F1 0.0 on epoch=49
05/21/2022 00:03:27 - INFO - __main__ - Step 110 Global step 110 Train loss 3.89 on epoch=54
05/21/2022 00:03:29 - INFO - __main__ - Step 120 Global step 120 Train loss 3.71 on epoch=59
05/21/2022 00:03:31 - INFO - __main__ - Step 130 Global step 130 Train loss 3.64 on epoch=64
05/21/2022 00:03:33 - INFO - __main__ - Step 140 Global step 140 Train loss 3.52 on epoch=69
05/21/2022 00:03:35 - INFO - __main__ - Step 150 Global step 150 Train loss 3.43 on epoch=74
05/21/2022 00:03:38 - INFO - __main__ - Global step 150 Train loss 3.64 Classification-F1 0.02 on epoch=74
05/21/2022 00:03:38 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.02 on epoch=74, global_step=150
05/21/2022 00:03:40 - INFO - __main__ - Step 160 Global step 160 Train loss 3.32 on epoch=79
05/21/2022 00:03:42 - INFO - __main__ - Step 170 Global step 170 Train loss 3.19 on epoch=84
05/21/2022 00:03:44 - INFO - __main__ - Step 180 Global step 180 Train loss 3.01 on epoch=89
05/21/2022 00:03:46 - INFO - __main__ - Step 190 Global step 190 Train loss 2.93 on epoch=94
05/21/2022 00:03:48 - INFO - __main__ - Step 200 Global step 200 Train loss 2.92 on epoch=99
05/21/2022 00:03:50 - INFO - __main__ - Global step 200 Train loss 3.07 Classification-F1 0.3333333333333333 on epoch=99
05/21/2022 00:03:50 - INFO - __main__ - Saving model with best Classification-F1: 0.02 -> 0.3333333333333333 on epoch=99, global_step=200
05/21/2022 00:03:52 - INFO - __main__ - Step 210 Global step 210 Train loss 2.77 on epoch=104
05/21/2022 00:03:54 - INFO - __main__ - Step 220 Global step 220 Train loss 2.58 on epoch=109
05/21/2022 00:03:56 - INFO - __main__ - Step 230 Global step 230 Train loss 2.53 on epoch=114
05/21/2022 00:03:58 - INFO - __main__ - Step 240 Global step 240 Train loss 2.47 on epoch=119
05/21/2022 00:04:00 - INFO - __main__ - Step 250 Global step 250 Train loss 2.33 on epoch=124
05/21/2022 00:04:02 - INFO - __main__ - Global step 250 Train loss 2.54 Classification-F1 0.3333333333333333 on epoch=124
05/21/2022 00:04:04 - INFO - __main__ - Step 260 Global step 260 Train loss 2.39 on epoch=129
05/21/2022 00:04:06 - INFO - __main__ - Step 270 Global step 270 Train loss 2.31 on epoch=134
05/21/2022 00:04:08 - INFO - __main__ - Step 280 Global step 280 Train loss 2.09 on epoch=139
05/21/2022 00:04:10 - INFO - __main__ - Step 290 Global step 290 Train loss 2.08 on epoch=144
05/21/2022 00:04:12 - INFO - __main__ - Step 300 Global step 300 Train loss 2.05 on epoch=149
05/21/2022 00:04:14 - INFO - __main__ - Global step 300 Train loss 2.18 Classification-F1 0.3333333333333333 on epoch=149
05/21/2022 00:04:16 - INFO - __main__ - Step 310 Global step 310 Train loss 1.99 on epoch=154
05/21/2022 00:04:18 - INFO - __main__ - Step 320 Global step 320 Train loss 1.95 on epoch=159
05/21/2022 00:04:20 - INFO - __main__ - Step 330 Global step 330 Train loss 1.73 on epoch=164
05/21/2022 00:04:22 - INFO - __main__ - Step 340 Global step 340 Train loss 1.70 on epoch=169
05/21/2022 00:04:24 - INFO - __main__ - Step 350 Global step 350 Train loss 1.65 on epoch=174
05/21/2022 00:04:25 - INFO - __main__ - Global step 350 Train loss 1.80 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 00:04:27 - INFO - __main__ - Step 360 Global step 360 Train loss 1.68 on epoch=179
05/21/2022 00:04:29 - INFO - __main__ - Step 370 Global step 370 Train loss 1.65 on epoch=184
05/21/2022 00:04:31 - INFO - __main__ - Step 380 Global step 380 Train loss 1.53 on epoch=189
05/21/2022 00:04:33 - INFO - __main__ - Step 390 Global step 390 Train loss 1.47 on epoch=194
05/21/2022 00:04:35 - INFO - __main__ - Step 400 Global step 400 Train loss 1.40 on epoch=199
05/21/2022 00:04:38 - INFO - __main__ - Global step 400 Train loss 1.55 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 00:04:40 - INFO - __main__ - Step 410 Global step 410 Train loss 1.40 on epoch=204
05/21/2022 00:04:42 - INFO - __main__ - Step 420 Global step 420 Train loss 1.36 on epoch=209
05/21/2022 00:04:44 - INFO - __main__ - Step 430 Global step 430 Train loss 1.24 on epoch=214
05/21/2022 00:04:46 - INFO - __main__ - Step 440 Global step 440 Train loss 1.19 on epoch=219
05/21/2022 00:04:48 - INFO - __main__ - Step 450 Global step 450 Train loss 1.18 on epoch=224
05/21/2022 00:04:49 - INFO - __main__ - Global step 450 Train loss 1.27 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 00:04:51 - INFO - __main__ - Step 460 Global step 460 Train loss 1.21 on epoch=229
05/21/2022 00:04:53 - INFO - __main__ - Step 470 Global step 470 Train loss 1.17 on epoch=234
05/21/2022 00:04:55 - INFO - __main__ - Step 480 Global step 480 Train loss 1.03 on epoch=239
05/21/2022 00:04:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.99 on epoch=244
05/21/2022 00:04:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.98 on epoch=249
05/21/2022 00:05:00 - INFO - __main__ - Global step 500 Train loss 1.08 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 00:05:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.87 on epoch=254
05/21/2022 00:05:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.85 on epoch=259
05/21/2022 00:05:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.91 on epoch=264
05/21/2022 00:05:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=269
05/21/2022 00:05:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.71 on epoch=274
05/21/2022 00:05:11 - INFO - __main__ - Global step 550 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 00:05:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=279
05/21/2022 00:05:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.78 on epoch=284
05/21/2022 00:05:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.87 on epoch=289
05/21/2022 00:05:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.75 on epoch=294
05/21/2022 00:05:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.72 on epoch=299
05/21/2022 00:05:22 - INFO - __main__ - Global step 600 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 00:05:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.71 on epoch=304
05/21/2022 00:05:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.68 on epoch=309
05/21/2022 00:05:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.79 on epoch=314
05/21/2022 00:05:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.80 on epoch=319
05/21/2022 00:05:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.70 on epoch=324
05/21/2022 00:05:32 - INFO - __main__ - Global step 650 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 00:05:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.65 on epoch=329
05/21/2022 00:05:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.59 on epoch=334
05/21/2022 00:05:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.66 on epoch=339
05/21/2022 00:05:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=344
05/21/2022 00:05:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.60 on epoch=349
05/21/2022 00:05:43 - INFO - __main__ - Global step 700 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 00:05:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.66 on epoch=354
05/21/2022 00:05:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=359
05/21/2022 00:05:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=364
05/21/2022 00:05:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.52 on epoch=369
05/21/2022 00:05:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.56 on epoch=374
05/21/2022 00:05:54 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 00:05:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=379
05/21/2022 00:05:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.54 on epoch=384
05/21/2022 00:06:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=389
05/21/2022 00:06:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.49 on epoch=394
05/21/2022 00:06:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=399
05/21/2022 00:06:05 - INFO - __main__ - Global step 800 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 00:06:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.50 on epoch=404
05/21/2022 00:06:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.54 on epoch=409
05/21/2022 00:06:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.58 on epoch=414
05/21/2022 00:06:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=419
05/21/2022 00:06:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=424
05/21/2022 00:06:16 - INFO - __main__ - Global step 850 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 00:06:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=429
05/21/2022 00:06:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=434
05/21/2022 00:06:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=439
05/21/2022 00:06:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
05/21/2022 00:06:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=449
05/21/2022 00:06:27 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 00:06:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=454
05/21/2022 00:06:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.52 on epoch=459
05/21/2022 00:06:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=464
05/21/2022 00:06:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=469
05/21/2022 00:06:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=474
05/21/2022 00:06:38 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 00:06:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=479
05/21/2022 00:06:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=484
05/21/2022 00:06:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=489
05/21/2022 00:06:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=494
05/21/2022 00:06:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=499
05/21/2022 00:06:49 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 00:06:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=504
05/21/2022 00:06:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.51 on epoch=509
05/21/2022 00:06:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=514
05/21/2022 00:06:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=519
05/21/2022 00:06:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=524
05/21/2022 00:07:00 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 00:07:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=529
05/21/2022 00:07:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=534
05/21/2022 00:07:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=539
05/21/2022 00:07:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=544
05/21/2022 00:07:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=549
05/21/2022 00:07:10 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 00:07:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=554
05/21/2022 00:07:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=559
05/21/2022 00:07:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=564
05/21/2022 00:07:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=569
05/21/2022 00:07:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=574
05/21/2022 00:07:21 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 00:07:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=579
05/21/2022 00:07:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=584
05/21/2022 00:07:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
05/21/2022 00:07:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.29 on epoch=594
05/21/2022 00:07:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.47 on epoch=599
05/21/2022 00:07:32 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 00:07:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=604
05/21/2022 00:07:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=609
05/21/2022 00:07:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.87 on epoch=614
05/21/2022 00:07:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.80 on epoch=619
05/21/2022 00:07:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=624
05/21/2022 00:07:42 - INFO - __main__ - Global step 1250 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 00:07:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=629
05/21/2022 00:07:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.52 on epoch=634
05/21/2022 00:07:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
05/21/2022 00:07:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=644
05/21/2022 00:07:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=649
05/21/2022 00:07:53 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 00:07:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=654
05/21/2022 00:07:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=659
05/21/2022 00:07:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=664
05/21/2022 00:08:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=669
05/21/2022 00:08:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=674
05/21/2022 00:08:03 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 00:08:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=679
05/21/2022 00:08:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=684
05/21/2022 00:08:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=689
05/21/2022 00:08:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=694
05/21/2022 00:08:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=699
05/21/2022 00:08:14 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 00:08:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=704
05/21/2022 00:08:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=709
05/21/2022 00:08:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
05/21/2022 00:08:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=719
05/21/2022 00:08:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=724
05/21/2022 00:08:24 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 00:08:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=729
05/21/2022 00:08:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=734
05/21/2022 00:08:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=739
05/21/2022 00:08:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=744
05/21/2022 00:08:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=749
05/21/2022 00:08:35 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 00:08:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=754
05/21/2022 00:08:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=759
05/21/2022 00:08:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.29 on epoch=764
05/21/2022 00:08:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=769
05/21/2022 00:08:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=774
05/21/2022 00:08:46 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 00:08:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=779
05/21/2022 00:08:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=784
05/21/2022 00:08:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=789
05/21/2022 00:08:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=794
05/21/2022 00:08:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=799
05/21/2022 00:08:56 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 00:08:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.28 on epoch=804
05/21/2022 00:09:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=809
05/21/2022 00:09:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=814
05/21/2022 00:09:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=819
05/21/2022 00:09:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=824
05/21/2022 00:09:07 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 00:09:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=829
05/21/2022 00:09:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=834
05/21/2022 00:09:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=839
05/21/2022 00:09:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=844
05/21/2022 00:09:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=849
05/21/2022 00:09:17 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 00:09:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=854
05/21/2022 00:09:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=859
05/21/2022 00:09:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=864
05/21/2022 00:09:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=869
05/21/2022 00:09:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/21/2022 00:09:28 - INFO - __main__ - Global step 1750 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 00:09:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.30 on epoch=879
05/21/2022 00:09:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=884
05/21/2022 00:09:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=889
05/21/2022 00:09:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=894
05/21/2022 00:09:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=899
05/21/2022 00:09:39 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 00:09:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=904
05/21/2022 00:09:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/21/2022 00:09:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=914
05/21/2022 00:09:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=919
05/21/2022 00:09:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=924
05/21/2022 00:09:49 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 00:09:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=929
05/21/2022 00:09:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=934
05/21/2022 00:09:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=939
05/21/2022 00:09:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=944
05/21/2022 00:09:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/21/2022 00:10:00 - INFO - __main__ - Global step 1900 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 00:10:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=954
05/21/2022 00:10:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=959
05/21/2022 00:10:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/21/2022 00:10:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
05/21/2022 00:10:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=974
05/21/2022 00:10:10 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 00:10:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=979
05/21/2022 00:10:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=984
05/21/2022 00:10:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
05/21/2022 00:10:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
05/21/2022 00:10:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=999
05/21/2022 00:10:21 - INFO - __main__ - Global step 2000 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 00:10:21 - INFO - __main__ - save last model!
05/21/2022 00:10:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 00:10:21 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 00:10:21 - INFO - __main__ - Printing 3 examples
05/21/2022 00:10:21 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:10:21 - INFO - __main__ - ['entailed']
05/21/2022 00:10:21 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:10:21 - INFO - __main__ - ['entailed']
05/21/2022 00:10:21 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:10:21 - INFO - __main__ - ['entailed']
05/21/2022 00:10:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:10:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:10:21 - INFO - __main__ - Printing 3 examples
05/21/2022 00:10:21 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/21/2022 00:10:21 - INFO - __main__ - ['refuted']
05/21/2022 00:10:21 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/21/2022 00:10:21 - INFO - __main__ - ['refuted']
05/21/2022 00:10:21 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/21/2022 00:10:21 - INFO - __main__ - ['refuted']
05/21/2022 00:10:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:10:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:10:21 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 00:10:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:10:21 - INFO - __main__ - Printing 3 examples
05/21/2022 00:10:21 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/21/2022 00:10:21 - INFO - __main__ - ['refuted']
05/21/2022 00:10:21 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/21/2022 00:10:21 - INFO - __main__ - ['refuted']
05/21/2022 00:10:21 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/21/2022 00:10:21 - INFO - __main__ - ['refuted']
05/21/2022 00:10:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:10:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:10:21 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 00:10:28 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 00:10:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 00:10:28 - INFO - __main__ - Starting training!
05/21/2022 00:10:45 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:10:57 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 00:15:09 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.4_8_predictions.txt
05/21/2022 00:15:09 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 00:15:10 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/21/2022 00:15:10 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.3, bsz=8 ...
05/21/2022 00:15:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:15:10 - INFO - __main__ - Printing 3 examples
05/21/2022 00:15:10 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/21/2022 00:15:10 - INFO - __main__ - ['refuted']
05/21/2022 00:15:10 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/21/2022 00:15:10 - INFO - __main__ - ['refuted']
05/21/2022 00:15:10 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/21/2022 00:15:10 - INFO - __main__ - ['refuted']
05/21/2022 00:15:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:15:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:15:11 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 00:15:11 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:15:11 - INFO - __main__ - Printing 3 examples
05/21/2022 00:15:11 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/21/2022 00:15:11 - INFO - __main__ - ['refuted']
05/21/2022 00:15:11 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/21/2022 00:15:11 - INFO - __main__ - ['refuted']
05/21/2022 00:15:11 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/21/2022 00:15:11 - INFO - __main__ - ['refuted']
05/21/2022 00:15:11 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:15:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:15:11 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 00:15:16 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 00:15:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 00:15:17 - INFO - __main__ - Starting training!
05/21/2022 00:15:20 - INFO - __main__ - Step 10 Global step 10 Train loss 4.98 on epoch=4
05/21/2022 00:15:22 - INFO - __main__ - Step 20 Global step 20 Train loss 4.94 on epoch=9
05/21/2022 00:15:24 - INFO - __main__ - Step 30 Global step 30 Train loss 4.93 on epoch=14
05/21/2022 00:15:26 - INFO - __main__ - Step 40 Global step 40 Train loss 4.88 on epoch=19
05/21/2022 00:15:28 - INFO - __main__ - Step 50 Global step 50 Train loss 4.83 on epoch=24
05/21/2022 00:15:30 - INFO - __main__ - Global step 50 Train loss 4.91 Classification-F1 0.0 on epoch=24
05/21/2022 00:15:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 00:15:32 - INFO - __main__ - Step 60 Global step 60 Train loss 4.75 on epoch=29
05/21/2022 00:15:33 - INFO - __main__ - Step 70 Global step 70 Train loss 4.59 on epoch=34
05/21/2022 00:15:36 - INFO - __main__ - Step 80 Global step 80 Train loss 4.61 on epoch=39
05/21/2022 00:15:38 - INFO - __main__ - Step 90 Global step 90 Train loss 4.43 on epoch=44
05/21/2022 00:15:40 - INFO - __main__ - Step 100 Global step 100 Train loss 4.46 on epoch=49
05/21/2022 00:15:41 - INFO - __main__ - Global step 100 Train loss 4.57 Classification-F1 0.0 on epoch=49
05/21/2022 00:15:43 - INFO - __main__ - Step 110 Global step 110 Train loss 4.39 on epoch=54
05/21/2022 00:15:45 - INFO - __main__ - Step 120 Global step 120 Train loss 4.11 on epoch=59
05/21/2022 00:15:47 - INFO - __main__ - Step 130 Global step 130 Train loss 4.18 on epoch=64
05/21/2022 00:15:49 - INFO - __main__ - Step 140 Global step 140 Train loss 3.98 on epoch=69
05/21/2022 00:15:51 - INFO - __main__ - Step 150 Global step 150 Train loss 3.96 on epoch=74
05/21/2022 00:15:54 - INFO - __main__ - Global step 150 Train loss 4.12 Classification-F1 0.0 on epoch=74
05/21/2022 00:15:56 - INFO - __main__ - Step 160 Global step 160 Train loss 3.85 on epoch=79
05/21/2022 00:15:58 - INFO - __main__ - Step 170 Global step 170 Train loss 3.87 on epoch=84
05/21/2022 00:16:00 - INFO - __main__ - Step 180 Global step 180 Train loss 3.88 on epoch=89
05/21/2022 00:16:02 - INFO - __main__ - Step 190 Global step 190 Train loss 3.78 on epoch=94
05/21/2022 00:16:04 - INFO - __main__ - Step 200 Global step 200 Train loss 3.64 on epoch=99
05/21/2022 00:16:06 - INFO - __main__ - Global step 200 Train loss 3.80 Classification-F1 0.038461538461538464 on epoch=99
05/21/2022 00:16:06 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.038461538461538464 on epoch=99, global_step=200
05/21/2022 00:16:08 - INFO - __main__ - Step 210 Global step 210 Train loss 3.56 on epoch=104
05/21/2022 00:16:10 - INFO - __main__ - Step 220 Global step 220 Train loss 3.37 on epoch=109
05/21/2022 00:16:12 - INFO - __main__ - Step 230 Global step 230 Train loss 3.41 on epoch=114
05/21/2022 00:16:14 - INFO - __main__ - Step 240 Global step 240 Train loss 3.28 on epoch=119
05/21/2022 00:16:16 - INFO - __main__ - Step 250 Global step 250 Train loss 3.26 on epoch=124
05/21/2022 00:16:18 - INFO - __main__ - Global step 250 Train loss 3.38 Classification-F1 0.3333333333333333 on epoch=124
05/21/2022 00:16:18 - INFO - __main__ - Saving model with best Classification-F1: 0.038461538461538464 -> 0.3333333333333333 on epoch=124, global_step=250
05/21/2022 00:16:20 - INFO - __main__ - Step 260 Global step 260 Train loss 3.09 on epoch=129
05/21/2022 00:16:22 - INFO - __main__ - Step 270 Global step 270 Train loss 3.06 on epoch=134
05/21/2022 00:16:24 - INFO - __main__ - Step 280 Global step 280 Train loss 3.08 on epoch=139
05/21/2022 00:16:26 - INFO - __main__ - Step 290 Global step 290 Train loss 2.93 on epoch=144
05/21/2022 00:16:28 - INFO - __main__ - Step 300 Global step 300 Train loss 2.87 on epoch=149
05/21/2022 00:16:34 - INFO - __main__ - Global step 300 Train loss 3.01 Classification-F1 0.3333333333333333 on epoch=149
05/21/2022 00:16:36 - INFO - __main__ - Step 310 Global step 310 Train loss 2.63 on epoch=154
05/21/2022 00:16:38 - INFO - __main__ - Step 320 Global step 320 Train loss 2.63 on epoch=159
05/21/2022 00:16:40 - INFO - __main__ - Step 330 Global step 330 Train loss 2.56 on epoch=164
05/21/2022 00:16:42 - INFO - __main__ - Step 340 Global step 340 Train loss 2.44 on epoch=169
05/21/2022 00:16:44 - INFO - __main__ - Step 350 Global step 350 Train loss 2.40 on epoch=174
05/21/2022 00:16:46 - INFO - __main__ - Global step 350 Train loss 2.53 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 00:16:48 - INFO - __main__ - Step 360 Global step 360 Train loss 2.41 on epoch=179
05/21/2022 00:16:50 - INFO - __main__ - Step 370 Global step 370 Train loss 2.35 on epoch=184
05/21/2022 00:16:52 - INFO - __main__ - Step 380 Global step 380 Train loss 2.19 on epoch=189
05/21/2022 00:16:54 - INFO - __main__ - Step 390 Global step 390 Train loss 2.18 on epoch=194
05/21/2022 00:16:56 - INFO - __main__ - Step 400 Global step 400 Train loss 2.07 on epoch=199
05/21/2022 00:16:58 - INFO - __main__ - Global step 400 Train loss 2.24 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 00:17:00 - INFO - __main__ - Step 410 Global step 410 Train loss 2.03 on epoch=204
05/21/2022 00:17:02 - INFO - __main__ - Step 420 Global step 420 Train loss 2.06 on epoch=209
05/21/2022 00:17:04 - INFO - __main__ - Step 430 Global step 430 Train loss 2.07 on epoch=214
05/21/2022 00:17:06 - INFO - __main__ - Step 440 Global step 440 Train loss 1.92 on epoch=219
05/21/2022 00:17:08 - INFO - __main__ - Step 450 Global step 450 Train loss 2.05 on epoch=224
05/21/2022 00:17:14 - INFO - __main__ - Global step 450 Train loss 2.03 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 00:17:16 - INFO - __main__ - Step 460 Global step 460 Train loss 2.05 on epoch=229
05/21/2022 00:17:18 - INFO - __main__ - Step 470 Global step 470 Train loss 1.83 on epoch=234
05/21/2022 00:17:20 - INFO - __main__ - Step 480 Global step 480 Train loss 1.72 on epoch=239
05/21/2022 00:17:22 - INFO - __main__ - Step 490 Global step 490 Train loss 1.77 on epoch=244
05/21/2022 00:17:24 - INFO - __main__ - Step 500 Global step 500 Train loss 1.70 on epoch=249
05/21/2022 00:17:25 - INFO - __main__ - Global step 500 Train loss 1.81 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 00:17:27 - INFO - __main__ - Step 510 Global step 510 Train loss 1.59 on epoch=254
05/21/2022 00:17:29 - INFO - __main__ - Step 520 Global step 520 Train loss 1.62 on epoch=259
05/21/2022 00:17:31 - INFO - __main__ - Step 530 Global step 530 Train loss 1.78 on epoch=264
05/21/2022 00:17:33 - INFO - __main__ - Step 540 Global step 540 Train loss 1.49 on epoch=269
05/21/2022 00:17:34 - INFO - __main__ - Step 550 Global step 550 Train loss 1.53 on epoch=274
05/21/2022 00:17:37 - INFO - __main__ - Global step 550 Train loss 1.60 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 00:17:39 - INFO - __main__ - Step 560 Global step 560 Train loss 1.45 on epoch=279
05/21/2022 00:17:41 - INFO - __main__ - Step 570 Global step 570 Train loss 1.48 on epoch=284
05/21/2022 00:17:43 - INFO - __main__ - Step 580 Global step 580 Train loss 1.20 on epoch=289
05/21/2022 00:17:45 - INFO - __main__ - Step 590 Global step 590 Train loss 1.32 on epoch=294
05/21/2022 00:17:47 - INFO - __main__ - Step 600 Global step 600 Train loss 1.30 on epoch=299
05/21/2022 00:17:51 - INFO - __main__ - Global step 600 Train loss 1.35 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 00:17:52 - INFO - __main__ - Step 610 Global step 610 Train loss 1.17 on epoch=304
05/21/2022 00:17:55 - INFO - __main__ - Step 620 Global step 620 Train loss 1.25 on epoch=309
05/21/2022 00:17:57 - INFO - __main__ - Step 630 Global step 630 Train loss 1.18 on epoch=314
05/21/2022 00:17:59 - INFO - __main__ - Step 640 Global step 640 Train loss 1.17 on epoch=319
05/21/2022 00:18:00 - INFO - __main__ - Step 650 Global step 650 Train loss 1.21 on epoch=324
05/21/2022 00:18:03 - INFO - __main__ - Global step 650 Train loss 1.20 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 00:18:05 - INFO - __main__ - Step 660 Global step 660 Train loss 1.13 on epoch=329
05/21/2022 00:18:07 - INFO - __main__ - Step 670 Global step 670 Train loss 1.01 on epoch=334
05/21/2022 00:18:09 - INFO - __main__ - Step 680 Global step 680 Train loss 1.03 on epoch=339
05/21/2022 00:18:11 - INFO - __main__ - Step 690 Global step 690 Train loss 1.02 on epoch=344
05/21/2022 00:18:13 - INFO - __main__ - Step 700 Global step 700 Train loss 1.02 on epoch=349
05/21/2022 00:18:14 - INFO - __main__ - Global step 700 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 00:18:15 - INFO - __main__ - Step 710 Global step 710 Train loss 1.08 on epoch=354
05/21/2022 00:18:17 - INFO - __main__ - Step 720 Global step 720 Train loss 1.00 on epoch=359
05/21/2022 00:18:19 - INFO - __main__ - Step 730 Global step 730 Train loss 1.16 on epoch=364
05/21/2022 00:18:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.96 on epoch=369
05/21/2022 00:18:23 - INFO - __main__ - Step 750 Global step 750 Train loss 1.01 on epoch=374
05/21/2022 00:18:24 - INFO - __main__ - Global step 750 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 00:18:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.90 on epoch=379
05/21/2022 00:18:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.91 on epoch=384
05/21/2022 00:18:30 - INFO - __main__ - Step 780 Global step 780 Train loss 1.03 on epoch=389
05/21/2022 00:18:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.89 on epoch=394
05/21/2022 00:18:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.87 on epoch=399
05/21/2022 00:18:40 - INFO - __main__ - Global step 800 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 00:18:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.92 on epoch=404
05/21/2022 00:18:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.90 on epoch=409
05/21/2022 00:18:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.80 on epoch=414
05/21/2022 00:18:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.84 on epoch=419
05/21/2022 00:18:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=424
05/21/2022 00:18:51 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 00:18:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.79 on epoch=429
05/21/2022 00:18:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.83 on epoch=434
05/21/2022 00:18:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.74 on epoch=439
05/21/2022 00:18:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.85 on epoch=444
05/21/2022 00:19:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.76 on epoch=449
05/21/2022 00:19:02 - INFO - __main__ - Global step 900 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 00:19:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.80 on epoch=454
05/21/2022 00:19:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.73 on epoch=459
05/21/2022 00:19:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.78 on epoch=464
05/21/2022 00:19:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.79 on epoch=469
05/21/2022 00:19:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.78 on epoch=474
05/21/2022 00:19:13 - INFO - __main__ - Global step 950 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 00:19:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.71 on epoch=479
05/21/2022 00:19:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.82 on epoch=484
05/21/2022 00:19:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.84 on epoch=489
05/21/2022 00:19:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=494
05/21/2022 00:19:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.77 on epoch=499
05/21/2022 00:19:24 - INFO - __main__ - Global step 1000 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 00:19:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.76 on epoch=504
05/21/2022 00:19:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.71 on epoch=509
05/21/2022 00:19:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.78 on epoch=514
05/21/2022 00:19:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.68 on epoch=519
05/21/2022 00:19:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.72 on epoch=524
05/21/2022 00:19:35 - INFO - __main__ - Global step 1050 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 00:19:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.73 on epoch=529
05/21/2022 00:19:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=534
05/21/2022 00:19:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.66 on epoch=539
05/21/2022 00:19:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.62 on epoch=544
05/21/2022 00:19:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.74 on epoch=549
05/21/2022 00:19:46 - INFO - __main__ - Global step 1100 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 00:19:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.71 on epoch=554
05/21/2022 00:19:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.78 on epoch=559
05/21/2022 00:19:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.70 on epoch=564
05/21/2022 00:19:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.72 on epoch=569
05/21/2022 00:19:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.71 on epoch=574
05/21/2022 00:19:57 - INFO - __main__ - Global step 1150 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 00:19:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.67 on epoch=579
05/21/2022 00:20:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.72 on epoch=584
05/21/2022 00:20:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.74 on epoch=589
05/21/2022 00:20:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.70 on epoch=594
05/21/2022 00:20:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.61 on epoch=599
05/21/2022 00:20:08 - INFO - __main__ - Global step 1200 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 00:20:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.66 on epoch=604
05/21/2022 00:20:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.63 on epoch=609
05/21/2022 00:20:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.66 on epoch=614
05/21/2022 00:20:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.66 on epoch=619
05/21/2022 00:20:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.59 on epoch=624
05/21/2022 00:20:19 - INFO - __main__ - Global step 1250 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 00:20:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.60 on epoch=629
05/21/2022 00:20:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.62 on epoch=634
05/21/2022 00:20:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.70 on epoch=639
05/21/2022 00:20:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.59 on epoch=644
05/21/2022 00:20:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.61 on epoch=649
05/21/2022 00:20:29 - INFO - __main__ - Global step 1300 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 00:20:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.54 on epoch=654
05/21/2022 00:20:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.54 on epoch=659
05/21/2022 00:20:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.55 on epoch=664
05/21/2022 00:20:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.54 on epoch=669
05/21/2022 00:20:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.62 on epoch=674
05/21/2022 00:20:40 - INFO - __main__ - Global step 1350 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 00:20:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.60 on epoch=679
05/21/2022 00:20:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.54 on epoch=684
05/21/2022 00:20:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
05/21/2022 00:20:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.53 on epoch=694
05/21/2022 00:20:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.58 on epoch=699
05/21/2022 00:20:51 - INFO - __main__ - Global step 1400 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 00:20:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.50 on epoch=704
05/21/2022 00:20:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=709
05/21/2022 00:20:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=714
05/21/2022 00:20:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.55 on epoch=719
05/21/2022 00:21:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.54 on epoch=724
05/21/2022 00:21:02 - INFO - __main__ - Global step 1450 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 00:21:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=729
05/21/2022 00:21:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.57 on epoch=734
05/21/2022 00:21:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.50 on epoch=739
05/21/2022 00:21:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.53 on epoch=744
05/21/2022 00:21:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.51 on epoch=749
05/21/2022 00:21:13 - INFO - __main__ - Global step 1500 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 00:21:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.50 on epoch=754
05/21/2022 00:21:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=759
05/21/2022 00:21:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.49 on epoch=764
05/21/2022 00:21:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.52 on epoch=769
05/21/2022 00:21:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.50 on epoch=774
05/21/2022 00:21:24 - INFO - __main__ - Global step 1550 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 00:21:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=779
05/21/2022 00:21:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.52 on epoch=784
05/21/2022 00:21:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
05/21/2022 00:21:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.52 on epoch=794
05/21/2022 00:21:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.46 on epoch=799
05/21/2022 00:21:34 - INFO - __main__ - Global step 1600 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 00:21:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
05/21/2022 00:21:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=809
05/21/2022 00:21:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=814
05/21/2022 00:21:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=819
05/21/2022 00:21:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=824
05/21/2022 00:21:45 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 00:21:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=829
05/21/2022 00:21:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=834
05/21/2022 00:21:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.48 on epoch=839
05/21/2022 00:21:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
05/21/2022 00:21:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=849
05/21/2022 00:21:56 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 00:21:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=854
05/21/2022 00:22:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=859
05/21/2022 00:22:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=864
05/21/2022 00:22:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=869
05/21/2022 00:22:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=874
05/21/2022 00:22:06 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 00:22:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=879
05/21/2022 00:22:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
05/21/2022 00:22:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=889
05/21/2022 00:22:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
05/21/2022 00:22:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
05/21/2022 00:22:17 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 00:22:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=904
05/21/2022 00:22:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
05/21/2022 00:22:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=914
05/21/2022 00:22:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=919
05/21/2022 00:22:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=924
05/21/2022 00:22:28 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 00:22:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
05/21/2022 00:22:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=934
05/21/2022 00:22:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.44 on epoch=939
05/21/2022 00:22:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.48 on epoch=944
05/21/2022 00:22:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=949
05/21/2022 00:22:39 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 00:22:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=954
05/21/2022 00:22:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.46 on epoch=959
05/21/2022 00:22:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.43 on epoch=964
05/21/2022 00:22:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
05/21/2022 00:22:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/21/2022 00:22:50 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 00:22:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=979
05/21/2022 00:22:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=984
05/21/2022 00:22:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=989
05/21/2022 00:22:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/21/2022 00:23:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=999
05/21/2022 00:23:01 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 00:23:01 - INFO - __main__ - save last model!
05/21/2022 00:23:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 00:23:01 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 00:23:01 - INFO - __main__ - Printing 3 examples
05/21/2022 00:23:01 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:23:01 - INFO - __main__ - ['entailed']
05/21/2022 00:23:01 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:23:01 - INFO - __main__ - ['entailed']
05/21/2022 00:23:01 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:23:01 - INFO - __main__ - ['entailed']
05/21/2022 00:23:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:23:01 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:23:01 - INFO - __main__ - Printing 3 examples
05/21/2022 00:23:01 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/21/2022 00:23:01 - INFO - __main__ - ['refuted']
05/21/2022 00:23:01 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/21/2022 00:23:01 - INFO - __main__ - ['refuted']
05/21/2022 00:23:01 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/21/2022 00:23:01 - INFO - __main__ - ['refuted']
05/21/2022 00:23:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:23:01 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:23:01 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 00:23:01 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:23:01 - INFO - __main__ - Printing 3 examples
05/21/2022 00:23:01 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/21/2022 00:23:01 - INFO - __main__ - ['refuted']
05/21/2022 00:23:01 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/21/2022 00:23:01 - INFO - __main__ - ['refuted']
05/21/2022 00:23:01 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/21/2022 00:23:01 - INFO - __main__ - ['refuted']
05/21/2022 00:23:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:23:01 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:23:02 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 00:23:07 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 00:23:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 00:23:08 - INFO - __main__ - Starting training!
05/21/2022 00:23:29 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:23:45 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 00:27:56 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.3_8_predictions.txt
05/21/2022 00:27:56 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 00:27:56 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/21/2022 00:27:56 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.2, bsz=8 ...
05/21/2022 00:27:57 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:27:57 - INFO - __main__ - Printing 3 examples
05/21/2022 00:27:57 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/21/2022 00:27:57 - INFO - __main__ - ['refuted']
05/21/2022 00:27:57 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/21/2022 00:27:57 - INFO - __main__ - ['refuted']
05/21/2022 00:27:57 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/21/2022 00:27:57 - INFO - __main__ - ['refuted']
05/21/2022 00:27:57 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:27:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:27:58 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 00:27:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:27:58 - INFO - __main__ - Printing 3 examples
05/21/2022 00:27:58 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/21/2022 00:27:58 - INFO - __main__ - ['refuted']
05/21/2022 00:27:58 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/21/2022 00:27:58 - INFO - __main__ - ['refuted']
05/21/2022 00:27:58 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/21/2022 00:27:58 - INFO - __main__ - ['refuted']
05/21/2022 00:27:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:27:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:27:58 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 00:28:04 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 00:28:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 00:28:04 - INFO - __main__ - Starting training!
05/21/2022 00:28:06 - INFO - __main__ - Step 10 Global step 10 Train loss 5.02 on epoch=4
05/21/2022 00:28:08 - INFO - __main__ - Step 20 Global step 20 Train loss 4.94 on epoch=9
05/21/2022 00:28:10 - INFO - __main__ - Step 30 Global step 30 Train loss 5.06 on epoch=14
05/21/2022 00:28:12 - INFO - __main__ - Step 40 Global step 40 Train loss 4.93 on epoch=19
05/21/2022 00:28:14 - INFO - __main__ - Step 50 Global step 50 Train loss 4.78 on epoch=24
05/21/2022 00:28:15 - INFO - __main__ - Global step 50 Train loss 4.95 Classification-F1 0.0 on epoch=24
05/21/2022 00:28:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 00:28:18 - INFO - __main__ - Step 60 Global step 60 Train loss 4.64 on epoch=29
05/21/2022 00:28:20 - INFO - __main__ - Step 70 Global step 70 Train loss 4.87 on epoch=34
05/21/2022 00:28:22 - INFO - __main__ - Step 80 Global step 80 Train loss 4.69 on epoch=39
05/21/2022 00:28:24 - INFO - __main__ - Step 90 Global step 90 Train loss 4.57 on epoch=44
05/21/2022 00:28:26 - INFO - __main__ - Step 100 Global step 100 Train loss 4.59 on epoch=49
05/21/2022 00:28:27 - INFO - __main__ - Global step 100 Train loss 4.67 Classification-F1 0.0 on epoch=49
05/21/2022 00:28:29 - INFO - __main__ - Step 110 Global step 110 Train loss 4.47 on epoch=54
05/21/2022 00:28:31 - INFO - __main__ - Step 120 Global step 120 Train loss 4.57 on epoch=59
05/21/2022 00:28:33 - INFO - __main__ - Step 130 Global step 130 Train loss 4.49 on epoch=64
05/21/2022 00:28:35 - INFO - __main__ - Step 140 Global step 140 Train loss 4.45 on epoch=69
05/21/2022 00:28:37 - INFO - __main__ - Step 150 Global step 150 Train loss 4.39 on epoch=74
05/21/2022 00:28:38 - INFO - __main__ - Global step 150 Train loss 4.47 Classification-F1 0.0 on epoch=74
05/21/2022 00:28:40 - INFO - __main__ - Step 160 Global step 160 Train loss 4.35 on epoch=79
05/21/2022 00:28:42 - INFO - __main__ - Step 170 Global step 170 Train loss 4.32 on epoch=84
05/21/2022 00:28:44 - INFO - __main__ - Step 180 Global step 180 Train loss 4.30 on epoch=89
05/21/2022 00:28:46 - INFO - __main__ - Step 190 Global step 190 Train loss 4.26 on epoch=94
05/21/2022 00:28:48 - INFO - __main__ - Step 200 Global step 200 Train loss 4.22 on epoch=99
05/21/2022 00:28:50 - INFO - __main__ - Global step 200 Train loss 4.29 Classification-F1 0.0 on epoch=99
05/21/2022 00:28:52 - INFO - __main__ - Step 210 Global step 210 Train loss 4.14 on epoch=104
05/21/2022 00:28:54 - INFO - __main__ - Step 220 Global step 220 Train loss 4.06 on epoch=109
05/21/2022 00:28:56 - INFO - __main__ - Step 230 Global step 230 Train loss 4.18 on epoch=114
05/21/2022 00:28:58 - INFO - __main__ - Step 240 Global step 240 Train loss 4.07 on epoch=119
05/21/2022 00:29:00 - INFO - __main__ - Step 250 Global step 250 Train loss 3.99 on epoch=124
05/21/2022 00:29:01 - INFO - __main__ - Global step 250 Train loss 4.09 Classification-F1 0.0 on epoch=124
05/21/2022 00:29:03 - INFO - __main__ - Step 260 Global step 260 Train loss 3.92 on epoch=129
05/21/2022 00:29:05 - INFO - __main__ - Step 270 Global step 270 Train loss 3.87 on epoch=134
05/21/2022 00:29:07 - INFO - __main__ - Step 280 Global step 280 Train loss 3.95 on epoch=139
05/21/2022 00:29:09 - INFO - __main__ - Step 290 Global step 290 Train loss 3.87 on epoch=144
05/21/2022 00:29:11 - INFO - __main__ - Step 300 Global step 300 Train loss 3.83 on epoch=149
05/21/2022 00:29:13 - INFO - __main__ - Global step 300 Train loss 3.89 Classification-F1 0.0 on epoch=149
05/21/2022 00:29:15 - INFO - __main__ - Step 310 Global step 310 Train loss 3.68 on epoch=154
05/21/2022 00:29:17 - INFO - __main__ - Step 320 Global step 320 Train loss 3.76 on epoch=159
05/21/2022 00:29:19 - INFO - __main__ - Step 330 Global step 330 Train loss 3.62 on epoch=164
05/21/2022 00:29:21 - INFO - __main__ - Step 340 Global step 340 Train loss 3.68 on epoch=169
05/21/2022 00:29:23 - INFO - __main__ - Step 350 Global step 350 Train loss 3.69 on epoch=174
05/21/2022 00:29:25 - INFO - __main__ - Global step 350 Train loss 3.69 Classification-F1 0.0 on epoch=174
05/21/2022 00:29:27 - INFO - __main__ - Step 360 Global step 360 Train loss 3.58 on epoch=179
05/21/2022 00:29:29 - INFO - __main__ - Step 370 Global step 370 Train loss 3.53 on epoch=184
05/21/2022 00:29:31 - INFO - __main__ - Step 380 Global step 380 Train loss 3.50 on epoch=189
05/21/2022 00:29:33 - INFO - __main__ - Step 390 Global step 390 Train loss 3.41 on epoch=194
05/21/2022 00:29:35 - INFO - __main__ - Step 400 Global step 400 Train loss 3.41 on epoch=199
05/21/2022 00:29:37 - INFO - __main__ - Global step 400 Train loss 3.48 Classification-F1 0.07777777777777778 on epoch=199
05/21/2022 00:29:37 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.07777777777777778 on epoch=199, global_step=400
05/21/2022 00:29:39 - INFO - __main__ - Step 410 Global step 410 Train loss 3.31 on epoch=204
05/21/2022 00:29:41 - INFO - __main__ - Step 420 Global step 420 Train loss 3.31 on epoch=209
05/21/2022 00:29:43 - INFO - __main__ - Step 430 Global step 430 Train loss 3.24 on epoch=214
05/21/2022 00:29:45 - INFO - __main__ - Step 440 Global step 440 Train loss 3.21 on epoch=219
05/21/2022 00:29:47 - INFO - __main__ - Step 450 Global step 450 Train loss 3.19 on epoch=224
05/21/2022 00:29:48 - INFO - __main__ - Global step 450 Train loss 3.25 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 00:29:48 - INFO - __main__ - Saving model with best Classification-F1: 0.07777777777777778 -> 0.3333333333333333 on epoch=224, global_step=450
05/21/2022 00:29:50 - INFO - __main__ - Step 460 Global step 460 Train loss 3.14 on epoch=229
05/21/2022 00:29:52 - INFO - __main__ - Step 470 Global step 470 Train loss 3.10 on epoch=234
05/21/2022 00:29:54 - INFO - __main__ - Step 480 Global step 480 Train loss 2.98 on epoch=239
05/21/2022 00:29:56 - INFO - __main__ - Step 490 Global step 490 Train loss 3.04 on epoch=244
05/21/2022 00:29:58 - INFO - __main__ - Step 500 Global step 500 Train loss 2.94 on epoch=249
05/21/2022 00:30:03 - INFO - __main__ - Global step 500 Train loss 3.04 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 00:30:05 - INFO - __main__ - Step 510 Global step 510 Train loss 2.93 on epoch=254
05/21/2022 00:30:07 - INFO - __main__ - Step 520 Global step 520 Train loss 2.88 on epoch=259
05/21/2022 00:30:09 - INFO - __main__ - Step 530 Global step 530 Train loss 2.80 on epoch=264
05/21/2022 00:30:11 - INFO - __main__ - Step 540 Global step 540 Train loss 2.80 on epoch=269
05/21/2022 00:30:13 - INFO - __main__ - Step 550 Global step 550 Train loss 2.69 on epoch=274
05/21/2022 00:30:16 - INFO - __main__ - Global step 550 Train loss 2.82 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 00:30:18 - INFO - __main__ - Step 560 Global step 560 Train loss 2.67 on epoch=279
05/21/2022 00:30:20 - INFO - __main__ - Step 570 Global step 570 Train loss 2.59 on epoch=284
05/21/2022 00:30:22 - INFO - __main__ - Step 580 Global step 580 Train loss 2.49 on epoch=289
05/21/2022 00:30:24 - INFO - __main__ - Step 590 Global step 590 Train loss 2.65 on epoch=294
05/21/2022 00:30:26 - INFO - __main__ - Step 600 Global step 600 Train loss 2.58 on epoch=299
05/21/2022 00:30:30 - INFO - __main__ - Global step 600 Train loss 2.60 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 00:30:32 - INFO - __main__ - Step 610 Global step 610 Train loss 2.54 on epoch=304
05/21/2022 00:30:34 - INFO - __main__ - Step 620 Global step 620 Train loss 2.37 on epoch=309
05/21/2022 00:30:36 - INFO - __main__ - Step 630 Global step 630 Train loss 2.29 on epoch=314
05/21/2022 00:30:38 - INFO - __main__ - Step 640 Global step 640 Train loss 2.32 on epoch=319
05/21/2022 00:30:40 - INFO - __main__ - Step 650 Global step 650 Train loss 2.40 on epoch=324
05/21/2022 00:30:44 - INFO - __main__ - Global step 650 Train loss 2.38 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 00:30:46 - INFO - __main__ - Step 660 Global step 660 Train loss 2.33 on epoch=329
05/21/2022 00:30:48 - INFO - __main__ - Step 670 Global step 670 Train loss 2.19 on epoch=334
05/21/2022 00:30:50 - INFO - __main__ - Step 680 Global step 680 Train loss 2.20 on epoch=339
05/21/2022 00:30:52 - INFO - __main__ - Step 690 Global step 690 Train loss 2.23 on epoch=344
05/21/2022 00:30:54 - INFO - __main__ - Step 700 Global step 700 Train loss 2.26 on epoch=349
05/21/2022 00:30:57 - INFO - __main__ - Global step 700 Train loss 2.24 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 00:30:59 - INFO - __main__ - Step 710 Global step 710 Train loss 2.04 on epoch=354
05/21/2022 00:31:01 - INFO - __main__ - Step 720 Global step 720 Train loss 2.10 on epoch=359
05/21/2022 00:31:03 - INFO - __main__ - Step 730 Global step 730 Train loss 2.03 on epoch=364
05/21/2022 00:31:05 - INFO - __main__ - Step 740 Global step 740 Train loss 2.04 on epoch=369
05/21/2022 00:31:07 - INFO - __main__ - Step 750 Global step 750 Train loss 2.06 on epoch=374
05/21/2022 00:31:11 - INFO - __main__ - Global step 750 Train loss 2.06 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 00:31:13 - INFO - __main__ - Step 760 Global step 760 Train loss 2.04 on epoch=379
05/21/2022 00:31:15 - INFO - __main__ - Step 770 Global step 770 Train loss 1.92 on epoch=384
05/21/2022 00:31:17 - INFO - __main__ - Step 780 Global step 780 Train loss 1.88 on epoch=389
05/21/2022 00:31:19 - INFO - __main__ - Step 790 Global step 790 Train loss 1.88 on epoch=394
05/21/2022 00:31:21 - INFO - __main__ - Step 800 Global step 800 Train loss 1.80 on epoch=399
05/21/2022 00:31:22 - INFO - __main__ - Global step 800 Train loss 1.91 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 00:31:24 - INFO - __main__ - Step 810 Global step 810 Train loss 1.77 on epoch=404
05/21/2022 00:31:26 - INFO - __main__ - Step 820 Global step 820 Train loss 1.66 on epoch=409
05/21/2022 00:31:28 - INFO - __main__ - Step 830 Global step 830 Train loss 1.76 on epoch=414
05/21/2022 00:31:30 - INFO - __main__ - Step 840 Global step 840 Train loss 1.59 on epoch=419
05/21/2022 00:31:32 - INFO - __main__ - Step 850 Global step 850 Train loss 1.57 on epoch=424
05/21/2022 00:31:33 - INFO - __main__ - Global step 850 Train loss 1.67 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 00:31:35 - INFO - __main__ - Step 860 Global step 860 Train loss 1.65 on epoch=429
05/21/2022 00:31:37 - INFO - __main__ - Step 870 Global step 870 Train loss 1.80 on epoch=434
05/21/2022 00:31:39 - INFO - __main__ - Step 880 Global step 880 Train loss 1.54 on epoch=439
05/21/2022 00:31:41 - INFO - __main__ - Step 890 Global step 890 Train loss 1.56 on epoch=444
05/21/2022 00:31:43 - INFO - __main__ - Step 900 Global step 900 Train loss 1.53 on epoch=449
05/21/2022 00:31:46 - INFO - __main__ - Global step 900 Train loss 1.62 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 00:31:48 - INFO - __main__ - Step 910 Global step 910 Train loss 1.41 on epoch=454
05/21/2022 00:31:50 - INFO - __main__ - Step 920 Global step 920 Train loss 1.49 on epoch=459
05/21/2022 00:31:52 - INFO - __main__ - Step 930 Global step 930 Train loss 1.50 on epoch=464
05/21/2022 00:31:54 - INFO - __main__ - Step 940 Global step 940 Train loss 1.32 on epoch=469
05/21/2022 00:31:56 - INFO - __main__ - Step 950 Global step 950 Train loss 1.31 on epoch=474
05/21/2022 00:31:59 - INFO - __main__ - Global step 950 Train loss 1.40 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 00:32:01 - INFO - __main__ - Step 960 Global step 960 Train loss 1.35 on epoch=479
05/21/2022 00:32:03 - INFO - __main__ - Step 970 Global step 970 Train loss 1.42 on epoch=484
05/21/2022 00:32:05 - INFO - __main__ - Step 980 Global step 980 Train loss 1.28 on epoch=489
05/21/2022 00:32:07 - INFO - __main__ - Step 990 Global step 990 Train loss 1.29 on epoch=494
05/21/2022 00:32:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.29 on epoch=499
05/21/2022 00:32:13 - INFO - __main__ - Global step 1000 Train loss 1.33 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 00:32:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.32 on epoch=504
05/21/2022 00:32:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.29 on epoch=509
05/21/2022 00:32:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.15 on epoch=514
05/21/2022 00:32:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.21 on epoch=519
05/21/2022 00:32:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.14 on epoch=524
05/21/2022 00:32:26 - INFO - __main__ - Global step 1050 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 00:32:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.13 on epoch=529
05/21/2022 00:32:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.14 on epoch=534
05/21/2022 00:32:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.12 on epoch=539
05/21/2022 00:32:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.16 on epoch=544
05/21/2022 00:32:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.12 on epoch=549
05/21/2022 00:32:39 - INFO - __main__ - Global step 1100 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 00:32:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.11 on epoch=554
05/21/2022 00:32:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.20 on epoch=559
05/21/2022 00:32:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.03 on epoch=564
05/21/2022 00:32:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.09 on epoch=569
05/21/2022 00:32:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.08 on epoch=574
05/21/2022 00:32:50 - INFO - __main__ - Global step 1150 Train loss 1.10 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 00:32:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.01 on epoch=579
05/21/2022 00:32:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.02 on epoch=584
05/21/2022 00:32:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.97 on epoch=589
05/21/2022 00:32:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.93 on epoch=594
05/21/2022 00:33:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.97 on epoch=599
05/21/2022 00:33:03 - INFO - __main__ - Global step 1200 Train loss 0.98 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 00:33:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.96 on epoch=604
05/21/2022 00:33:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.94 on epoch=609
05/21/2022 00:33:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.88 on epoch=614
05/21/2022 00:33:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.87 on epoch=619
05/21/2022 00:33:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.91 on epoch=624
05/21/2022 00:33:13 - INFO - __main__ - Global step 1250 Train loss 0.91 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 00:33:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.00 on epoch=629
05/21/2022 00:33:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.98 on epoch=634
05/21/2022 00:33:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.82 on epoch=639
05/21/2022 00:33:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.94 on epoch=644
05/21/2022 00:33:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.88 on epoch=649
05/21/2022 00:33:25 - INFO - __main__ - Global step 1300 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 00:33:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.82 on epoch=654
05/21/2022 00:33:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.88 on epoch=659
05/21/2022 00:33:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.83 on epoch=664
05/21/2022 00:33:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.97 on epoch=669
05/21/2022 00:33:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=674
05/21/2022 00:33:36 - INFO - __main__ - Global step 1350 Train loss 0.87 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 00:33:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.80 on epoch=679
05/21/2022 00:33:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.73 on epoch=684
05/21/2022 00:33:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.78 on epoch=689
05/21/2022 00:33:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.78 on epoch=694
05/21/2022 00:33:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.69 on epoch=699
05/21/2022 00:33:47 - INFO - __main__ - Global step 1400 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 00:33:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.81 on epoch=704
05/21/2022 00:33:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.73 on epoch=709
05/21/2022 00:33:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.69 on epoch=714
05/21/2022 00:33:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.73 on epoch=719
05/21/2022 00:33:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.70 on epoch=724
05/21/2022 00:33:58 - INFO - __main__ - Global step 1450 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 00:34:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.70 on epoch=729
05/21/2022 00:34:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.68 on epoch=734
05/21/2022 00:34:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.76 on epoch=739
05/21/2022 00:34:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.70 on epoch=744
05/21/2022 00:34:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.72 on epoch=749
05/21/2022 00:34:09 - INFO - __main__ - Global step 1500 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 00:34:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.72 on epoch=754
05/21/2022 00:34:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.66 on epoch=759
05/21/2022 00:34:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.67 on epoch=764
05/21/2022 00:34:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.68 on epoch=769
05/21/2022 00:34:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.68 on epoch=774
05/21/2022 00:34:20 - INFO - __main__ - Global step 1550 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 00:34:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.66 on epoch=779
05/21/2022 00:34:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.73 on epoch=784
05/21/2022 00:34:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.59 on epoch=789
05/21/2022 00:34:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.64 on epoch=794
05/21/2022 00:34:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.71 on epoch=799
05/21/2022 00:34:31 - INFO - __main__ - Global step 1600 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 00:34:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.71 on epoch=804
05/21/2022 00:34:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.64 on epoch=809
05/21/2022 00:34:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.55 on epoch=814
05/21/2022 00:34:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.64 on epoch=819
05/21/2022 00:34:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.62 on epoch=824
05/21/2022 00:34:42 - INFO - __main__ - Global step 1650 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 00:34:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.67 on epoch=829
05/21/2022 00:34:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.63 on epoch=834
05/21/2022 00:34:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.63 on epoch=839
05/21/2022 00:34:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.65 on epoch=844
05/21/2022 00:34:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.57 on epoch=849
05/21/2022 00:34:53 - INFO - __main__ - Global step 1700 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 00:34:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.53 on epoch=854
05/21/2022 00:34:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.59 on epoch=859
05/21/2022 00:34:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.67 on epoch=864
05/21/2022 00:35:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.57 on epoch=869
05/21/2022 00:35:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.57 on epoch=874
05/21/2022 00:35:04 - INFO - __main__ - Global step 1750 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 00:35:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.61 on epoch=879
05/21/2022 00:35:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.58 on epoch=884
05/21/2022 00:35:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.57 on epoch=889
05/21/2022 00:35:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=894
05/21/2022 00:35:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.55 on epoch=899
05/21/2022 00:35:16 - INFO - __main__ - Global step 1800 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 00:35:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.59 on epoch=904
05/21/2022 00:35:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.56 on epoch=909
05/21/2022 00:35:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=914
05/21/2022 00:35:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.54 on epoch=919
05/21/2022 00:35:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.50 on epoch=924
05/21/2022 00:35:27 - INFO - __main__ - Global step 1850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 00:35:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.52 on epoch=929
05/21/2022 00:35:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.49 on epoch=934
05/21/2022 00:35:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=939
05/21/2022 00:35:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
05/21/2022 00:35:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.57 on epoch=949
05/21/2022 00:35:38 - INFO - __main__ - Global step 1900 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 00:35:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.59 on epoch=954
05/21/2022 00:35:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.60 on epoch=959
05/21/2022 00:35:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.53 on epoch=964
05/21/2022 00:35:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.52 on epoch=969
05/21/2022 00:35:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.54 on epoch=974
05/21/2022 00:35:49 - INFO - __main__ - Global step 1950 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 00:35:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.48 on epoch=979
05/21/2022 00:35:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.51 on epoch=984
05/21/2022 00:35:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.56 on epoch=989
05/21/2022 00:35:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.51 on epoch=994
05/21/2022 00:35:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.50 on epoch=999
05/21/2022 00:36:00 - INFO - __main__ - Global step 2000 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 00:36:00 - INFO - __main__ - save last model!
05/21/2022 00:36:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 00:36:01 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 00:36:01 - INFO - __main__ - Printing 3 examples
05/21/2022 00:36:01 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:36:01 - INFO - __main__ - ['entailed']
05/21/2022 00:36:01 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:36:01 - INFO - __main__ - ['entailed']
05/21/2022 00:36:01 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:36:01 - INFO - __main__ - ['entailed']
05/21/2022 00:36:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:36:01 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:36:01 - INFO - __main__ - Printing 3 examples
05/21/2022 00:36:01 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/21/2022 00:36:01 - INFO - __main__ - ['entailed']
05/21/2022 00:36:01 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/21/2022 00:36:01 - INFO - __main__ - ['entailed']
05/21/2022 00:36:01 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/21/2022 00:36:01 - INFO - __main__ - ['entailed']
05/21/2022 00:36:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:36:01 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:36:01 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 00:36:01 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:36:01 - INFO - __main__ - Printing 3 examples
05/21/2022 00:36:01 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/21/2022 00:36:01 - INFO - __main__ - ['entailed']
05/21/2022 00:36:01 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/21/2022 00:36:01 - INFO - __main__ - ['entailed']
05/21/2022 00:36:01 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/21/2022 00:36:01 - INFO - __main__ - ['entailed']
05/21/2022 00:36:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:36:01 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:36:01 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 00:36:08 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 00:36:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 00:36:08 - INFO - __main__ - Starting training!
05/21/2022 00:36:25 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:36:38 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 00:42:37 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.2_8_predictions.txt
05/21/2022 00:42:37 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 00:42:37 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.2, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/21/2022 00:42:37 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.5, bsz=8 ...
05/21/2022 00:42:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:42:38 - INFO - __main__ - Printing 3 examples
05/21/2022 00:42:38 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/21/2022 00:42:38 - INFO - __main__ - ['entailed']
05/21/2022 00:42:38 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/21/2022 00:42:38 - INFO - __main__ - ['entailed']
05/21/2022 00:42:38 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/21/2022 00:42:38 - INFO - __main__ - ['entailed']
05/21/2022 00:42:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:42:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:42:38 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 00:42:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:42:38 - INFO - __main__ - Printing 3 examples
05/21/2022 00:42:38 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/21/2022 00:42:38 - INFO - __main__ - ['entailed']
05/21/2022 00:42:38 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/21/2022 00:42:38 - INFO - __main__ - ['entailed']
05/21/2022 00:42:38 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/21/2022 00:42:38 - INFO - __main__ - ['entailed']
05/21/2022 00:42:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:42:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:42:38 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 00:42:44 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 00:42:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 00:42:44 - INFO - __main__ - Starting training!
05/21/2022 00:42:46 - INFO - __main__ - Step 10 Global step 10 Train loss 5.10 on epoch=4
05/21/2022 00:42:48 - INFO - __main__ - Step 20 Global step 20 Train loss 4.95 on epoch=9
05/21/2022 00:42:50 - INFO - __main__ - Step 30 Global step 30 Train loss 4.84 on epoch=14
05/21/2022 00:42:52 - INFO - __main__ - Step 40 Global step 40 Train loss 4.85 on epoch=19
05/21/2022 00:42:54 - INFO - __main__ - Step 50 Global step 50 Train loss 4.77 on epoch=24
05/21/2022 00:43:05 - INFO - __main__ - Global step 50 Train loss 4.90 Classification-F1 0.0 on epoch=24
05/21/2022 00:43:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 00:43:07 - INFO - __main__ - Step 60 Global step 60 Train loss 4.63 on epoch=29
05/21/2022 00:43:09 - INFO - __main__ - Step 70 Global step 70 Train loss 4.45 on epoch=34
05/21/2022 00:43:11 - INFO - __main__ - Step 80 Global step 80 Train loss 4.54 on epoch=39
05/21/2022 00:43:13 - INFO - __main__ - Step 90 Global step 90 Train loss 4.29 on epoch=44
05/21/2022 00:43:15 - INFO - __main__ - Step 100 Global step 100 Train loss 4.30 on epoch=49
05/21/2022 00:43:27 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/21/2022 00:43:28 - INFO - __main__ - Step 110 Global step 110 Train loss 4.17 on epoch=54
05/21/2022 00:43:30 - INFO - __main__ - Step 120 Global step 120 Train loss 3.95 on epoch=59
05/21/2022 00:43:32 - INFO - __main__ - Step 130 Global step 130 Train loss 4.03 on epoch=64
05/21/2022 00:43:34 - INFO - __main__ - Step 140 Global step 140 Train loss 3.80 on epoch=69
05/21/2022 00:43:36 - INFO - __main__ - Step 150 Global step 150 Train loss 3.72 on epoch=74
05/21/2022 00:43:43 - INFO - __main__ - Global step 150 Train loss 3.93 Classification-F1 0.04105571847507331 on epoch=74
05/21/2022 00:43:43 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.04105571847507331 on epoch=74, global_step=150
05/21/2022 00:43:45 - INFO - __main__ - Step 160 Global step 160 Train loss 3.54 on epoch=79
05/21/2022 00:43:47 - INFO - __main__ - Step 170 Global step 170 Train loss 3.37 on epoch=84
05/21/2022 00:43:49 - INFO - __main__ - Step 180 Global step 180 Train loss 3.27 on epoch=89
05/21/2022 00:43:51 - INFO - __main__ - Step 190 Global step 190 Train loss 3.11 on epoch=94
05/21/2022 00:43:53 - INFO - __main__ - Step 200 Global step 200 Train loss 2.94 on epoch=99
05/21/2022 00:43:58 - INFO - __main__ - Global step 200 Train loss 3.25 Classification-F1 0.3333333333333333 on epoch=99
05/21/2022 00:43:58 - INFO - __main__ - Saving model with best Classification-F1: 0.04105571847507331 -> 0.3333333333333333 on epoch=99, global_step=200
05/21/2022 00:44:00 - INFO - __main__ - Step 210 Global step 210 Train loss 2.80 on epoch=104
05/21/2022 00:44:01 - INFO - __main__ - Step 220 Global step 220 Train loss 2.64 on epoch=109
05/21/2022 00:44:03 - INFO - __main__ - Step 230 Global step 230 Train loss 2.63 on epoch=114
05/21/2022 00:44:05 - INFO - __main__ - Step 240 Global step 240 Train loss 2.53 on epoch=119
05/21/2022 00:44:07 - INFO - __main__ - Step 250 Global step 250 Train loss 2.28 on epoch=124
05/21/2022 00:44:11 - INFO - __main__ - Global step 250 Train loss 2.58 Classification-F1 0.3333333333333333 on epoch=124
05/21/2022 00:44:13 - INFO - __main__ - Step 260 Global step 260 Train loss 2.17 on epoch=129
05/21/2022 00:44:15 - INFO - __main__ - Step 270 Global step 270 Train loss 2.15 on epoch=134
05/21/2022 00:44:17 - INFO - __main__ - Step 280 Global step 280 Train loss 1.98 on epoch=139
05/21/2022 00:44:19 - INFO - __main__ - Step 290 Global step 290 Train loss 1.82 on epoch=144
05/21/2022 00:44:21 - INFO - __main__ - Step 300 Global step 300 Train loss 1.75 on epoch=149
05/21/2022 00:44:24 - INFO - __main__ - Global step 300 Train loss 1.97 Classification-F1 0.3333333333333333 on epoch=149
05/21/2022 00:44:26 - INFO - __main__ - Step 310 Global step 310 Train loss 1.81 on epoch=154
05/21/2022 00:44:28 - INFO - __main__ - Step 320 Global step 320 Train loss 1.71 on epoch=159
05/21/2022 00:44:30 - INFO - __main__ - Step 330 Global step 330 Train loss 1.73 on epoch=164
05/21/2022 00:44:31 - INFO - __main__ - Step 340 Global step 340 Train loss 1.57 on epoch=169
05/21/2022 00:44:34 - INFO - __main__ - Step 350 Global step 350 Train loss 1.63 on epoch=174
05/21/2022 00:44:36 - INFO - __main__ - Global step 350 Train loss 1.69 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 00:44:38 - INFO - __main__ - Step 360 Global step 360 Train loss 1.44 on epoch=179
05/21/2022 00:44:40 - INFO - __main__ - Step 370 Global step 370 Train loss 1.26 on epoch=184
05/21/2022 00:44:42 - INFO - __main__ - Step 380 Global step 380 Train loss 1.31 on epoch=189
05/21/2022 00:44:44 - INFO - __main__ - Step 390 Global step 390 Train loss 1.25 on epoch=194
05/21/2022 00:44:46 - INFO - __main__ - Step 400 Global step 400 Train loss 1.09 on epoch=199
05/21/2022 00:44:48 - INFO - __main__ - Global step 400 Train loss 1.27 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 00:44:50 - INFO - __main__ - Step 410 Global step 410 Train loss 1.02 on epoch=204
05/21/2022 00:44:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.94 on epoch=209
05/21/2022 00:44:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.97 on epoch=214
05/21/2022 00:44:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=219
05/21/2022 00:44:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=224
05/21/2022 00:45:00 - INFO - __main__ - Global step 450 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 00:45:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.86 on epoch=229
05/21/2022 00:45:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.81 on epoch=234
05/21/2022 00:45:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.83 on epoch=239
05/21/2022 00:45:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.78 on epoch=244
05/21/2022 00:45:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.74 on epoch=249
05/21/2022 00:45:11 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 00:45:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.67 on epoch=254
05/21/2022 00:45:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.76 on epoch=259
05/21/2022 00:45:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.64 on epoch=264
05/21/2022 00:45:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=269
05/21/2022 00:45:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.60 on epoch=274
05/21/2022 00:45:22 - INFO - __main__ - Global step 550 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 00:45:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=279
05/21/2022 00:45:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=284
05/21/2022 00:45:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=289
05/21/2022 00:45:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.57 on epoch=294
05/21/2022 00:45:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=299
05/21/2022 00:45:33 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 00:45:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.50 on epoch=304
05/21/2022 00:45:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=309
05/21/2022 00:45:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.63 on epoch=314
05/21/2022 00:45:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=319
05/21/2022 00:45:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.57 on epoch=324
05/21/2022 00:45:43 - INFO - __main__ - Global step 650 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 00:45:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=329
05/21/2022 00:45:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=334
05/21/2022 00:45:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.53 on epoch=339
05/21/2022 00:45:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=344
05/21/2022 00:45:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.53 on epoch=349
05/21/2022 00:45:54 - INFO - __main__ - Global step 700 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 00:45:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=354
05/21/2022 00:45:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=359
05/21/2022 00:46:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=364
05/21/2022 00:46:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=369
05/21/2022 00:46:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=374
05/21/2022 00:46:05 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 00:46:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=379
05/21/2022 00:46:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=384
05/21/2022 00:46:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=389
05/21/2022 00:46:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=394
05/21/2022 00:46:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=399
05/21/2022 00:46:16 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 00:46:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=404
05/21/2022 00:46:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=409
05/21/2022 00:46:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=414
05/21/2022 00:46:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=419
05/21/2022 00:46:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=424
05/21/2022 00:46:26 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 00:46:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=429
05/21/2022 00:46:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
05/21/2022 00:46:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=439
05/21/2022 00:46:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=444
05/21/2022 00:46:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=449
05/21/2022 00:46:37 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 00:46:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=454
05/21/2022 00:46:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=459
05/21/2022 00:46:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=464
05/21/2022 00:46:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=469
05/21/2022 00:46:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=474
05/21/2022 00:46:48 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 00:46:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=479
05/21/2022 00:46:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=484
05/21/2022 00:46:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=489
05/21/2022 00:46:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=494
05/21/2022 00:46:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=499
05/21/2022 00:46:58 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 00:47:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=504
05/21/2022 00:47:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=509
05/21/2022 00:47:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=514
05/21/2022 00:47:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=519
05/21/2022 00:47:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=524
05/21/2022 00:47:09 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 00:47:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=529
05/21/2022 00:47:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=534
05/21/2022 00:47:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=539
05/21/2022 00:47:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=544
05/21/2022 00:47:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=549
05/21/2022 00:47:19 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 00:47:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=554
05/21/2022 00:47:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=559
05/21/2022 00:47:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=564
05/21/2022 00:47:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=569
05/21/2022 00:47:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=574
05/21/2022 00:47:30 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 00:47:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=579
05/21/2022 00:47:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=584
05/21/2022 00:47:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
05/21/2022 00:47:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=594
05/21/2022 00:47:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=599
05/21/2022 00:47:40 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 00:47:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=604
05/21/2022 00:47:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=609
05/21/2022 00:47:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=614
05/21/2022 00:47:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=619
05/21/2022 00:47:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=624
05/21/2022 00:47:51 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 00:47:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=629
05/21/2022 00:47:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=634
05/21/2022 00:47:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=639
05/21/2022 00:47:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=644
05/21/2022 00:48:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=649
05/21/2022 00:48:01 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 00:48:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=654
05/21/2022 00:48:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=659
05/21/2022 00:48:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=664
05/21/2022 00:48:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=669
05/21/2022 00:48:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=674
05/21/2022 00:48:11 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 00:48:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=679
05/21/2022 00:48:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=684
05/21/2022 00:48:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=689
05/21/2022 00:48:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=694
05/21/2022 00:48:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=699
05/21/2022 00:48:22 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 00:48:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=704
05/21/2022 00:48:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=709
05/21/2022 00:48:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=714
05/21/2022 00:48:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=719
05/21/2022 00:48:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=724
05/21/2022 00:48:33 - INFO - __main__ - Global step 1450 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 00:48:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=729
05/21/2022 00:48:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=734
05/21/2022 00:48:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=739
05/21/2022 00:48:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=744
05/21/2022 00:48:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.28 on epoch=749
05/21/2022 00:48:43 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 00:48:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=754
05/21/2022 00:48:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=759
05/21/2022 00:48:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=764
05/21/2022 00:48:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=769
05/21/2022 00:48:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=774
05/21/2022 00:48:54 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 00:48:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=779
05/21/2022 00:48:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=784
05/21/2022 00:49:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=789
05/21/2022 00:49:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
05/21/2022 00:49:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=799
05/21/2022 00:49:04 - INFO - __main__ - Global step 1600 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 00:49:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=804
05/21/2022 00:49:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=809
05/21/2022 00:49:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.25 on epoch=814
05/21/2022 00:49:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.29 on epoch=819
05/21/2022 00:49:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=824
05/21/2022 00:49:15 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 00:49:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=829
05/21/2022 00:49:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=834
05/21/2022 00:49:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=839
05/21/2022 00:49:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=844
05/21/2022 00:49:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=849
05/21/2022 00:49:25 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 00:49:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.32 on epoch=854
05/21/2022 00:49:29 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=859
05/21/2022 00:49:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=864
05/21/2022 00:49:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=869
05/21/2022 00:49:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/21/2022 00:49:36 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 00:49:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=879
05/21/2022 00:49:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=884
05/21/2022 00:49:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=889
05/21/2022 00:49:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.29 on epoch=894
05/21/2022 00:49:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=899
05/21/2022 00:49:47 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 00:49:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=904
05/21/2022 00:49:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=909
05/21/2022 00:49:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.28 on epoch=914
05/21/2022 00:49:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=919
05/21/2022 00:49:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=924
05/21/2022 00:49:57 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 00:49:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=929
05/21/2022 00:50:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=934
05/21/2022 00:50:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=939
05/21/2022 00:50:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=944
05/21/2022 00:50:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.28 on epoch=949
05/21/2022 00:50:08 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 00:50:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=954
05/21/2022 00:50:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=959
05/21/2022 00:50:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=964
05/21/2022 00:50:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=969
05/21/2022 00:50:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=974
05/21/2022 00:50:19 - INFO - __main__ - Global step 1950 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 00:50:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=979
05/21/2022 00:50:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=984
05/21/2022 00:50:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.30 on epoch=989
05/21/2022 00:50:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=994
05/21/2022 00:50:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=999
05/21/2022 00:50:29 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 00:50:29 - INFO - __main__ - save last model!
05/21/2022 00:50:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 00:50:29 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 00:50:29 - INFO - __main__ - Printing 3 examples
05/21/2022 00:50:29 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:50:29 - INFO - __main__ - ['entailed']
05/21/2022 00:50:29 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:50:29 - INFO - __main__ - ['entailed']
05/21/2022 00:50:29 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 00:50:29 - INFO - __main__ - ['entailed']
05/21/2022 00:50:29 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:50:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:50:30 - INFO - __main__ - Printing 3 examples
05/21/2022 00:50:30 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/21/2022 00:50:30 - INFO - __main__ - ['entailed']
05/21/2022 00:50:30 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/21/2022 00:50:30 - INFO - __main__ - ['entailed']
05/21/2022 00:50:30 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/21/2022 00:50:30 - INFO - __main__ - ['entailed']
05/21/2022 00:50:30 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:50:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:50:30 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 00:50:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:50:30 - INFO - __main__ - Printing 3 examples
05/21/2022 00:50:30 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/21/2022 00:50:30 - INFO - __main__ - ['entailed']
05/21/2022 00:50:30 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/21/2022 00:50:30 - INFO - __main__ - ['entailed']
05/21/2022 00:50:30 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/21/2022 00:50:30 - INFO - __main__ - ['entailed']
05/21/2022 00:50:30 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:50:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:50:30 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 00:50:37 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 00:50:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 00:50:38 - INFO - __main__ - Starting training!
05/21/2022 00:50:59 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:51:12 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 00:55:20 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.5_8_predictions.txt
05/21/2022 00:55:20 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 00:55:20 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/21/2022 00:55:20 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.4, bsz=8 ...
05/21/2022 00:55:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:55:21 - INFO - __main__ - Printing 3 examples
05/21/2022 00:55:21 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/21/2022 00:55:21 - INFO - __main__ - ['entailed']
05/21/2022 00:55:21 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/21/2022 00:55:21 - INFO - __main__ - ['entailed']
05/21/2022 00:55:21 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/21/2022 00:55:21 - INFO - __main__ - ['entailed']
05/21/2022 00:55:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:55:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:55:21 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 00:55:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 00:55:21 - INFO - __main__ - Printing 3 examples
05/21/2022 00:55:21 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/21/2022 00:55:21 - INFO - __main__ - ['entailed']
05/21/2022 00:55:21 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/21/2022 00:55:21 - INFO - __main__ - ['entailed']
05/21/2022 00:55:21 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/21/2022 00:55:21 - INFO - __main__ - ['entailed']
05/21/2022 00:55:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 00:55:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 00:55:21 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 00:55:27 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 00:55:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 00:55:27 - INFO - __main__ - Starting training!
05/21/2022 00:55:29 - INFO - __main__ - Step 10 Global step 10 Train loss 5.05 on epoch=4
05/21/2022 00:55:31 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/21/2022 00:55:33 - INFO - __main__ - Step 30 Global step 30 Train loss 4.91 on epoch=14
05/21/2022 00:55:35 - INFO - __main__ - Step 40 Global step 40 Train loss 4.82 on epoch=19
05/21/2022 00:55:37 - INFO - __main__ - Step 50 Global step 50 Train loss 4.74 on epoch=24
05/21/2022 00:55:38 - INFO - __main__ - Global step 50 Train loss 4.91 Classification-F1 0.0 on epoch=24
05/21/2022 00:55:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 00:55:40 - INFO - __main__ - Step 60 Global step 60 Train loss 4.68 on epoch=29
05/21/2022 00:55:42 - INFO - __main__ - Step 70 Global step 70 Train loss 4.55 on epoch=34
05/21/2022 00:55:44 - INFO - __main__ - Step 80 Global step 80 Train loss 4.47 on epoch=39
05/21/2022 00:55:46 - INFO - __main__ - Step 90 Global step 90 Train loss 4.28 on epoch=44
05/21/2022 00:55:48 - INFO - __main__ - Step 100 Global step 100 Train loss 4.26 on epoch=49
05/21/2022 00:55:50 - INFO - __main__ - Global step 100 Train loss 4.45 Classification-F1 0.0 on epoch=49
05/21/2022 00:55:52 - INFO - __main__ - Step 110 Global step 110 Train loss 4.06 on epoch=54
05/21/2022 00:55:54 - INFO - __main__ - Step 120 Global step 120 Train loss 3.95 on epoch=59
05/21/2022 00:55:56 - INFO - __main__ - Step 130 Global step 130 Train loss 3.91 on epoch=64
05/21/2022 00:55:58 - INFO - __main__ - Step 140 Global step 140 Train loss 3.88 on epoch=69
05/21/2022 00:56:00 - INFO - __main__ - Step 150 Global step 150 Train loss 3.79 on epoch=74
05/21/2022 00:56:01 - INFO - __main__ - Global step 150 Train loss 3.92 Classification-F1 0.13333333333333336 on epoch=74
05/21/2022 00:56:01 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.13333333333333336 on epoch=74, global_step=150
05/21/2022 00:56:03 - INFO - __main__ - Step 160 Global step 160 Train loss 3.65 on epoch=79
05/21/2022 00:56:04 - INFO - __main__ - Step 170 Global step 170 Train loss 3.44 on epoch=84
05/21/2022 00:56:06 - INFO - __main__ - Step 180 Global step 180 Train loss 3.48 on epoch=89
05/21/2022 00:56:08 - INFO - __main__ - Step 190 Global step 190 Train loss 3.32 on epoch=94
05/21/2022 00:56:10 - INFO - __main__ - Step 200 Global step 200 Train loss 3.28 on epoch=99
05/21/2022 00:56:12 - INFO - __main__ - Global step 200 Train loss 3.43 Classification-F1 0.3333333333333333 on epoch=99
05/21/2022 00:56:12 - INFO - __main__ - Saving model with best Classification-F1: 0.13333333333333336 -> 0.3333333333333333 on epoch=99, global_step=200
05/21/2022 00:56:14 - INFO - __main__ - Step 210 Global step 210 Train loss 3.14 on epoch=104
05/21/2022 00:56:16 - INFO - __main__ - Step 220 Global step 220 Train loss 3.05 on epoch=109
05/21/2022 00:56:18 - INFO - __main__ - Step 230 Global step 230 Train loss 2.92 on epoch=114
05/21/2022 00:56:20 - INFO - __main__ - Step 240 Global step 240 Train loss 2.86 on epoch=119
05/21/2022 00:56:22 - INFO - __main__ - Step 250 Global step 250 Train loss 2.73 on epoch=124
05/21/2022 00:56:24 - INFO - __main__ - Global step 250 Train loss 2.94 Classification-F1 0.3333333333333333 on epoch=124
05/21/2022 00:56:26 - INFO - __main__ - Step 260 Global step 260 Train loss 2.49 on epoch=129
05/21/2022 00:56:28 - INFO - __main__ - Step 270 Global step 270 Train loss 2.55 on epoch=134
05/21/2022 00:56:30 - INFO - __main__ - Step 280 Global step 280 Train loss 2.44 on epoch=139
05/21/2022 00:56:32 - INFO - __main__ - Step 290 Global step 290 Train loss 2.42 on epoch=144
05/21/2022 00:56:34 - INFO - __main__ - Step 300 Global step 300 Train loss 2.22 on epoch=149
05/21/2022 00:56:36 - INFO - __main__ - Global step 300 Train loss 2.42 Classification-F1 0.3333333333333333 on epoch=149
05/21/2022 00:56:38 - INFO - __main__ - Step 310 Global step 310 Train loss 2.33 on epoch=154
05/21/2022 00:56:40 - INFO - __main__ - Step 320 Global step 320 Train loss 2.20 on epoch=159
05/21/2022 00:56:42 - INFO - __main__ - Step 330 Global step 330 Train loss 2.22 on epoch=164
05/21/2022 00:56:44 - INFO - __main__ - Step 340 Global step 340 Train loss 2.09 on epoch=169
05/21/2022 00:56:46 - INFO - __main__ - Step 350 Global step 350 Train loss 2.01 on epoch=174
05/21/2022 00:56:48 - INFO - __main__ - Global step 350 Train loss 2.17 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 00:56:50 - INFO - __main__ - Step 360 Global step 360 Train loss 1.94 on epoch=179
05/21/2022 00:56:52 - INFO - __main__ - Step 370 Global step 370 Train loss 1.76 on epoch=184
05/21/2022 00:56:54 - INFO - __main__ - Step 380 Global step 380 Train loss 1.66 on epoch=189
05/21/2022 00:56:56 - INFO - __main__ - Step 390 Global step 390 Train loss 1.67 on epoch=194
05/21/2022 00:56:58 - INFO - __main__ - Step 400 Global step 400 Train loss 1.61 on epoch=199
05/21/2022 00:57:00 - INFO - __main__ - Global step 400 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 00:57:02 - INFO - __main__ - Step 410 Global step 410 Train loss 1.52 on epoch=204
05/21/2022 00:57:04 - INFO - __main__ - Step 420 Global step 420 Train loss 1.40 on epoch=209
05/21/2022 00:57:06 - INFO - __main__ - Step 430 Global step 430 Train loss 1.27 on epoch=214
05/21/2022 00:57:08 - INFO - __main__ - Step 440 Global step 440 Train loss 1.21 on epoch=219
05/21/2022 00:57:10 - INFO - __main__ - Step 450 Global step 450 Train loss 1.30 on epoch=224
05/21/2022 00:57:11 - INFO - __main__ - Global step 450 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 00:57:13 - INFO - __main__ - Step 460 Global step 460 Train loss 1.18 on epoch=229
05/21/2022 00:57:15 - INFO - __main__ - Step 470 Global step 470 Train loss 1.13 on epoch=234
05/21/2022 00:57:17 - INFO - __main__ - Step 480 Global step 480 Train loss 1.06 on epoch=239
05/21/2022 00:57:19 - INFO - __main__ - Step 490 Global step 490 Train loss 1.01 on epoch=244
05/21/2022 00:57:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.95 on epoch=249
05/21/2022 00:57:22 - INFO - __main__ - Global step 500 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 00:57:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.94 on epoch=254
05/21/2022 00:57:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=259
05/21/2022 00:57:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.83 on epoch=264
05/21/2022 00:57:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.76 on epoch=269
05/21/2022 00:57:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=274
05/21/2022 00:57:33 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 00:57:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.81 on epoch=279
05/21/2022 00:57:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.93 on epoch=284
05/21/2022 00:57:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.69 on epoch=289
05/21/2022 00:57:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.86 on epoch=294
05/21/2022 00:57:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.77 on epoch=299
05/21/2022 00:57:44 - INFO - __main__ - Global step 600 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 00:57:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=304
05/21/2022 00:57:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.72 on epoch=309
05/21/2022 00:57:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.73 on epoch=314
05/21/2022 00:57:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.72 on epoch=319
05/21/2022 00:57:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.70 on epoch=324
05/21/2022 00:57:55 - INFO - __main__ - Global step 650 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 00:57:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.66 on epoch=329
05/21/2022 00:57:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=334
05/21/2022 00:58:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=339
05/21/2022 00:58:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=344
05/21/2022 00:58:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.66 on epoch=349
05/21/2022 00:58:05 - INFO - __main__ - Global step 700 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 00:58:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.56 on epoch=354
05/21/2022 00:58:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
05/21/2022 00:58:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.69 on epoch=364
05/21/2022 00:58:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.57 on epoch=369
05/21/2022 00:58:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=374
05/21/2022 00:58:16 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 00:58:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.62 on epoch=379
05/21/2022 00:58:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=384
05/21/2022 00:58:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.67 on epoch=389
05/21/2022 00:58:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.54 on epoch=394
05/21/2022 00:58:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.56 on epoch=399
05/21/2022 00:58:27 - INFO - __main__ - Global step 800 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 00:58:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.58 on epoch=404
05/21/2022 00:58:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.56 on epoch=409
05/21/2022 00:58:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.55 on epoch=414
05/21/2022 00:58:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=419
05/21/2022 00:58:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=424
05/21/2022 00:58:38 - INFO - __main__ - Global step 850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 00:58:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=429
05/21/2022 00:58:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=434
05/21/2022 00:58:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=439
05/21/2022 00:58:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
05/21/2022 00:58:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=449
05/21/2022 00:58:48 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 00:58:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=454
05/21/2022 00:58:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.53 on epoch=459
05/21/2022 00:58:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=464
05/21/2022 00:58:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=469
05/21/2022 00:58:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=474
05/21/2022 00:58:59 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 00:59:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.51 on epoch=479
05/21/2022 00:59:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=484
05/21/2022 00:59:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=489
05/21/2022 00:59:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=494
05/21/2022 00:59:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=499
05/21/2022 00:59:10 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 00:59:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=504
05/21/2022 00:59:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=509
05/21/2022 00:59:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=514
05/21/2022 00:59:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=519
05/21/2022 00:59:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=524
05/21/2022 00:59:21 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 00:59:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=529
05/21/2022 00:59:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=534
05/21/2022 00:59:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.51 on epoch=539
05/21/2022 00:59:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=544
05/21/2022 00:59:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=549
05/21/2022 00:59:32 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 00:59:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=554
05/21/2022 00:59:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=559
05/21/2022 00:59:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=564
05/21/2022 00:59:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=569
05/21/2022 00:59:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=574
05/21/2022 00:59:42 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 00:59:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=579
05/21/2022 00:59:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=584
05/21/2022 00:59:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=589
05/21/2022 00:59:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=594
05/21/2022 00:59:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=599
05/21/2022 00:59:53 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=599
05/21/2022 00:59:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=599, global_step=1200
05/21/2022 00:59:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=604
05/21/2022 00:59:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=609
05/21/2022 00:59:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=614
05/21/2022 01:00:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=619
05/21/2022 01:00:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=624
05/21/2022 01:00:03 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 01:00:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=629
05/21/2022 01:00:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=634
05/21/2022 01:00:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=639
05/21/2022 01:00:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=644
05/21/2022 01:00:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=649
05/21/2022 01:00:14 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.3191489361702127 on epoch=649
05/21/2022 01:00:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=654
05/21/2022 01:00:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=659
05/21/2022 01:00:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=664
05/21/2022 01:00:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=669
05/21/2022 01:00:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=674
05/21/2022 01:00:25 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 01:00:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=679
05/21/2022 01:00:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=684
05/21/2022 01:00:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=689
05/21/2022 01:00:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=694
05/21/2022 01:00:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=699
05/21/2022 01:00:35 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 01:00:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=704
05/21/2022 01:00:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=709
05/21/2022 01:00:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=714
05/21/2022 01:00:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=719
05/21/2022 01:00:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=724
05/21/2022 01:00:46 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 01:00:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=729
05/21/2022 01:00:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=734
05/21/2022 01:00:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=739
05/21/2022 01:00:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=744
05/21/2022 01:00:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=749
05/21/2022 01:00:57 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 01:00:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=754
05/21/2022 01:01:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=759
05/21/2022 01:01:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=764
05/21/2022 01:01:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=769
05/21/2022 01:01:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=774
05/21/2022 01:01:07 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 01:01:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=779
05/21/2022 01:01:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
05/21/2022 01:01:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
05/21/2022 01:01:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=794
05/21/2022 01:01:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=799
05/21/2022 01:01:18 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 01:01:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=804
05/21/2022 01:01:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.33 on epoch=809
05/21/2022 01:01:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=814
05/21/2022 01:01:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=819
05/21/2022 01:01:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=824
05/21/2022 01:01:29 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 01:01:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=829
05/21/2022 01:01:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=834
05/21/2022 01:01:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
05/21/2022 01:01:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=844
05/21/2022 01:01:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=849
05/21/2022 01:01:40 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 01:01:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=854
05/21/2022 01:01:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.31 on epoch=859
05/21/2022 01:01:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/21/2022 01:01:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=869
05/21/2022 01:01:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=874
05/21/2022 01:01:51 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 01:01:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=879
05/21/2022 01:01:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=884
05/21/2022 01:01:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=889
05/21/2022 01:01:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=894
05/21/2022 01:02:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
05/21/2022 01:02:02 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 01:02:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=904
05/21/2022 01:02:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/21/2022 01:02:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=914
05/21/2022 01:02:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=919
05/21/2022 01:02:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
05/21/2022 01:02:12 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 01:02:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
05/21/2022 01:02:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
05/21/2022 01:02:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.32 on epoch=939
05/21/2022 01:02:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=944
05/21/2022 01:02:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
05/21/2022 01:02:23 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 01:02:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
05/21/2022 01:02:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=959
05/21/2022 01:02:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=964
05/21/2022 01:02:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
05/21/2022 01:02:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
05/21/2022 01:02:34 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 01:02:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
05/21/2022 01:02:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=984
05/21/2022 01:02:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.27 on epoch=989
05/21/2022 01:02:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/21/2022 01:02:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=999
05/21/2022 01:02:44 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 01:02:44 - INFO - __main__ - save last model!
05/21/2022 01:02:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 01:02:44 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 01:02:44 - INFO - __main__ - Printing 3 examples
05/21/2022 01:02:44 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:02:44 - INFO - __main__ - ['entailed']
05/21/2022 01:02:44 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:02:44 - INFO - __main__ - ['entailed']
05/21/2022 01:02:44 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:02:44 - INFO - __main__ - ['entailed']
05/21/2022 01:02:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:02:45 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:02:45 - INFO - __main__ - Printing 3 examples
05/21/2022 01:02:45 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/21/2022 01:02:45 - INFO - __main__ - ['entailed']
05/21/2022 01:02:45 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/21/2022 01:02:45 - INFO - __main__ - ['entailed']
05/21/2022 01:02:45 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/21/2022 01:02:45 - INFO - __main__ - ['entailed']
05/21/2022 01:02:45 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:02:45 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:02:45 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 01:02:45 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:02:45 - INFO - __main__ - Printing 3 examples
05/21/2022 01:02:45 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/21/2022 01:02:45 - INFO - __main__ - ['entailed']
05/21/2022 01:02:45 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/21/2022 01:02:45 - INFO - __main__ - ['entailed']
05/21/2022 01:02:45 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/21/2022 01:02:45 - INFO - __main__ - ['entailed']
05/21/2022 01:02:45 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:02:45 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:02:45 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 01:02:52 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 01:02:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 01:02:53 - INFO - __main__ - Starting training!
05/21/2022 01:03:09 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:03:23 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 01:07:29 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.4_8_predictions.txt
05/21/2022 01:07:29 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 01:07:29 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.4, bsz=8, dev_performance=0.3992490613266583, test_performance=0.33047210300429186
05/21/2022 01:07:29 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.3, bsz=8 ...
05/21/2022 01:07:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:07:30 - INFO - __main__ - Printing 3 examples
05/21/2022 01:07:30 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/21/2022 01:07:30 - INFO - __main__ - ['entailed']
05/21/2022 01:07:30 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/21/2022 01:07:30 - INFO - __main__ - ['entailed']
05/21/2022 01:07:30 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/21/2022 01:07:30 - INFO - __main__ - ['entailed']
05/21/2022 01:07:30 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:07:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:07:30 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 01:07:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:07:30 - INFO - __main__ - Printing 3 examples
05/21/2022 01:07:30 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/21/2022 01:07:30 - INFO - __main__ - ['entailed']
05/21/2022 01:07:30 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/21/2022 01:07:30 - INFO - __main__ - ['entailed']
05/21/2022 01:07:30 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/21/2022 01:07:30 - INFO - __main__ - ['entailed']
05/21/2022 01:07:30 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:07:31 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:07:31 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 01:07:37 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 01:07:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 01:07:37 - INFO - __main__ - Starting training!
05/21/2022 01:07:39 - INFO - __main__ - Step 10 Global step 10 Train loss 5.11 on epoch=4
05/21/2022 01:07:41 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/21/2022 01:07:43 - INFO - __main__ - Step 30 Global step 30 Train loss 4.91 on epoch=14
05/21/2022 01:07:45 - INFO - __main__ - Step 40 Global step 40 Train loss 4.90 on epoch=19
05/21/2022 01:07:47 - INFO - __main__ - Step 50 Global step 50 Train loss 4.86 on epoch=24
05/21/2022 01:07:49 - INFO - __main__ - Global step 50 Train loss 4.95 Classification-F1 0.0 on epoch=24
05/21/2022 01:07:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 01:07:51 - INFO - __main__ - Step 60 Global step 60 Train loss 4.75 on epoch=29
05/21/2022 01:07:53 - INFO - __main__ - Step 70 Global step 70 Train loss 4.66 on epoch=34
05/21/2022 01:07:55 - INFO - __main__ - Step 80 Global step 80 Train loss 4.62 on epoch=39
05/21/2022 01:07:57 - INFO - __main__ - Step 90 Global step 90 Train loss 4.59 on epoch=44
05/21/2022 01:07:58 - INFO - __main__ - Step 100 Global step 100 Train loss 4.54 on epoch=49
05/21/2022 01:08:10 - INFO - __main__ - Global step 100 Train loss 4.63 Classification-F1 0.0 on epoch=49
05/21/2022 01:08:12 - INFO - __main__ - Step 110 Global step 110 Train loss 4.44 on epoch=54
05/21/2022 01:08:14 - INFO - __main__ - Step 120 Global step 120 Train loss 4.30 on epoch=59
05/21/2022 01:08:15 - INFO - __main__ - Step 130 Global step 130 Train loss 4.27 on epoch=64
05/21/2022 01:08:17 - INFO - __main__ - Step 140 Global step 140 Train loss 4.18 on epoch=69
05/21/2022 01:08:19 - INFO - __main__ - Step 150 Global step 150 Train loss 4.14 on epoch=74
05/21/2022 01:08:25 - INFO - __main__ - Global step 150 Train loss 4.27 Classification-F1 0.0 on epoch=74
05/21/2022 01:08:27 - INFO - __main__ - Step 160 Global step 160 Train loss 4.08 on epoch=79
05/21/2022 01:08:29 - INFO - __main__ - Step 170 Global step 170 Train loss 4.13 on epoch=84
05/21/2022 01:08:31 - INFO - __main__ - Step 180 Global step 180 Train loss 3.95 on epoch=89
05/21/2022 01:08:33 - INFO - __main__ - Step 190 Global step 190 Train loss 3.97 on epoch=94
05/21/2022 01:08:35 - INFO - __main__ - Step 200 Global step 200 Train loss 3.73 on epoch=99
05/21/2022 01:08:42 - INFO - __main__ - Global step 200 Train loss 3.97 Classification-F1 0.041666666666666664 on epoch=99
05/21/2022 01:08:42 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.041666666666666664 on epoch=99, global_step=200
05/21/2022 01:08:44 - INFO - __main__ - Step 210 Global step 210 Train loss 3.72 on epoch=104
05/21/2022 01:08:46 - INFO - __main__ - Step 220 Global step 220 Train loss 3.63 on epoch=109
05/21/2022 01:08:48 - INFO - __main__ - Step 230 Global step 230 Train loss 3.66 on epoch=114
05/21/2022 01:08:50 - INFO - __main__ - Step 240 Global step 240 Train loss 3.57 on epoch=119
05/21/2022 01:08:52 - INFO - __main__ - Step 250 Global step 250 Train loss 3.38 on epoch=124
05/21/2022 01:08:55 - INFO - __main__ - Global step 250 Train loss 3.59 Classification-F1 0.17045454545454544 on epoch=124
05/21/2022 01:08:55 - INFO - __main__ - Saving model with best Classification-F1: 0.041666666666666664 -> 0.17045454545454544 on epoch=124, global_step=250
05/21/2022 01:08:57 - INFO - __main__ - Step 260 Global step 260 Train loss 3.43 on epoch=129
05/21/2022 01:08:59 - INFO - __main__ - Step 270 Global step 270 Train loss 3.31 on epoch=134
05/21/2022 01:09:01 - INFO - __main__ - Step 280 Global step 280 Train loss 3.24 on epoch=139
05/21/2022 01:09:03 - INFO - __main__ - Step 290 Global step 290 Train loss 3.25 on epoch=144
05/21/2022 01:09:05 - INFO - __main__ - Step 300 Global step 300 Train loss 3.07 on epoch=149
05/21/2022 01:09:09 - INFO - __main__ - Global step 300 Train loss 3.26 Classification-F1 0.12727272727272726 on epoch=149
05/21/2022 01:09:11 - INFO - __main__ - Step 310 Global step 310 Train loss 3.09 on epoch=154
05/21/2022 01:09:13 - INFO - __main__ - Step 320 Global step 320 Train loss 3.08 on epoch=159
05/21/2022 01:09:15 - INFO - __main__ - Step 330 Global step 330 Train loss 2.98 on epoch=164
05/21/2022 01:09:17 - INFO - __main__ - Step 340 Global step 340 Train loss 2.89 on epoch=169
05/21/2022 01:09:19 - INFO - __main__ - Step 350 Global step 350 Train loss 2.79 on epoch=174
05/21/2022 01:09:22 - INFO - __main__ - Global step 350 Train loss 2.96 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 01:09:22 - INFO - __main__ - Saving model with best Classification-F1: 0.17045454545454544 -> 0.3333333333333333 on epoch=174, global_step=350
05/21/2022 01:09:24 - INFO - __main__ - Step 360 Global step 360 Train loss 2.87 on epoch=179
05/21/2022 01:09:26 - INFO - __main__ - Step 370 Global step 370 Train loss 2.70 on epoch=184
05/21/2022 01:09:28 - INFO - __main__ - Step 380 Global step 380 Train loss 2.64 on epoch=189
05/21/2022 01:09:30 - INFO - __main__ - Step 390 Global step 390 Train loss 2.52 on epoch=194
05/21/2022 01:09:32 - INFO - __main__ - Step 400 Global step 400 Train loss 2.54 on epoch=199
05/21/2022 01:09:38 - INFO - __main__ - Global step 400 Train loss 2.66 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 01:09:40 - INFO - __main__ - Step 410 Global step 410 Train loss 2.58 on epoch=204
05/21/2022 01:09:42 - INFO - __main__ - Step 420 Global step 420 Train loss 2.52 on epoch=209
05/21/2022 01:09:44 - INFO - __main__ - Step 430 Global step 430 Train loss 2.52 on epoch=214
05/21/2022 01:09:46 - INFO - __main__ - Step 440 Global step 440 Train loss 2.37 on epoch=219
05/21/2022 01:09:48 - INFO - __main__ - Step 450 Global step 450 Train loss 2.34 on epoch=224
05/21/2022 01:09:51 - INFO - __main__ - Global step 450 Train loss 2.47 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 01:09:53 - INFO - __main__ - Step 460 Global step 460 Train loss 2.21 on epoch=229
05/21/2022 01:09:55 - INFO - __main__ - Step 470 Global step 470 Train loss 2.20 on epoch=234
05/21/2022 01:09:58 - INFO - __main__ - Step 480 Global step 480 Train loss 2.15 on epoch=239
05/21/2022 01:10:00 - INFO - __main__ - Step 490 Global step 490 Train loss 2.12 on epoch=244
05/21/2022 01:10:02 - INFO - __main__ - Step 500 Global step 500 Train loss 2.07 on epoch=249
05/21/2022 01:10:04 - INFO - __main__ - Global step 500 Train loss 2.15 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 01:10:06 - INFO - __main__ - Step 510 Global step 510 Train loss 2.11 on epoch=254
05/21/2022 01:10:08 - INFO - __main__ - Step 520 Global step 520 Train loss 1.92 on epoch=259
05/21/2022 01:10:10 - INFO - __main__ - Step 530 Global step 530 Train loss 1.90 on epoch=264
05/21/2022 01:10:12 - INFO - __main__ - Step 540 Global step 540 Train loss 1.90 on epoch=269
05/21/2022 01:10:14 - INFO - __main__ - Step 550 Global step 550 Train loss 1.80 on epoch=274
05/21/2022 01:10:16 - INFO - __main__ - Global step 550 Train loss 1.93 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 01:10:18 - INFO - __main__ - Step 560 Global step 560 Train loss 1.68 on epoch=279
05/21/2022 01:10:20 - INFO - __main__ - Step 570 Global step 570 Train loss 1.80 on epoch=284
05/21/2022 01:10:22 - INFO - __main__ - Step 580 Global step 580 Train loss 1.83 on epoch=289
05/21/2022 01:10:24 - INFO - __main__ - Step 590 Global step 590 Train loss 1.61 on epoch=294
05/21/2022 01:10:26 - INFO - __main__ - Step 600 Global step 600 Train loss 1.72 on epoch=299
05/21/2022 01:10:27 - INFO - __main__ - Global step 600 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 01:10:29 - INFO - __main__ - Step 610 Global step 610 Train loss 1.61 on epoch=304
05/21/2022 01:10:31 - INFO - __main__ - Step 620 Global step 620 Train loss 1.70 on epoch=309
05/21/2022 01:10:33 - INFO - __main__ - Step 630 Global step 630 Train loss 1.57 on epoch=314
05/21/2022 01:10:35 - INFO - __main__ - Step 640 Global step 640 Train loss 1.45 on epoch=319
05/21/2022 01:10:38 - INFO - __main__ - Step 650 Global step 650 Train loss 1.45 on epoch=324
05/21/2022 01:10:38 - INFO - __main__ - Global step 650 Train loss 1.56 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 01:10:40 - INFO - __main__ - Step 660 Global step 660 Train loss 1.59 on epoch=329
05/21/2022 01:10:43 - INFO - __main__ - Step 670 Global step 670 Train loss 1.38 on epoch=334
05/21/2022 01:10:44 - INFO - __main__ - Step 680 Global step 680 Train loss 1.45 on epoch=339
05/21/2022 01:10:46 - INFO - __main__ - Step 690 Global step 690 Train loss 1.30 on epoch=344
05/21/2022 01:10:49 - INFO - __main__ - Step 700 Global step 700 Train loss 1.25 on epoch=349
05/21/2022 01:10:49 - INFO - __main__ - Global step 700 Train loss 1.39 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 01:10:51 - INFO - __main__ - Step 710 Global step 710 Train loss 1.25 on epoch=354
05/21/2022 01:10:53 - INFO - __main__ - Step 720 Global step 720 Train loss 1.28 on epoch=359
05/21/2022 01:10:55 - INFO - __main__ - Step 730 Global step 730 Train loss 1.19 on epoch=364
05/21/2022 01:10:57 - INFO - __main__ - Step 740 Global step 740 Train loss 1.16 on epoch=369
05/21/2022 01:10:59 - INFO - __main__ - Step 750 Global step 750 Train loss 1.25 on epoch=374
05/21/2022 01:11:00 - INFO - __main__ - Global step 750 Train loss 1.23 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 01:11:02 - INFO - __main__ - Step 760 Global step 760 Train loss 1.29 on epoch=379
05/21/2022 01:11:04 - INFO - __main__ - Step 770 Global step 770 Train loss 1.14 on epoch=384
05/21/2022 01:11:06 - INFO - __main__ - Step 780 Global step 780 Train loss 1.11 on epoch=389
05/21/2022 01:11:08 - INFO - __main__ - Step 790 Global step 790 Train loss 1.13 on epoch=394
05/21/2022 01:11:10 - INFO - __main__ - Step 800 Global step 800 Train loss 1.07 on epoch=399
05/21/2022 01:11:11 - INFO - __main__ - Global step 800 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 01:11:13 - INFO - __main__ - Step 810 Global step 810 Train loss 1.11 on epoch=404
05/21/2022 01:11:15 - INFO - __main__ - Step 820 Global step 820 Train loss 1.11 on epoch=409
05/21/2022 01:11:17 - INFO - __main__ - Step 830 Global step 830 Train loss 1.04 on epoch=414
05/21/2022 01:11:19 - INFO - __main__ - Step 840 Global step 840 Train loss 1.05 on epoch=419
05/21/2022 01:11:21 - INFO - __main__ - Step 850 Global step 850 Train loss 1.06 on epoch=424
05/21/2022 01:11:22 - INFO - __main__ - Global step 850 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 01:11:24 - INFO - __main__ - Step 860 Global step 860 Train loss 1.04 on epoch=429
05/21/2022 01:11:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.95 on epoch=434
05/21/2022 01:11:28 - INFO - __main__ - Step 880 Global step 880 Train loss 1.02 on epoch=439
05/21/2022 01:11:30 - INFO - __main__ - Step 890 Global step 890 Train loss 1.05 on epoch=444
05/21/2022 01:11:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.89 on epoch=449
05/21/2022 01:11:33 - INFO - __main__ - Global step 900 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 01:11:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.97 on epoch=454
05/21/2022 01:11:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.91 on epoch=459
05/21/2022 01:11:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.79 on epoch=464
05/21/2022 01:11:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.95 on epoch=469
05/21/2022 01:11:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.89 on epoch=474
05/21/2022 01:11:44 - INFO - __main__ - Global step 950 Train loss 0.90 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 01:11:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.86 on epoch=479
05/21/2022 01:11:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.86 on epoch=484
05/21/2022 01:11:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.78 on epoch=489
05/21/2022 01:11:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.93 on epoch=494
05/21/2022 01:11:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.84 on epoch=499
05/21/2022 01:11:56 - INFO - __main__ - Global step 1000 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 01:11:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.80 on epoch=504
05/21/2022 01:12:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.79 on epoch=509
05/21/2022 01:12:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.83 on epoch=514
05/21/2022 01:12:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.83 on epoch=519
05/21/2022 01:12:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.71 on epoch=524
05/21/2022 01:12:06 - INFO - __main__ - Global step 1050 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 01:12:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.79 on epoch=529
05/21/2022 01:12:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.75 on epoch=534
05/21/2022 01:12:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.73 on epoch=539
05/21/2022 01:12:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.75 on epoch=544
05/21/2022 01:12:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.72 on epoch=549
05/21/2022 01:12:17 - INFO - __main__ - Global step 1100 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 01:12:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.67 on epoch=554
05/21/2022 01:12:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.74 on epoch=559
05/21/2022 01:12:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.72 on epoch=564
05/21/2022 01:12:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.70 on epoch=569
05/21/2022 01:12:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.67 on epoch=574
05/21/2022 01:12:28 - INFO - __main__ - Global step 1150 Train loss 0.70 Classification-F1 0.3191489361702127 on epoch=574
05/21/2022 01:12:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.76 on epoch=579
05/21/2022 01:12:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.79 on epoch=584
05/21/2022 01:12:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.74 on epoch=589
05/21/2022 01:12:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.69 on epoch=594
05/21/2022 01:12:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.70 on epoch=599
05/21/2022 01:12:39 - INFO - __main__ - Global step 1200 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 01:12:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.67 on epoch=604
05/21/2022 01:12:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.71 on epoch=609
05/21/2022 01:12:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.66 on epoch=614
05/21/2022 01:12:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.68 on epoch=619
05/21/2022 01:12:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=624
05/21/2022 01:12:50 - INFO - __main__ - Global step 1250 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 01:12:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.67 on epoch=629
05/21/2022 01:12:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.64 on epoch=634
05/21/2022 01:12:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.68 on epoch=639
05/21/2022 01:12:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.61 on epoch=644
05/21/2022 01:13:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.60 on epoch=649
05/21/2022 01:13:01 - INFO - __main__ - Global step 1300 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 01:13:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.66 on epoch=654
05/21/2022 01:13:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.56 on epoch=659
05/21/2022 01:13:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.53 on epoch=664
05/21/2022 01:13:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.59 on epoch=669
05/21/2022 01:13:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.62 on epoch=674
05/21/2022 01:13:12 - INFO - __main__ - Global step 1350 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 01:13:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.57 on epoch=679
05/21/2022 01:13:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.55 on epoch=684
05/21/2022 01:13:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
05/21/2022 01:13:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.55 on epoch=694
05/21/2022 01:13:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.51 on epoch=699
05/21/2022 01:13:23 - INFO - __main__ - Global step 1400 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 01:13:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.62 on epoch=704
05/21/2022 01:13:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.59 on epoch=709
05/21/2022 01:13:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.63 on epoch=714
05/21/2022 01:13:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.56 on epoch=719
05/21/2022 01:13:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.55 on epoch=724
05/21/2022 01:13:34 - INFO - __main__ - Global step 1450 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 01:13:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
05/21/2022 01:13:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
05/21/2022 01:13:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=739
05/21/2022 01:13:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.53 on epoch=744
05/21/2022 01:13:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.58 on epoch=749
05/21/2022 01:13:44 - INFO - __main__ - Global step 1500 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 01:13:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.55 on epoch=754
05/21/2022 01:13:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.50 on epoch=759
05/21/2022 01:13:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.60 on epoch=764
05/21/2022 01:13:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.51 on epoch=769
05/21/2022 01:13:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.53 on epoch=774
05/21/2022 01:13:55 - INFO - __main__ - Global step 1550 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 01:13:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.50 on epoch=779
05/21/2022 01:13:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
05/21/2022 01:14:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.63 on epoch=789
05/21/2022 01:14:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.54 on epoch=794
05/21/2022 01:14:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=799
05/21/2022 01:14:06 - INFO - __main__ - Global step 1600 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 01:14:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=804
05/21/2022 01:14:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.58 on epoch=809
05/21/2022 01:14:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
05/21/2022 01:14:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=819
05/21/2022 01:14:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.48 on epoch=824
05/21/2022 01:14:16 - INFO - __main__ - Global step 1650 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 01:14:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=829
05/21/2022 01:14:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.48 on epoch=834
05/21/2022 01:14:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.53 on epoch=839
05/21/2022 01:14:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=844
05/21/2022 01:14:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=849
05/21/2022 01:14:27 - INFO - __main__ - Global step 1700 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 01:14:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=854
05/21/2022 01:14:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=859
05/21/2022 01:14:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
05/21/2022 01:14:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=869
05/21/2022 01:14:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.48 on epoch=874
05/21/2022 01:14:38 - INFO - __main__ - Global step 1750 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 01:14:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=879
05/21/2022 01:14:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.49 on epoch=884
05/21/2022 01:14:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=889
05/21/2022 01:14:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
05/21/2022 01:14:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=899
05/21/2022 01:14:48 - INFO - __main__ - Global step 1800 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 01:14:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.56 on epoch=904
05/21/2022 01:14:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
05/21/2022 01:14:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=914
05/21/2022 01:14:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=919
05/21/2022 01:14:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=924
05/21/2022 01:14:59 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 01:15:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
05/21/2022 01:15:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.48 on epoch=934
05/21/2022 01:15:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
05/21/2022 01:15:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
05/21/2022 01:15:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.48 on epoch=949
05/21/2022 01:15:10 - INFO - __main__ - Global step 1900 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 01:15:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
05/21/2022 01:15:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
05/21/2022 01:15:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=964
05/21/2022 01:15:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=969
05/21/2022 01:15:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
05/21/2022 01:15:20 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 01:15:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
05/21/2022 01:15:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
05/21/2022 01:15:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=989
05/21/2022 01:15:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=994
05/21/2022 01:15:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=999
05/21/2022 01:15:31 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 01:15:31 - INFO - __main__ - save last model!
05/21/2022 01:15:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 01:15:31 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 01:15:31 - INFO - __main__ - Printing 3 examples
05/21/2022 01:15:31 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:15:31 - INFO - __main__ - ['entailed']
05/21/2022 01:15:31 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:15:31 - INFO - __main__ - ['entailed']
05/21/2022 01:15:31 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:15:31 - INFO - __main__ - ['entailed']
05/21/2022 01:15:31 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:15:31 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:15:31 - INFO - __main__ - Printing 3 examples
05/21/2022 01:15:31 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/21/2022 01:15:31 - INFO - __main__ - ['entailed']
05/21/2022 01:15:31 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/21/2022 01:15:31 - INFO - __main__ - ['entailed']
05/21/2022 01:15:31 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/21/2022 01:15:31 - INFO - __main__ - ['entailed']
05/21/2022 01:15:31 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:15:32 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:15:32 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 01:15:32 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:15:32 - INFO - __main__ - Printing 3 examples
05/21/2022 01:15:32 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/21/2022 01:15:32 - INFO - __main__ - ['entailed']
05/21/2022 01:15:32 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/21/2022 01:15:32 - INFO - __main__ - ['entailed']
05/21/2022 01:15:32 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/21/2022 01:15:32 - INFO - __main__ - ['entailed']
05/21/2022 01:15:32 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:15:32 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:15:32 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 01:15:38 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 01:15:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 01:15:38 - INFO - __main__ - Starting training!
05/21/2022 01:15:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:16:08 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 01:20:18 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.3_8_predictions.txt
05/21/2022 01:20:18 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 01:20:19 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/21/2022 01:20:19 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.2, bsz=8 ...
05/21/2022 01:20:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:20:21 - INFO - __main__ - Printing 3 examples
05/21/2022 01:20:21 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/21/2022 01:20:21 - INFO - __main__ - ['entailed']
05/21/2022 01:20:21 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/21/2022 01:20:21 - INFO - __main__ - ['entailed']
05/21/2022 01:20:21 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/21/2022 01:20:21 - INFO - __main__ - ['entailed']
05/21/2022 01:20:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:20:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:20:21 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 01:20:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:20:21 - INFO - __main__ - Printing 3 examples
05/21/2022 01:20:21 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/21/2022 01:20:21 - INFO - __main__ - ['entailed']
05/21/2022 01:20:21 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/21/2022 01:20:21 - INFO - __main__ - ['entailed']
05/21/2022 01:20:21 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/21/2022 01:20:21 - INFO - __main__ - ['entailed']
05/21/2022 01:20:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:20:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:20:21 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 01:20:27 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 01:20:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 01:20:27 - INFO - __main__ - Starting training!
05/21/2022 01:20:30 - INFO - __main__ - Step 10 Global step 10 Train loss 5.05 on epoch=4
05/21/2022 01:20:32 - INFO - __main__ - Step 20 Global step 20 Train loss 5.04 on epoch=9
05/21/2022 01:20:33 - INFO - __main__ - Step 30 Global step 30 Train loss 4.99 on epoch=14
05/21/2022 01:20:35 - INFO - __main__ - Step 40 Global step 40 Train loss 4.92 on epoch=19
05/21/2022 01:20:37 - INFO - __main__ - Step 50 Global step 50 Train loss 4.83 on epoch=24
05/21/2022 01:20:39 - INFO - __main__ - Global step 50 Train loss 4.97 Classification-F1 0.0 on epoch=24
05/21/2022 01:20:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 01:20:41 - INFO - __main__ - Step 60 Global step 60 Train loss 4.92 on epoch=29
05/21/2022 01:20:43 - INFO - __main__ - Step 70 Global step 70 Train loss 4.83 on epoch=34
05/21/2022 01:20:45 - INFO - __main__ - Step 80 Global step 80 Train loss 4.73 on epoch=39
05/21/2022 01:20:47 - INFO - __main__ - Step 90 Global step 90 Train loss 4.64 on epoch=44
05/21/2022 01:20:49 - INFO - __main__ - Step 100 Global step 100 Train loss 4.64 on epoch=49
05/21/2022 01:20:50 - INFO - __main__ - Global step 100 Train loss 4.75 Classification-F1 0.0 on epoch=49
05/21/2022 01:20:52 - INFO - __main__ - Step 110 Global step 110 Train loss 4.56 on epoch=54
05/21/2022 01:20:54 - INFO - __main__ - Step 120 Global step 120 Train loss 4.47 on epoch=59
05/21/2022 01:20:56 - INFO - __main__ - Step 130 Global step 130 Train loss 4.45 on epoch=64
05/21/2022 01:20:58 - INFO - __main__ - Step 140 Global step 140 Train loss 4.42 on epoch=69
05/21/2022 01:21:00 - INFO - __main__ - Step 150 Global step 150 Train loss 4.32 on epoch=74
05/21/2022 01:21:02 - INFO - __main__ - Global step 150 Train loss 4.45 Classification-F1 0.0 on epoch=74
05/21/2022 01:21:04 - INFO - __main__ - Step 160 Global step 160 Train loss 4.29 on epoch=79
05/21/2022 01:21:06 - INFO - __main__ - Step 170 Global step 170 Train loss 4.24 on epoch=84
05/21/2022 01:21:08 - INFO - __main__ - Step 180 Global step 180 Train loss 4.11 on epoch=89
05/21/2022 01:21:10 - INFO - __main__ - Step 190 Global step 190 Train loss 4.08 on epoch=94
05/21/2022 01:21:12 - INFO - __main__ - Step 200 Global step 200 Train loss 3.98 on epoch=99
05/21/2022 01:21:13 - INFO - __main__ - Global step 200 Train loss 4.14 Classification-F1 0.0 on epoch=99
05/21/2022 01:21:15 - INFO - __main__ - Step 210 Global step 210 Train loss 3.86 on epoch=104
05/21/2022 01:21:17 - INFO - __main__ - Step 220 Global step 220 Train loss 3.86 on epoch=109
05/21/2022 01:21:19 - INFO - __main__ - Step 230 Global step 230 Train loss 3.75 on epoch=114
05/21/2022 01:21:21 - INFO - __main__ - Step 240 Global step 240 Train loss 3.74 on epoch=119
05/21/2022 01:21:23 - INFO - __main__ - Step 250 Global step 250 Train loss 3.72 on epoch=124
05/21/2022 01:21:26 - INFO - __main__ - Global step 250 Train loss 3.78 Classification-F1 0.0 on epoch=124
05/21/2022 01:21:28 - INFO - __main__ - Step 260 Global step 260 Train loss 3.59 on epoch=129
05/21/2022 01:21:30 - INFO - __main__ - Step 270 Global step 270 Train loss 3.54 on epoch=134
05/21/2022 01:21:32 - INFO - __main__ - Step 280 Global step 280 Train loss 3.55 on epoch=139
05/21/2022 01:21:34 - INFO - __main__ - Step 290 Global step 290 Train loss 3.45 on epoch=144
05/21/2022 01:21:36 - INFO - __main__ - Step 300 Global step 300 Train loss 3.45 on epoch=149
05/21/2022 01:21:38 - INFO - __main__ - Global step 300 Train loss 3.51 Classification-F1 0.11666666666666668 on epoch=149
05/21/2022 01:21:38 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.11666666666666668 on epoch=149, global_step=300
05/21/2022 01:21:40 - INFO - __main__ - Step 310 Global step 310 Train loss 3.39 on epoch=154
05/21/2022 01:21:42 - INFO - __main__ - Step 320 Global step 320 Train loss 3.32 on epoch=159
05/21/2022 01:21:44 - INFO - __main__ - Step 330 Global step 330 Train loss 3.37 on epoch=164
05/21/2022 01:21:46 - INFO - __main__ - Step 340 Global step 340 Train loss 3.34 on epoch=169
05/21/2022 01:21:48 - INFO - __main__ - Step 350 Global step 350 Train loss 3.16 on epoch=174
05/21/2022 01:21:51 - INFO - __main__ - Global step 350 Train loss 3.32 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 01:21:51 - INFO - __main__ - Saving model with best Classification-F1: 0.11666666666666668 -> 0.3333333333333333 on epoch=174, global_step=350
05/21/2022 01:21:53 - INFO - __main__ - Step 360 Global step 360 Train loss 3.21 on epoch=179
05/21/2022 01:21:55 - INFO - __main__ - Step 370 Global step 370 Train loss 3.11 on epoch=184
05/21/2022 01:21:57 - INFO - __main__ - Step 380 Global step 380 Train loss 3.11 on epoch=189
05/21/2022 01:21:59 - INFO - __main__ - Step 390 Global step 390 Train loss 3.14 on epoch=194
05/21/2022 01:22:01 - INFO - __main__ - Step 400 Global step 400 Train loss 2.96 on epoch=199
05/21/2022 01:22:03 - INFO - __main__ - Global step 400 Train loss 3.11 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 01:22:05 - INFO - __main__ - Step 410 Global step 410 Train loss 2.97 on epoch=204
05/21/2022 01:22:07 - INFO - __main__ - Step 420 Global step 420 Train loss 2.98 on epoch=209
05/21/2022 01:22:09 - INFO - __main__ - Step 430 Global step 430 Train loss 2.88 on epoch=214
05/21/2022 01:22:11 - INFO - __main__ - Step 440 Global step 440 Train loss 2.76 on epoch=219
05/21/2022 01:22:13 - INFO - __main__ - Step 450 Global step 450 Train loss 2.77 on epoch=224
05/21/2022 01:22:16 - INFO - __main__ - Global step 450 Train loss 2.88 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 01:22:18 - INFO - __main__ - Step 460 Global step 460 Train loss 2.71 on epoch=229
05/21/2022 01:22:20 - INFO - __main__ - Step 470 Global step 470 Train loss 2.64 on epoch=234
05/21/2022 01:22:22 - INFO - __main__ - Step 480 Global step 480 Train loss 2.58 on epoch=239
05/21/2022 01:22:24 - INFO - __main__ - Step 490 Global step 490 Train loss 2.51 on epoch=244
05/21/2022 01:22:26 - INFO - __main__ - Step 500 Global step 500 Train loss 2.51 on epoch=249
05/21/2022 01:22:28 - INFO - __main__ - Global step 500 Train loss 2.59 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 01:22:30 - INFO - __main__ - Step 510 Global step 510 Train loss 2.37 on epoch=254
05/21/2022 01:22:32 - INFO - __main__ - Step 520 Global step 520 Train loss 2.36 on epoch=259
05/21/2022 01:22:34 - INFO - __main__ - Step 530 Global step 530 Train loss 2.40 on epoch=264
05/21/2022 01:22:36 - INFO - __main__ - Step 540 Global step 540 Train loss 2.21 on epoch=269
05/21/2022 01:22:38 - INFO - __main__ - Step 550 Global step 550 Train loss 2.17 on epoch=274
05/21/2022 01:22:40 - INFO - __main__ - Global step 550 Train loss 2.30 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 01:22:42 - INFO - __main__ - Step 560 Global step 560 Train loss 2.21 on epoch=279
05/21/2022 01:22:44 - INFO - __main__ - Step 570 Global step 570 Train loss 2.24 on epoch=284
05/21/2022 01:22:47 - INFO - __main__ - Step 580 Global step 580 Train loss 2.23 on epoch=289
05/21/2022 01:22:49 - INFO - __main__ - Step 590 Global step 590 Train loss 2.12 on epoch=294
05/21/2022 01:22:51 - INFO - __main__ - Step 600 Global step 600 Train loss 2.04 on epoch=299
05/21/2022 01:22:52 - INFO - __main__ - Global step 600 Train loss 2.17 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 01:22:54 - INFO - __main__ - Step 610 Global step 610 Train loss 2.15 on epoch=304
05/21/2022 01:22:56 - INFO - __main__ - Step 620 Global step 620 Train loss 2.10 on epoch=309
05/21/2022 01:22:58 - INFO - __main__ - Step 630 Global step 630 Train loss 2.05 on epoch=314
05/21/2022 01:23:00 - INFO - __main__ - Step 640 Global step 640 Train loss 1.92 on epoch=319
05/21/2022 01:23:02 - INFO - __main__ - Step 650 Global step 650 Train loss 1.89 on epoch=324
05/21/2022 01:23:03 - INFO - __main__ - Global step 650 Train loss 2.02 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 01:23:05 - INFO - __main__ - Step 660 Global step 660 Train loss 1.94 on epoch=329
05/21/2022 01:23:07 - INFO - __main__ - Step 670 Global step 670 Train loss 1.96 on epoch=334
05/21/2022 01:23:09 - INFO - __main__ - Step 680 Global step 680 Train loss 1.95 on epoch=339
05/21/2022 01:23:11 - INFO - __main__ - Step 690 Global step 690 Train loss 1.83 on epoch=344
05/21/2022 01:23:13 - INFO - __main__ - Step 700 Global step 700 Train loss 1.90 on epoch=349
05/21/2022 01:23:14 - INFO - __main__ - Global step 700 Train loss 1.92 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 01:23:16 - INFO - __main__ - Step 710 Global step 710 Train loss 1.81 on epoch=354
05/21/2022 01:23:18 - INFO - __main__ - Step 720 Global step 720 Train loss 1.69 on epoch=359
05/21/2022 01:23:20 - INFO - __main__ - Step 730 Global step 730 Train loss 1.81 on epoch=364
05/21/2022 01:23:22 - INFO - __main__ - Step 740 Global step 740 Train loss 1.58 on epoch=369
05/21/2022 01:23:24 - INFO - __main__ - Step 750 Global step 750 Train loss 1.62 on epoch=374
05/21/2022 01:23:27 - INFO - __main__ - Global step 750 Train loss 1.70 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 01:23:29 - INFO - __main__ - Step 760 Global step 760 Train loss 1.72 on epoch=379
05/21/2022 01:23:31 - INFO - __main__ - Step 770 Global step 770 Train loss 1.56 on epoch=384
05/21/2022 01:23:33 - INFO - __main__ - Step 780 Global step 780 Train loss 1.70 on epoch=389
05/21/2022 01:23:35 - INFO - __main__ - Step 790 Global step 790 Train loss 1.53 on epoch=394
05/21/2022 01:23:37 - INFO - __main__ - Step 800 Global step 800 Train loss 1.63 on epoch=399
05/21/2022 01:23:39 - INFO - __main__ - Global step 800 Train loss 1.63 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 01:23:41 - INFO - __main__ - Step 810 Global step 810 Train loss 1.70 on epoch=404
05/21/2022 01:23:43 - INFO - __main__ - Step 820 Global step 820 Train loss 1.58 on epoch=409
05/21/2022 01:23:45 - INFO - __main__ - Step 830 Global step 830 Train loss 1.50 on epoch=414
05/21/2022 01:23:47 - INFO - __main__ - Step 840 Global step 840 Train loss 1.55 on epoch=419
05/21/2022 01:23:49 - INFO - __main__ - Step 850 Global step 850 Train loss 1.60 on epoch=424
05/21/2022 01:23:52 - INFO - __main__ - Global step 850 Train loss 1.59 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 01:23:53 - INFO - __main__ - Step 860 Global step 860 Train loss 1.42 on epoch=429
05/21/2022 01:23:55 - INFO - __main__ - Step 870 Global step 870 Train loss 1.46 on epoch=434
05/21/2022 01:23:57 - INFO - __main__ - Step 880 Global step 880 Train loss 1.43 on epoch=439
05/21/2022 01:23:59 - INFO - __main__ - Step 890 Global step 890 Train loss 1.40 on epoch=444
05/21/2022 01:24:01 - INFO - __main__ - Step 900 Global step 900 Train loss 1.42 on epoch=449
05/21/2022 01:24:04 - INFO - __main__ - Global step 900 Train loss 1.43 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 01:24:06 - INFO - __main__ - Step 910 Global step 910 Train loss 1.36 on epoch=454
05/21/2022 01:24:08 - INFO - __main__ - Step 920 Global step 920 Train loss 1.39 on epoch=459
05/21/2022 01:24:10 - INFO - __main__ - Step 930 Global step 930 Train loss 1.49 on epoch=464
05/21/2022 01:24:11 - INFO - __main__ - Step 940 Global step 940 Train loss 1.42 on epoch=469
05/21/2022 01:24:13 - INFO - __main__ - Step 950 Global step 950 Train loss 1.41 on epoch=474
05/21/2022 01:24:16 - INFO - __main__ - Global step 950 Train loss 1.41 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 01:24:18 - INFO - __main__ - Step 960 Global step 960 Train loss 1.25 on epoch=479
05/21/2022 01:24:20 - INFO - __main__ - Step 970 Global step 970 Train loss 1.30 on epoch=484
05/21/2022 01:24:22 - INFO - __main__ - Step 980 Global step 980 Train loss 1.24 on epoch=489
05/21/2022 01:24:24 - INFO - __main__ - Step 990 Global step 990 Train loss 1.25 on epoch=494
05/21/2022 01:24:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.20 on epoch=499
05/21/2022 01:24:27 - INFO - __main__ - Global step 1000 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 01:24:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.18 on epoch=504
05/21/2022 01:24:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.18 on epoch=509
05/21/2022 01:24:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.22 on epoch=514
05/21/2022 01:24:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.27 on epoch=519
05/21/2022 01:24:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.21 on epoch=524
05/21/2022 01:24:37 - INFO - __main__ - Global step 1050 Train loss 1.21 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 01:24:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.17 on epoch=529
05/21/2022 01:24:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.16 on epoch=534
05/21/2022 01:24:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.19 on epoch=539
05/21/2022 01:24:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.30 on epoch=544
05/21/2022 01:24:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.07 on epoch=549
05/21/2022 01:24:48 - INFO - __main__ - Global step 1100 Train loss 1.18 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 01:24:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.05 on epoch=554
05/21/2022 01:24:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.15 on epoch=559
05/21/2022 01:24:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.98 on epoch=564
05/21/2022 01:24:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.08 on epoch=569
05/21/2022 01:24:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.02 on epoch=574
05/21/2022 01:24:58 - INFO - __main__ - Global step 1150 Train loss 1.06 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 01:25:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.98 on epoch=579
05/21/2022 01:25:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.04 on epoch=584
05/21/2022 01:25:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.07 on epoch=589
05/21/2022 01:25:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.04 on epoch=594
05/21/2022 01:25:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.05 on epoch=599
05/21/2022 01:25:08 - INFO - __main__ - Global step 1200 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 01:25:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.94 on epoch=604
05/21/2022 01:25:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.95 on epoch=609
05/21/2022 01:25:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.13 on epoch=614
05/21/2022 01:25:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.01 on epoch=619
05/21/2022 01:25:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.93 on epoch=624
05/21/2022 01:25:19 - INFO - __main__ - Global step 1250 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 01:25:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.88 on epoch=629
05/21/2022 01:25:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.91 on epoch=634
05/21/2022 01:25:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.87 on epoch=639
05/21/2022 01:25:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.90 on epoch=644
05/21/2022 01:25:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.89 on epoch=649
05/21/2022 01:25:29 - INFO - __main__ - Global step 1300 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 01:25:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.91 on epoch=654
05/21/2022 01:25:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.97 on epoch=659
05/21/2022 01:25:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.95 on epoch=664
05/21/2022 01:25:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.93 on epoch=669
05/21/2022 01:25:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=674
05/21/2022 01:25:39 - INFO - __main__ - Global step 1350 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 01:25:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.88 on epoch=679
05/21/2022 01:25:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.86 on epoch=684
05/21/2022 01:25:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.84 on epoch=689
05/21/2022 01:25:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.75 on epoch=694
05/21/2022 01:25:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.81 on epoch=699
05/21/2022 01:25:50 - INFO - __main__ - Global step 1400 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 01:25:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.79 on epoch=704
05/21/2022 01:25:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.80 on epoch=709
05/21/2022 01:25:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.81 on epoch=714
05/21/2022 01:25:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.84 on epoch=719
05/21/2022 01:25:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=724
05/21/2022 01:26:00 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 01:26:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.83 on epoch=729
05/21/2022 01:26:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.79 on epoch=734
05/21/2022 01:26:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.83 on epoch=739
05/21/2022 01:26:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.71 on epoch=744
05/21/2022 01:26:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.73 on epoch=749
05/21/2022 01:26:11 - INFO - __main__ - Global step 1500 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 01:26:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.71 on epoch=754
05/21/2022 01:26:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.79 on epoch=759
05/21/2022 01:26:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.69 on epoch=764
05/21/2022 01:26:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.64 on epoch=769
05/21/2022 01:26:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.65 on epoch=774
05/21/2022 01:26:21 - INFO - __main__ - Global step 1550 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 01:26:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.59 on epoch=779
05/21/2022 01:26:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.67 on epoch=784
05/21/2022 01:26:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.75 on epoch=789
05/21/2022 01:26:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.69 on epoch=794
05/21/2022 01:26:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.64 on epoch=799
05/21/2022 01:26:31 - INFO - __main__ - Global step 1600 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 01:26:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.70 on epoch=804
05/21/2022 01:26:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.68 on epoch=809
05/21/2022 01:26:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.66 on epoch=814
05/21/2022 01:26:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.56 on epoch=819
05/21/2022 01:26:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.60 on epoch=824
05/21/2022 01:26:42 - INFO - __main__ - Global step 1650 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 01:26:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.60 on epoch=829
05/21/2022 01:26:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.64 on epoch=834
05/21/2022 01:26:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.68 on epoch=839
05/21/2022 01:26:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.60 on epoch=844
05/21/2022 01:26:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.65 on epoch=849
05/21/2022 01:26:52 - INFO - __main__ - Global step 1700 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 01:26:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.53 on epoch=854
05/21/2022 01:26:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.69 on epoch=859
05/21/2022 01:26:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.65 on epoch=864
05/21/2022 01:27:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.55 on epoch=869
05/21/2022 01:27:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.57 on epoch=874
05/21/2022 01:27:02 - INFO - __main__ - Global step 1750 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 01:27:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.59 on epoch=879
05/21/2022 01:27:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.58 on epoch=884
05/21/2022 01:27:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
05/21/2022 01:27:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.56 on epoch=894
05/21/2022 01:27:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.64 on epoch=899
05/21/2022 01:27:13 - INFO - __main__ - Global step 1800 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 01:27:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.56 on epoch=904
05/21/2022 01:27:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.58 on epoch=909
05/21/2022 01:27:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.55 on epoch=914
05/21/2022 01:27:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.52 on epoch=919
05/21/2022 01:27:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.51 on epoch=924
05/21/2022 01:27:23 - INFO - __main__ - Global step 1850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 01:27:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.61 on epoch=929
05/21/2022 01:27:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.59 on epoch=934
05/21/2022 01:27:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.50 on epoch=939
05/21/2022 01:27:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.62 on epoch=944
05/21/2022 01:27:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=949
05/21/2022 01:27:33 - INFO - __main__ - Global step 1900 Train loss 0.55 Classification-F1 0.3191489361702127 on epoch=949
05/21/2022 01:27:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.55 on epoch=954
05/21/2022 01:27:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.49 on epoch=959
05/21/2022 01:27:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.50 on epoch=964
05/21/2022 01:27:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.51 on epoch=969
05/21/2022 01:27:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=974
05/21/2022 01:27:44 - INFO - __main__ - Global step 1950 Train loss 0.51 Classification-F1 0.5151515151515151 on epoch=974
05/21/2022 01:27:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5151515151515151 on epoch=974, global_step=1950
05/21/2022 01:27:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
05/21/2022 01:27:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=984
05/21/2022 01:27:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=989
05/21/2022 01:27:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.50 on epoch=994
05/21/2022 01:27:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=999
05/21/2022 01:27:54 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 01:27:54 - INFO - __main__ - save last model!
05/21/2022 01:27:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 01:27:54 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 01:27:54 - INFO - __main__ - Printing 3 examples
05/21/2022 01:27:54 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:27:54 - INFO - __main__ - ['entailed']
05/21/2022 01:27:54 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:27:54 - INFO - __main__ - ['entailed']
05/21/2022 01:27:54 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:27:54 - INFO - __main__ - ['entailed']
05/21/2022 01:27:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:27:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:27:54 - INFO - __main__ - Printing 3 examples
05/21/2022 01:27:54 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/21/2022 01:27:54 - INFO - __main__ - ['refuted']
05/21/2022 01:27:54 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/21/2022 01:27:54 - INFO - __main__ - ['refuted']
05/21/2022 01:27:54 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/21/2022 01:27:54 - INFO - __main__ - ['refuted']
05/21/2022 01:27:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:27:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:27:54 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 01:27:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:27:54 - INFO - __main__ - Printing 3 examples
05/21/2022 01:27:54 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/21/2022 01:27:54 - INFO - __main__ - ['refuted']
05/21/2022 01:27:54 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/21/2022 01:27:54 - INFO - __main__ - ['refuted']
05/21/2022 01:27:54 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/21/2022 01:27:54 - INFO - __main__ - ['refuted']
05/21/2022 01:27:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:27:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:27:54 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 01:28:00 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 01:28:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 01:28:00 - INFO - __main__ - Starting training!
05/21/2022 01:28:18 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:28:31 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 01:34:23 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.2_8_predictions.txt
05/21/2022 01:34:23 - INFO - __main__ - Classification-F1 on test data: 0.3350
05/21/2022 01:34:23 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.2, bsz=8, dev_performance=0.5151515151515151, test_performance=0.3349734394132839
05/21/2022 01:34:23 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.5, bsz=8 ...
05/21/2022 01:34:24 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:34:24 - INFO - __main__ - Printing 3 examples
05/21/2022 01:34:24 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/21/2022 01:34:24 - INFO - __main__ - ['refuted']
05/21/2022 01:34:24 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/21/2022 01:34:24 - INFO - __main__ - ['refuted']
05/21/2022 01:34:24 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/21/2022 01:34:24 - INFO - __main__ - ['refuted']
05/21/2022 01:34:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:34:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:34:24 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 01:34:24 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:34:24 - INFO - __main__ - Printing 3 examples
05/21/2022 01:34:24 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/21/2022 01:34:24 - INFO - __main__ - ['refuted']
05/21/2022 01:34:24 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/21/2022 01:34:24 - INFO - __main__ - ['refuted']
05/21/2022 01:34:24 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/21/2022 01:34:24 - INFO - __main__ - ['refuted']
05/21/2022 01:34:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:34:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:34:24 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 01:34:30 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 01:34:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 01:34:30 - INFO - __main__ - Starting training!
05/21/2022 01:34:33 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/21/2022 01:34:35 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/21/2022 01:34:36 - INFO - __main__ - Step 30 Global step 30 Train loss 4.83 on epoch=14
05/21/2022 01:34:38 - INFO - __main__ - Step 40 Global step 40 Train loss 4.83 on epoch=19
05/21/2022 01:34:40 - INFO - __main__ - Step 50 Global step 50 Train loss 4.62 on epoch=24
05/21/2022 01:34:41 - INFO - __main__ - Global step 50 Train loss 4.86 Classification-F1 0.0 on epoch=24
05/21/2022 01:34:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 01:34:43 - INFO - __main__ - Step 60 Global step 60 Train loss 4.56 on epoch=29
05/21/2022 01:34:45 - INFO - __main__ - Step 70 Global step 70 Train loss 4.38 on epoch=34
05/21/2022 01:34:47 - INFO - __main__ - Step 80 Global step 80 Train loss 4.20 on epoch=39
05/21/2022 01:34:49 - INFO - __main__ - Step 90 Global step 90 Train loss 4.13 on epoch=44
05/21/2022 01:34:51 - INFO - __main__ - Step 100 Global step 100 Train loss 4.00 on epoch=49
05/21/2022 01:34:53 - INFO - __main__ - Global step 100 Train loss 4.25 Classification-F1 0.0 on epoch=49
05/21/2022 01:34:55 - INFO - __main__ - Step 110 Global step 110 Train loss 3.94 on epoch=54
05/21/2022 01:34:56 - INFO - __main__ - Step 120 Global step 120 Train loss 3.78 on epoch=59
05/21/2022 01:34:58 - INFO - __main__ - Step 130 Global step 130 Train loss 3.62 on epoch=64
05/21/2022 01:35:00 - INFO - __main__ - Step 140 Global step 140 Train loss 3.48 on epoch=69
05/21/2022 01:35:02 - INFO - __main__ - Step 150 Global step 150 Train loss 3.34 on epoch=74
05/21/2022 01:35:05 - INFO - __main__ - Global step 150 Train loss 3.63 Classification-F1 0.10526315789473684 on epoch=74
05/21/2022 01:35:05 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.10526315789473684 on epoch=74, global_step=150
05/21/2022 01:35:07 - INFO - __main__ - Step 160 Global step 160 Train loss 3.24 on epoch=79
05/21/2022 01:35:09 - INFO - __main__ - Step 170 Global step 170 Train loss 3.04 on epoch=84
05/21/2022 01:35:11 - INFO - __main__ - Step 180 Global step 180 Train loss 2.89 on epoch=89
05/21/2022 01:35:13 - INFO - __main__ - Step 190 Global step 190 Train loss 2.63 on epoch=94
05/21/2022 01:35:15 - INFO - __main__ - Step 200 Global step 200 Train loss 2.74 on epoch=99
05/21/2022 01:35:16 - INFO - __main__ - Global step 200 Train loss 2.91 Classification-F1 0.3333333333333333 on epoch=99
05/21/2022 01:35:16 - INFO - __main__ - Saving model with best Classification-F1: 0.10526315789473684 -> 0.3333333333333333 on epoch=99, global_step=200
05/21/2022 01:35:18 - INFO - __main__ - Step 210 Global step 210 Train loss 2.49 on epoch=104
05/21/2022 01:35:20 - INFO - __main__ - Step 220 Global step 220 Train loss 2.25 on epoch=109
05/21/2022 01:35:21 - INFO - __main__ - Step 230 Global step 230 Train loss 2.25 on epoch=114
05/21/2022 01:35:23 - INFO - __main__ - Step 240 Global step 240 Train loss 2.11 on epoch=119
05/21/2022 01:35:25 - INFO - __main__ - Step 250 Global step 250 Train loss 1.89 on epoch=124
05/21/2022 01:35:28 - INFO - __main__ - Global step 250 Train loss 2.20 Classification-F1 0.3333333333333333 on epoch=124
05/21/2022 01:35:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.77 on epoch=129
05/21/2022 01:35:32 - INFO - __main__ - Step 270 Global step 270 Train loss 1.76 on epoch=134
05/21/2022 01:35:33 - INFO - __main__ - Step 280 Global step 280 Train loss 1.68 on epoch=139
05/21/2022 01:35:35 - INFO - __main__ - Step 290 Global step 290 Train loss 1.60 on epoch=144
05/21/2022 01:35:37 - INFO - __main__ - Step 300 Global step 300 Train loss 1.60 on epoch=149
05/21/2022 01:35:38 - INFO - __main__ - Global step 300 Train loss 1.68 Classification-F1 0.3333333333333333 on epoch=149
05/21/2022 01:35:40 - INFO - __main__ - Step 310 Global step 310 Train loss 1.46 on epoch=154
05/21/2022 01:35:42 - INFO - __main__ - Step 320 Global step 320 Train loss 1.54 on epoch=159
05/21/2022 01:35:44 - INFO - __main__ - Step 330 Global step 330 Train loss 1.45 on epoch=164
05/21/2022 01:35:46 - INFO - __main__ - Step 340 Global step 340 Train loss 1.50 on epoch=169
05/21/2022 01:35:48 - INFO - __main__ - Step 350 Global step 350 Train loss 1.38 on epoch=174
05/21/2022 01:35:51 - INFO - __main__ - Global step 350 Train loss 1.47 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 01:35:52 - INFO - __main__ - Step 360 Global step 360 Train loss 1.30 on epoch=179
05/21/2022 01:35:54 - INFO - __main__ - Step 370 Global step 370 Train loss 1.31 on epoch=184
05/21/2022 01:35:56 - INFO - __main__ - Step 380 Global step 380 Train loss 1.16 on epoch=189
05/21/2022 01:35:58 - INFO - __main__ - Step 390 Global step 390 Train loss 1.28 on epoch=194
05/21/2022 01:36:00 - INFO - __main__ - Step 400 Global step 400 Train loss 1.20 on epoch=199
05/21/2022 01:36:02 - INFO - __main__ - Global step 400 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 01:36:04 - INFO - __main__ - Step 410 Global step 410 Train loss 1.14 on epoch=204
05/21/2022 01:36:06 - INFO - __main__ - Step 420 Global step 420 Train loss 1.20 on epoch=209
05/21/2022 01:36:08 - INFO - __main__ - Step 430 Global step 430 Train loss 1.10 on epoch=214
05/21/2022 01:36:10 - INFO - __main__ - Step 440 Global step 440 Train loss 1.02 on epoch=219
05/21/2022 01:36:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.99 on epoch=224
05/21/2022 01:36:13 - INFO - __main__ - Global step 450 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 01:36:15 - INFO - __main__ - Step 460 Global step 460 Train loss 1.07 on epoch=229
05/21/2022 01:36:17 - INFO - __main__ - Step 470 Global step 470 Train loss 1.01 on epoch=234
05/21/2022 01:36:19 - INFO - __main__ - Step 480 Global step 480 Train loss 1.03 on epoch=239
05/21/2022 01:36:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.93 on epoch=244
05/21/2022 01:36:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.92 on epoch=249
05/21/2022 01:36:23 - INFO - __main__ - Global step 500 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 01:36:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.86 on epoch=254
05/21/2022 01:36:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=259
05/21/2022 01:36:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.85 on epoch=264
05/21/2022 01:36:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.82 on epoch=269
05/21/2022 01:36:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=274
05/21/2022 01:36:34 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 01:36:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.91 on epoch=279
05/21/2022 01:36:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.73 on epoch=284
05/21/2022 01:36:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.71 on epoch=289
05/21/2022 01:36:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.68 on epoch=294
05/21/2022 01:36:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.75 on epoch=299
05/21/2022 01:36:44 - INFO - __main__ - Global step 600 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 01:36:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.81 on epoch=304
05/21/2022 01:36:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.75 on epoch=309
05/21/2022 01:36:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.77 on epoch=314
05/21/2022 01:36:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.66 on epoch=319
05/21/2022 01:36:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.63 on epoch=324
05/21/2022 01:36:54 - INFO - __main__ - Global step 650 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 01:36:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.70 on epoch=329
05/21/2022 01:36:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.65 on epoch=334
05/21/2022 01:37:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=339
05/21/2022 01:37:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.65 on epoch=344
05/21/2022 01:37:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.75 on epoch=349
05/21/2022 01:37:05 - INFO - __main__ - Global step 700 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 01:37:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.61 on epoch=354
05/21/2022 01:37:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
05/21/2022 01:37:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.61 on epoch=364
05/21/2022 01:37:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.59 on epoch=369
05/21/2022 01:37:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=374
05/21/2022 01:37:15 - INFO - __main__ - Global step 750 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 01:37:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.58 on epoch=379
05/21/2022 01:37:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.67 on epoch=384
05/21/2022 01:37:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.56 on epoch=389
05/21/2022 01:37:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.59 on epoch=394
05/21/2022 01:37:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.62 on epoch=399
05/21/2022 01:37:26 - INFO - __main__ - Global step 800 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 01:37:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.54 on epoch=404
05/21/2022 01:37:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.59 on epoch=409
05/21/2022 01:37:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.61 on epoch=414
05/21/2022 01:37:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.53 on epoch=419
05/21/2022 01:37:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=424
05/21/2022 01:37:36 - INFO - __main__ - Global step 850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 01:37:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=429
05/21/2022 01:37:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=434
05/21/2022 01:37:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=439
05/21/2022 01:37:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=444
05/21/2022 01:37:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=449
05/21/2022 01:37:47 - INFO - __main__ - Global step 900 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 01:37:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=454
05/21/2022 01:37:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.55 on epoch=459
05/21/2022 01:37:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.49 on epoch=464
05/21/2022 01:37:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=469
05/21/2022 01:37:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=474
05/21/2022 01:37:57 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 01:37:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.52 on epoch=479
05/21/2022 01:38:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=484
05/21/2022 01:38:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=489
05/21/2022 01:38:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=494
05/21/2022 01:38:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=499
05/21/2022 01:38:08 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 01:38:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=504
05/21/2022 01:38:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=509
05/21/2022 01:38:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.55 on epoch=514
05/21/2022 01:38:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=519
05/21/2022 01:38:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=524
05/21/2022 01:38:18 - INFO - __main__ - Global step 1050 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 01:38:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=529
05/21/2022 01:38:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=534
05/21/2022 01:38:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=539
05/21/2022 01:38:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
05/21/2022 01:38:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=549
05/21/2022 01:38:29 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 01:38:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=554
05/21/2022 01:38:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=559
05/21/2022 01:38:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=564
05/21/2022 01:38:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.49 on epoch=569
05/21/2022 01:38:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=574
05/21/2022 01:38:39 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 01:38:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=579
05/21/2022 01:38:43 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=584
05/21/2022 01:38:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=589
05/21/2022 01:38:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=594
05/21/2022 01:38:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=599
05/21/2022 01:38:49 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 01:38:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=604
05/21/2022 01:38:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.46 on epoch=609
05/21/2022 01:38:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=614
05/21/2022 01:38:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=619
05/21/2022 01:38:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=624
05/21/2022 01:39:00 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 01:39:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=629
05/21/2022 01:39:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=634
05/21/2022 01:39:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=639
05/21/2022 01:39:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=644
05/21/2022 01:39:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=649
05/21/2022 01:39:11 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.3191489361702127 on epoch=649
05/21/2022 01:39:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=654
05/21/2022 01:39:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=659
05/21/2022 01:39:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=664
05/21/2022 01:39:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=669
05/21/2022 01:39:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=674
05/21/2022 01:39:21 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 01:39:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=679
05/21/2022 01:39:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=684
05/21/2022 01:39:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=689
05/21/2022 01:39:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=694
05/21/2022 01:39:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=699
05/21/2022 01:39:32 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 01:39:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=704
05/21/2022 01:39:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=709
05/21/2022 01:39:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=714
05/21/2022 01:39:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=719
05/21/2022 01:39:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=724
05/21/2022 01:39:42 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 01:39:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=729
05/21/2022 01:39:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=734
05/21/2022 01:39:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=739
05/21/2022 01:39:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=744
05/21/2022 01:39:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=749
05/21/2022 01:39:53 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 01:39:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=754
05/21/2022 01:39:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=759
05/21/2022 01:39:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=764
05/21/2022 01:40:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=769
05/21/2022 01:40:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=774
05/21/2022 01:40:03 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 01:40:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=779
05/21/2022 01:40:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=784
05/21/2022 01:40:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
05/21/2022 01:40:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
05/21/2022 01:40:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=799
05/21/2022 01:40:13 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 01:40:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=804
05/21/2022 01:40:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=809
05/21/2022 01:40:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=814
05/21/2022 01:40:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=819
05/21/2022 01:40:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=824
05/21/2022 01:40:24 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 01:40:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=829
05/21/2022 01:40:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=834
05/21/2022 01:40:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.36 on epoch=839
05/21/2022 01:40:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=844
05/21/2022 01:40:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=849
05/21/2022 01:40:34 - INFO - __main__ - Global step 1700 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 01:40:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/21/2022 01:40:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=859
05/21/2022 01:40:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=864
05/21/2022 01:40:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=869
05/21/2022 01:40:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
05/21/2022 01:40:45 - INFO - __main__ - Global step 1750 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 01:40:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=879
05/21/2022 01:40:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
05/21/2022 01:40:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.31 on epoch=889
05/21/2022 01:40:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=894
05/21/2022 01:40:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
05/21/2022 01:40:55 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 01:40:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=904
05/21/2022 01:40:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/21/2022 01:41:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=914
05/21/2022 01:41:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.29 on epoch=919
05/21/2022 01:41:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
05/21/2022 01:41:05 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 01:41:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=929
05/21/2022 01:41:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=934
05/21/2022 01:41:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=939
05/21/2022 01:41:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=944
05/21/2022 01:41:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=949
05/21/2022 01:41:16 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 01:41:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.30 on epoch=954
05/21/2022 01:41:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.34 on epoch=959
05/21/2022 01:41:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/21/2022 01:41:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
05/21/2022 01:41:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=974
05/21/2022 01:41:26 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 01:41:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.33 on epoch=979
05/21/2022 01:41:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=984
05/21/2022 01:41:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.25 on epoch=989
05/21/2022 01:41:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/21/2022 01:41:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=999
05/21/2022 01:41:36 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 01:41:36 - INFO - __main__ - save last model!
05/21/2022 01:41:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 01:41:36 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 01:41:36 - INFO - __main__ - Printing 3 examples
05/21/2022 01:41:36 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:41:36 - INFO - __main__ - ['entailed']
05/21/2022 01:41:36 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:41:36 - INFO - __main__ - ['entailed']
05/21/2022 01:41:36 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:41:36 - INFO - __main__ - ['entailed']
05/21/2022 01:41:36 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:41:37 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:41:37 - INFO - __main__ - Printing 3 examples
05/21/2022 01:41:37 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/21/2022 01:41:37 - INFO - __main__ - ['refuted']
05/21/2022 01:41:37 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/21/2022 01:41:37 - INFO - __main__ - ['refuted']
05/21/2022 01:41:37 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/21/2022 01:41:37 - INFO - __main__ - ['refuted']
05/21/2022 01:41:37 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:41:37 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:41:37 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 01:41:37 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:41:37 - INFO - __main__ - Printing 3 examples
05/21/2022 01:41:37 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/21/2022 01:41:37 - INFO - __main__ - ['refuted']
05/21/2022 01:41:37 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/21/2022 01:41:37 - INFO - __main__ - ['refuted']
05/21/2022 01:41:37 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/21/2022 01:41:37 - INFO - __main__ - ['refuted']
05/21/2022 01:41:37 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:41:37 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:41:37 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 01:41:43 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 01:41:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 01:41:43 - INFO - __main__ - Starting training!
05/21/2022 01:42:00 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:42:13 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 01:46:42 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.5_8_predictions.txt
05/21/2022 01:46:42 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 01:46:43 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/21/2022 01:46:43 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.4, bsz=8 ...
05/21/2022 01:46:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:46:44 - INFO - __main__ - Printing 3 examples
05/21/2022 01:46:44 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/21/2022 01:46:44 - INFO - __main__ - ['refuted']
05/21/2022 01:46:44 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/21/2022 01:46:44 - INFO - __main__ - ['refuted']
05/21/2022 01:46:44 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/21/2022 01:46:44 - INFO - __main__ - ['refuted']
05/21/2022 01:46:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:46:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:46:44 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 01:46:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:46:44 - INFO - __main__ - Printing 3 examples
05/21/2022 01:46:44 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/21/2022 01:46:44 - INFO - __main__ - ['refuted']
05/21/2022 01:46:44 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/21/2022 01:46:44 - INFO - __main__ - ['refuted']
05/21/2022 01:46:44 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/21/2022 01:46:44 - INFO - __main__ - ['refuted']
05/21/2022 01:46:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:46:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:46:44 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 01:46:49 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 01:46:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 01:46:49 - INFO - __main__ - Starting training!
05/21/2022 01:46:53 - INFO - __main__ - Step 10 Global step 10 Train loss 4.97 on epoch=4
05/21/2022 01:46:54 - INFO - __main__ - Step 20 Global step 20 Train loss 5.00 on epoch=9
05/21/2022 01:46:56 - INFO - __main__ - Step 30 Global step 30 Train loss 4.73 on epoch=14
05/21/2022 01:46:58 - INFO - __main__ - Step 40 Global step 40 Train loss 4.75 on epoch=19
05/21/2022 01:47:00 - INFO - __main__ - Step 50 Global step 50 Train loss 4.65 on epoch=24
05/21/2022 01:47:06 - INFO - __main__ - Global step 50 Train loss 4.82 Classification-F1 0.0 on epoch=24
05/21/2022 01:47:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 01:47:08 - INFO - __main__ - Step 60 Global step 60 Train loss 4.51 on epoch=29
05/21/2022 01:47:10 - INFO - __main__ - Step 70 Global step 70 Train loss 4.44 on epoch=34
05/21/2022 01:47:12 - INFO - __main__ - Step 80 Global step 80 Train loss 4.27 on epoch=39
05/21/2022 01:47:14 - INFO - __main__ - Step 90 Global step 90 Train loss 4.15 on epoch=44
05/21/2022 01:47:16 - INFO - __main__ - Step 100 Global step 100 Train loss 4.08 on epoch=49
05/21/2022 01:47:23 - INFO - __main__ - Global step 100 Train loss 4.29 Classification-F1 0.0 on epoch=49
05/21/2022 01:47:24 - INFO - __main__ - Step 110 Global step 110 Train loss 3.96 on epoch=54
05/21/2022 01:47:26 - INFO - __main__ - Step 120 Global step 120 Train loss 3.76 on epoch=59
05/21/2022 01:47:28 - INFO - __main__ - Step 130 Global step 130 Train loss 3.70 on epoch=64
05/21/2022 01:47:30 - INFO - __main__ - Step 140 Global step 140 Train loss 3.53 on epoch=69
05/21/2022 01:47:32 - INFO - __main__ - Step 150 Global step 150 Train loss 3.63 on epoch=74
05/21/2022 01:47:36 - INFO - __main__ - Global step 150 Train loss 3.72 Classification-F1 0.016304347826086953 on epoch=74
05/21/2022 01:47:37 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.016304347826086953 on epoch=74, global_step=150
05/21/2022 01:47:38 - INFO - __main__ - Step 160 Global step 160 Train loss 3.35 on epoch=79
05/21/2022 01:47:40 - INFO - __main__ - Step 170 Global step 170 Train loss 3.28 on epoch=84
05/21/2022 01:47:42 - INFO - __main__ - Step 180 Global step 180 Train loss 3.14 on epoch=89
05/21/2022 01:47:44 - INFO - __main__ - Step 190 Global step 190 Train loss 3.07 on epoch=94
05/21/2022 01:47:46 - INFO - __main__ - Step 200 Global step 200 Train loss 3.01 on epoch=99
05/21/2022 01:47:50 - INFO - __main__ - Global step 200 Train loss 3.17 Classification-F1 0.3333333333333333 on epoch=99
05/21/2022 01:47:50 - INFO - __main__ - Saving model with best Classification-F1: 0.016304347826086953 -> 0.3333333333333333 on epoch=99, global_step=200
05/21/2022 01:47:52 - INFO - __main__ - Step 210 Global step 210 Train loss 2.93 on epoch=104
05/21/2022 01:47:54 - INFO - __main__ - Step 220 Global step 220 Train loss 2.89 on epoch=109
05/21/2022 01:47:56 - INFO - __main__ - Step 230 Global step 230 Train loss 2.64 on epoch=114
05/21/2022 01:47:58 - INFO - __main__ - Step 240 Global step 240 Train loss 2.50 on epoch=119
05/21/2022 01:48:00 - INFO - __main__ - Step 250 Global step 250 Train loss 2.54 on epoch=124
05/21/2022 01:48:02 - INFO - __main__ - Global step 250 Train loss 2.70 Classification-F1 0.3333333333333333 on epoch=124
05/21/2022 01:48:04 - INFO - __main__ - Step 260 Global step 260 Train loss 2.38 on epoch=129
05/21/2022 01:48:06 - INFO - __main__ - Step 270 Global step 270 Train loss 2.28 on epoch=134
05/21/2022 01:48:08 - INFO - __main__ - Step 280 Global step 280 Train loss 2.24 on epoch=139
05/21/2022 01:48:10 - INFO - __main__ - Step 290 Global step 290 Train loss 2.22 on epoch=144
05/21/2022 01:48:11 - INFO - __main__ - Step 300 Global step 300 Train loss 2.20 on epoch=149
05/21/2022 01:48:23 - INFO - __main__ - Global step 300 Train loss 2.26 Classification-F1 0.10852713178294572 on epoch=149
05/21/2022 01:48:25 - INFO - __main__ - Step 310 Global step 310 Train loss 2.17 on epoch=154
05/21/2022 01:48:27 - INFO - __main__ - Step 320 Global step 320 Train loss 2.00 on epoch=159
05/21/2022 01:48:29 - INFO - __main__ - Step 330 Global step 330 Train loss 2.00 on epoch=164
05/21/2022 01:48:31 - INFO - __main__ - Step 340 Global step 340 Train loss 1.97 on epoch=169
05/21/2022 01:48:33 - INFO - __main__ - Step 350 Global step 350 Train loss 1.86 on epoch=174
05/21/2022 01:48:44 - INFO - __main__ - Global step 350 Train loss 2.00 Classification-F1 0.22695035460992907 on epoch=174
05/21/2022 01:48:46 - INFO - __main__ - Step 360 Global step 360 Train loss 1.90 on epoch=179
05/21/2022 01:48:47 - INFO - __main__ - Step 370 Global step 370 Train loss 1.89 on epoch=184
05/21/2022 01:48:49 - INFO - __main__ - Step 380 Global step 380 Train loss 1.74 on epoch=189
05/21/2022 01:48:51 - INFO - __main__ - Step 390 Global step 390 Train loss 1.78 on epoch=194
05/21/2022 01:48:53 - INFO - __main__ - Step 400 Global step 400 Train loss 1.75 on epoch=199
05/21/2022 01:49:04 - INFO - __main__ - Global step 400 Train loss 1.81 Classification-F1 0.21276595744680848 on epoch=199
05/21/2022 01:49:06 - INFO - __main__ - Step 410 Global step 410 Train loss 1.72 on epoch=204
05/21/2022 01:49:08 - INFO - __main__ - Step 420 Global step 420 Train loss 1.73 on epoch=209
05/21/2022 01:49:10 - INFO - __main__ - Step 430 Global step 430 Train loss 1.65 on epoch=214
05/21/2022 01:49:11 - INFO - __main__ - Step 440 Global step 440 Train loss 1.60 on epoch=219
05/21/2022 01:49:13 - INFO - __main__ - Step 450 Global step 450 Train loss 1.50 on epoch=224
05/21/2022 01:49:15 - INFO - __main__ - Global step 450 Train loss 1.64 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 01:49:18 - INFO - __main__ - Step 460 Global step 460 Train loss 1.40 on epoch=229
05/21/2022 01:49:19 - INFO - __main__ - Step 470 Global step 470 Train loss 1.41 on epoch=234
05/21/2022 01:49:21 - INFO - __main__ - Step 480 Global step 480 Train loss 1.41 on epoch=239
05/21/2022 01:49:23 - INFO - __main__ - Step 490 Global step 490 Train loss 1.25 on epoch=244
05/21/2022 01:49:25 - INFO - __main__ - Step 500 Global step 500 Train loss 1.29 on epoch=249
05/21/2022 01:49:26 - INFO - __main__ - Global step 500 Train loss 1.35 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 01:49:28 - INFO - __main__ - Step 510 Global step 510 Train loss 1.32 on epoch=254
05/21/2022 01:49:30 - INFO - __main__ - Step 520 Global step 520 Train loss 1.34 on epoch=259
05/21/2022 01:49:32 - INFO - __main__ - Step 530 Global step 530 Train loss 1.21 on epoch=264
05/21/2022 01:49:34 - INFO - __main__ - Step 540 Global step 540 Train loss 1.16 on epoch=269
05/21/2022 01:49:35 - INFO - __main__ - Step 550 Global step 550 Train loss 1.08 on epoch=274
05/21/2022 01:49:36 - INFO - __main__ - Global step 550 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 01:49:38 - INFO - __main__ - Step 560 Global step 560 Train loss 1.09 on epoch=279
05/21/2022 01:49:40 - INFO - __main__ - Step 570 Global step 570 Train loss 1.17 on epoch=284
05/21/2022 01:49:42 - INFO - __main__ - Step 580 Global step 580 Train loss 1.16 on epoch=289
05/21/2022 01:49:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.97 on epoch=294
05/21/2022 01:49:46 - INFO - __main__ - Step 600 Global step 600 Train loss 1.04 on epoch=299
05/21/2022 01:49:47 - INFO - __main__ - Global step 600 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 01:49:49 - INFO - __main__ - Step 610 Global step 610 Train loss 1.00 on epoch=304
05/21/2022 01:49:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.98 on epoch=309
05/21/2022 01:49:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.95 on epoch=314
05/21/2022 01:49:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.90 on epoch=319
05/21/2022 01:49:56 - INFO - __main__ - Step 650 Global step 650 Train loss 1.05 on epoch=324
05/21/2022 01:49:57 - INFO - __main__ - Global step 650 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 01:49:59 - INFO - __main__ - Step 660 Global step 660 Train loss 1.03 on epoch=329
05/21/2022 01:50:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.87 on epoch=334
05/21/2022 01:50:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.98 on epoch=339
05/21/2022 01:50:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.91 on epoch=344
05/21/2022 01:50:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.96 on epoch=349
05/21/2022 01:50:08 - INFO - __main__ - Global step 700 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 01:50:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.90 on epoch=354
05/21/2022 01:50:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=359
05/21/2022 01:50:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.81 on epoch=364
05/21/2022 01:50:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.81 on epoch=369
05/21/2022 01:50:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.77 on epoch=374
05/21/2022 01:50:18 - INFO - __main__ - Global step 750 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 01:50:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.73 on epoch=379
05/21/2022 01:50:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=384
05/21/2022 01:50:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.72 on epoch=389
05/21/2022 01:50:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.66 on epoch=394
05/21/2022 01:50:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.75 on epoch=399
05/21/2022 01:50:29 - INFO - __main__ - Global step 800 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 01:50:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.66 on epoch=404
05/21/2022 01:50:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.64 on epoch=409
05/21/2022 01:50:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.76 on epoch=414
05/21/2022 01:50:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.68 on epoch=419
05/21/2022 01:50:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.68 on epoch=424
05/21/2022 01:50:39 - INFO - __main__ - Global step 850 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 01:50:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.77 on epoch=429
05/21/2022 01:50:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.70 on epoch=434
05/21/2022 01:50:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.58 on epoch=439
05/21/2022 01:50:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.60 on epoch=444
05/21/2022 01:50:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.58 on epoch=449
05/21/2022 01:50:50 - INFO - __main__ - Global step 900 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 01:50:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.59 on epoch=454
05/21/2022 01:50:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.64 on epoch=459
05/21/2022 01:50:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.61 on epoch=464
05/21/2022 01:50:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.66 on epoch=469
05/21/2022 01:50:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.62 on epoch=474
05/21/2022 01:51:00 - INFO - __main__ - Global step 950 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 01:51:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.64 on epoch=479
05/21/2022 01:51:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.67 on epoch=484
05/21/2022 01:51:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.55 on epoch=489
05/21/2022 01:51:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.60 on epoch=494
05/21/2022 01:51:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.58 on epoch=499
05/21/2022 01:51:11 - INFO - __main__ - Global step 1000 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 01:51:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=504
05/21/2022 01:51:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=509
05/21/2022 01:51:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=514
05/21/2022 01:51:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=519
05/21/2022 01:51:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=524
05/21/2022 01:51:21 - INFO - __main__ - Global step 1050 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 01:51:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.53 on epoch=529
05/21/2022 01:51:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=534
05/21/2022 01:51:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.55 on epoch=539
05/21/2022 01:51:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.60 on epoch=544
05/21/2022 01:51:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=549
05/21/2022 01:51:32 - INFO - __main__ - Global step 1100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 01:51:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=554
05/21/2022 01:51:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.53 on epoch=559
05/21/2022 01:51:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.55 on epoch=564
05/21/2022 01:51:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=569
05/21/2022 01:51:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=574
05/21/2022 01:51:42 - INFO - __main__ - Global step 1150 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 01:51:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=579
05/21/2022 01:51:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=584
05/21/2022 01:51:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=589
05/21/2022 01:51:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=594
05/21/2022 01:51:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=599
05/21/2022 01:51:52 - INFO - __main__ - Global step 1200 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 01:51:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=604
05/21/2022 01:51:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
05/21/2022 01:51:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=614
05/21/2022 01:52:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=619
05/21/2022 01:52:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=624
05/21/2022 01:52:03 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 01:52:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=629
05/21/2022 01:52:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=634
05/21/2022 01:52:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=639
05/21/2022 01:52:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.51 on epoch=644
05/21/2022 01:52:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=649
05/21/2022 01:52:13 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 01:52:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=654
05/21/2022 01:52:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
05/21/2022 01:52:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=664
05/21/2022 01:52:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=669
05/21/2022 01:52:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=674
05/21/2022 01:52:24 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 01:52:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=679
05/21/2022 01:52:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=684
05/21/2022 01:52:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=689
05/21/2022 01:52:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.54 on epoch=694
05/21/2022 01:52:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.55 on epoch=699
05/21/2022 01:52:34 - INFO - __main__ - Global step 1400 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 01:52:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=704
05/21/2022 01:52:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=709
05/21/2022 01:52:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=714
05/21/2022 01:52:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=719
05/21/2022 01:52:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
05/21/2022 01:52:44 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 01:52:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=729
05/21/2022 01:52:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=734
05/21/2022 01:52:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/21/2022 01:52:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=744
05/21/2022 01:52:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=749
05/21/2022 01:52:55 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 01:52:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.45 on epoch=754
05/21/2022 01:52:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=759
05/21/2022 01:53:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=764
05/21/2022 01:53:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=769
05/21/2022 01:53:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=774
05/21/2022 01:53:06 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 01:53:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=779
05/21/2022 01:53:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=784
05/21/2022 01:53:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=789
05/21/2022 01:53:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=794
05/21/2022 01:53:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=799
05/21/2022 01:53:16 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 01:53:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=804
05/21/2022 01:53:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=809
05/21/2022 01:53:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=814
05/21/2022 01:53:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
05/21/2022 01:53:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=824
05/21/2022 01:53:27 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 01:53:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=829
05/21/2022 01:53:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=834
05/21/2022 01:53:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=839
05/21/2022 01:53:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=844
05/21/2022 01:53:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.52 on epoch=849
05/21/2022 01:53:37 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 01:53:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/21/2022 01:53:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=859
05/21/2022 01:53:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=864
05/21/2022 01:53:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=869
05/21/2022 01:53:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
05/21/2022 01:53:47 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 01:53:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=879
05/21/2022 01:53:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
05/21/2022 01:53:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=889
05/21/2022 01:53:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
05/21/2022 01:53:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
05/21/2022 01:53:58 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 01:54:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=904
05/21/2022 01:54:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
05/21/2022 01:54:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=914
05/21/2022 01:54:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=919
05/21/2022 01:54:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
05/21/2022 01:54:08 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 01:54:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=929
05/21/2022 01:54:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=934
05/21/2022 01:54:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.52 on epoch=939
05/21/2022 01:54:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=944
05/21/2022 01:54:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=949
05/21/2022 01:54:19 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 01:54:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=954
05/21/2022 01:54:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
05/21/2022 01:54:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=964
05/21/2022 01:54:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.37 on epoch=969
05/21/2022 01:54:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=974
05/21/2022 01:54:29 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 01:54:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
05/21/2022 01:54:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/21/2022 01:54:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
05/21/2022 01:54:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/21/2022 01:54:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=999
05/21/2022 01:54:39 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 01:54:39 - INFO - __main__ - save last model!
05/21/2022 01:54:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 01:54:39 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 01:54:39 - INFO - __main__ - Printing 3 examples
05/21/2022 01:54:39 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:54:39 - INFO - __main__ - ['entailed']
05/21/2022 01:54:39 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:54:39 - INFO - __main__ - ['entailed']
05/21/2022 01:54:39 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 01:54:39 - INFO - __main__ - ['entailed']
05/21/2022 01:54:39 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:54:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:54:40 - INFO - __main__ - Printing 3 examples
05/21/2022 01:54:40 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/21/2022 01:54:40 - INFO - __main__ - ['refuted']
05/21/2022 01:54:40 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/21/2022 01:54:40 - INFO - __main__ - ['refuted']
05/21/2022 01:54:40 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/21/2022 01:54:40 - INFO - __main__ - ['refuted']
05/21/2022 01:54:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:54:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:54:40 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 01:54:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:54:40 - INFO - __main__ - Printing 3 examples
05/21/2022 01:54:40 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/21/2022 01:54:40 - INFO - __main__ - ['refuted']
05/21/2022 01:54:40 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/21/2022 01:54:40 - INFO - __main__ - ['refuted']
05/21/2022 01:54:40 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/21/2022 01:54:40 - INFO - __main__ - ['refuted']
05/21/2022 01:54:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:54:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:54:40 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 01:54:46 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 01:54:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 01:54:46 - INFO - __main__ - Starting training!
05/21/2022 01:55:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:55:16 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 01:59:25 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.4_8_predictions.txt
05/21/2022 01:59:25 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 01:59:25 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/21/2022 01:59:25 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.3, bsz=8 ...
05/21/2022 01:59:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:59:26 - INFO - __main__ - Printing 3 examples
05/21/2022 01:59:26 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/21/2022 01:59:26 - INFO - __main__ - ['refuted']
05/21/2022 01:59:26 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/21/2022 01:59:26 - INFO - __main__ - ['refuted']
05/21/2022 01:59:26 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/21/2022 01:59:26 - INFO - __main__ - ['refuted']
05/21/2022 01:59:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:59:26 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:59:26 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 01:59:26 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 01:59:26 - INFO - __main__ - Printing 3 examples
05/21/2022 01:59:26 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/21/2022 01:59:26 - INFO - __main__ - ['refuted']
05/21/2022 01:59:26 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/21/2022 01:59:26 - INFO - __main__ - ['refuted']
05/21/2022 01:59:26 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/21/2022 01:59:26 - INFO - __main__ - ['refuted']
05/21/2022 01:59:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 01:59:26 - INFO - __main__ - Tokenizing Output ...
05/21/2022 01:59:26 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 01:59:32 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 01:59:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 01:59:32 - INFO - __main__ - Starting training!
05/21/2022 01:59:34 - INFO - __main__ - Step 10 Global step 10 Train loss 4.93 on epoch=4
05/21/2022 01:59:36 - INFO - __main__ - Step 20 Global step 20 Train loss 4.96 on epoch=9
05/21/2022 01:59:38 - INFO - __main__ - Step 30 Global step 30 Train loss 4.83 on epoch=14
05/21/2022 01:59:40 - INFO - __main__ - Step 40 Global step 40 Train loss 4.79 on epoch=19
05/21/2022 01:59:42 - INFO - __main__ - Step 50 Global step 50 Train loss 4.76 on epoch=24
05/21/2022 01:59:49 - INFO - __main__ - Global step 50 Train loss 4.85 Classification-F1 0.0 on epoch=24
05/21/2022 01:59:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 01:59:51 - INFO - __main__ - Step 60 Global step 60 Train loss 4.55 on epoch=29
05/21/2022 01:59:53 - INFO - __main__ - Step 70 Global step 70 Train loss 4.49 on epoch=34
05/21/2022 01:59:55 - INFO - __main__ - Step 80 Global step 80 Train loss 4.46 on epoch=39
05/21/2022 01:59:57 - INFO - __main__ - Step 90 Global step 90 Train loss 4.41 on epoch=44
05/21/2022 01:59:59 - INFO - __main__ - Step 100 Global step 100 Train loss 4.27 on epoch=49
05/21/2022 02:00:00 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/21/2022 02:00:02 - INFO - __main__ - Step 110 Global step 110 Train loss 4.04 on epoch=54
05/21/2022 02:00:04 - INFO - __main__ - Step 120 Global step 120 Train loss 3.99 on epoch=59
05/21/2022 02:00:06 - INFO - __main__ - Step 130 Global step 130 Train loss 3.90 on epoch=64
05/21/2022 02:00:07 - INFO - __main__ - Step 140 Global step 140 Train loss 3.86 on epoch=69
05/21/2022 02:00:09 - INFO - __main__ - Step 150 Global step 150 Train loss 3.85 on epoch=74
05/21/2022 02:00:11 - INFO - __main__ - Global step 150 Train loss 3.93 Classification-F1 0.0 on epoch=74
05/21/2022 02:00:13 - INFO - __main__ - Step 160 Global step 160 Train loss 3.69 on epoch=79
05/21/2022 02:00:14 - INFO - __main__ - Step 170 Global step 170 Train loss 3.57 on epoch=84
05/21/2022 02:00:16 - INFO - __main__ - Step 180 Global step 180 Train loss 3.63 on epoch=89
05/21/2022 02:00:18 - INFO - __main__ - Step 190 Global step 190 Train loss 3.55 on epoch=94
05/21/2022 02:00:20 - INFO - __main__ - Step 200 Global step 200 Train loss 3.35 on epoch=99
05/21/2022 02:00:21 - INFO - __main__ - Global step 200 Train loss 3.56 Classification-F1 0.0 on epoch=99
05/21/2022 02:00:23 - INFO - __main__ - Step 210 Global step 210 Train loss 3.37 on epoch=104
05/21/2022 02:00:25 - INFO - __main__ - Step 220 Global step 220 Train loss 3.39 on epoch=109
05/21/2022 02:00:27 - INFO - __main__ - Step 230 Global step 230 Train loss 3.29 on epoch=114
05/21/2022 02:00:29 - INFO - __main__ - Step 240 Global step 240 Train loss 3.10 on epoch=119
05/21/2022 02:00:31 - INFO - __main__ - Step 250 Global step 250 Train loss 3.17 on epoch=124
05/21/2022 02:00:32 - INFO - __main__ - Global step 250 Train loss 3.26 Classification-F1 0.14285714285714285 on epoch=124
05/21/2022 02:00:32 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.14285714285714285 on epoch=124, global_step=250
05/21/2022 02:00:34 - INFO - __main__ - Step 260 Global step 260 Train loss 2.88 on epoch=129
05/21/2022 02:00:36 - INFO - __main__ - Step 270 Global step 270 Train loss 2.99 on epoch=134
05/21/2022 02:00:38 - INFO - __main__ - Step 280 Global step 280 Train loss 2.91 on epoch=139
05/21/2022 02:00:40 - INFO - __main__ - Step 290 Global step 290 Train loss 2.79 on epoch=144
05/21/2022 02:00:42 - INFO - __main__ - Step 300 Global step 300 Train loss 2.69 on epoch=149
05/21/2022 02:00:44 - INFO - __main__ - Global step 300 Train loss 2.85 Classification-F1 0.3333333333333333 on epoch=149
05/21/2022 02:00:44 - INFO - __main__ - Saving model with best Classification-F1: 0.14285714285714285 -> 0.3333333333333333 on epoch=149, global_step=300
05/21/2022 02:00:46 - INFO - __main__ - Step 310 Global step 310 Train loss 2.65 on epoch=154
05/21/2022 02:00:48 - INFO - __main__ - Step 320 Global step 320 Train loss 2.49 on epoch=159
05/21/2022 02:00:50 - INFO - __main__ - Step 330 Global step 330 Train loss 2.46 on epoch=164
05/21/2022 02:00:52 - INFO - __main__ - Step 340 Global step 340 Train loss 2.35 on epoch=169
05/21/2022 02:00:54 - INFO - __main__ - Step 350 Global step 350 Train loss 2.37 on epoch=174
05/21/2022 02:00:55 - INFO - __main__ - Global step 350 Train loss 2.46 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 02:00:57 - INFO - __main__ - Step 360 Global step 360 Train loss 2.20 on epoch=179
05/21/2022 02:00:59 - INFO - __main__ - Step 370 Global step 370 Train loss 2.00 on epoch=184
05/21/2022 02:01:01 - INFO - __main__ - Step 380 Global step 380 Train loss 1.99 on epoch=189
05/21/2022 02:01:03 - INFO - __main__ - Step 390 Global step 390 Train loss 1.93 on epoch=194
05/21/2022 02:01:05 - INFO - __main__ - Step 400 Global step 400 Train loss 1.79 on epoch=199
05/21/2022 02:01:08 - INFO - __main__ - Global step 400 Train loss 1.98 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 02:01:09 - INFO - __main__ - Step 410 Global step 410 Train loss 1.78 on epoch=204
05/21/2022 02:01:11 - INFO - __main__ - Step 420 Global step 420 Train loss 1.79 on epoch=209
05/21/2022 02:01:13 - INFO - __main__ - Step 430 Global step 430 Train loss 1.69 on epoch=214
05/21/2022 02:01:15 - INFO - __main__ - Step 440 Global step 440 Train loss 1.56 on epoch=219
05/21/2022 02:01:17 - INFO - __main__ - Step 450 Global step 450 Train loss 1.54 on epoch=224
05/21/2022 02:01:20 - INFO - __main__ - Global step 450 Train loss 1.67 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 02:01:22 - INFO - __main__ - Step 460 Global step 460 Train loss 1.67 on epoch=229
05/21/2022 02:01:24 - INFO - __main__ - Step 470 Global step 470 Train loss 1.39 on epoch=234
05/21/2022 02:01:26 - INFO - __main__ - Step 480 Global step 480 Train loss 1.46 on epoch=239
05/21/2022 02:01:28 - INFO - __main__ - Step 490 Global step 490 Train loss 1.44 on epoch=244
05/21/2022 02:01:29 - INFO - __main__ - Step 500 Global step 500 Train loss 1.30 on epoch=249
05/21/2022 02:01:36 - INFO - __main__ - Global step 500 Train loss 1.45 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 02:01:37 - INFO - __main__ - Step 510 Global step 510 Train loss 1.29 on epoch=254
05/21/2022 02:01:39 - INFO - __main__ - Step 520 Global step 520 Train loss 1.30 on epoch=259
05/21/2022 02:01:41 - INFO - __main__ - Step 530 Global step 530 Train loss 1.34 on epoch=264
05/21/2022 02:01:43 - INFO - __main__ - Step 540 Global step 540 Train loss 1.18 on epoch=269
05/21/2022 02:01:45 - INFO - __main__ - Step 550 Global step 550 Train loss 1.17 on epoch=274
05/21/2022 02:01:51 - INFO - __main__ - Global step 550 Train loss 1.26 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 02:01:53 - INFO - __main__ - Step 560 Global step 560 Train loss 1.27 on epoch=279
05/21/2022 02:01:55 - INFO - __main__ - Step 570 Global step 570 Train loss 1.20 on epoch=284
05/21/2022 02:01:57 - INFO - __main__ - Step 580 Global step 580 Train loss 1.12 on epoch=289
05/21/2022 02:01:59 - INFO - __main__ - Step 590 Global step 590 Train loss 1.11 on epoch=294
05/21/2022 02:02:01 - INFO - __main__ - Step 600 Global step 600 Train loss 1.02 on epoch=299
05/21/2022 02:02:04 - INFO - __main__ - Global step 600 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 02:02:06 - INFO - __main__ - Step 610 Global step 610 Train loss 1.07 on epoch=304
05/21/2022 02:02:08 - INFO - __main__ - Step 620 Global step 620 Train loss 1.07 on epoch=309
05/21/2022 02:02:09 - INFO - __main__ - Step 630 Global step 630 Train loss 1.07 on epoch=314
05/21/2022 02:02:11 - INFO - __main__ - Step 640 Global step 640 Train loss 1.04 on epoch=319
05/21/2022 02:02:13 - INFO - __main__ - Step 650 Global step 650 Train loss 1.07 on epoch=324
05/21/2022 02:02:14 - INFO - __main__ - Global step 650 Train loss 1.06 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 02:02:16 - INFO - __main__ - Step 660 Global step 660 Train loss 1.06 on epoch=329
05/21/2022 02:02:18 - INFO - __main__ - Step 670 Global step 670 Train loss 1.01 on epoch=334
05/21/2022 02:02:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.98 on epoch=339
05/21/2022 02:02:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.98 on epoch=344
05/21/2022 02:02:24 - INFO - __main__ - Step 700 Global step 700 Train loss 1.04 on epoch=349
05/21/2022 02:02:25 - INFO - __main__ - Global step 700 Train loss 1.01 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 02:02:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.89 on epoch=354
05/21/2022 02:02:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.95 on epoch=359
05/21/2022 02:02:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.95 on epoch=364
05/21/2022 02:02:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.92 on epoch=369
05/21/2022 02:02:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.92 on epoch=374
05/21/2022 02:02:35 - INFO - __main__ - Global step 750 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 02:02:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=379
05/21/2022 02:02:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.92 on epoch=384
05/21/2022 02:02:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.81 on epoch=389
05/21/2022 02:02:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=394
05/21/2022 02:02:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.87 on epoch=399
05/21/2022 02:02:45 - INFO - __main__ - Global step 800 Train loss 0.86 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 02:02:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=404
05/21/2022 02:02:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.75 on epoch=409
05/21/2022 02:02:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=414
05/21/2022 02:02:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.78 on epoch=419
05/21/2022 02:02:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.85 on epoch=424
05/21/2022 02:02:56 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 02:02:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.75 on epoch=429
05/21/2022 02:03:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.81 on epoch=434
05/21/2022 02:03:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.68 on epoch=439
05/21/2022 02:03:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.68 on epoch=444
05/21/2022 02:03:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.68 on epoch=449
05/21/2022 02:03:06 - INFO - __main__ - Global step 900 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 02:03:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.68 on epoch=454
05/21/2022 02:03:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.71 on epoch=459
05/21/2022 02:03:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.63 on epoch=464
05/21/2022 02:03:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.70 on epoch=469
05/21/2022 02:03:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.75 on epoch=474
05/21/2022 02:03:17 - INFO - __main__ - Global step 950 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 02:03:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.63 on epoch=479
05/21/2022 02:03:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.64 on epoch=484
05/21/2022 02:03:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.62 on epoch=489
05/21/2022 02:03:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.66 on epoch=494
05/21/2022 02:03:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.76 on epoch=499
05/21/2022 02:03:27 - INFO - __main__ - Global step 1000 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 02:03:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.63 on epoch=504
05/21/2022 02:03:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.71 on epoch=509
05/21/2022 02:03:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=514
05/21/2022 02:03:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.68 on epoch=519
05/21/2022 02:03:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=524
05/21/2022 02:03:37 - INFO - __main__ - Global step 1050 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 02:03:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.65 on epoch=529
05/21/2022 02:03:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.65 on epoch=534
05/21/2022 02:03:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.65 on epoch=539
05/21/2022 02:03:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.57 on epoch=544
05/21/2022 02:03:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=549
05/21/2022 02:03:48 - INFO - __main__ - Global step 1100 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 02:03:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.61 on epoch=554
05/21/2022 02:03:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.55 on epoch=559
05/21/2022 02:03:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.56 on epoch=564
05/21/2022 02:03:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.56 on epoch=569
05/21/2022 02:03:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.57 on epoch=574
05/21/2022 02:03:58 - INFO - __main__ - Global step 1150 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 02:04:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.53 on epoch=579
05/21/2022 02:04:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=584
05/21/2022 02:04:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=589
05/21/2022 02:04:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.51 on epoch=594
05/21/2022 02:04:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.50 on epoch=599
05/21/2022 02:04:09 - INFO - __main__ - Global step 1200 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 02:04:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.48 on epoch=604
05/21/2022 02:04:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.52 on epoch=609
05/21/2022 02:04:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=614
05/21/2022 02:04:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.53 on epoch=619
05/21/2022 02:04:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=624
05/21/2022 02:04:19 - INFO - __main__ - Global step 1250 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 02:04:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=629
05/21/2022 02:04:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=634
05/21/2022 02:04:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=639
05/21/2022 02:04:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=644
05/21/2022 02:04:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=649
05/21/2022 02:04:30 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 02:04:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=654
05/21/2022 02:04:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.52 on epoch=659
05/21/2022 02:04:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=664
05/21/2022 02:04:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=669
05/21/2022 02:04:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=674
05/21/2022 02:04:40 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 02:04:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=679
05/21/2022 02:04:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
05/21/2022 02:04:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=689
05/21/2022 02:04:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=694
05/21/2022 02:04:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=699
05/21/2022 02:04:50 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 02:04:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.47 on epoch=704
05/21/2022 02:04:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
05/21/2022 02:04:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
05/21/2022 02:04:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
05/21/2022 02:05:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=724
05/21/2022 02:05:01 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 02:05:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=729
05/21/2022 02:05:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=734
05/21/2022 02:05:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/21/2022 02:05:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=744
05/21/2022 02:05:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=749
05/21/2022 02:05:11 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 02:05:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=754
05/21/2022 02:05:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=759
05/21/2022 02:05:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=764
05/21/2022 02:05:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=769
05/21/2022 02:05:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=774
05/21/2022 02:05:21 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 02:05:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=779
05/21/2022 02:05:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
05/21/2022 02:05:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
05/21/2022 02:05:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=794
05/21/2022 02:05:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
05/21/2022 02:05:31 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 02:05:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=804
05/21/2022 02:05:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=809
05/21/2022 02:05:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=814
05/21/2022 02:05:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=819
05/21/2022 02:05:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=824
05/21/2022 02:05:42 - INFO - __main__ - Global step 1650 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 02:05:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=829
05/21/2022 02:05:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
05/21/2022 02:05:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
05/21/2022 02:05:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=844
05/21/2022 02:05:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=849
05/21/2022 02:05:52 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 02:05:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/21/2022 02:05:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=859
05/21/2022 02:05:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=864
05/21/2022 02:06:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
05/21/2022 02:06:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/21/2022 02:06:02 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 02:06:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=879
05/21/2022 02:06:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=884
05/21/2022 02:06:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=889
05/21/2022 02:06:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=894
05/21/2022 02:06:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=899
05/21/2022 02:06:12 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 02:06:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=904
05/21/2022 02:06:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/21/2022 02:06:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=914
05/21/2022 02:06:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=919
05/21/2022 02:06:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
05/21/2022 02:06:23 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 02:06:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=929
05/21/2022 02:06:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
05/21/2022 02:06:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=939
05/21/2022 02:06:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=944
05/21/2022 02:06:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
05/21/2022 02:06:33 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 02:06:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=954
05/21/2022 02:06:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=959
05/21/2022 02:06:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/21/2022 02:06:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=969
05/21/2022 02:06:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/21/2022 02:06:43 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 02:06:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
05/21/2022 02:06:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=984
05/21/2022 02:06:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
05/21/2022 02:06:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/21/2022 02:06:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=999
05/21/2022 02:06:54 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 02:06:54 - INFO - __main__ - save last model!
05/21/2022 02:06:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 02:06:54 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 02:06:54 - INFO - __main__ - Printing 3 examples
05/21/2022 02:06:54 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:06:54 - INFO - __main__ - ['entailed']
05/21/2022 02:06:54 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:06:54 - INFO - __main__ - ['entailed']
05/21/2022 02:06:54 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:06:54 - INFO - __main__ - ['entailed']
05/21/2022 02:06:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:06:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:06:54 - INFO - __main__ - Printing 3 examples
05/21/2022 02:06:54 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/21/2022 02:06:54 - INFO - __main__ - ['refuted']
05/21/2022 02:06:54 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/21/2022 02:06:54 - INFO - __main__ - ['refuted']
05/21/2022 02:06:54 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/21/2022 02:06:54 - INFO - __main__ - ['refuted']
05/21/2022 02:06:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:06:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:06:54 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 02:06:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:06:54 - INFO - __main__ - Printing 3 examples
05/21/2022 02:06:54 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/21/2022 02:06:54 - INFO - __main__ - ['refuted']
05/21/2022 02:06:54 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/21/2022 02:06:54 - INFO - __main__ - ['refuted']
05/21/2022 02:06:54 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/21/2022 02:06:54 - INFO - __main__ - ['refuted']
05/21/2022 02:06:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:06:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:06:54 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 02:06:59 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 02:07:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 02:07:00 - INFO - __main__ - Starting training!
05/21/2022 02:07:18 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:07:30 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 02:11:41 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.3_8_predictions.txt
05/21/2022 02:11:41 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 02:11:41 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/21/2022 02:11:41 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.2, bsz=8 ...
05/21/2022 02:11:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:11:42 - INFO - __main__ - Printing 3 examples
05/21/2022 02:11:42 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/21/2022 02:11:42 - INFO - __main__ - ['refuted']
05/21/2022 02:11:42 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/21/2022 02:11:42 - INFO - __main__ - ['refuted']
05/21/2022 02:11:42 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/21/2022 02:11:42 - INFO - __main__ - ['refuted']
05/21/2022 02:11:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:11:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:11:42 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 02:11:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:11:42 - INFO - __main__ - Printing 3 examples
05/21/2022 02:11:42 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/21/2022 02:11:42 - INFO - __main__ - ['refuted']
05/21/2022 02:11:42 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/21/2022 02:11:42 - INFO - __main__ - ['refuted']
05/21/2022 02:11:42 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/21/2022 02:11:42 - INFO - __main__ - ['refuted']
05/21/2022 02:11:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:11:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:11:42 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 02:11:47 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 02:11:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 02:11:48 - INFO - __main__ - Starting training!
05/21/2022 02:11:51 - INFO - __main__ - Step 10 Global step 10 Train loss 4.93 on epoch=4
05/21/2022 02:11:53 - INFO - __main__ - Step 20 Global step 20 Train loss 4.93 on epoch=9
05/21/2022 02:11:55 - INFO - __main__ - Step 30 Global step 30 Train loss 4.94 on epoch=14
05/21/2022 02:11:57 - INFO - __main__ - Step 40 Global step 40 Train loss 4.89 on epoch=19
05/21/2022 02:11:59 - INFO - __main__ - Step 50 Global step 50 Train loss 4.86 on epoch=24
05/21/2022 02:12:00 - INFO - __main__ - Global step 50 Train loss 4.91 Classification-F1 0.0 on epoch=24
05/21/2022 02:12:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 02:12:02 - INFO - __main__ - Step 60 Global step 60 Train loss 4.71 on epoch=29
05/21/2022 02:12:04 - INFO - __main__ - Step 70 Global step 70 Train loss 4.81 on epoch=34
05/21/2022 02:12:06 - INFO - __main__ - Step 80 Global step 80 Train loss 4.81 on epoch=39
05/21/2022 02:12:08 - INFO - __main__ - Step 90 Global step 90 Train loss 4.82 on epoch=44
05/21/2022 02:12:10 - INFO - __main__ - Step 100 Global step 100 Train loss 4.63 on epoch=49
05/21/2022 02:12:11 - INFO - __main__ - Global step 100 Train loss 4.76 Classification-F1 0.0 on epoch=49
05/21/2022 02:12:13 - INFO - __main__ - Step 110 Global step 110 Train loss 4.68 on epoch=54
05/21/2022 02:12:15 - INFO - __main__ - Step 120 Global step 120 Train loss 4.58 on epoch=59
05/21/2022 02:12:16 - INFO - __main__ - Step 130 Global step 130 Train loss 4.53 on epoch=64
05/21/2022 02:12:18 - INFO - __main__ - Step 140 Global step 140 Train loss 4.56 on epoch=69
05/21/2022 02:12:20 - INFO - __main__ - Step 150 Global step 150 Train loss 4.47 on epoch=74
05/21/2022 02:12:22 - INFO - __main__ - Global step 150 Train loss 4.57 Classification-F1 0.0 on epoch=74
05/21/2022 02:12:24 - INFO - __main__ - Step 160 Global step 160 Train loss 4.39 on epoch=79
05/21/2022 02:12:26 - INFO - __main__ - Step 170 Global step 170 Train loss 4.40 on epoch=84
05/21/2022 02:12:28 - INFO - __main__ - Step 180 Global step 180 Train loss 4.32 on epoch=89
05/21/2022 02:12:30 - INFO - __main__ - Step 190 Global step 190 Train loss 4.13 on epoch=94
05/21/2022 02:12:32 - INFO - __main__ - Step 200 Global step 200 Train loss 4.18 on epoch=99
05/21/2022 02:12:34 - INFO - __main__ - Global step 200 Train loss 4.28 Classification-F1 0.0 on epoch=99
05/21/2022 02:12:36 - INFO - __main__ - Step 210 Global step 210 Train loss 4.16 on epoch=104
05/21/2022 02:12:38 - INFO - __main__ - Step 220 Global step 220 Train loss 4.06 on epoch=109
05/21/2022 02:12:40 - INFO - __main__ - Step 230 Global step 230 Train loss 3.93 on epoch=114
05/21/2022 02:12:42 - INFO - __main__ - Step 240 Global step 240 Train loss 4.01 on epoch=119
05/21/2022 02:12:44 - INFO - __main__ - Step 250 Global step 250 Train loss 3.95 on epoch=124
05/21/2022 02:12:46 - INFO - __main__ - Global step 250 Train loss 4.02 Classification-F1 0.0 on epoch=124
05/21/2022 02:12:48 - INFO - __main__ - Step 260 Global step 260 Train loss 3.93 on epoch=129
05/21/2022 02:12:50 - INFO - __main__ - Step 270 Global step 270 Train loss 3.83 on epoch=134
05/21/2022 02:12:52 - INFO - __main__ - Step 280 Global step 280 Train loss 3.80 on epoch=139
05/21/2022 02:12:53 - INFO - __main__ - Step 290 Global step 290 Train loss 3.77 on epoch=144
05/21/2022 02:12:55 - INFO - __main__ - Step 300 Global step 300 Train loss 3.70 on epoch=149
05/21/2022 02:12:57 - INFO - __main__ - Global step 300 Train loss 3.81 Classification-F1 0.0 on epoch=149
05/21/2022 02:12:59 - INFO - __main__ - Step 310 Global step 310 Train loss 3.56 on epoch=154
05/21/2022 02:13:01 - INFO - __main__ - Step 320 Global step 320 Train loss 3.63 on epoch=159
05/21/2022 02:13:03 - INFO - __main__ - Step 330 Global step 330 Train loss 3.55 on epoch=164
05/21/2022 02:13:05 - INFO - __main__ - Step 340 Global step 340 Train loss 3.53 on epoch=169
05/21/2022 02:13:06 - INFO - __main__ - Step 350 Global step 350 Train loss 3.49 on epoch=174
05/21/2022 02:13:08 - INFO - __main__ - Global step 350 Train loss 3.55 Classification-F1 0.08637873754152824 on epoch=174
05/21/2022 02:13:08 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.08637873754152824 on epoch=174, global_step=350
05/21/2022 02:13:10 - INFO - __main__ - Step 360 Global step 360 Train loss 3.48 on epoch=179
05/21/2022 02:13:12 - INFO - __main__ - Step 370 Global step 370 Train loss 3.44 on epoch=184
05/21/2022 02:13:14 - INFO - __main__ - Step 380 Global step 380 Train loss 3.40 on epoch=189
05/21/2022 02:13:15 - INFO - __main__ - Step 390 Global step 390 Train loss 3.21 on epoch=194
05/21/2022 02:13:17 - INFO - __main__ - Step 400 Global step 400 Train loss 3.29 on epoch=199
05/21/2022 02:13:19 - INFO - __main__ - Global step 400 Train loss 3.36 Classification-F1 0.21276595744680848 on epoch=199
05/21/2022 02:13:19 - INFO - __main__ - Saving model with best Classification-F1: 0.08637873754152824 -> 0.21276595744680848 on epoch=199, global_step=400
05/21/2022 02:13:21 - INFO - __main__ - Step 410 Global step 410 Train loss 3.18 on epoch=204
05/21/2022 02:13:23 - INFO - __main__ - Step 420 Global step 420 Train loss 3.16 on epoch=209
05/21/2022 02:13:25 - INFO - __main__ - Step 430 Global step 430 Train loss 3.07 on epoch=214
05/21/2022 02:13:26 - INFO - __main__ - Step 440 Global step 440 Train loss 3.14 on epoch=219
05/21/2022 02:13:28 - INFO - __main__ - Step 450 Global step 450 Train loss 2.96 on epoch=224
05/21/2022 02:13:32 - INFO - __main__ - Global step 450 Train loss 3.10 Classification-F1 0.1590909090909091 on epoch=224
05/21/2022 02:13:33 - INFO - __main__ - Step 460 Global step 460 Train loss 2.87 on epoch=229
05/21/2022 02:13:35 - INFO - __main__ - Step 470 Global step 470 Train loss 3.00 on epoch=234
05/21/2022 02:13:37 - INFO - __main__ - Step 480 Global step 480 Train loss 2.81 on epoch=239
05/21/2022 02:13:39 - INFO - __main__ - Step 490 Global step 490 Train loss 2.82 on epoch=244
05/21/2022 02:13:41 - INFO - __main__ - Step 500 Global step 500 Train loss 2.80 on epoch=249
05/21/2022 02:13:44 - INFO - __main__ - Global step 500 Train loss 2.86 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 02:13:44 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.3333333333333333 on epoch=249, global_step=500
05/21/2022 02:13:46 - INFO - __main__ - Step 510 Global step 510 Train loss 2.76 on epoch=254
05/21/2022 02:13:48 - INFO - __main__ - Step 520 Global step 520 Train loss 2.67 on epoch=259
05/21/2022 02:13:50 - INFO - __main__ - Step 530 Global step 530 Train loss 2.52 on epoch=264
05/21/2022 02:13:52 - INFO - __main__ - Step 540 Global step 540 Train loss 2.53 on epoch=269
05/21/2022 02:13:54 - INFO - __main__ - Step 550 Global step 550 Train loss 2.52 on epoch=274
05/21/2022 02:13:57 - INFO - __main__ - Global step 550 Train loss 2.60 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 02:13:59 - INFO - __main__ - Step 560 Global step 560 Train loss 2.54 on epoch=279
05/21/2022 02:14:01 - INFO - __main__ - Step 570 Global step 570 Train loss 2.45 on epoch=284
05/21/2022 02:14:03 - INFO - __main__ - Step 580 Global step 580 Train loss 2.34 on epoch=289
05/21/2022 02:14:05 - INFO - __main__ - Step 590 Global step 590 Train loss 2.43 on epoch=294
05/21/2022 02:14:07 - INFO - __main__ - Step 600 Global step 600 Train loss 2.28 on epoch=299
05/21/2022 02:14:10 - INFO - __main__ - Global step 600 Train loss 2.41 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 02:14:12 - INFO - __main__ - Step 610 Global step 610 Train loss 2.26 on epoch=304
05/21/2022 02:14:14 - INFO - __main__ - Step 620 Global step 620 Train loss 2.27 on epoch=309
05/21/2022 02:14:16 - INFO - __main__ - Step 630 Global step 630 Train loss 2.15 on epoch=314
05/21/2022 02:14:18 - INFO - __main__ - Step 640 Global step 640 Train loss 2.18 on epoch=319
05/21/2022 02:14:20 - INFO - __main__ - Step 650 Global step 650 Train loss 2.09 on epoch=324
05/21/2022 02:14:24 - INFO - __main__ - Global step 650 Train loss 2.19 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 02:14:26 - INFO - __main__ - Step 660 Global step 660 Train loss 2.01 on epoch=329
05/21/2022 02:14:28 - INFO - __main__ - Step 670 Global step 670 Train loss 2.02 on epoch=334
05/21/2022 02:14:30 - INFO - __main__ - Step 680 Global step 680 Train loss 2.06 on epoch=339
05/21/2022 02:14:32 - INFO - __main__ - Step 690 Global step 690 Train loss 2.03 on epoch=344
05/21/2022 02:14:33 - INFO - __main__ - Step 700 Global step 700 Train loss 1.95 on epoch=349
05/21/2022 02:14:37 - INFO - __main__ - Global step 700 Train loss 2.02 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 02:14:39 - INFO - __main__ - Step 710 Global step 710 Train loss 1.86 on epoch=354
05/21/2022 02:14:41 - INFO - __main__ - Step 720 Global step 720 Train loss 1.92 on epoch=359
05/21/2022 02:14:43 - INFO - __main__ - Step 730 Global step 730 Train loss 1.83 on epoch=364
05/21/2022 02:14:45 - INFO - __main__ - Step 740 Global step 740 Train loss 1.80 on epoch=369
05/21/2022 02:14:47 - INFO - __main__ - Step 750 Global step 750 Train loss 1.82 on epoch=374
05/21/2022 02:14:50 - INFO - __main__ - Global step 750 Train loss 1.85 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 02:14:52 - INFO - __main__ - Step 760 Global step 760 Train loss 1.65 on epoch=379
05/21/2022 02:14:54 - INFO - __main__ - Step 770 Global step 770 Train loss 1.83 on epoch=384
05/21/2022 02:14:56 - INFO - __main__ - Step 780 Global step 780 Train loss 1.74 on epoch=389
05/21/2022 02:14:58 - INFO - __main__ - Step 790 Global step 790 Train loss 1.65 on epoch=394
05/21/2022 02:15:00 - INFO - __main__ - Step 800 Global step 800 Train loss 1.57 on epoch=399
05/21/2022 02:15:03 - INFO - __main__ - Global step 800 Train loss 1.69 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 02:15:05 - INFO - __main__ - Step 810 Global step 810 Train loss 1.61 on epoch=404
05/21/2022 02:15:07 - INFO - __main__ - Step 820 Global step 820 Train loss 1.57 on epoch=409
05/21/2022 02:15:09 - INFO - __main__ - Step 830 Global step 830 Train loss 1.54 on epoch=414
05/21/2022 02:15:11 - INFO - __main__ - Step 840 Global step 840 Train loss 1.48 on epoch=419
05/21/2022 02:15:13 - INFO - __main__ - Step 850 Global step 850 Train loss 1.46 on epoch=424
05/21/2022 02:15:16 - INFO - __main__ - Global step 850 Train loss 1.53 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 02:15:18 - INFO - __main__ - Step 860 Global step 860 Train loss 1.38 on epoch=429
05/21/2022 02:15:20 - INFO - __main__ - Step 870 Global step 870 Train loss 1.30 on epoch=434
05/21/2022 02:15:21 - INFO - __main__ - Step 880 Global step 880 Train loss 1.33 on epoch=439
05/21/2022 02:15:23 - INFO - __main__ - Step 890 Global step 890 Train loss 1.44 on epoch=444
05/21/2022 02:15:25 - INFO - __main__ - Step 900 Global step 900 Train loss 1.25 on epoch=449
05/21/2022 02:15:29 - INFO - __main__ - Global step 900 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 02:15:30 - INFO - __main__ - Step 910 Global step 910 Train loss 1.29 on epoch=454
05/21/2022 02:15:32 - INFO - __main__ - Step 920 Global step 920 Train loss 1.28 on epoch=459
05/21/2022 02:15:34 - INFO - __main__ - Step 930 Global step 930 Train loss 1.16 on epoch=464
05/21/2022 02:15:36 - INFO - __main__ - Step 940 Global step 940 Train loss 1.19 on epoch=469
05/21/2022 02:15:38 - INFO - __main__ - Step 950 Global step 950 Train loss 1.10 on epoch=474
05/21/2022 02:15:40 - INFO - __main__ - Global step 950 Train loss 1.20 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 02:15:42 - INFO - __main__ - Step 960 Global step 960 Train loss 1.16 on epoch=479
05/21/2022 02:15:44 - INFO - __main__ - Step 970 Global step 970 Train loss 1.20 on epoch=484
05/21/2022 02:15:46 - INFO - __main__ - Step 980 Global step 980 Train loss 1.08 on epoch=489
05/21/2022 02:15:48 - INFO - __main__ - Step 990 Global step 990 Train loss 1.10 on epoch=494
05/21/2022 02:15:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.14 on epoch=499
05/21/2022 02:15:51 - INFO - __main__ - Global step 1000 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 02:15:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.09 on epoch=504
05/21/2022 02:15:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.15 on epoch=509
05/21/2022 02:15:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.12 on epoch=514
05/21/2022 02:15:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.03 on epoch=519
05/21/2022 02:16:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.01 on epoch=524
05/21/2022 02:16:01 - INFO - __main__ - Global step 1050 Train loss 1.08 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 02:16:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.99 on epoch=529
05/21/2022 02:16:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.09 on epoch=534
05/21/2022 02:16:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.92 on epoch=539
05/21/2022 02:16:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.11 on epoch=544
05/21/2022 02:16:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.02 on epoch=549
05/21/2022 02:16:12 - INFO - __main__ - Global step 1100 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 02:16:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.88 on epoch=554
05/21/2022 02:16:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.02 on epoch=559
05/21/2022 02:16:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.96 on epoch=564
05/21/2022 02:16:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.90 on epoch=569
05/21/2022 02:16:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.86 on epoch=574
05/21/2022 02:16:22 - INFO - __main__ - Global step 1150 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 02:16:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.92 on epoch=579
05/21/2022 02:16:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=584
05/21/2022 02:16:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.94 on epoch=589
05/21/2022 02:16:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.89 on epoch=594
05/21/2022 02:16:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.85 on epoch=599
05/21/2022 02:16:33 - INFO - __main__ - Global step 1200 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 02:16:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.88 on epoch=604
05/21/2022 02:16:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.76 on epoch=609
05/21/2022 02:16:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.77 on epoch=614
05/21/2022 02:16:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=619
05/21/2022 02:16:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.84 on epoch=624
05/21/2022 02:16:43 - INFO - __main__ - Global step 1250 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 02:16:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.81 on epoch=629
05/21/2022 02:16:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.80 on epoch=634
05/21/2022 02:16:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.71 on epoch=639
05/21/2022 02:16:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.78 on epoch=644
05/21/2022 02:16:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.85 on epoch=649
05/21/2022 02:16:54 - INFO - __main__ - Global step 1300 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 02:16:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=654
05/21/2022 02:16:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.73 on epoch=659
05/21/2022 02:16:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.71 on epoch=664
05/21/2022 02:17:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=669
05/21/2022 02:17:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.73 on epoch=674
05/21/2022 02:17:04 - INFO - __main__ - Global step 1350 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 02:17:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.76 on epoch=679
05/21/2022 02:17:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.77 on epoch=684
05/21/2022 02:17:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.72 on epoch=689
05/21/2022 02:17:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.67 on epoch=694
05/21/2022 02:17:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.69 on epoch=699
05/21/2022 02:17:15 - INFO - __main__ - Global step 1400 Train loss 0.72 Classification-F1 0.3191489361702127 on epoch=699
05/21/2022 02:17:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.68 on epoch=704
05/21/2022 02:17:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.72 on epoch=709
05/21/2022 02:17:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.76 on epoch=714
05/21/2022 02:17:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.63 on epoch=719
05/21/2022 02:17:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.59 on epoch=724
05/21/2022 02:17:25 - INFO - __main__ - Global step 1450 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 02:17:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.73 on epoch=729
05/21/2022 02:17:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.63 on epoch=734
05/21/2022 02:17:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.69 on epoch=739
05/21/2022 02:17:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.65 on epoch=744
05/21/2022 02:17:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.63 on epoch=749
05/21/2022 02:17:36 - INFO - __main__ - Global step 1500 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 02:17:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.72 on epoch=754
05/21/2022 02:17:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.64 on epoch=759
05/21/2022 02:17:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.62 on epoch=764
05/21/2022 02:17:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=769
05/21/2022 02:17:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.53 on epoch=774
05/21/2022 02:17:46 - INFO - __main__ - Global step 1550 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 02:17:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=779
05/21/2022 02:17:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
05/21/2022 02:17:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.69 on epoch=789
05/21/2022 02:17:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.58 on epoch=794
05/21/2022 02:17:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.54 on epoch=799
05/21/2022 02:17:57 - INFO - __main__ - Global step 1600 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 02:17:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.59 on epoch=804
05/21/2022 02:18:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.55 on epoch=809
05/21/2022 02:18:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=814
05/21/2022 02:18:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.59 on epoch=819
05/21/2022 02:18:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.57 on epoch=824
05/21/2022 02:18:07 - INFO - __main__ - Global step 1650 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 02:18:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.53 on epoch=829
05/21/2022 02:18:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.61 on epoch=834
05/21/2022 02:18:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.59 on epoch=839
05/21/2022 02:18:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
05/21/2022 02:18:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.59 on epoch=849
05/21/2022 02:18:17 - INFO - __main__ - Global step 1700 Train loss 0.56 Classification-F1 0.3043478260869565 on epoch=849
05/21/2022 02:18:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.58 on epoch=854
05/21/2022 02:18:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.53 on epoch=859
05/21/2022 02:18:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.52 on epoch=864
05/21/2022 02:18:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=869
05/21/2022 02:18:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.53 on epoch=874
05/21/2022 02:18:28 - INFO - __main__ - Global step 1750 Train loss 0.52 Classification-F1 0.39139139139139134 on epoch=874
05/21/2022 02:18:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39139139139139134 on epoch=874, global_step=1750
05/21/2022 02:18:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.51 on epoch=879
05/21/2022 02:18:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.51 on epoch=884
05/21/2022 02:18:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.57 on epoch=889
05/21/2022 02:18:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.55 on epoch=894
05/21/2022 02:18:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
05/21/2022 02:18:38 - INFO - __main__ - Global step 1800 Train loss 0.51 Classification-F1 0.3043478260869565 on epoch=899
05/21/2022 02:18:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=904
05/21/2022 02:18:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.47 on epoch=909
05/21/2022 02:18:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.55 on epoch=914
05/21/2022 02:18:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=919
05/21/2022 02:18:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.62 on epoch=924
05/21/2022 02:18:49 - INFO - __main__ - Global step 1850 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 02:18:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
05/21/2022 02:18:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.53 on epoch=934
05/21/2022 02:18:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.53 on epoch=939
05/21/2022 02:18:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.48 on epoch=944
05/21/2022 02:18:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.58 on epoch=949
05/21/2022 02:18:59 - INFO - __main__ - Global step 1900 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 02:19:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=954
05/21/2022 02:19:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=959
05/21/2022 02:19:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=964
05/21/2022 02:19:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=969
05/21/2022 02:19:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=974
05/21/2022 02:19:10 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 02:19:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=979
05/21/2022 02:19:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.56 on epoch=984
05/21/2022 02:19:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.49 on epoch=989
05/21/2022 02:19:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=994
05/21/2022 02:19:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.50 on epoch=999
05/21/2022 02:19:20 - INFO - __main__ - Global step 2000 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 02:19:20 - INFO - __main__ - save last model!
05/21/2022 02:19:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 02:19:20 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 02:19:20 - INFO - __main__ - Printing 3 examples
05/21/2022 02:19:20 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:19:20 - INFO - __main__ - ['entailed']
05/21/2022 02:19:20 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:19:20 - INFO - __main__ - ['entailed']
05/21/2022 02:19:20 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:19:20 - INFO - __main__ - ['entailed']
05/21/2022 02:19:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:19:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:19:21 - INFO - __main__ - Printing 3 examples
05/21/2022 02:19:21 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/21/2022 02:19:21 - INFO - __main__ - ['entailed']
05/21/2022 02:19:21 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/21/2022 02:19:21 - INFO - __main__ - ['entailed']
05/21/2022 02:19:21 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/21/2022 02:19:21 - INFO - __main__ - ['entailed']
05/21/2022 02:19:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:19:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:19:21 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 02:19:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:19:21 - INFO - __main__ - Printing 3 examples
05/21/2022 02:19:21 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/21/2022 02:19:21 - INFO - __main__ - ['entailed']
05/21/2022 02:19:21 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/21/2022 02:19:21 - INFO - __main__ - ['entailed']
05/21/2022 02:19:21 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/21/2022 02:19:21 - INFO - __main__ - ['entailed']
05/21/2022 02:19:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:19:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:19:21 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 02:19:26 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 02:19:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 02:19:26 - INFO - __main__ - Starting training!
05/21/2022 02:19:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:19:56 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 02:24:17 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.2_8_predictions.txt
05/21/2022 02:24:17 - INFO - __main__ - Classification-F1 on test data: 0.3373
05/21/2022 02:24:19 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.2, bsz=8, dev_performance=0.39139139139139134, test_performance=0.3373174086208301
05/21/2022 02:24:19 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.5, bsz=8 ...
05/21/2022 02:24:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:24:20 - INFO - __main__ - Printing 3 examples
05/21/2022 02:24:20 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/21/2022 02:24:20 - INFO - __main__ - ['entailed']
05/21/2022 02:24:20 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/21/2022 02:24:20 - INFO - __main__ - ['entailed']
05/21/2022 02:24:20 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/21/2022 02:24:20 - INFO - __main__ - ['entailed']
05/21/2022 02:24:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:24:20 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:24:20 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 02:24:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:24:20 - INFO - __main__ - Printing 3 examples
05/21/2022 02:24:20 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/21/2022 02:24:20 - INFO - __main__ - ['entailed']
05/21/2022 02:24:20 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/21/2022 02:24:20 - INFO - __main__ - ['entailed']
05/21/2022 02:24:20 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/21/2022 02:24:20 - INFO - __main__ - ['entailed']
05/21/2022 02:24:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:24:20 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:24:20 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 02:24:26 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 02:24:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 02:24:26 - INFO - __main__ - Starting training!
05/21/2022 02:24:28 - INFO - __main__ - Step 10 Global step 10 Train loss 5.10 on epoch=4
05/21/2022 02:24:30 - INFO - __main__ - Step 20 Global step 20 Train loss 4.90 on epoch=9
05/21/2022 02:24:32 - INFO - __main__ - Step 30 Global step 30 Train loss 4.87 on epoch=14
05/21/2022 02:24:34 - INFO - __main__ - Step 40 Global step 40 Train loss 4.81 on epoch=19
05/21/2022 02:24:36 - INFO - __main__ - Step 50 Global step 50 Train loss 4.67 on epoch=24
05/21/2022 02:24:42 - INFO - __main__ - Global step 50 Train loss 4.87 Classification-F1 0.0 on epoch=24
05/21/2022 02:24:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 02:24:44 - INFO - __main__ - Step 60 Global step 60 Train loss 4.56 on epoch=29
05/21/2022 02:24:46 - INFO - __main__ - Step 70 Global step 70 Train loss 4.45 on epoch=34
05/21/2022 02:24:48 - INFO - __main__ - Step 80 Global step 80 Train loss 4.29 on epoch=39
05/21/2022 02:24:50 - INFO - __main__ - Step 90 Global step 90 Train loss 4.23 on epoch=44
05/21/2022 02:24:52 - INFO - __main__ - Step 100 Global step 100 Train loss 4.07 on epoch=49
05/21/2022 02:24:58 - INFO - __main__ - Global step 100 Train loss 4.32 Classification-F1 0.0 on epoch=49
05/21/2022 02:25:00 - INFO - __main__ - Step 110 Global step 110 Train loss 3.94 on epoch=54
05/21/2022 02:25:02 - INFO - __main__ - Step 120 Global step 120 Train loss 3.75 on epoch=59
05/21/2022 02:25:04 - INFO - __main__ - Step 130 Global step 130 Train loss 3.72 on epoch=64
05/21/2022 02:25:06 - INFO - __main__ - Step 140 Global step 140 Train loss 3.47 on epoch=69
05/21/2022 02:25:08 - INFO - __main__ - Step 150 Global step 150 Train loss 3.33 on epoch=74
05/21/2022 02:25:12 - INFO - __main__ - Global step 150 Train loss 3.64 Classification-F1 0.3333333333333333 on epoch=74
05/21/2022 02:25:12 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=74, global_step=150
05/21/2022 02:25:14 - INFO - __main__ - Step 160 Global step 160 Train loss 3.34 on epoch=79
05/21/2022 02:25:16 - INFO - __main__ - Step 170 Global step 170 Train loss 3.11 on epoch=84
05/21/2022 02:25:17 - INFO - __main__ - Step 180 Global step 180 Train loss 2.85 on epoch=89
05/21/2022 02:25:19 - INFO - __main__ - Step 190 Global step 190 Train loss 2.89 on epoch=94
05/21/2022 02:25:21 - INFO - __main__ - Step 200 Global step 200 Train loss 2.64 on epoch=99
05/21/2022 02:25:25 - INFO - __main__ - Global step 200 Train loss 2.97 Classification-F1 0.3333333333333333 on epoch=99
05/21/2022 02:25:27 - INFO - __main__ - Step 210 Global step 210 Train loss 2.48 on epoch=104
05/21/2022 02:25:29 - INFO - __main__ - Step 220 Global step 220 Train loss 2.30 on epoch=109
05/21/2022 02:25:30 - INFO - __main__ - Step 230 Global step 230 Train loss 2.37 on epoch=114
05/21/2022 02:25:32 - INFO - __main__ - Step 240 Global step 240 Train loss 2.18 on epoch=119
05/21/2022 02:25:34 - INFO - __main__ - Step 250 Global step 250 Train loss 2.12 on epoch=124
05/21/2022 02:25:38 - INFO - __main__ - Global step 250 Train loss 2.29 Classification-F1 0.3333333333333333 on epoch=124
05/21/2022 02:25:40 - INFO - __main__ - Step 260 Global step 260 Train loss 2.00 on epoch=129
05/21/2022 02:25:42 - INFO - __main__ - Step 270 Global step 270 Train loss 1.89 on epoch=134
05/21/2022 02:25:44 - INFO - __main__ - Step 280 Global step 280 Train loss 1.66 on epoch=139
05/21/2022 02:25:46 - INFO - __main__ - Step 290 Global step 290 Train loss 1.62 on epoch=144
05/21/2022 02:25:48 - INFO - __main__ - Step 300 Global step 300 Train loss 1.49 on epoch=149
05/21/2022 02:25:53 - INFO - __main__ - Global step 300 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=149
05/21/2022 02:25:55 - INFO - __main__ - Step 310 Global step 310 Train loss 1.41 on epoch=154
05/21/2022 02:25:57 - INFO - __main__ - Step 320 Global step 320 Train loss 1.34 on epoch=159
05/21/2022 02:25:59 - INFO - __main__ - Step 330 Global step 330 Train loss 1.17 on epoch=164
05/21/2022 02:26:01 - INFO - __main__ - Step 340 Global step 340 Train loss 1.19 on epoch=169
05/21/2022 02:26:03 - INFO - __main__ - Step 350 Global step 350 Train loss 1.17 on epoch=174
05/21/2022 02:26:04 - INFO - __main__ - Global step 350 Train loss 1.26 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 02:26:06 - INFO - __main__ - Step 360 Global step 360 Train loss 1.23 on epoch=179
05/21/2022 02:26:08 - INFO - __main__ - Step 370 Global step 370 Train loss 1.04 on epoch=184
05/21/2022 02:26:10 - INFO - __main__ - Step 380 Global step 380 Train loss 1.10 on epoch=189
05/21/2022 02:26:12 - INFO - __main__ - Step 390 Global step 390 Train loss 1.01 on epoch=194
05/21/2022 02:26:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.95 on epoch=199
05/21/2022 02:26:15 - INFO - __main__ - Global step 400 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 02:26:17 - INFO - __main__ - Step 410 Global step 410 Train loss 1.10 on epoch=204
05/21/2022 02:26:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.89 on epoch=209
05/21/2022 02:26:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.91 on epoch=214
05/21/2022 02:26:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.87 on epoch=219
05/21/2022 02:26:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=224
05/21/2022 02:26:25 - INFO - __main__ - Global step 450 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 02:26:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.92 on epoch=229
05/21/2022 02:26:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.89 on epoch=234
05/21/2022 02:26:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=239
05/21/2022 02:26:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.68 on epoch=244
05/21/2022 02:26:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.74 on epoch=249
05/21/2022 02:26:35 - INFO - __main__ - Global step 500 Train loss 0.79 Classification-F1 0.3191489361702127 on epoch=249
05/21/2022 02:26:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.64 on epoch=254
05/21/2022 02:26:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.71 on epoch=259
05/21/2022 02:26:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.74 on epoch=264
05/21/2022 02:26:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.61 on epoch=269
05/21/2022 02:26:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.61 on epoch=274
05/21/2022 02:26:46 - INFO - __main__ - Global step 550 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 02:26:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=279
05/21/2022 02:26:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=284
05/21/2022 02:26:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=289
05/21/2022 02:26:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=294
05/21/2022 02:26:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=299
05/21/2022 02:26:56 - INFO - __main__ - Global step 600 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 02:26:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.62 on epoch=304
05/21/2022 02:27:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.55 on epoch=309
05/21/2022 02:27:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=314
05/21/2022 02:27:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=319
05/21/2022 02:27:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=324
05/21/2022 02:27:07 - INFO - __main__ - Global step 650 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 02:27:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=329
05/21/2022 02:27:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.56 on epoch=334
05/21/2022 02:27:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=339
05/21/2022 02:27:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=344
05/21/2022 02:27:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=349
05/21/2022 02:27:17 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.4458874458874459 on epoch=349
05/21/2022 02:27:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4458874458874459 on epoch=349, global_step=700
05/21/2022 02:27:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=354
05/21/2022 02:27:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=359
05/21/2022 02:27:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=364
05/21/2022 02:27:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=369
05/21/2022 02:27:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.51 on epoch=374
05/21/2022 02:27:27 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.4181818181818182 on epoch=374
05/21/2022 02:27:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=379
05/21/2022 02:27:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=384
05/21/2022 02:27:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=389
05/21/2022 02:27:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=394
05/21/2022 02:27:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=399
05/21/2022 02:27:37 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.3816425120772947 on epoch=399
05/21/2022 02:27:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=404
05/21/2022 02:27:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=409
05/21/2022 02:27:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=414
05/21/2022 02:27:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=419
05/21/2022 02:27:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=424
05/21/2022 02:27:47 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 02:27:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=429
05/21/2022 02:27:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=434
05/21/2022 02:27:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=439
05/21/2022 02:27:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=444
05/21/2022 02:27:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=449
05/21/2022 02:27:58 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 02:28:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=454
05/21/2022 02:28:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=459
05/21/2022 02:28:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=464
05/21/2022 02:28:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=469
05/21/2022 02:28:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=474
05/21/2022 02:28:08 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 02:28:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=479
05/21/2022 02:28:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=484
05/21/2022 02:28:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=489
05/21/2022 02:28:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=494
05/21/2022 02:28:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
05/21/2022 02:28:18 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 02:28:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=504
05/21/2022 02:28:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=509
05/21/2022 02:28:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=514
05/21/2022 02:28:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=519
05/21/2022 02:28:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=524
05/21/2022 02:28:28 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.3191489361702127 on epoch=524
05/21/2022 02:28:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=529
05/21/2022 02:28:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=534
05/21/2022 02:28:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=539
05/21/2022 02:28:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=544
05/21/2022 02:28:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=549
05/21/2022 02:28:38 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.3992490613266583 on epoch=549
05/21/2022 02:28:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=554
05/21/2022 02:28:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=559
05/21/2022 02:28:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=564
05/21/2022 02:28:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=569
05/21/2022 02:28:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=574
05/21/2022 02:28:48 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 02:28:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=579
05/21/2022 02:28:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=584
05/21/2022 02:28:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
05/21/2022 02:28:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=594
05/21/2022 02:28:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=599
05/21/2022 02:28:58 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 02:29:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
05/21/2022 02:29:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=609
05/21/2022 02:29:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=614
05/21/2022 02:29:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=619
05/21/2022 02:29:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=624
05/21/2022 02:29:08 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 02:29:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=629
05/21/2022 02:29:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=634
05/21/2022 02:29:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=639
05/21/2022 02:29:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=644
05/21/2022 02:29:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=649
05/21/2022 02:29:18 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 02:29:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=654
05/21/2022 02:29:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=659
05/21/2022 02:29:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=664
05/21/2022 02:29:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.28 on epoch=669
05/21/2022 02:29:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=674
05/21/2022 02:29:28 - INFO - __main__ - Global step 1350 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 02:29:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=679
05/21/2022 02:29:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=684
05/21/2022 02:29:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=689
05/21/2022 02:29:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.32 on epoch=694
05/21/2022 02:29:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=699
05/21/2022 02:29:38 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 02:29:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=704
05/21/2022 02:29:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=709
05/21/2022 02:29:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=714
05/21/2022 02:29:46 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.26 on epoch=719
05/21/2022 02:29:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=724
05/21/2022 02:29:49 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 02:29:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
05/21/2022 02:29:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=734
05/21/2022 02:29:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=739
05/21/2022 02:29:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=744
05/21/2022 02:29:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=749
05/21/2022 02:29:59 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 02:30:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=754
05/21/2022 02:30:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=759
05/21/2022 02:30:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=764
05/21/2022 02:30:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=769
05/21/2022 02:30:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=774
05/21/2022 02:30:09 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 02:30:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=779
05/21/2022 02:30:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=784
05/21/2022 02:30:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=789
05/21/2022 02:30:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=794
05/21/2022 02:30:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=799
05/21/2022 02:30:19 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 02:30:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=804
05/21/2022 02:30:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=809
05/21/2022 02:30:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=814
05/21/2022 02:30:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=819
05/21/2022 02:30:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=824
05/21/2022 02:30:29 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 02:30:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=829
05/21/2022 02:30:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=834
05/21/2022 02:30:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.29 on epoch=839
05/21/2022 02:30:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=844
05/21/2022 02:30:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.27 on epoch=849
05/21/2022 02:30:39 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 02:30:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=854
05/21/2022 02:30:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=859
05/21/2022 02:30:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/21/2022 02:30:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=869
05/21/2022 02:30:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
05/21/2022 02:30:49 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 02:30:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.31 on epoch=879
05/21/2022 02:30:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=884
05/21/2022 02:30:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
05/21/2022 02:30:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.30 on epoch=894
05/21/2022 02:30:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=899
05/21/2022 02:30:59 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 02:31:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=904
05/21/2022 02:31:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=909
05/21/2022 02:31:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=914
05/21/2022 02:31:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.29 on epoch=919
05/21/2022 02:31:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
05/21/2022 02:31:09 - INFO - __main__ - Global step 1850 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 02:31:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.31 on epoch=929
05/21/2022 02:31:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=934
05/21/2022 02:31:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=939
05/21/2022 02:31:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=944
05/21/2022 02:31:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.29 on epoch=949
05/21/2022 02:31:19 - INFO - __main__ - Global step 1900 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 02:31:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=954
05/21/2022 02:31:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=959
05/21/2022 02:31:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=964
05/21/2022 02:31:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=969
05/21/2022 02:31:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=974
05/21/2022 02:31:29 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 02:31:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=979
05/21/2022 02:31:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=984
05/21/2022 02:31:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/21/2022 02:31:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=994
05/21/2022 02:31:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=999
05/21/2022 02:31:40 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 02:31:40 - INFO - __main__ - save last model!
05/21/2022 02:31:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 02:31:40 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 02:31:40 - INFO - __main__ - Printing 3 examples
05/21/2022 02:31:40 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:31:40 - INFO - __main__ - ['entailed']
05/21/2022 02:31:40 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:31:40 - INFO - __main__ - ['entailed']
05/21/2022 02:31:40 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:31:40 - INFO - __main__ - ['entailed']
05/21/2022 02:31:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:31:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:31:40 - INFO - __main__ - Printing 3 examples
05/21/2022 02:31:40 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/21/2022 02:31:40 - INFO - __main__ - ['entailed']
05/21/2022 02:31:40 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/21/2022 02:31:40 - INFO - __main__ - ['entailed']
05/21/2022 02:31:40 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/21/2022 02:31:40 - INFO - __main__ - ['entailed']
05/21/2022 02:31:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:31:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:31:40 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 02:31:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:31:40 - INFO - __main__ - Printing 3 examples
05/21/2022 02:31:40 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/21/2022 02:31:40 - INFO - __main__ - ['entailed']
05/21/2022 02:31:40 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/21/2022 02:31:40 - INFO - __main__ - ['entailed']
05/21/2022 02:31:40 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/21/2022 02:31:40 - INFO - __main__ - ['entailed']
05/21/2022 02:31:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:31:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:31:40 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 02:31:45 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 02:31:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 02:31:46 - INFO - __main__ - Starting training!
05/21/2022 02:32:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:32:16 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 02:36:21 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.5_8_predictions.txt
05/21/2022 02:36:21 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 02:36:21 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.5, bsz=8, dev_performance=0.4458874458874459, test_performance=0.33047210300429186
05/21/2022 02:36:21 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.4, bsz=8 ...
05/21/2022 02:36:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:36:22 - INFO - __main__ - Printing 3 examples
05/21/2022 02:36:22 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/21/2022 02:36:22 - INFO - __main__ - ['entailed']
05/21/2022 02:36:22 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/21/2022 02:36:22 - INFO - __main__ - ['entailed']
05/21/2022 02:36:22 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/21/2022 02:36:22 - INFO - __main__ - ['entailed']
05/21/2022 02:36:22 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:36:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:36:22 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 02:36:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:36:22 - INFO - __main__ - Printing 3 examples
05/21/2022 02:36:22 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/21/2022 02:36:22 - INFO - __main__ - ['entailed']
05/21/2022 02:36:22 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/21/2022 02:36:22 - INFO - __main__ - ['entailed']
05/21/2022 02:36:22 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/21/2022 02:36:22 - INFO - __main__ - ['entailed']
05/21/2022 02:36:22 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:36:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:36:22 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 02:36:28 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 02:36:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 02:36:28 - INFO - __main__ - Starting training!
05/21/2022 02:36:30 - INFO - __main__ - Step 10 Global step 10 Train loss 4.97 on epoch=4
05/21/2022 02:36:32 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/21/2022 02:36:34 - INFO - __main__ - Step 30 Global step 30 Train loss 4.80 on epoch=14
05/21/2022 02:36:36 - INFO - __main__ - Step 40 Global step 40 Train loss 4.77 on epoch=19
05/21/2022 02:36:38 - INFO - __main__ - Step 50 Global step 50 Train loss 4.72 on epoch=24
05/21/2022 02:36:39 - INFO - __main__ - Global step 50 Train loss 4.86 Classification-F1 0.0 on epoch=24
05/21/2022 02:36:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 02:36:41 - INFO - __main__ - Step 60 Global step 60 Train loss 4.71 on epoch=29
05/21/2022 02:36:43 - INFO - __main__ - Step 70 Global step 70 Train loss 4.70 on epoch=34
05/21/2022 02:36:44 - INFO - __main__ - Step 80 Global step 80 Train loss 4.60 on epoch=39
05/21/2022 02:36:46 - INFO - __main__ - Step 90 Global step 90 Train loss 4.56 on epoch=44
05/21/2022 02:36:48 - INFO - __main__ - Step 100 Global step 100 Train loss 4.38 on epoch=49
05/21/2022 02:36:49 - INFO - __main__ - Global step 100 Train loss 4.59 Classification-F1 0.0 on epoch=49
05/21/2022 02:36:51 - INFO - __main__ - Step 110 Global step 110 Train loss 4.34 on epoch=54
05/21/2022 02:36:53 - INFO - __main__ - Step 120 Global step 120 Train loss 4.35 on epoch=59
05/21/2022 02:36:55 - INFO - __main__ - Step 130 Global step 130 Train loss 4.22 on epoch=64
05/21/2022 02:36:57 - INFO - __main__ - Step 140 Global step 140 Train loss 4.29 on epoch=69
05/21/2022 02:36:59 - INFO - __main__ - Step 150 Global step 150 Train loss 4.15 on epoch=74
05/21/2022 02:37:00 - INFO - __main__ - Global step 150 Train loss 4.27 Classification-F1 0.0 on epoch=74
05/21/2022 02:37:02 - INFO - __main__ - Step 160 Global step 160 Train loss 4.11 on epoch=79
05/21/2022 02:37:04 - INFO - __main__ - Step 170 Global step 170 Train loss 3.98 on epoch=84
05/21/2022 02:37:06 - INFO - __main__ - Step 180 Global step 180 Train loss 3.89 on epoch=89
05/21/2022 02:37:08 - INFO - __main__ - Step 190 Global step 190 Train loss 3.76 on epoch=94
05/21/2022 02:37:10 - INFO - __main__ - Step 200 Global step 200 Train loss 3.77 on epoch=99
05/21/2022 02:37:16 - INFO - __main__ - Global step 200 Train loss 3.90 Classification-F1 0.0058823529411764705 on epoch=99
05/21/2022 02:37:16 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0058823529411764705 on epoch=99, global_step=200
05/21/2022 02:37:18 - INFO - __main__ - Step 210 Global step 210 Train loss 3.70 on epoch=104
05/21/2022 02:37:20 - INFO - __main__ - Step 220 Global step 220 Train loss 3.61 on epoch=109
05/21/2022 02:37:22 - INFO - __main__ - Step 230 Global step 230 Train loss 3.59 on epoch=114
05/21/2022 02:37:24 - INFO - __main__ - Step 240 Global step 240 Train loss 3.50 on epoch=119
05/21/2022 02:37:26 - INFO - __main__ - Step 250 Global step 250 Train loss 3.44 on epoch=124
05/21/2022 02:37:33 - INFO - __main__ - Global step 250 Train loss 3.57 Classification-F1 0.3333333333333333 on epoch=124
05/21/2022 02:37:33 - INFO - __main__ - Saving model with best Classification-F1: 0.0058823529411764705 -> 0.3333333333333333 on epoch=124, global_step=250
05/21/2022 02:37:35 - INFO - __main__ - Step 260 Global step 260 Train loss 3.33 on epoch=129
05/21/2022 02:37:36 - INFO - __main__ - Step 270 Global step 270 Train loss 3.18 on epoch=134
05/21/2022 02:37:38 - INFO - __main__ - Step 280 Global step 280 Train loss 3.07 on epoch=139
05/21/2022 02:37:40 - INFO - __main__ - Step 290 Global step 290 Train loss 2.91 on epoch=144
05/21/2022 02:37:42 - INFO - __main__ - Step 300 Global step 300 Train loss 2.78 on epoch=149
05/21/2022 02:37:46 - INFO - __main__ - Global step 300 Train loss 3.05 Classification-F1 0.3333333333333333 on epoch=149
05/21/2022 02:37:47 - INFO - __main__ - Step 310 Global step 310 Train loss 2.87 on epoch=154
05/21/2022 02:37:49 - INFO - __main__ - Step 320 Global step 320 Train loss 2.75 on epoch=159
05/21/2022 02:37:51 - INFO - __main__ - Step 330 Global step 330 Train loss 2.62 on epoch=164
05/21/2022 02:37:53 - INFO - __main__ - Step 340 Global step 340 Train loss 2.57 on epoch=169
05/21/2022 02:37:55 - INFO - __main__ - Step 350 Global step 350 Train loss 2.50 on epoch=174
05/21/2022 02:37:59 - INFO - __main__ - Global step 350 Train loss 2.66 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 02:38:00 - INFO - __main__ - Step 360 Global step 360 Train loss 2.47 on epoch=179
05/21/2022 02:38:02 - INFO - __main__ - Step 370 Global step 370 Train loss 2.33 on epoch=184
05/21/2022 02:38:04 - INFO - __main__ - Step 380 Global step 380 Train loss 2.28 on epoch=189
05/21/2022 02:38:06 - INFO - __main__ - Step 390 Global step 390 Train loss 2.15 on epoch=194
05/21/2022 02:38:08 - INFO - __main__ - Step 400 Global step 400 Train loss 2.11 on epoch=199
05/21/2022 02:38:11 - INFO - __main__ - Global step 400 Train loss 2.27 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 02:38:13 - INFO - __main__ - Step 410 Global step 410 Train loss 2.04 on epoch=204
05/21/2022 02:38:15 - INFO - __main__ - Step 420 Global step 420 Train loss 2.01 on epoch=209
05/21/2022 02:38:17 - INFO - __main__ - Step 430 Global step 430 Train loss 1.99 on epoch=214
05/21/2022 02:38:19 - INFO - __main__ - Step 440 Global step 440 Train loss 1.84 on epoch=219
05/21/2022 02:38:21 - INFO - __main__ - Step 450 Global step 450 Train loss 1.71 on epoch=224
05/21/2022 02:38:24 - INFO - __main__ - Global step 450 Train loss 1.91 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 02:38:26 - INFO - __main__ - Step 460 Global step 460 Train loss 1.78 on epoch=229
05/21/2022 02:38:28 - INFO - __main__ - Step 470 Global step 470 Train loss 1.74 on epoch=234
05/21/2022 02:38:30 - INFO - __main__ - Step 480 Global step 480 Train loss 1.74 on epoch=239
05/21/2022 02:38:32 - INFO - __main__ - Step 490 Global step 490 Train loss 1.69 on epoch=244
05/21/2022 02:38:34 - INFO - __main__ - Step 500 Global step 500 Train loss 1.71 on epoch=249
05/21/2022 02:38:41 - INFO - __main__ - Global step 500 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 02:38:43 - INFO - __main__ - Step 510 Global step 510 Train loss 1.66 on epoch=254
05/21/2022 02:38:44 - INFO - __main__ - Step 520 Global step 520 Train loss 1.72 on epoch=259
05/21/2022 02:38:46 - INFO - __main__ - Step 530 Global step 530 Train loss 1.55 on epoch=264
05/21/2022 02:38:48 - INFO - __main__ - Step 540 Global step 540 Train loss 1.54 on epoch=269
05/21/2022 02:38:50 - INFO - __main__ - Step 550 Global step 550 Train loss 1.51 on epoch=274
05/21/2022 02:38:53 - INFO - __main__ - Global step 550 Train loss 1.59 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 02:38:55 - INFO - __main__ - Step 560 Global step 560 Train loss 1.41 on epoch=279
05/21/2022 02:38:57 - INFO - __main__ - Step 570 Global step 570 Train loss 1.49 on epoch=284
05/21/2022 02:38:59 - INFO - __main__ - Step 580 Global step 580 Train loss 1.39 on epoch=289
05/21/2022 02:39:01 - INFO - __main__ - Step 590 Global step 590 Train loss 1.35 on epoch=294
05/21/2022 02:39:02 - INFO - __main__ - Step 600 Global step 600 Train loss 1.29 on epoch=299
05/21/2022 02:39:05 - INFO - __main__ - Global step 600 Train loss 1.39 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 02:39:07 - INFO - __main__ - Step 610 Global step 610 Train loss 1.25 on epoch=304
05/21/2022 02:39:09 - INFO - __main__ - Step 620 Global step 620 Train loss 1.27 on epoch=309
05/21/2022 02:39:11 - INFO - __main__ - Step 630 Global step 630 Train loss 1.24 on epoch=314
05/21/2022 02:39:13 - INFO - __main__ - Step 640 Global step 640 Train loss 1.31 on epoch=319
05/21/2022 02:39:15 - INFO - __main__ - Step 650 Global step 650 Train loss 1.17 on epoch=324
05/21/2022 02:39:16 - INFO - __main__ - Global step 650 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 02:39:17 - INFO - __main__ - Step 660 Global step 660 Train loss 1.34 on epoch=329
05/21/2022 02:39:19 - INFO - __main__ - Step 670 Global step 670 Train loss 1.17 on epoch=334
05/21/2022 02:39:21 - INFO - __main__ - Step 680 Global step 680 Train loss 1.12 on epoch=339
05/21/2022 02:39:23 - INFO - __main__ - Step 690 Global step 690 Train loss 1.13 on epoch=344
05/21/2022 02:39:25 - INFO - __main__ - Step 700 Global step 700 Train loss 1.01 on epoch=349
05/21/2022 02:39:26 - INFO - __main__ - Global step 700 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 02:39:28 - INFO - __main__ - Step 710 Global step 710 Train loss 1.22 on epoch=354
05/21/2022 02:39:30 - INFO - __main__ - Step 720 Global step 720 Train loss 1.05 on epoch=359
05/21/2022 02:39:32 - INFO - __main__ - Step 730 Global step 730 Train loss 1.01 on epoch=364
05/21/2022 02:39:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.91 on epoch=369
05/21/2022 02:39:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.92 on epoch=374
05/21/2022 02:39:36 - INFO - __main__ - Global step 750 Train loss 1.02 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 02:39:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.93 on epoch=379
05/21/2022 02:39:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.96 on epoch=384
05/21/2022 02:39:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.87 on epoch=389
05/21/2022 02:39:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.86 on epoch=394
05/21/2022 02:39:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.80 on epoch=399
05/21/2022 02:39:47 - INFO - __main__ - Global step 800 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 02:39:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.86 on epoch=404
05/21/2022 02:39:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.84 on epoch=409
05/21/2022 02:39:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=414
05/21/2022 02:39:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.83 on epoch=419
05/21/2022 02:39:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.77 on epoch=424
05/21/2022 02:39:57 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 02:39:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.71 on epoch=429
05/21/2022 02:40:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.69 on epoch=434
05/21/2022 02:40:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.77 on epoch=439
05/21/2022 02:40:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.77 on epoch=444
05/21/2022 02:40:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.67 on epoch=449
05/21/2022 02:40:08 - INFO - __main__ - Global step 900 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 02:40:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.73 on epoch=454
05/21/2022 02:40:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.72 on epoch=459
05/21/2022 02:40:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.71 on epoch=464
05/21/2022 02:40:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.65 on epoch=469
05/21/2022 02:40:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.65 on epoch=474
05/21/2022 02:40:19 - INFO - __main__ - Global step 950 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 02:40:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.58 on epoch=479
05/21/2022 02:40:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.62 on epoch=484
05/21/2022 02:40:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.65 on epoch=489
05/21/2022 02:40:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.65 on epoch=494
05/21/2022 02:40:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.66 on epoch=499
05/21/2022 02:40:29 - INFO - __main__ - Global step 1000 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 02:40:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.57 on epoch=504
05/21/2022 02:40:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.68 on epoch=509
05/21/2022 02:40:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.56 on epoch=514
05/21/2022 02:40:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.59 on epoch=519
05/21/2022 02:40:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.58 on epoch=524
05/21/2022 02:40:40 - INFO - __main__ - Global step 1050 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 02:40:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.50 on epoch=529
05/21/2022 02:40:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.55 on epoch=534
05/21/2022 02:40:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.55 on epoch=539
05/21/2022 02:40:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.60 on epoch=544
05/21/2022 02:40:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=549
05/21/2022 02:40:50 - INFO - __main__ - Global step 1100 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 02:40:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.53 on epoch=554
05/21/2022 02:40:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.52 on epoch=559
05/21/2022 02:40:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.54 on epoch=564
05/21/2022 02:40:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.55 on epoch=569
05/21/2022 02:40:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.58 on epoch=574
05/21/2022 02:41:00 - INFO - __main__ - Global step 1150 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 02:41:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.53 on epoch=579
05/21/2022 02:41:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.54 on epoch=584
05/21/2022 02:41:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=589
05/21/2022 02:41:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.53 on epoch=594
05/21/2022 02:41:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=599
05/21/2022 02:41:11 - INFO - __main__ - Global step 1200 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 02:41:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=604
05/21/2022 02:41:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=609
05/21/2022 02:41:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=614
05/21/2022 02:41:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.51 on epoch=619
05/21/2022 02:41:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.54 on epoch=624
05/21/2022 02:41:21 - INFO - __main__ - Global step 1250 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 02:41:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=629
05/21/2022 02:41:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=634
05/21/2022 02:41:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=639
05/21/2022 02:41:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=644
05/21/2022 02:41:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=649
05/21/2022 02:41:31 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 02:41:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=654
05/21/2022 02:41:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
05/21/2022 02:41:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=664
05/21/2022 02:41:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=669
05/21/2022 02:41:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.47 on epoch=674
05/21/2022 02:41:42 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 02:41:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=679
05/21/2022 02:41:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
05/21/2022 02:41:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=689
05/21/2022 02:41:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=694
05/21/2022 02:41:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=699
05/21/2022 02:41:52 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 02:41:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=704
05/21/2022 02:41:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
05/21/2022 02:41:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
05/21/2022 02:41:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=719
05/21/2022 02:42:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=724
05/21/2022 02:42:02 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 02:42:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=729
05/21/2022 02:42:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
05/21/2022 02:42:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=739
05/21/2022 02:42:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
05/21/2022 02:42:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.50 on epoch=749
05/21/2022 02:42:12 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 02:42:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=754
05/21/2022 02:42:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=759
05/21/2022 02:42:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=764
05/21/2022 02:42:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=769
05/21/2022 02:42:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=774
05/21/2022 02:42:22 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 02:42:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=779
05/21/2022 02:42:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=784
05/21/2022 02:42:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=789
05/21/2022 02:42:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=794
05/21/2022 02:42:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
05/21/2022 02:42:32 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 02:42:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=804
05/21/2022 02:42:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=809
05/21/2022 02:42:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=814
05/21/2022 02:42:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
05/21/2022 02:42:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=824
05/21/2022 02:42:43 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 02:42:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=829
05/21/2022 02:42:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=834
05/21/2022 02:42:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
05/21/2022 02:42:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
05/21/2022 02:42:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=849
05/21/2022 02:42:53 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 02:42:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=854
05/21/2022 02:42:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=859
05/21/2022 02:42:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/21/2022 02:43:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=869
05/21/2022 02:43:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=874
05/21/2022 02:43:03 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 02:43:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=879
05/21/2022 02:43:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
05/21/2022 02:43:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=889
05/21/2022 02:43:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
05/21/2022 02:43:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=899
05/21/2022 02:43:13 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 02:43:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=904
05/21/2022 02:43:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=909
05/21/2022 02:43:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=914
05/21/2022 02:43:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=919
05/21/2022 02:43:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=924
05/21/2022 02:43:23 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 02:43:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=929
05/21/2022 02:43:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
05/21/2022 02:43:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=939
05/21/2022 02:43:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
05/21/2022 02:43:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/21/2022 02:43:33 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 02:43:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=954
05/21/2022 02:43:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
05/21/2022 02:43:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=964
05/21/2022 02:43:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=969
05/21/2022 02:43:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/21/2022 02:43:44 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 02:43:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=979
05/21/2022 02:43:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/21/2022 02:43:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/21/2022 02:43:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.34 on epoch=994
05/21/2022 02:43:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=999
05/21/2022 02:43:54 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 02:43:54 - INFO - __main__ - save last model!
05/21/2022 02:43:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 02:43:54 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 02:43:54 - INFO - __main__ - Printing 3 examples
05/21/2022 02:43:54 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:43:54 - INFO - __main__ - ['entailed']
05/21/2022 02:43:54 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:43:54 - INFO - __main__ - ['entailed']
05/21/2022 02:43:54 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:43:54 - INFO - __main__ - ['entailed']
05/21/2022 02:43:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:43:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:43:54 - INFO - __main__ - Printing 3 examples
05/21/2022 02:43:54 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/21/2022 02:43:54 - INFO - __main__ - ['entailed']
05/21/2022 02:43:54 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/21/2022 02:43:54 - INFO - __main__ - ['entailed']
05/21/2022 02:43:54 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/21/2022 02:43:54 - INFO - __main__ - ['entailed']
05/21/2022 02:43:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:43:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:43:54 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 02:43:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:43:54 - INFO - __main__ - Printing 3 examples
05/21/2022 02:43:54 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/21/2022 02:43:54 - INFO - __main__ - ['entailed']
05/21/2022 02:43:54 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/21/2022 02:43:54 - INFO - __main__ - ['entailed']
05/21/2022 02:43:54 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/21/2022 02:43:54 - INFO - __main__ - ['entailed']
05/21/2022 02:43:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:43:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:43:54 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 02:44:00 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 02:44:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 02:44:00 - INFO - __main__ - Starting training!
05/21/2022 02:44:18 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:44:30 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 02:48:41 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.4_8_predictions.txt
05/21/2022 02:48:41 - INFO - __main__ - Classification-F1 on test data: 0.3306
05/21/2022 02:48:41 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.3306437454867474
05/21/2022 02:48:41 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.3, bsz=8 ...
05/21/2022 02:48:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:48:42 - INFO - __main__ - Printing 3 examples
05/21/2022 02:48:42 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/21/2022 02:48:42 - INFO - __main__ - ['entailed']
05/21/2022 02:48:42 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/21/2022 02:48:42 - INFO - __main__ - ['entailed']
05/21/2022 02:48:42 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/21/2022 02:48:42 - INFO - __main__ - ['entailed']
05/21/2022 02:48:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:48:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:48:42 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 02:48:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:48:42 - INFO - __main__ - Printing 3 examples
05/21/2022 02:48:42 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/21/2022 02:48:42 - INFO - __main__ - ['entailed']
05/21/2022 02:48:42 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/21/2022 02:48:42 - INFO - __main__ - ['entailed']
05/21/2022 02:48:42 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/21/2022 02:48:42 - INFO - __main__ - ['entailed']
05/21/2022 02:48:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:48:43 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:48:43 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 02:48:48 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 02:48:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 02:48:48 - INFO - __main__ - Starting training!
05/21/2022 02:48:51 - INFO - __main__ - Step 10 Global step 10 Train loss 5.01 on epoch=4
05/21/2022 02:48:53 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/21/2022 02:48:55 - INFO - __main__ - Step 30 Global step 30 Train loss 4.96 on epoch=14
05/21/2022 02:48:57 - INFO - __main__ - Step 40 Global step 40 Train loss 4.83 on epoch=19
05/21/2022 02:48:59 - INFO - __main__ - Step 50 Global step 50 Train loss 4.86 on epoch=24
05/21/2022 02:49:00 - INFO - __main__ - Global step 50 Train loss 4.93 Classification-F1 0.0 on epoch=24
05/21/2022 02:49:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 02:49:02 - INFO - __main__ - Step 60 Global step 60 Train loss 4.69 on epoch=29
05/21/2022 02:49:03 - INFO - __main__ - Step 70 Global step 70 Train loss 4.51 on epoch=34
05/21/2022 02:49:05 - INFO - __main__ - Step 80 Global step 80 Train loss 4.46 on epoch=39
05/21/2022 02:49:07 - INFO - __main__ - Step 90 Global step 90 Train loss 4.33 on epoch=44
05/21/2022 02:49:09 - INFO - __main__ - Step 100 Global step 100 Train loss 4.19 on epoch=49
05/21/2022 02:49:10 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/21/2022 02:49:12 - INFO - __main__ - Step 110 Global step 110 Train loss 4.12 on epoch=54
05/21/2022 02:49:14 - INFO - __main__ - Step 120 Global step 120 Train loss 3.96 on epoch=59
05/21/2022 02:49:16 - INFO - __main__ - Step 130 Global step 130 Train loss 3.95 on epoch=64
05/21/2022 02:49:18 - INFO - __main__ - Step 140 Global step 140 Train loss 3.91 on epoch=69
05/21/2022 02:49:20 - INFO - __main__ - Step 150 Global step 150 Train loss 3.81 on epoch=74
05/21/2022 02:49:21 - INFO - __main__ - Global step 150 Train loss 3.95 Classification-F1 0.0 on epoch=74
05/21/2022 02:49:23 - INFO - __main__ - Step 160 Global step 160 Train loss 3.79 on epoch=79
05/21/2022 02:49:25 - INFO - __main__ - Step 170 Global step 170 Train loss 3.65 on epoch=84
05/21/2022 02:49:27 - INFO - __main__ - Step 180 Global step 180 Train loss 3.62 on epoch=89
05/21/2022 02:49:29 - INFO - __main__ - Step 190 Global step 190 Train loss 3.61 on epoch=94
05/21/2022 02:49:31 - INFO - __main__ - Step 200 Global step 200 Train loss 3.52 on epoch=99
05/21/2022 02:49:33 - INFO - __main__ - Global step 200 Train loss 3.64 Classification-F1 0.13 on epoch=99
05/21/2022 02:49:34 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.13 on epoch=99, global_step=200
05/21/2022 02:49:35 - INFO - __main__ - Step 210 Global step 210 Train loss 3.45 on epoch=104
05/21/2022 02:49:37 - INFO - __main__ - Step 220 Global step 220 Train loss 3.31 on epoch=109
05/21/2022 02:49:39 - INFO - __main__ - Step 230 Global step 230 Train loss 3.34 on epoch=114
05/21/2022 02:49:41 - INFO - __main__ - Step 240 Global step 240 Train loss 3.22 on epoch=119
05/21/2022 02:49:43 - INFO - __main__ - Step 250 Global step 250 Train loss 3.17 on epoch=124
05/21/2022 02:49:45 - INFO - __main__ - Global step 250 Train loss 3.30 Classification-F1 0.3333333333333333 on epoch=124
05/21/2022 02:49:45 - INFO - __main__ - Saving model with best Classification-F1: 0.13 -> 0.3333333333333333 on epoch=124, global_step=250
05/21/2022 02:49:47 - INFO - __main__ - Step 260 Global step 260 Train loss 3.02 on epoch=129
05/21/2022 02:49:49 - INFO - __main__ - Step 270 Global step 270 Train loss 3.06 on epoch=134
05/21/2022 02:49:51 - INFO - __main__ - Step 280 Global step 280 Train loss 2.93 on epoch=139
05/21/2022 02:49:53 - INFO - __main__ - Step 290 Global step 290 Train loss 2.82 on epoch=144
05/21/2022 02:49:55 - INFO - __main__ - Step 300 Global step 300 Train loss 2.70 on epoch=149
05/21/2022 02:49:57 - INFO - __main__ - Global step 300 Train loss 2.91 Classification-F1 0.3333333333333333 on epoch=149
05/21/2022 02:49:59 - INFO - __main__ - Step 310 Global step 310 Train loss 2.70 on epoch=154
05/21/2022 02:50:01 - INFO - __main__ - Step 320 Global step 320 Train loss 2.72 on epoch=159
05/21/2022 02:50:03 - INFO - __main__ - Step 330 Global step 330 Train loss 2.53 on epoch=164
05/21/2022 02:50:05 - INFO - __main__ - Step 340 Global step 340 Train loss 2.60 on epoch=169
05/21/2022 02:50:07 - INFO - __main__ - Step 350 Global step 350 Train loss 2.45 on epoch=174
05/21/2022 02:50:09 - INFO - __main__ - Global step 350 Train loss 2.60 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 02:50:11 - INFO - __main__ - Step 360 Global step 360 Train loss 2.36 on epoch=179
05/21/2022 02:50:13 - INFO - __main__ - Step 370 Global step 370 Train loss 2.42 on epoch=184
05/21/2022 02:50:15 - INFO - __main__ - Step 380 Global step 380 Train loss 2.39 on epoch=189
05/21/2022 02:50:17 - INFO - __main__ - Step 390 Global step 390 Train loss 2.32 on epoch=194
05/21/2022 02:50:19 - INFO - __main__ - Step 400 Global step 400 Train loss 2.23 on epoch=199
05/21/2022 02:50:21 - INFO - __main__ - Global step 400 Train loss 2.34 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 02:50:23 - INFO - __main__ - Step 410 Global step 410 Train loss 2.27 on epoch=204
05/21/2022 02:50:25 - INFO - __main__ - Step 420 Global step 420 Train loss 2.29 on epoch=209
05/21/2022 02:50:27 - INFO - __main__ - Step 430 Global step 430 Train loss 2.24 on epoch=214
05/21/2022 02:50:29 - INFO - __main__ - Step 440 Global step 440 Train loss 2.06 on epoch=219
05/21/2022 02:50:31 - INFO - __main__ - Step 450 Global step 450 Train loss 1.93 on epoch=224
05/21/2022 02:50:32 - INFO - __main__ - Global step 450 Train loss 2.16 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 02:50:33 - INFO - __main__ - Step 460 Global step 460 Train loss 2.06 on epoch=229
05/21/2022 02:50:35 - INFO - __main__ - Step 470 Global step 470 Train loss 1.86 on epoch=234
05/21/2022 02:50:37 - INFO - __main__ - Step 480 Global step 480 Train loss 1.87 on epoch=239
05/21/2022 02:50:39 - INFO - __main__ - Step 490 Global step 490 Train loss 1.88 on epoch=244
05/21/2022 02:50:41 - INFO - __main__ - Step 500 Global step 500 Train loss 1.90 on epoch=249
05/21/2022 02:50:42 - INFO - __main__ - Global step 500 Train loss 1.91 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 02:50:44 - INFO - __main__ - Step 510 Global step 510 Train loss 1.76 on epoch=254
05/21/2022 02:50:46 - INFO - __main__ - Step 520 Global step 520 Train loss 1.78 on epoch=259
05/21/2022 02:50:48 - INFO - __main__ - Step 530 Global step 530 Train loss 1.80 on epoch=264
05/21/2022 02:50:50 - INFO - __main__ - Step 540 Global step 540 Train loss 1.58 on epoch=269
05/21/2022 02:50:52 - INFO - __main__ - Step 550 Global step 550 Train loss 1.61 on epoch=274
05/21/2022 02:50:53 - INFO - __main__ - Global step 550 Train loss 1.71 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 02:50:55 - INFO - __main__ - Step 560 Global step 560 Train loss 1.59 on epoch=279
05/21/2022 02:50:57 - INFO - __main__ - Step 570 Global step 570 Train loss 1.56 on epoch=284
05/21/2022 02:50:58 - INFO - __main__ - Step 580 Global step 580 Train loss 1.54 on epoch=289
05/21/2022 02:51:00 - INFO - __main__ - Step 590 Global step 590 Train loss 1.56 on epoch=294
05/21/2022 02:51:02 - INFO - __main__ - Step 600 Global step 600 Train loss 1.59 on epoch=299
05/21/2022 02:51:03 - INFO - __main__ - Global step 600 Train loss 1.57 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 02:51:05 - INFO - __main__ - Step 610 Global step 610 Train loss 1.50 on epoch=304
05/21/2022 02:51:07 - INFO - __main__ - Step 620 Global step 620 Train loss 1.54 on epoch=309
05/21/2022 02:51:09 - INFO - __main__ - Step 630 Global step 630 Train loss 1.44 on epoch=314
05/21/2022 02:51:11 - INFO - __main__ - Step 640 Global step 640 Train loss 1.54 on epoch=319
05/21/2022 02:51:13 - INFO - __main__ - Step 650 Global step 650 Train loss 1.44 on epoch=324
05/21/2022 02:51:14 - INFO - __main__ - Global step 650 Train loss 1.49 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 02:51:16 - INFO - __main__ - Step 660 Global step 660 Train loss 1.31 on epoch=329
05/21/2022 02:51:18 - INFO - __main__ - Step 670 Global step 670 Train loss 1.28 on epoch=334
05/21/2022 02:51:20 - INFO - __main__ - Step 680 Global step 680 Train loss 1.25 on epoch=339
05/21/2022 02:51:22 - INFO - __main__ - Step 690 Global step 690 Train loss 1.20 on epoch=344
05/21/2022 02:51:24 - INFO - __main__ - Step 700 Global step 700 Train loss 1.30 on epoch=349
05/21/2022 02:51:25 - INFO - __main__ - Global step 700 Train loss 1.27 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 02:51:27 - INFO - __main__ - Step 710 Global step 710 Train loss 1.10 on epoch=354
05/21/2022 02:51:29 - INFO - __main__ - Step 720 Global step 720 Train loss 1.16 on epoch=359
05/21/2022 02:51:31 - INFO - __main__ - Step 730 Global step 730 Train loss 1.15 on epoch=364
05/21/2022 02:51:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.99 on epoch=369
05/21/2022 02:51:35 - INFO - __main__ - Step 750 Global step 750 Train loss 1.04 on epoch=374
05/21/2022 02:51:37 - INFO - __main__ - Global step 750 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 02:51:39 - INFO - __main__ - Step 760 Global step 760 Train loss 1.12 on epoch=379
05/21/2022 02:51:41 - INFO - __main__ - Step 770 Global step 770 Train loss 1.14 on epoch=384
05/21/2022 02:51:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.93 on epoch=389
05/21/2022 02:51:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.97 on epoch=394
05/21/2022 02:51:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.98 on epoch=399
05/21/2022 02:51:48 - INFO - __main__ - Global step 800 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 02:51:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.95 on epoch=404
05/21/2022 02:51:52 - INFO - __main__ - Step 820 Global step 820 Train loss 1.02 on epoch=409
05/21/2022 02:51:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.90 on epoch=414
05/21/2022 02:51:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.92 on epoch=419
05/21/2022 02:51:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.89 on epoch=424
05/21/2022 02:51:59 - INFO - __main__ - Global step 850 Train loss 0.93 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 02:52:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.93 on epoch=429
05/21/2022 02:52:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.90 on epoch=434
05/21/2022 02:52:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=439
05/21/2022 02:52:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.75 on epoch=444
05/21/2022 02:52:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.80 on epoch=449
05/21/2022 02:52:09 - INFO - __main__ - Global step 900 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 02:52:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.73 on epoch=454
05/21/2022 02:52:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.75 on epoch=459
05/21/2022 02:52:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.76 on epoch=464
05/21/2022 02:52:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.75 on epoch=469
05/21/2022 02:52:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.69 on epoch=474
05/21/2022 02:52:20 - INFO - __main__ - Global step 950 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 02:52:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.80 on epoch=479
05/21/2022 02:52:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.66 on epoch=484
05/21/2022 02:52:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.82 on epoch=489
05/21/2022 02:52:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.71 on epoch=494
05/21/2022 02:52:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.69 on epoch=499
05/21/2022 02:52:31 - INFO - __main__ - Global step 1000 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 02:52:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.76 on epoch=504
05/21/2022 02:52:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.73 on epoch=509
05/21/2022 02:52:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.68 on epoch=514
05/21/2022 02:52:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.70 on epoch=519
05/21/2022 02:52:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.69 on epoch=524
05/21/2022 02:52:41 - INFO - __main__ - Global step 1050 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 02:52:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.70 on epoch=529
05/21/2022 02:52:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.69 on epoch=534
05/21/2022 02:52:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.67 on epoch=539
05/21/2022 02:52:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.59 on epoch=544
05/21/2022 02:52:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.59 on epoch=549
05/21/2022 02:52:52 - INFO - __main__ - Global step 1100 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 02:52:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.56 on epoch=554
05/21/2022 02:52:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.62 on epoch=559
05/21/2022 02:52:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.58 on epoch=564
05/21/2022 02:53:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.55 on epoch=569
05/21/2022 02:53:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.61 on epoch=574
05/21/2022 02:53:02 - INFO - __main__ - Global step 1150 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 02:53:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.55 on epoch=579
05/21/2022 02:53:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.61 on epoch=584
05/21/2022 02:53:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.59 on epoch=589
05/21/2022 02:53:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.65 on epoch=594
05/21/2022 02:53:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.54 on epoch=599
05/21/2022 02:53:14 - INFO - __main__ - Global step 1200 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 02:53:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=604
05/21/2022 02:53:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.54 on epoch=609
05/21/2022 02:53:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=614
05/21/2022 02:53:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.60 on epoch=619
05/21/2022 02:53:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=624
05/21/2022 02:53:24 - INFO - __main__ - Global step 1250 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 02:53:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=629
05/21/2022 02:53:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.51 on epoch=634
05/21/2022 02:53:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
05/21/2022 02:53:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.60 on epoch=644
05/21/2022 02:53:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.57 on epoch=649
05/21/2022 02:53:34 - INFO - __main__ - Global step 1300 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 02:53:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.51 on epoch=654
05/21/2022 02:53:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=659
05/21/2022 02:53:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.54 on epoch=664
05/21/2022 02:53:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=669
05/21/2022 02:53:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=674
05/21/2022 02:53:45 - INFO - __main__ - Global step 1350 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 02:53:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.52 on epoch=679
05/21/2022 02:53:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.51 on epoch=684
05/21/2022 02:53:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=689
05/21/2022 02:53:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=694
05/21/2022 02:53:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=699
05/21/2022 02:53:55 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 02:53:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=704
05/21/2022 02:53:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=709
05/21/2022 02:54:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
05/21/2022 02:54:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=719
05/21/2022 02:54:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=724
05/21/2022 02:54:05 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=724
05/21/2022 02:54:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=729
05/21/2022 02:54:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.49 on epoch=734
05/21/2022 02:54:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.50 on epoch=739
05/21/2022 02:54:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=744
05/21/2022 02:54:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=749
05/21/2022 02:54:16 - INFO - __main__ - Global step 1500 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 02:54:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
05/21/2022 02:54:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=759
05/21/2022 02:54:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=764
05/21/2022 02:54:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=769
05/21/2022 02:54:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=774
05/21/2022 02:54:26 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=774
05/21/2022 02:54:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=779
05/21/2022 02:54:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.42 on epoch=784
05/21/2022 02:54:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=789
05/21/2022 02:54:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=794
05/21/2022 02:54:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.41 on epoch=799
05/21/2022 02:54:36 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 02:54:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=804
05/21/2022 02:54:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=809
05/21/2022 02:54:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=814
05/21/2022 02:54:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
05/21/2022 02:54:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=824
05/21/2022 02:54:46 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 02:54:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=829
05/21/2022 02:54:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=834
05/21/2022 02:54:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
05/21/2022 02:54:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=844
05/21/2022 02:54:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=849
05/21/2022 02:54:56 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 02:54:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=854
05/21/2022 02:55:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.18 on epoch=859
05/21/2022 02:55:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=864
05/21/2022 02:55:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=869
05/21/2022 02:55:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
05/21/2022 02:55:07 - INFO - __main__ - Global step 1750 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 02:55:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=879
05/21/2022 02:55:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
05/21/2022 02:55:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=889
05/21/2022 02:55:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=894
05/21/2022 02:55:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=899
05/21/2022 02:55:17 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 02:55:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=904
05/21/2022 02:55:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=909
05/21/2022 02:55:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=914
05/21/2022 02:55:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=919
05/21/2022 02:55:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=924
05/21/2022 02:55:27 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 02:55:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=929
05/21/2022 02:55:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
05/21/2022 02:55:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=939
05/21/2022 02:55:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=944
05/21/2022 02:55:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
05/21/2022 02:55:37 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 02:55:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=954
05/21/2022 02:55:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
05/21/2022 02:55:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=964
05/21/2022 02:55:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=969
05/21/2022 02:55:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=974
05/21/2022 02:55:48 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 02:55:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=979
05/21/2022 02:55:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=984
05/21/2022 02:55:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=989
05/21/2022 02:55:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=994
05/21/2022 02:55:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=999
05/21/2022 02:55:58 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=999
05/21/2022 02:55:58 - INFO - __main__ - save last model!
05/21/2022 02:55:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 02:55:58 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 02:55:58 - INFO - __main__ - Printing 3 examples
05/21/2022 02:55:58 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:55:58 - INFO - __main__ - ['entailed']
05/21/2022 02:55:58 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:55:58 - INFO - __main__ - ['entailed']
05/21/2022 02:55:58 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 02:55:58 - INFO - __main__ - ['entailed']
05/21/2022 02:55:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:55:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:55:58 - INFO - __main__ - Printing 3 examples
05/21/2022 02:55:58 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/21/2022 02:55:58 - INFO - __main__ - ['entailed']
05/21/2022 02:55:58 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/21/2022 02:55:58 - INFO - __main__ - ['entailed']
05/21/2022 02:55:58 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/21/2022 02:55:58 - INFO - __main__ - ['entailed']
05/21/2022 02:55:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:55:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:55:58 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 02:55:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 02:55:58 - INFO - __main__ - Printing 3 examples
05/21/2022 02:55:58 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/21/2022 02:55:58 - INFO - __main__ - ['entailed']
05/21/2022 02:55:58 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/21/2022 02:55:58 - INFO - __main__ - ['entailed']
05/21/2022 02:55:58 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/21/2022 02:55:58 - INFO - __main__ - ['entailed']
05/21/2022 02:55:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 02:55:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:55:58 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 02:56:04 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 02:56:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 02:56:04 - INFO - __main__ - Starting training!
05/21/2022 02:56:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 02:56:34 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 03:00:45 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.3_8_predictions.txt
05/21/2022 03:00:45 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/21/2022 03:00:47 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/21/2022 03:00:47 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.2, bsz=8 ...
05/21/2022 03:00:48 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 03:00:48 - INFO - __main__ - Printing 3 examples
05/21/2022 03:00:48 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/21/2022 03:00:48 - INFO - __main__ - ['entailed']
05/21/2022 03:00:48 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/21/2022 03:00:48 - INFO - __main__ - ['entailed']
05/21/2022 03:00:48 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/21/2022 03:00:48 - INFO - __main__ - ['entailed']
05/21/2022 03:00:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 03:00:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 03:00:48 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 03:00:48 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 03:00:48 - INFO - __main__ - Printing 3 examples
05/21/2022 03:00:48 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/21/2022 03:00:48 - INFO - __main__ - ['entailed']
05/21/2022 03:00:48 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/21/2022 03:00:48 - INFO - __main__ - ['entailed']
05/21/2022 03:00:48 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/21/2022 03:00:48 - INFO - __main__ - ['entailed']
05/21/2022 03:00:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 03:00:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 03:00:48 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 03:00:53 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 03:00:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 03:00:54 - INFO - __main__ - Starting training!
05/21/2022 03:00:56 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/21/2022 03:00:58 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/21/2022 03:01:00 - INFO - __main__ - Step 30 Global step 30 Train loss 4.96 on epoch=14
05/21/2022 03:01:01 - INFO - __main__ - Step 40 Global step 40 Train loss 4.86 on epoch=19
05/21/2022 03:01:03 - INFO - __main__ - Step 50 Global step 50 Train loss 4.80 on epoch=24
05/21/2022 03:01:06 - INFO - __main__ - Global step 50 Train loss 4.93 Classification-F1 0.0 on epoch=24
05/21/2022 03:01:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/21/2022 03:01:08 - INFO - __main__ - Step 60 Global step 60 Train loss 4.86 on epoch=29
05/21/2022 03:01:10 - INFO - __main__ - Step 70 Global step 70 Train loss 4.82 on epoch=34
05/21/2022 03:01:11 - INFO - __main__ - Step 80 Global step 80 Train loss 4.67 on epoch=39
05/21/2022 03:01:13 - INFO - __main__ - Step 90 Global step 90 Train loss 4.71 on epoch=44
05/21/2022 03:01:15 - INFO - __main__ - Step 100 Global step 100 Train loss 4.65 on epoch=49
05/21/2022 03:01:16 - INFO - __main__ - Global step 100 Train loss 4.74 Classification-F1 0.0 on epoch=49
05/21/2022 03:01:18 - INFO - __main__ - Step 110 Global step 110 Train loss 4.58 on epoch=54
05/21/2022 03:01:20 - INFO - __main__ - Step 120 Global step 120 Train loss 4.59 on epoch=59
05/21/2022 03:01:22 - INFO - __main__ - Step 130 Global step 130 Train loss 4.59 on epoch=64
05/21/2022 03:01:24 - INFO - __main__ - Step 140 Global step 140 Train loss 4.45 on epoch=69
05/21/2022 03:01:26 - INFO - __main__ - Step 150 Global step 150 Train loss 4.52 on epoch=74
05/21/2022 03:01:28 - INFO - __main__ - Global step 150 Train loss 4.55 Classification-F1 0.0 on epoch=74
05/21/2022 03:01:30 - INFO - __main__ - Step 160 Global step 160 Train loss 4.51 on epoch=79
05/21/2022 03:01:32 - INFO - __main__ - Step 170 Global step 170 Train loss 4.40 on epoch=84
05/21/2022 03:01:34 - INFO - __main__ - Step 180 Global step 180 Train loss 4.29 on epoch=89
05/21/2022 03:01:36 - INFO - __main__ - Step 190 Global step 190 Train loss 4.24 on epoch=94
05/21/2022 03:01:37 - INFO - __main__ - Step 200 Global step 200 Train loss 4.27 on epoch=99
05/21/2022 03:01:39 - INFO - __main__ - Global step 200 Train loss 4.34 Classification-F1 0.0 on epoch=99
05/21/2022 03:01:41 - INFO - __main__ - Step 210 Global step 210 Train loss 4.27 on epoch=104
05/21/2022 03:01:43 - INFO - __main__ - Step 220 Global step 220 Train loss 4.16 on epoch=109
05/21/2022 03:01:45 - INFO - __main__ - Step 230 Global step 230 Train loss 4.01 on epoch=114
05/21/2022 03:01:47 - INFO - __main__ - Step 240 Global step 240 Train loss 4.02 on epoch=119
05/21/2022 03:01:49 - INFO - __main__ - Step 250 Global step 250 Train loss 3.99 on epoch=124
05/21/2022 03:01:51 - INFO - __main__ - Global step 250 Train loss 4.09 Classification-F1 0.0 on epoch=124
05/21/2022 03:01:53 - INFO - __main__ - Step 260 Global step 260 Train loss 3.88 on epoch=129
05/21/2022 03:01:55 - INFO - __main__ - Step 270 Global step 270 Train loss 3.93 on epoch=134
05/21/2022 03:01:57 - INFO - __main__ - Step 280 Global step 280 Train loss 3.83 on epoch=139
05/21/2022 03:01:59 - INFO - __main__ - Step 290 Global step 290 Train loss 3.84 on epoch=144
05/21/2022 03:02:01 - INFO - __main__ - Step 300 Global step 300 Train loss 3.73 on epoch=149
05/21/2022 03:02:02 - INFO - __main__ - Global step 300 Train loss 3.84 Classification-F1 0.0 on epoch=149
05/21/2022 03:02:04 - INFO - __main__ - Step 310 Global step 310 Train loss 3.75 on epoch=154
05/21/2022 03:02:06 - INFO - __main__ - Step 320 Global step 320 Train loss 3.62 on epoch=159
05/21/2022 03:02:08 - INFO - __main__ - Step 330 Global step 330 Train loss 3.63 on epoch=164
05/21/2022 03:02:10 - INFO - __main__ - Step 340 Global step 340 Train loss 3.62 on epoch=169
05/21/2022 03:02:12 - INFO - __main__ - Step 350 Global step 350 Train loss 3.54 on epoch=174
05/21/2022 03:02:14 - INFO - __main__ - Global step 350 Train loss 3.63 Classification-F1 0.0 on epoch=174
05/21/2022 03:02:16 - INFO - __main__ - Step 360 Global step 360 Train loss 3.50 on epoch=179
05/21/2022 03:02:18 - INFO - __main__ - Step 370 Global step 370 Train loss 3.48 on epoch=184
05/21/2022 03:02:20 - INFO - __main__ - Step 380 Global step 380 Train loss 3.51 on epoch=189
05/21/2022 03:02:22 - INFO - __main__ - Step 390 Global step 390 Train loss 3.39 on epoch=194
05/21/2022 03:02:23 - INFO - __main__ - Step 400 Global step 400 Train loss 3.29 on epoch=199
05/21/2022 03:02:27 - INFO - __main__ - Global step 400 Train loss 3.43 Classification-F1 0.0625 on epoch=199
05/21/2022 03:02:27 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0625 on epoch=199, global_step=400
05/21/2022 03:02:28 - INFO - __main__ - Step 410 Global step 410 Train loss 3.35 on epoch=204
05/21/2022 03:02:30 - INFO - __main__ - Step 420 Global step 420 Train loss 3.26 on epoch=209
05/21/2022 03:02:32 - INFO - __main__ - Step 430 Global step 430 Train loss 3.23 on epoch=214
05/21/2022 03:02:34 - INFO - __main__ - Step 440 Global step 440 Train loss 3.30 on epoch=219
05/21/2022 03:02:36 - INFO - __main__ - Step 450 Global step 450 Train loss 3.18 on epoch=224
05/21/2022 03:02:39 - INFO - __main__ - Global step 450 Train loss 3.26 Classification-F1 0.10317460317460318 on epoch=224
05/21/2022 03:02:39 - INFO - __main__ - Saving model with best Classification-F1: 0.0625 -> 0.10317460317460318 on epoch=224, global_step=450
05/21/2022 03:02:41 - INFO - __main__ - Step 460 Global step 460 Train loss 3.17 on epoch=229
05/21/2022 03:02:43 - INFO - __main__ - Step 470 Global step 470 Train loss 3.11 on epoch=234
05/21/2022 03:02:45 - INFO - __main__ - Step 480 Global step 480 Train loss 3.01 on epoch=239
05/21/2022 03:02:47 - INFO - __main__ - Step 490 Global step 490 Train loss 3.09 on epoch=244
05/21/2022 03:02:49 - INFO - __main__ - Step 500 Global step 500 Train loss 2.92 on epoch=249
05/21/2022 03:02:51 - INFO - __main__ - Global step 500 Train loss 3.06 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 03:02:51 - INFO - __main__ - Saving model with best Classification-F1: 0.10317460317460318 -> 0.3333333333333333 on epoch=249, global_step=500
05/21/2022 03:02:53 - INFO - __main__ - Step 510 Global step 510 Train loss 3.00 on epoch=254
05/21/2022 03:02:55 - INFO - __main__ - Step 520 Global step 520 Train loss 2.88 on epoch=259
05/21/2022 03:02:57 - INFO - __main__ - Step 530 Global step 530 Train loss 2.94 on epoch=264
05/21/2022 03:02:59 - INFO - __main__ - Step 540 Global step 540 Train loss 2.75 on epoch=269
05/21/2022 03:03:01 - INFO - __main__ - Step 550 Global step 550 Train loss 2.89 on epoch=274
05/21/2022 03:03:04 - INFO - __main__ - Global step 550 Train loss 2.89 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 03:03:06 - INFO - __main__ - Step 560 Global step 560 Train loss 2.72 on epoch=279
05/21/2022 03:03:08 - INFO - __main__ - Step 570 Global step 570 Train loss 2.73 on epoch=284
05/21/2022 03:03:10 - INFO - __main__ - Step 580 Global step 580 Train loss 2.69 on epoch=289
05/21/2022 03:03:12 - INFO - __main__ - Step 590 Global step 590 Train loss 2.71 on epoch=294
05/21/2022 03:03:14 - INFO - __main__ - Step 600 Global step 600 Train loss 2.70 on epoch=299
05/21/2022 03:03:17 - INFO - __main__ - Global step 600 Train loss 2.71 Classification-F1 0.3333333333333333 on epoch=299
05/21/2022 03:03:19 - INFO - __main__ - Step 610 Global step 610 Train loss 2.58 on epoch=304
05/21/2022 03:03:21 - INFO - __main__ - Step 620 Global step 620 Train loss 2.63 on epoch=309
05/21/2022 03:03:23 - INFO - __main__ - Step 630 Global step 630 Train loss 2.50 on epoch=314
05/21/2022 03:03:25 - INFO - __main__ - Step 640 Global step 640 Train loss 2.43 on epoch=319
05/21/2022 03:03:27 - INFO - __main__ - Step 650 Global step 650 Train loss 2.49 on epoch=324
05/21/2022 03:03:30 - INFO - __main__ - Global step 650 Train loss 2.53 Classification-F1 0.3333333333333333 on epoch=324
05/21/2022 03:03:32 - INFO - __main__ - Step 660 Global step 660 Train loss 2.48 on epoch=329
05/21/2022 03:03:34 - INFO - __main__ - Step 670 Global step 670 Train loss 2.36 on epoch=334
05/21/2022 03:03:36 - INFO - __main__ - Step 680 Global step 680 Train loss 2.28 on epoch=339
05/21/2022 03:03:37 - INFO - __main__ - Step 690 Global step 690 Train loss 2.31 on epoch=344
05/21/2022 03:03:39 - INFO - __main__ - Step 700 Global step 700 Train loss 2.34 on epoch=349
05/21/2022 03:03:43 - INFO - __main__ - Global step 700 Train loss 2.35 Classification-F1 0.3333333333333333 on epoch=349
05/21/2022 03:03:45 - INFO - __main__ - Step 710 Global step 710 Train loss 2.20 on epoch=354
05/21/2022 03:03:47 - INFO - __main__ - Step 720 Global step 720 Train loss 2.09 on epoch=359
05/21/2022 03:03:49 - INFO - __main__ - Step 730 Global step 730 Train loss 2.13 on epoch=364
05/21/2022 03:03:50 - INFO - __main__ - Step 740 Global step 740 Train loss 2.08 on epoch=369
05/21/2022 03:03:52 - INFO - __main__ - Step 750 Global step 750 Train loss 2.09 on epoch=374
05/21/2022 03:03:56 - INFO - __main__ - Global step 750 Train loss 2.12 Classification-F1 0.3333333333333333 on epoch=374
05/21/2022 03:03:58 - INFO - __main__ - Step 760 Global step 760 Train loss 2.04 on epoch=379
05/21/2022 03:04:00 - INFO - __main__ - Step 770 Global step 770 Train loss 1.91 on epoch=384
05/21/2022 03:04:02 - INFO - __main__ - Step 780 Global step 780 Train loss 1.88 on epoch=389
05/21/2022 03:04:03 - INFO - __main__ - Step 790 Global step 790 Train loss 1.89 on epoch=394
05/21/2022 03:04:05 - INFO - __main__ - Step 800 Global step 800 Train loss 1.88 on epoch=399
05/21/2022 03:04:09 - INFO - __main__ - Global step 800 Train loss 1.92 Classification-F1 0.3333333333333333 on epoch=399
05/21/2022 03:04:10 - INFO - __main__ - Step 810 Global step 810 Train loss 1.82 on epoch=404
05/21/2022 03:04:12 - INFO - __main__ - Step 820 Global step 820 Train loss 1.90 on epoch=409
05/21/2022 03:04:14 - INFO - __main__ - Step 830 Global step 830 Train loss 1.75 on epoch=414
05/21/2022 03:04:16 - INFO - __main__ - Step 840 Global step 840 Train loss 1.61 on epoch=419
05/21/2022 03:04:18 - INFO - __main__ - Step 850 Global step 850 Train loss 1.66 on epoch=424
05/21/2022 03:04:22 - INFO - __main__ - Global step 850 Train loss 1.75 Classification-F1 0.3333333333333333 on epoch=424
05/21/2022 03:04:23 - INFO - __main__ - Step 860 Global step 860 Train loss 1.55 on epoch=429
05/21/2022 03:04:25 - INFO - __main__ - Step 870 Global step 870 Train loss 1.59 on epoch=434
05/21/2022 03:04:27 - INFO - __main__ - Step 880 Global step 880 Train loss 1.49 on epoch=439
05/21/2022 03:04:29 - INFO - __main__ - Step 890 Global step 890 Train loss 1.53 on epoch=444
05/21/2022 03:04:31 - INFO - __main__ - Step 900 Global step 900 Train loss 1.55 on epoch=449
05/21/2022 03:04:34 - INFO - __main__ - Global step 900 Train loss 1.54 Classification-F1 0.3333333333333333 on epoch=449
05/21/2022 03:04:36 - INFO - __main__ - Step 910 Global step 910 Train loss 1.46 on epoch=454
05/21/2022 03:04:38 - INFO - __main__ - Step 920 Global step 920 Train loss 1.56 on epoch=459
05/21/2022 03:04:40 - INFO - __main__ - Step 930 Global step 930 Train loss 1.42 on epoch=464
05/21/2022 03:04:41 - INFO - __main__ - Step 940 Global step 940 Train loss 1.33 on epoch=469
05/21/2022 03:04:43 - INFO - __main__ - Step 950 Global step 950 Train loss 1.35 on epoch=474
05/21/2022 03:04:45 - INFO - __main__ - Global step 950 Train loss 1.42 Classification-F1 0.3333333333333333 on epoch=474
05/21/2022 03:04:47 - INFO - __main__ - Step 960 Global step 960 Train loss 1.26 on epoch=479
05/21/2022 03:04:49 - INFO - __main__ - Step 970 Global step 970 Train loss 1.34 on epoch=484
05/21/2022 03:04:51 - INFO - __main__ - Step 980 Global step 980 Train loss 1.37 on epoch=489
05/21/2022 03:04:53 - INFO - __main__ - Step 990 Global step 990 Train loss 1.42 on epoch=494
05/21/2022 03:04:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.24 on epoch=499
05/21/2022 03:04:55 - INFO - __main__ - Global step 1000 Train loss 1.33 Classification-F1 0.3333333333333333 on epoch=499
05/21/2022 03:04:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.31 on epoch=504
05/21/2022 03:04:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.19 on epoch=509
05/21/2022 03:05:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.23 on epoch=514
05/21/2022 03:05:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.11 on epoch=519
05/21/2022 03:05:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.16 on epoch=524
05/21/2022 03:05:06 - INFO - __main__ - Global step 1050 Train loss 1.20 Classification-F1 0.3333333333333333 on epoch=524
05/21/2022 03:05:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.14 on epoch=529
05/21/2022 03:05:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.05 on epoch=534
05/21/2022 03:05:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.03 on epoch=539
05/21/2022 03:05:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.03 on epoch=544
05/21/2022 03:05:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.96 on epoch=549
05/21/2022 03:05:16 - INFO - __main__ - Global step 1100 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=549
05/21/2022 03:05:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.05 on epoch=554
05/21/2022 03:05:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.87 on epoch=559
05/21/2022 03:05:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.02 on epoch=564
05/21/2022 03:05:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.94 on epoch=569
05/21/2022 03:05:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.88 on epoch=574
05/21/2022 03:05:27 - INFO - __main__ - Global step 1150 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=574
05/21/2022 03:05:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.87 on epoch=579
05/21/2022 03:05:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.94 on epoch=584
05/21/2022 03:05:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.93 on epoch=589
05/21/2022 03:05:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.94 on epoch=594
05/21/2022 03:05:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=599
05/21/2022 03:05:37 - INFO - __main__ - Global step 1200 Train loss 0.90 Classification-F1 0.3333333333333333 on epoch=599
05/21/2022 03:05:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.86 on epoch=604
05/21/2022 03:05:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.90 on epoch=609
05/21/2022 03:05:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.85 on epoch=614
05/21/2022 03:05:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.85 on epoch=619
05/21/2022 03:05:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.71 on epoch=624
05/21/2022 03:05:48 - INFO - __main__ - Global step 1250 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=624
05/21/2022 03:05:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.82 on epoch=629
05/21/2022 03:05:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=634
05/21/2022 03:05:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.78 on epoch=639
05/21/2022 03:05:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.79 on epoch=644
05/21/2022 03:05:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.75 on epoch=649
05/21/2022 03:05:58 - INFO - __main__ - Global step 1300 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=649
05/21/2022 03:06:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.67 on epoch=654
05/21/2022 03:06:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.74 on epoch=659
05/21/2022 03:06:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.79 on epoch=664
05/21/2022 03:06:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.67 on epoch=669
05/21/2022 03:06:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.74 on epoch=674
05/21/2022 03:06:08 - INFO - __main__ - Global step 1350 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=674
05/21/2022 03:06:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.72 on epoch=679
05/21/2022 03:06:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.59 on epoch=684
05/21/2022 03:06:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.66 on epoch=689
05/21/2022 03:06:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.63 on epoch=694
05/21/2022 03:06:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.64 on epoch=699
05/21/2022 03:06:19 - INFO - __main__ - Global step 1400 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=699
05/21/2022 03:06:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.72 on epoch=704
05/21/2022 03:06:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.56 on epoch=709
05/21/2022 03:06:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.61 on epoch=714
05/21/2022 03:06:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=719
05/21/2022 03:06:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=724
05/21/2022 03:06:29 - INFO - __main__ - Global step 1450 Train loss 0.59 Classification-F1 0.3992490613266583 on epoch=724
05/21/2022 03:06:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=724, global_step=1450
05/21/2022 03:06:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.61 on epoch=729
05/21/2022 03:06:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=734
05/21/2022 03:06:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=739
05/21/2022 03:06:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.60 on epoch=744
05/21/2022 03:06:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.57 on epoch=749
05/21/2022 03:06:40 - INFO - __main__ - Global step 1500 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=749
05/21/2022 03:06:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.56 on epoch=754
05/21/2022 03:06:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.55 on epoch=759
05/21/2022 03:06:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.50 on epoch=764
05/21/2022 03:06:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.52 on epoch=769
05/21/2022 03:06:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.55 on epoch=774
05/21/2022 03:06:50 - INFO - __main__ - Global step 1550 Train loss 0.54 Classification-F1 0.3992490613266583 on epoch=774
05/21/2022 03:06:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=779
05/21/2022 03:06:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.46 on epoch=784
05/21/2022 03:06:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.51 on epoch=789
05/21/2022 03:06:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.61 on epoch=794
05/21/2022 03:07:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=799
05/21/2022 03:07:01 - INFO - __main__ - Global step 1600 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=799
05/21/2022 03:07:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=804
05/21/2022 03:07:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.56 on epoch=809
05/21/2022 03:07:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
05/21/2022 03:07:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.50 on epoch=819
05/21/2022 03:07:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.56 on epoch=824
05/21/2022 03:07:11 - INFO - __main__ - Global step 1650 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=824
05/21/2022 03:07:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=829
05/21/2022 03:07:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=834
05/21/2022 03:07:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=839
05/21/2022 03:07:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=844
05/21/2022 03:07:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=849
05/21/2022 03:07:22 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=849
05/21/2022 03:07:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=854
05/21/2022 03:07:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.46 on epoch=859
05/21/2022 03:07:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=864
05/21/2022 03:07:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=869
05/21/2022 03:07:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.47 on epoch=874
05/21/2022 03:07:32 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=874
05/21/2022 03:07:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=879
05/21/2022 03:07:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
05/21/2022 03:07:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=889
05/21/2022 03:07:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=894
05/21/2022 03:07:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=899
05/21/2022 03:07:42 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=899
05/21/2022 03:07:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=904
05/21/2022 03:07:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=909
05/21/2022 03:07:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=914
05/21/2022 03:07:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=919
05/21/2022 03:07:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=924
05/21/2022 03:07:52 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=924
05/21/2022 03:07:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=929
05/21/2022 03:07:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=934
05/21/2022 03:07:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
05/21/2022 03:08:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.50 on epoch=944
05/21/2022 03:08:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=949
05/21/2022 03:08:02 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=949
05/21/2022 03:08:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
05/21/2022 03:08:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
05/21/2022 03:08:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
05/21/2022 03:08:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=969
05/21/2022 03:08:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=974
05/21/2022 03:08:13 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=974
05/21/2022 03:08:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=979
05/21/2022 03:08:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/21/2022 03:08:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=989
05/21/2022 03:08:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=994
05/21/2022 03:08:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=999
05/21/2022 03:08:23 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.3191489361702127 on epoch=999
05/21/2022 03:08:23 - INFO - __main__ - save last model!
05/21/2022 03:08:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 03:08:23 - INFO - __main__ - Start tokenizing ... 12792 instances
05/21/2022 03:08:23 - INFO - __main__ - Printing 3 examples
05/21/2022 03:08:23 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 03:08:23 - INFO - __main__ - ['entailed']
05/21/2022 03:08:23 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 03:08:23 - INFO - __main__ - ['entailed']
05/21/2022 03:08:23 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/21/2022 03:08:23 - INFO - __main__ - ['entailed']
05/21/2022 03:08:23 - INFO - __main__ - Tokenizing Input ...
05/21/2022 03:08:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 03:09:01 - INFO - __main__ - Loaded 12792 examples from test data
05/21/2022 03:13:08 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.2_8_predictions.txt
05/21/2022 03:13:08 - INFO - __main__ - Classification-F1 on test data: 0.3372
05/21/2022 03:13:08 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.2, bsz=8, dev_performance=0.3992490613266583, test_performance=0.33724740154282734
05/30/2022 11:21:38 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-base-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
05/30/2022 11:21:38 - INFO - __main__ - models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact
05/30/2022 11:21:39 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-base-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='2,3')
05/30/2022 11:21:39 - INFO - __main__ - models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact
05/30/2022 11:21:39 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/30/2022 11:21:39 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/30/2022 11:21:39 - INFO - __main__ - args.device: cuda:0
05/30/2022 11:21:39 - INFO - __main__ - Using 2 gpus
05/30/2022 11:21:39 - INFO - __main__ - args.device: cuda:1
05/30/2022 11:21:39 - INFO - __main__ - Using 2 gpus
05/30/2022 11:21:39 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
05/30/2022 11:21:39 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
05/30/2022 11:21:44 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.5, bsz=8 ...
05/30/2022 11:21:45 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:21:45 - INFO - __main__ - Printing 3 examples
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:21:45 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:21:45 - INFO - __main__ - Printing 3 examples
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:21:45 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:21:45 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:21:45 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 11:21:45 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:21:45 - INFO - __main__ - Printing 3 examples
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:21:45 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 11:21:45 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:21:45 - INFO - __main__ - Printing 3 examples
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/30/2022 11:21:45 - INFO - __main__ - ['refuted']
05/30/2022 11:21:45 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:21:45 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:21:45 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:21:45 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 11:21:45 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 11:21:51 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 11:21:51 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 11:21:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 11:21:51 - INFO - __main__ - Starting training!
05/30/2022 11:21:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 11:21:56 - INFO - __main__ - Starting training!
05/30/2022 11:21:58 - INFO - __main__ - Step 10 Global step 10 Train loss 5.01 on epoch=4
05/30/2022 11:22:00 - INFO - __main__ - Step 20 Global step 20 Train loss 4.89 on epoch=9
05/30/2022 11:22:02 - INFO - __main__ - Step 30 Global step 30 Train loss 4.82 on epoch=14
05/30/2022 11:22:04 - INFO - __main__ - Step 40 Global step 40 Train loss 4.63 on epoch=19
05/30/2022 11:22:06 - INFO - __main__ - Step 50 Global step 50 Train loss 4.47 on epoch=24
05/30/2022 11:22:07 - INFO - __main__ - Global step 50 Train loss 4.76 Classification-F1 0.0 on epoch=24
05/30/2022 11:22:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 11:22:09 - INFO - __main__ - Step 60 Global step 60 Train loss 4.52 on epoch=29
05/30/2022 11:22:11 - INFO - __main__ - Step 70 Global step 70 Train loss 4.22 on epoch=34
05/30/2022 11:22:13 - INFO - __main__ - Step 80 Global step 80 Train loss 4.14 on epoch=39
05/30/2022 11:22:15 - INFO - __main__ - Step 90 Global step 90 Train loss 4.04 on epoch=44
05/30/2022 11:22:17 - INFO - __main__ - Step 100 Global step 100 Train loss 3.88 on epoch=49
05/30/2022 11:22:19 - INFO - __main__ - Global step 100 Train loss 4.16 Classification-F1 0.0 on epoch=49
05/30/2022 11:22:21 - INFO - __main__ - Step 110 Global step 110 Train loss 3.72 on epoch=54
05/30/2022 11:22:23 - INFO - __main__ - Step 120 Global step 120 Train loss 3.64 on epoch=59
05/30/2022 11:22:25 - INFO - __main__ - Step 130 Global step 130 Train loss 3.54 on epoch=64
05/30/2022 11:22:27 - INFO - __main__ - Step 140 Global step 140 Train loss 3.41 on epoch=69
05/30/2022 11:22:29 - INFO - __main__ - Step 150 Global step 150 Train loss 3.34 on epoch=74
05/30/2022 11:22:32 - INFO - __main__ - Global step 150 Train loss 3.53 Classification-F1 0.21276595744680848 on epoch=74
05/30/2022 11:22:32 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.21276595744680848 on epoch=74, global_step=150
05/30/2022 11:22:34 - INFO - __main__ - Step 160 Global step 160 Train loss 3.17 on epoch=79
05/30/2022 11:22:36 - INFO - __main__ - Step 170 Global step 170 Train loss 2.98 on epoch=84
05/30/2022 11:22:38 - INFO - __main__ - Step 180 Global step 180 Train loss 2.84 on epoch=89
05/30/2022 11:22:40 - INFO - __main__ - Step 190 Global step 190 Train loss 2.77 on epoch=94
05/30/2022 11:22:42 - INFO - __main__ - Step 200 Global step 200 Train loss 2.64 on epoch=99
05/30/2022 11:22:45 - INFO - __main__ - Global step 200 Train loss 2.88 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 11:22:45 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.3333333333333333 on epoch=99, global_step=200
05/30/2022 11:22:47 - INFO - __main__ - Step 210 Global step 210 Train loss 2.52 on epoch=104
05/30/2022 11:22:49 - INFO - __main__ - Step 220 Global step 220 Train loss 2.41 on epoch=109
05/30/2022 11:22:51 - INFO - __main__ - Step 230 Global step 230 Train loss 2.36 on epoch=114
05/30/2022 11:22:53 - INFO - __main__ - Step 240 Global step 240 Train loss 2.26 on epoch=119
05/30/2022 11:22:55 - INFO - __main__ - Step 250 Global step 250 Train loss 2.11 on epoch=124
05/30/2022 11:22:59 - INFO - __main__ - Global step 250 Train loss 2.33 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 11:23:01 - INFO - __main__ - Step 260 Global step 260 Train loss 2.13 on epoch=129
05/30/2022 11:23:03 - INFO - __main__ - Step 270 Global step 270 Train loss 1.91 on epoch=134
05/30/2022 11:23:05 - INFO - __main__ - Step 280 Global step 280 Train loss 1.90 on epoch=139
05/30/2022 11:23:06 - INFO - __main__ - Step 290 Global step 290 Train loss 1.84 on epoch=144
05/30/2022 11:23:08 - INFO - __main__ - Step 300 Global step 300 Train loss 1.67 on epoch=149
05/30/2022 11:23:12 - INFO - __main__ - Global step 300 Train loss 1.89 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 11:23:14 - INFO - __main__ - Step 310 Global step 310 Train loss 1.73 on epoch=154
05/30/2022 11:23:16 - INFO - __main__ - Step 320 Global step 320 Train loss 1.55 on epoch=159
05/30/2022 11:23:18 - INFO - __main__ - Step 330 Global step 330 Train loss 1.60 on epoch=164
05/30/2022 11:23:19 - INFO - __main__ - Step 340 Global step 340 Train loss 1.55 on epoch=169
05/30/2022 11:23:21 - INFO - __main__ - Step 350 Global step 350 Train loss 1.38 on epoch=174
05/30/2022 11:23:24 - INFO - __main__ - Global step 350 Train loss 1.56 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 11:23:26 - INFO - __main__ - Step 360 Global step 360 Train loss 1.35 on epoch=179
05/30/2022 11:23:28 - INFO - __main__ - Step 370 Global step 370 Train loss 1.26 on epoch=184
05/30/2022 11:23:30 - INFO - __main__ - Step 380 Global step 380 Train loss 1.30 on epoch=189
05/30/2022 11:23:32 - INFO - __main__ - Step 390 Global step 390 Train loss 1.13 on epoch=194
05/30/2022 11:23:34 - INFO - __main__ - Step 400 Global step 400 Train loss 1.07 on epoch=199
05/30/2022 11:23:35 - INFO - __main__ - Global step 400 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 11:23:37 - INFO - __main__ - Step 410 Global step 410 Train loss 1.04 on epoch=204
05/30/2022 11:23:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.98 on epoch=209
05/30/2022 11:23:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.95 on epoch=214
05/30/2022 11:23:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=219
05/30/2022 11:23:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.83 on epoch=224
05/30/2022 11:23:47 - INFO - __main__ - Global step 450 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 11:23:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.88 on epoch=229
05/30/2022 11:23:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.87 on epoch=234
05/30/2022 11:23:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.79 on epoch=239
05/30/2022 11:23:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.73 on epoch=244
05/30/2022 11:23:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=249
05/30/2022 11:23:57 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 11:23:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.62 on epoch=254
05/30/2022 11:24:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=259
05/30/2022 11:24:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.68 on epoch=264
05/30/2022 11:24:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.67 on epoch=269
05/30/2022 11:24:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.63 on epoch=274
05/30/2022 11:24:08 - INFO - __main__ - Global step 550 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 11:24:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=279
05/30/2022 11:24:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.58 on epoch=284
05/30/2022 11:24:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=289
05/30/2022 11:24:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.60 on epoch=294
05/30/2022 11:24:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=299
05/30/2022 11:24:18 - INFO - __main__ - Global step 600 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 11:24:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.54 on epoch=304
05/30/2022 11:24:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=309
05/30/2022 11:24:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=314
05/30/2022 11:24:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=319
05/30/2022 11:24:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=324
05/30/2022 11:24:28 - INFO - __main__ - Global step 650 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 11:24:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=329
05/30/2022 11:24:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=334
05/30/2022 11:24:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=339
05/30/2022 11:24:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=344
05/30/2022 11:24:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=349
05/30/2022 11:24:39 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 11:24:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=354
05/30/2022 11:24:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=359
05/30/2022 11:24:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=364
05/30/2022 11:24:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=369
05/30/2022 11:24:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=374
05/30/2022 11:24:49 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 11:24:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=379
05/30/2022 11:24:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=384
05/30/2022 11:24:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=389
05/30/2022 11:24:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=394
05/30/2022 11:24:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=399
05/30/2022 11:25:00 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 11:25:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=404
05/30/2022 11:25:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=409
05/30/2022 11:25:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
05/30/2022 11:25:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=419
05/30/2022 11:25:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=424
05/30/2022 11:25:10 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 11:25:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=429
05/30/2022 11:25:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=434
05/30/2022 11:25:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=439
05/30/2022 11:25:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
05/30/2022 11:25:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=449
05/30/2022 11:25:20 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 11:25:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=454
05/30/2022 11:25:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=459
05/30/2022 11:25:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=464
05/30/2022 11:25:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=469
05/30/2022 11:25:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=474
05/30/2022 11:25:30 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 11:25:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=479
05/30/2022 11:25:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=484
05/30/2022 11:25:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=489
05/30/2022 11:25:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=494
05/30/2022 11:25:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
05/30/2022 11:25:41 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 11:25:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=504
05/30/2022 11:25:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=509
05/30/2022 11:25:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=514
05/30/2022 11:25:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=519
05/30/2022 11:25:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=524
05/30/2022 11:25:51 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 11:25:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=529
05/30/2022 11:25:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=534
05/30/2022 11:25:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=539
05/30/2022 11:25:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=544
05/30/2022 11:26:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=549
05/30/2022 11:26:01 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 11:26:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=554
05/30/2022 11:26:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=559
05/30/2022 11:26:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=564
05/30/2022 11:26:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=569
05/30/2022 11:26:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=574
05/30/2022 11:26:11 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 11:26:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=579
05/30/2022 11:26:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=584
05/30/2022 11:26:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
05/30/2022 11:26:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=594
05/30/2022 11:26:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=599
05/30/2022 11:26:22 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 11:26:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
05/30/2022 11:26:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.31 on epoch=609
05/30/2022 11:26:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=614
05/30/2022 11:26:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.31 on epoch=619
05/30/2022 11:26:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=624
05/30/2022 11:26:32 - INFO - __main__ - Global step 1250 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 11:26:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=629
05/30/2022 11:26:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=634
05/30/2022 11:26:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=639
05/30/2022 11:26:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=644
05/30/2022 11:26:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=649
05/30/2022 11:26:42 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 11:26:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=654
05/30/2022 11:26:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=659
05/30/2022 11:26:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=664
05/30/2022 11:26:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=669
05/30/2022 11:26:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=674
05/30/2022 11:26:52 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 11:26:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=679
05/30/2022 11:26:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=684
05/30/2022 11:26:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=689
05/30/2022 11:27:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=694
05/30/2022 11:27:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=699
05/30/2022 11:27:03 - INFO - __main__ - Global step 1400 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 11:27:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=704
05/30/2022 11:27:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=709
05/30/2022 11:27:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=714
05/30/2022 11:27:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.28 on epoch=719
05/30/2022 11:27:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.27 on epoch=724
05/30/2022 11:27:13 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 11:27:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=729
05/30/2022 11:27:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=734
05/30/2022 11:27:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=739
05/30/2022 11:27:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=744
05/30/2022 11:27:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=749
05/30/2022 11:27:23 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 11:27:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=754
05/30/2022 11:27:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=759
05/30/2022 11:27:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=764
05/30/2022 11:27:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=769
05/30/2022 11:27:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=774
05/30/2022 11:27:34 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 11:27:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=779
05/30/2022 11:27:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=784
05/30/2022 11:27:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.30 on epoch=789
05/30/2022 11:27:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=794
05/30/2022 11:27:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=799
05/30/2022 11:27:44 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 11:27:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=804
05/30/2022 11:27:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=809
05/30/2022 11:27:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=814
05/30/2022 11:27:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=819
05/30/2022 11:27:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=824
05/30/2022 11:27:54 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 11:27:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=829
05/30/2022 11:27:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=834
05/30/2022 11:28:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=839
05/30/2022 11:28:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=844
05/30/2022 11:28:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=849
05/30/2022 11:28:05 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 11:28:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=854
05/30/2022 11:28:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=859
05/30/2022 11:28:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=864
05/30/2022 11:28:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=869
05/30/2022 11:28:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.30 on epoch=874
05/30/2022 11:28:15 - INFO - __main__ - Global step 1750 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 11:28:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=879
05/30/2022 11:28:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=884
05/30/2022 11:28:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=889
05/30/2022 11:28:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=894
05/30/2022 11:28:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=899
05/30/2022 11:28:25 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 11:28:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=904
05/30/2022 11:28:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=909
05/30/2022 11:28:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=914
05/30/2022 11:28:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.31 on epoch=919
05/30/2022 11:28:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=924
05/30/2022 11:28:36 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 11:28:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=929
05/30/2022 11:28:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=934
05/30/2022 11:28:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=939
05/30/2022 11:28:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.28 on epoch=944
05/30/2022 11:28:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=949
05/30/2022 11:28:46 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 11:28:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=954
05/30/2022 11:28:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=959
05/30/2022 11:28:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=964
05/30/2022 11:28:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=969
05/30/2022 11:28:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=974
05/30/2022 11:28:56 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 11:28:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=979
05/30/2022 11:29:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=984
05/30/2022 11:29:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
05/30/2022 11:29:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=994
05/30/2022 11:29:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=999
05/30/2022 11:29:07 - INFO - __main__ - Global step 2000 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 11:29:07 - INFO - __main__ - save last model!
05/30/2022 11:29:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 11:29:07 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 11:29:07 - INFO - __main__ - Printing 3 examples
05/30/2022 11:29:07 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:29:07 - INFO - __main__ - ['entailed']
05/30/2022 11:29:07 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:29:07 - INFO - __main__ - ['entailed']
05/30/2022 11:29:07 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:29:07 - INFO - __main__ - ['entailed']
05/30/2022 11:29:07 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:29:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:29:07 - INFO - __main__ - Printing 3 examples
05/30/2022 11:29:07 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 11:29:07 - INFO - __main__ - ['refuted']
05/30/2022 11:29:07 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 11:29:07 - INFO - __main__ - ['refuted']
05/30/2022 11:29:07 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 11:29:07 - INFO - __main__ - ['refuted']
05/30/2022 11:29:07 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:29:07 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:29:07 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 11:29:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:29:07 - INFO - __main__ - Printing 3 examples
05/30/2022 11:29:07 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/30/2022 11:29:07 - INFO - __main__ - ['refuted']
05/30/2022 11:29:07 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/30/2022 11:29:07 - INFO - __main__ - ['refuted']
05/30/2022 11:29:07 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/30/2022 11:29:07 - INFO - __main__ - ['refuted']
05/30/2022 11:29:07 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:29:07 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:29:07 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 11:29:13 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 11:29:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 11:29:13 - INFO - __main__ - Starting training!
05/30/2022 11:29:31 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:29:44 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 11:33:51 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.5_8_predictions.txt
05/30/2022 11:33:51 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 11:33:51 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 11:33:51 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.4, bsz=8 ...
05/30/2022 11:33:52 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:33:52 - INFO - __main__ - Printing 3 examples
05/30/2022 11:33:52 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 11:33:52 - INFO - __main__ - ['refuted']
05/30/2022 11:33:52 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 11:33:52 - INFO - __main__ - ['refuted']
05/30/2022 11:33:52 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 11:33:52 - INFO - __main__ - ['refuted']
05/30/2022 11:33:52 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:33:52 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:33:52 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 11:33:52 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:33:52 - INFO - __main__ - Printing 3 examples
05/30/2022 11:33:52 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/30/2022 11:33:52 - INFO - __main__ - ['refuted']
05/30/2022 11:33:52 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/30/2022 11:33:52 - INFO - __main__ - ['refuted']
05/30/2022 11:33:52 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/30/2022 11:33:52 - INFO - __main__ - ['refuted']
05/30/2022 11:33:52 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:33:52 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:33:53 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 11:33:59 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 11:33:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 11:33:59 - INFO - __main__ - Starting training!
05/30/2022 11:34:01 - INFO - __main__ - Step 10 Global step 10 Train loss 5.06 on epoch=4
05/30/2022 11:34:03 - INFO - __main__ - Step 20 Global step 20 Train loss 4.99 on epoch=9
05/30/2022 11:34:05 - INFO - __main__ - Step 30 Global step 30 Train loss 4.88 on epoch=14
05/30/2022 11:34:07 - INFO - __main__ - Step 40 Global step 40 Train loss 4.85 on epoch=19
05/30/2022 11:34:09 - INFO - __main__ - Step 50 Global step 50 Train loss 4.59 on epoch=24
05/30/2022 11:34:10 - INFO - __main__ - Global step 50 Train loss 4.88 Classification-F1 0.0 on epoch=24
05/30/2022 11:34:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 11:34:12 - INFO - __main__ - Step 60 Global step 60 Train loss 4.69 on epoch=29
05/30/2022 11:34:14 - INFO - __main__ - Step 70 Global step 70 Train loss 4.53 on epoch=34
05/30/2022 11:34:16 - INFO - __main__ - Step 80 Global step 80 Train loss 4.48 on epoch=39
05/30/2022 11:34:18 - INFO - __main__ - Step 90 Global step 90 Train loss 4.24 on epoch=44
05/30/2022 11:34:20 - INFO - __main__ - Step 100 Global step 100 Train loss 4.29 on epoch=49
05/30/2022 11:34:23 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/30/2022 11:34:25 - INFO - __main__ - Step 110 Global step 110 Train loss 4.08 on epoch=54
05/30/2022 11:34:27 - INFO - __main__ - Step 120 Global step 120 Train loss 4.21 on epoch=59
05/30/2022 11:34:29 - INFO - __main__ - Step 130 Global step 130 Train loss 3.97 on epoch=64
05/30/2022 11:34:31 - INFO - __main__ - Step 140 Global step 140 Train loss 3.87 on epoch=69
05/30/2022 11:34:33 - INFO - __main__ - Step 150 Global step 150 Train loss 3.72 on epoch=74
05/30/2022 11:34:36 - INFO - __main__ - Global step 150 Train loss 3.97 Classification-F1 0.0 on epoch=74
05/30/2022 11:34:38 - INFO - __main__ - Step 160 Global step 160 Train loss 3.66 on epoch=79
05/30/2022 11:34:40 - INFO - __main__ - Step 170 Global step 170 Train loss 3.57 on epoch=84
05/30/2022 11:34:42 - INFO - __main__ - Step 180 Global step 180 Train loss 3.59 on epoch=89
05/30/2022 11:34:44 - INFO - __main__ - Step 190 Global step 190 Train loss 3.51 on epoch=94
05/30/2022 11:34:46 - INFO - __main__ - Step 200 Global step 200 Train loss 3.35 on epoch=99
05/30/2022 11:34:51 - INFO - __main__ - Global step 200 Train loss 3.53 Classification-F1 0.07973421926910298 on epoch=99
05/30/2022 11:34:51 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.07973421926910298 on epoch=99, global_step=200
05/30/2022 11:34:53 - INFO - __main__ - Step 210 Global step 210 Train loss 3.34 on epoch=104
05/30/2022 11:34:55 - INFO - __main__ - Step 220 Global step 220 Train loss 3.29 on epoch=109
05/30/2022 11:34:57 - INFO - __main__ - Step 230 Global step 230 Train loss 3.04 on epoch=114
05/30/2022 11:34:59 - INFO - __main__ - Step 240 Global step 240 Train loss 3.09 on epoch=119
05/30/2022 11:35:01 - INFO - __main__ - Step 250 Global step 250 Train loss 2.88 on epoch=124
05/30/2022 11:35:04 - INFO - __main__ - Global step 250 Train loss 3.13 Classification-F1 0.1739130434782609 on epoch=124
05/30/2022 11:35:04 - INFO - __main__ - Saving model with best Classification-F1: 0.07973421926910298 -> 0.1739130434782609 on epoch=124, global_step=250
05/30/2022 11:35:06 - INFO - __main__ - Step 260 Global step 260 Train loss 2.89 on epoch=129
05/30/2022 11:35:08 - INFO - __main__ - Step 270 Global step 270 Train loss 2.84 on epoch=134
05/30/2022 11:35:10 - INFO - __main__ - Step 280 Global step 280 Train loss 2.83 on epoch=139
05/30/2022 11:35:12 - INFO - __main__ - Step 290 Global step 290 Train loss 2.72 on epoch=144
05/30/2022 11:35:14 - INFO - __main__ - Step 300 Global step 300 Train loss 2.50 on epoch=149
05/30/2022 11:35:17 - INFO - __main__ - Global step 300 Train loss 2.75 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 11:35:17 - INFO - __main__ - Saving model with best Classification-F1: 0.1739130434782609 -> 0.3333333333333333 on epoch=149, global_step=300
05/30/2022 11:35:19 - INFO - __main__ - Step 310 Global step 310 Train loss 2.59 on epoch=154
05/30/2022 11:35:21 - INFO - __main__ - Step 320 Global step 320 Train loss 2.30 on epoch=159
05/30/2022 11:35:23 - INFO - __main__ - Step 330 Global step 330 Train loss 2.41 on epoch=164
05/30/2022 11:35:25 - INFO - __main__ - Step 340 Global step 340 Train loss 2.39 on epoch=169
05/30/2022 11:35:26 - INFO - __main__ - Step 350 Global step 350 Train loss 2.35 on epoch=174
05/30/2022 11:35:30 - INFO - __main__ - Global step 350 Train loss 2.41 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 11:35:32 - INFO - __main__ - Step 360 Global step 360 Train loss 2.29 on epoch=179
05/30/2022 11:35:34 - INFO - __main__ - Step 370 Global step 370 Train loss 2.17 on epoch=184
05/30/2022 11:35:36 - INFO - __main__ - Step 380 Global step 380 Train loss 2.15 on epoch=189
05/30/2022 11:35:38 - INFO - __main__ - Step 390 Global step 390 Train loss 2.05 on epoch=194
05/30/2022 11:35:40 - INFO - __main__ - Step 400 Global step 400 Train loss 1.97 on epoch=199
05/30/2022 11:35:47 - INFO - __main__ - Global step 400 Train loss 2.13 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 11:35:49 - INFO - __main__ - Step 410 Global step 410 Train loss 2.00 on epoch=204
05/30/2022 11:35:50 - INFO - __main__ - Step 420 Global step 420 Train loss 1.99 on epoch=209
05/30/2022 11:35:52 - INFO - __main__ - Step 430 Global step 430 Train loss 2.04 on epoch=214
05/30/2022 11:35:54 - INFO - __main__ - Step 440 Global step 440 Train loss 1.99 on epoch=219
05/30/2022 11:35:56 - INFO - __main__ - Step 450 Global step 450 Train loss 1.96 on epoch=224
05/30/2022 11:36:03 - INFO - __main__ - Global step 450 Train loss 2.00 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 11:36:05 - INFO - __main__ - Step 460 Global step 460 Train loss 1.81 on epoch=229
05/30/2022 11:36:07 - INFO - __main__ - Step 470 Global step 470 Train loss 1.71 on epoch=234
05/30/2022 11:36:09 - INFO - __main__ - Step 480 Global step 480 Train loss 1.64 on epoch=239
05/30/2022 11:36:11 - INFO - __main__ - Step 490 Global step 490 Train loss 1.71 on epoch=244
05/30/2022 11:36:13 - INFO - __main__ - Step 500 Global step 500 Train loss 1.60 on epoch=249
05/30/2022 11:36:16 - INFO - __main__ - Global step 500 Train loss 1.69 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 11:36:18 - INFO - __main__ - Step 510 Global step 510 Train loss 1.59 on epoch=254
05/30/2022 11:36:20 - INFO - __main__ - Step 520 Global step 520 Train loss 1.53 on epoch=259
05/30/2022 11:36:22 - INFO - __main__ - Step 530 Global step 530 Train loss 1.49 on epoch=264
05/30/2022 11:36:24 - INFO - __main__ - Step 540 Global step 540 Train loss 1.47 on epoch=269
05/30/2022 11:36:25 - INFO - __main__ - Step 550 Global step 550 Train loss 1.48 on epoch=274
05/30/2022 11:36:28 - INFO - __main__ - Global step 550 Train loss 1.51 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 11:36:30 - INFO - __main__ - Step 560 Global step 560 Train loss 1.47 on epoch=279
05/30/2022 11:36:32 - INFO - __main__ - Step 570 Global step 570 Train loss 1.39 on epoch=284
05/30/2022 11:36:34 - INFO - __main__ - Step 580 Global step 580 Train loss 1.33 on epoch=289
05/30/2022 11:36:36 - INFO - __main__ - Step 590 Global step 590 Train loss 1.24 on epoch=294
05/30/2022 11:36:38 - INFO - __main__ - Step 600 Global step 600 Train loss 1.30 on epoch=299
05/30/2022 11:36:39 - INFO - __main__ - Global step 600 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 11:36:41 - INFO - __main__ - Step 610 Global step 610 Train loss 1.25 on epoch=304
05/30/2022 11:36:42 - INFO - __main__ - Step 620 Global step 620 Train loss 1.20 on epoch=309
05/30/2022 11:36:44 - INFO - __main__ - Step 630 Global step 630 Train loss 1.18 on epoch=314
05/30/2022 11:36:46 - INFO - __main__ - Step 640 Global step 640 Train loss 1.29 on epoch=319
05/30/2022 11:36:48 - INFO - __main__ - Step 650 Global step 650 Train loss 1.13 on epoch=324
05/30/2022 11:36:49 - INFO - __main__ - Global step 650 Train loss 1.21 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 11:36:51 - INFO - __main__ - Step 660 Global step 660 Train loss 1.14 on epoch=329
05/30/2022 11:36:53 - INFO - __main__ - Step 670 Global step 670 Train loss 1.08 on epoch=334
05/30/2022 11:36:55 - INFO - __main__ - Step 680 Global step 680 Train loss 1.15 on epoch=339
05/30/2022 11:36:57 - INFO - __main__ - Step 690 Global step 690 Train loss 1.15 on epoch=344
05/30/2022 11:36:59 - INFO - __main__ - Step 700 Global step 700 Train loss 1.08 on epoch=349
05/30/2022 11:37:00 - INFO - __main__ - Global step 700 Train loss 1.12 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 11:37:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.95 on epoch=354
05/30/2022 11:37:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.98 on epoch=359
05/30/2022 11:37:05 - INFO - __main__ - Step 730 Global step 730 Train loss 1.05 on epoch=364
05/30/2022 11:37:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.99 on epoch=369
05/30/2022 11:37:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.97 on epoch=374
05/30/2022 11:37:10 - INFO - __main__ - Global step 750 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 11:37:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.88 on epoch=379
05/30/2022 11:37:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=384
05/30/2022 11:37:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.91 on epoch=389
05/30/2022 11:37:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.93 on epoch=394
05/30/2022 11:37:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.92 on epoch=399
05/30/2022 11:37:21 - INFO - __main__ - Global step 800 Train loss 0.91 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 11:37:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=404
05/30/2022 11:37:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.88 on epoch=409
05/30/2022 11:37:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.85 on epoch=414
05/30/2022 11:37:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.81 on epoch=419
05/30/2022 11:37:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.85 on epoch=424
05/30/2022 11:37:31 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 11:37:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.74 on epoch=429
05/30/2022 11:37:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=434
05/30/2022 11:37:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.82 on epoch=439
05/30/2022 11:37:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.73 on epoch=444
05/30/2022 11:37:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.76 on epoch=449
05/30/2022 11:37:42 - INFO - __main__ - Global step 900 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 11:37:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.73 on epoch=454
05/30/2022 11:37:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.83 on epoch=459
05/30/2022 11:37:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.69 on epoch=464
05/30/2022 11:37:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.74 on epoch=469
05/30/2022 11:37:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.72 on epoch=474
05/30/2022 11:37:52 - INFO - __main__ - Global step 950 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 11:37:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.72 on epoch=479
05/30/2022 11:37:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.74 on epoch=484
05/30/2022 11:37:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.69 on epoch=489
05/30/2022 11:38:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.68 on epoch=494
05/30/2022 11:38:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.78 on epoch=499
05/30/2022 11:38:03 - INFO - __main__ - Global step 1000 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 11:38:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.62 on epoch=504
05/30/2022 11:38:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.78 on epoch=509
05/30/2022 11:38:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.77 on epoch=514
05/30/2022 11:38:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.62 on epoch=519
05/30/2022 11:38:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.66 on epoch=524
05/30/2022 11:38:13 - INFO - __main__ - Global step 1050 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 11:38:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.64 on epoch=529
05/30/2022 11:38:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.65 on epoch=534
05/30/2022 11:38:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.62 on epoch=539
05/30/2022 11:38:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.57 on epoch=544
05/30/2022 11:38:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.63 on epoch=549
05/30/2022 11:38:24 - INFO - __main__ - Global step 1100 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 11:38:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=554
05/30/2022 11:38:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.65 on epoch=559
05/30/2022 11:38:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.56 on epoch=564
05/30/2022 11:38:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.53 on epoch=569
05/30/2022 11:38:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.60 on epoch=574
05/30/2022 11:38:34 - INFO - __main__ - Global step 1150 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 11:38:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=579
05/30/2022 11:38:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.58 on epoch=584
05/30/2022 11:38:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=589
05/30/2022 11:38:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.63 on epoch=594
05/30/2022 11:38:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.53 on epoch=599
05/30/2022 11:38:44 - INFO - __main__ - Global step 1200 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 11:38:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.52 on epoch=604
05/30/2022 11:38:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=609
05/30/2022 11:38:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.53 on epoch=614
05/30/2022 11:38:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.58 on epoch=619
05/30/2022 11:38:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=624
05/30/2022 11:38:55 - INFO - __main__ - Global step 1250 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 11:38:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=629
05/30/2022 11:38:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.58 on epoch=634
05/30/2022 11:39:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=639
05/30/2022 11:39:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.57 on epoch=644
05/30/2022 11:39:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.54 on epoch=649
05/30/2022 11:39:05 - INFO - __main__ - Global step 1300 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 11:39:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=654
05/30/2022 11:39:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=659
05/30/2022 11:39:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.50 on epoch=664
05/30/2022 11:39:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.53 on epoch=669
05/30/2022 11:39:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=674
05/30/2022 11:39:16 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 11:39:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=679
05/30/2022 11:39:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.53 on epoch=684
05/30/2022 11:39:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=689
05/30/2022 11:39:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=694
05/30/2022 11:39:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=699
05/30/2022 11:39:26 - INFO - __main__ - Global step 1400 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 11:39:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=704
05/30/2022 11:39:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
05/30/2022 11:39:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=714
05/30/2022 11:39:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.53 on epoch=719
05/30/2022 11:39:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
05/30/2022 11:39:37 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 11:39:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=729
05/30/2022 11:39:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=734
05/30/2022 11:39:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/30/2022 11:39:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=744
05/30/2022 11:39:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=749
05/30/2022 11:39:47 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 11:39:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.47 on epoch=754
05/30/2022 11:39:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=759
05/30/2022 11:39:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=764
05/30/2022 11:39:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=769
05/30/2022 11:39:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=774
05/30/2022 11:39:58 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 11:40:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.47 on epoch=779
05/30/2022 11:40:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
05/30/2022 11:40:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=789
05/30/2022 11:40:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=794
05/30/2022 11:40:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=799
05/30/2022 11:40:08 - INFO - __main__ - Global step 1600 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 11:40:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=804
05/30/2022 11:40:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=809
05/30/2022 11:40:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=814
05/30/2022 11:40:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
05/30/2022 11:40:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=824
05/30/2022 11:40:19 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 11:40:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=829
05/30/2022 11:40:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=834
05/30/2022 11:40:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
05/30/2022 11:40:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=844
05/30/2022 11:40:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
05/30/2022 11:40:29 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 11:40:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=854
05/30/2022 11:40:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=859
05/30/2022 11:40:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/30/2022 11:40:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=869
05/30/2022 11:40:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=874
05/30/2022 11:40:40 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 11:40:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=879
05/30/2022 11:40:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=884
05/30/2022 11:40:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
05/30/2022 11:40:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=894
05/30/2022 11:40:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
05/30/2022 11:40:50 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 11:40:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=904
05/30/2022 11:40:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=909
05/30/2022 11:40:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=914
05/30/2022 11:40:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=919
05/30/2022 11:41:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=924
05/30/2022 11:41:00 - INFO - __main__ - Global step 1850 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 11:41:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
05/30/2022 11:41:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=934
05/30/2022 11:41:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=939
05/30/2022 11:41:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=944
05/30/2022 11:41:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/30/2022 11:41:11 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 11:41:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
05/30/2022 11:41:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
05/30/2022 11:41:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.34 on epoch=964
05/30/2022 11:41:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=969
05/30/2022 11:41:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=974
05/30/2022 11:41:21 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 11:41:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
05/30/2022 11:41:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/30/2022 11:41:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/30/2022 11:41:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=994
05/30/2022 11:41:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=999
05/30/2022 11:41:31 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 11:41:31 - INFO - __main__ - save last model!
05/30/2022 11:41:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 11:41:31 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 11:41:31 - INFO - __main__ - Printing 3 examples
05/30/2022 11:41:31 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:41:31 - INFO - __main__ - ['entailed']
05/30/2022 11:41:31 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:41:31 - INFO - __main__ - ['entailed']
05/30/2022 11:41:31 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:41:31 - INFO - __main__ - ['entailed']
05/30/2022 11:41:31 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:41:32 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:41:32 - INFO - __main__ - Printing 3 examples
05/30/2022 11:41:32 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 11:41:32 - INFO - __main__ - ['refuted']
05/30/2022 11:41:32 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 11:41:32 - INFO - __main__ - ['refuted']
05/30/2022 11:41:32 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 11:41:32 - INFO - __main__ - ['refuted']
05/30/2022 11:41:32 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:41:32 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:41:32 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 11:41:32 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:41:32 - INFO - __main__ - Printing 3 examples
05/30/2022 11:41:32 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/30/2022 11:41:32 - INFO - __main__ - ['refuted']
05/30/2022 11:41:32 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/30/2022 11:41:32 - INFO - __main__ - ['refuted']
05/30/2022 11:41:32 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/30/2022 11:41:32 - INFO - __main__ - ['refuted']
05/30/2022 11:41:32 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:41:32 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:41:32 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 11:41:38 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 11:41:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 11:41:38 - INFO - __main__ - Starting training!
05/30/2022 11:41:56 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:42:08 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 11:46:15 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.4_8_predictions.txt
05/30/2022 11:46:16 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 11:46:16 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 11:46:16 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.3, bsz=8 ...
05/30/2022 11:46:17 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:46:17 - INFO - __main__ - Printing 3 examples
05/30/2022 11:46:17 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 11:46:17 - INFO - __main__ - ['refuted']
05/30/2022 11:46:17 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 11:46:17 - INFO - __main__ - ['refuted']
05/30/2022 11:46:17 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 11:46:17 - INFO - __main__ - ['refuted']
05/30/2022 11:46:17 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:46:17 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:46:17 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 11:46:17 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:46:17 - INFO - __main__ - Printing 3 examples
05/30/2022 11:46:17 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/30/2022 11:46:17 - INFO - __main__ - ['refuted']
05/30/2022 11:46:17 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/30/2022 11:46:17 - INFO - __main__ - ['refuted']
05/30/2022 11:46:17 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/30/2022 11:46:17 - INFO - __main__ - ['refuted']
05/30/2022 11:46:17 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:46:17 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:46:17 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 11:46:23 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 11:46:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 11:46:23 - INFO - __main__ - Starting training!
05/30/2022 11:46:25 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/30/2022 11:46:27 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/30/2022 11:46:29 - INFO - __main__ - Step 30 Global step 30 Train loss 4.98 on epoch=14
05/30/2022 11:46:31 - INFO - __main__ - Step 40 Global step 40 Train loss 4.81 on epoch=19
05/30/2022 11:46:33 - INFO - __main__ - Step 50 Global step 50 Train loss 4.73 on epoch=24
05/30/2022 11:46:35 - INFO - __main__ - Global step 50 Train loss 4.90 Classification-F1 0.0 on epoch=24
05/30/2022 11:46:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 11:46:37 - INFO - __main__ - Step 60 Global step 60 Train loss 4.69 on epoch=29
05/30/2022 11:46:39 - INFO - __main__ - Step 70 Global step 70 Train loss 4.64 on epoch=34
05/30/2022 11:46:41 - INFO - __main__ - Step 80 Global step 80 Train loss 4.54 on epoch=39
05/30/2022 11:46:43 - INFO - __main__ - Step 90 Global step 90 Train loss 4.52 on epoch=44
05/30/2022 11:46:44 - INFO - __main__ - Step 100 Global step 100 Train loss 4.46 on epoch=49
05/30/2022 11:46:46 - INFO - __main__ - Global step 100 Train loss 4.57 Classification-F1 0.0 on epoch=49
05/30/2022 11:46:48 - INFO - __main__ - Step 110 Global step 110 Train loss 4.43 on epoch=54
05/30/2022 11:46:50 - INFO - __main__ - Step 120 Global step 120 Train loss 4.40 on epoch=59
05/30/2022 11:46:51 - INFO - __main__ - Step 130 Global step 130 Train loss 4.23 on epoch=64
05/30/2022 11:46:53 - INFO - __main__ - Step 140 Global step 140 Train loss 4.11 on epoch=69
05/30/2022 11:46:55 - INFO - __main__ - Step 150 Global step 150 Train loss 4.04 on epoch=74
05/30/2022 11:46:57 - INFO - __main__ - Global step 150 Train loss 4.24 Classification-F1 0.0 on epoch=74
05/30/2022 11:46:59 - INFO - __main__ - Step 160 Global step 160 Train loss 4.00 on epoch=79
05/30/2022 11:47:01 - INFO - __main__ - Step 170 Global step 170 Train loss 3.86 on epoch=84
05/30/2022 11:47:03 - INFO - __main__ - Step 180 Global step 180 Train loss 3.70 on epoch=89
05/30/2022 11:47:05 - INFO - __main__ - Step 190 Global step 190 Train loss 3.73 on epoch=94
05/30/2022 11:47:06 - INFO - __main__ - Step 200 Global step 200 Train loss 3.67 on epoch=99
05/30/2022 11:47:08 - INFO - __main__ - Global step 200 Train loss 3.79 Classification-F1 0.0 on epoch=99
05/30/2022 11:47:10 - INFO - __main__ - Step 210 Global step 210 Train loss 3.64 on epoch=104
05/30/2022 11:47:12 - INFO - __main__ - Step 220 Global step 220 Train loss 3.49 on epoch=109
05/30/2022 11:47:14 - INFO - __main__ - Step 230 Global step 230 Train loss 3.41 on epoch=114
05/30/2022 11:47:16 - INFO - __main__ - Step 240 Global step 240 Train loss 3.38 on epoch=119
05/30/2022 11:47:18 - INFO - __main__ - Step 250 Global step 250 Train loss 3.28 on epoch=124
05/30/2022 11:47:19 - INFO - __main__ - Global step 250 Train loss 3.44 Classification-F1 0.16666666666666669 on epoch=124
05/30/2022 11:47:19 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.16666666666666669 on epoch=124, global_step=250
05/30/2022 11:47:21 - INFO - __main__ - Step 260 Global step 260 Train loss 3.10 on epoch=129
05/30/2022 11:47:23 - INFO - __main__ - Step 270 Global step 270 Train loss 3.12 on epoch=134
05/30/2022 11:47:25 - INFO - __main__ - Step 280 Global step 280 Train loss 3.04 on epoch=139
05/30/2022 11:47:27 - INFO - __main__ - Step 290 Global step 290 Train loss 2.97 on epoch=144
05/30/2022 11:47:29 - INFO - __main__ - Step 300 Global step 300 Train loss 2.84 on epoch=149
05/30/2022 11:47:31 - INFO - __main__ - Global step 300 Train loss 3.01 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 11:47:31 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666669 -> 0.3333333333333333 on epoch=149, global_step=300
05/30/2022 11:47:33 - INFO - __main__ - Step 310 Global step 310 Train loss 2.76 on epoch=154
05/30/2022 11:47:35 - INFO - __main__ - Step 320 Global step 320 Train loss 2.74 on epoch=159
05/30/2022 11:47:37 - INFO - __main__ - Step 330 Global step 330 Train loss 2.53 on epoch=164
05/30/2022 11:47:39 - INFO - __main__ - Step 340 Global step 340 Train loss 2.35 on epoch=169
05/30/2022 11:47:41 - INFO - __main__ - Step 350 Global step 350 Train loss 2.26 on epoch=174
05/30/2022 11:47:44 - INFO - __main__ - Global step 350 Train loss 2.53 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 11:47:46 - INFO - __main__ - Step 360 Global step 360 Train loss 2.25 on epoch=179
05/30/2022 11:47:48 - INFO - __main__ - Step 370 Global step 370 Train loss 2.33 on epoch=184
05/30/2022 11:47:49 - INFO - __main__ - Step 380 Global step 380 Train loss 2.10 on epoch=189
05/30/2022 11:47:51 - INFO - __main__ - Step 390 Global step 390 Train loss 2.02 on epoch=194
05/30/2022 11:47:53 - INFO - __main__ - Step 400 Global step 400 Train loss 1.97 on epoch=199
05/30/2022 11:47:56 - INFO - __main__ - Global step 400 Train loss 2.14 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 11:47:58 - INFO - __main__ - Step 410 Global step 410 Train loss 1.92 on epoch=204
05/30/2022 11:48:00 - INFO - __main__ - Step 420 Global step 420 Train loss 1.88 on epoch=209
05/30/2022 11:48:02 - INFO - __main__ - Step 430 Global step 430 Train loss 1.84 on epoch=214
05/30/2022 11:48:04 - INFO - __main__ - Step 440 Global step 440 Train loss 1.75 on epoch=219
05/30/2022 11:48:06 - INFO - __main__ - Step 450 Global step 450 Train loss 1.66 on epoch=224
05/30/2022 11:48:08 - INFO - __main__ - Global step 450 Train loss 1.81 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 11:48:10 - INFO - __main__ - Step 460 Global step 460 Train loss 1.61 on epoch=229
05/30/2022 11:48:12 - INFO - __main__ - Step 470 Global step 470 Train loss 1.55 on epoch=234
05/30/2022 11:48:14 - INFO - __main__ - Step 480 Global step 480 Train loss 1.51 on epoch=239
05/30/2022 11:48:16 - INFO - __main__ - Step 490 Global step 490 Train loss 1.53 on epoch=244
05/30/2022 11:48:18 - INFO - __main__ - Step 500 Global step 500 Train loss 1.37 on epoch=249
05/30/2022 11:48:19 - INFO - __main__ - Global step 500 Train loss 1.51 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 11:48:21 - INFO - __main__ - Step 510 Global step 510 Train loss 1.41 on epoch=254
05/30/2022 11:48:23 - INFO - __main__ - Step 520 Global step 520 Train loss 1.32 on epoch=259
05/30/2022 11:48:25 - INFO - __main__ - Step 530 Global step 530 Train loss 1.32 on epoch=264
05/30/2022 11:48:27 - INFO - __main__ - Step 540 Global step 540 Train loss 1.21 on epoch=269
05/30/2022 11:48:29 - INFO - __main__ - Step 550 Global step 550 Train loss 1.13 on epoch=274
05/30/2022 11:48:30 - INFO - __main__ - Global step 550 Train loss 1.28 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 11:48:32 - INFO - __main__ - Step 560 Global step 560 Train loss 1.17 on epoch=279
05/30/2022 11:48:34 - INFO - __main__ - Step 570 Global step 570 Train loss 1.10 on epoch=284
05/30/2022 11:48:36 - INFO - __main__ - Step 580 Global step 580 Train loss 1.06 on epoch=289
05/30/2022 11:48:38 - INFO - __main__ - Step 590 Global step 590 Train loss 1.13 on epoch=294
05/30/2022 11:48:40 - INFO - __main__ - Step 600 Global step 600 Train loss 1.13 on epoch=299
05/30/2022 11:48:40 - INFO - __main__ - Global step 600 Train loss 1.12 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 11:48:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.99 on epoch=304
05/30/2022 11:48:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.92 on epoch=309
05/30/2022 11:48:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.96 on epoch=314
05/30/2022 11:48:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.87 on epoch=319
05/30/2022 11:48:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.94 on epoch=324
05/30/2022 11:48:51 - INFO - __main__ - Global step 650 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 11:48:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.95 on epoch=329
05/30/2022 11:48:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.84 on epoch=334
05/30/2022 11:48:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.87 on epoch=339
05/30/2022 11:48:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.79 on epoch=344
05/30/2022 11:49:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.79 on epoch=349
05/30/2022 11:49:01 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 11:49:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.78 on epoch=354
05/30/2022 11:49:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.76 on epoch=359
05/30/2022 11:49:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.76 on epoch=364
05/30/2022 11:49:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.75 on epoch=369
05/30/2022 11:49:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.72 on epoch=374
05/30/2022 11:49:12 - INFO - __main__ - Global step 750 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 11:49:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.73 on epoch=379
05/30/2022 11:49:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=384
05/30/2022 11:49:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.71 on epoch=389
05/30/2022 11:49:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.58 on epoch=394
05/30/2022 11:49:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.63 on epoch=399
05/30/2022 11:49:23 - INFO - __main__ - Global step 800 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 11:49:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.68 on epoch=404
05/30/2022 11:49:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.69 on epoch=409
05/30/2022 11:49:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.71 on epoch=414
05/30/2022 11:49:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.65 on epoch=419
05/30/2022 11:49:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.69 on epoch=424
05/30/2022 11:49:33 - INFO - __main__ - Global step 850 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 11:49:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.55 on epoch=429
05/30/2022 11:49:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.57 on epoch=434
05/30/2022 11:49:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.56 on epoch=439
05/30/2022 11:49:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.59 on epoch=444
05/30/2022 11:49:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=449
05/30/2022 11:49:44 - INFO - __main__ - Global step 900 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 11:49:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.60 on epoch=454
05/30/2022 11:49:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.60 on epoch=459
05/30/2022 11:49:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.56 on epoch=464
05/30/2022 11:49:51 - INFO - __main__ - Step 940 Global step 940 Train loss 0.53 on epoch=469
05/30/2022 11:49:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.57 on epoch=474
05/30/2022 11:49:54 - INFO - __main__ - Global step 950 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 11:49:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.53 on epoch=479
05/30/2022 11:49:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=484
05/30/2022 11:50:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=489
05/30/2022 11:50:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=494
05/30/2022 11:50:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.55 on epoch=499
05/30/2022 11:50:05 - INFO - __main__ - Global step 1000 Train loss 0.51 Classification-F1 0.3992490613266583 on epoch=499
05/30/2022 11:50:05 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=499, global_step=1000
05/30/2022 11:50:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=504
05/30/2022 11:50:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.53 on epoch=509
05/30/2022 11:50:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=514
05/30/2022 11:50:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=519
05/30/2022 11:50:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=524
05/30/2022 11:50:15 - INFO - __main__ - Global step 1050 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 11:50:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.52 on epoch=529
05/30/2022 11:50:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.50 on epoch=534
05/30/2022 11:50:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.56 on epoch=539
05/30/2022 11:50:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=544
05/30/2022 11:50:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=549
05/30/2022 11:50:26 - INFO - __main__ - Global step 1100 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 11:50:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=554
05/30/2022 11:50:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=559
05/30/2022 11:50:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.49 on epoch=564
05/30/2022 11:50:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=569
05/30/2022 11:50:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=574
05/30/2022 11:50:36 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 11:50:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=579
05/30/2022 11:50:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=584
05/30/2022 11:50:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
05/30/2022 11:50:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.50 on epoch=594
05/30/2022 11:50:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=599
05/30/2022 11:50:47 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 11:50:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.48 on epoch=604
05/30/2022 11:50:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
05/30/2022 11:50:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=614
05/30/2022 11:50:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=619
05/30/2022 11:50:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=624
05/30/2022 11:50:57 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 11:50:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=629
05/30/2022 11:51:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.52 on epoch=634
05/30/2022 11:51:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=639
05/30/2022 11:51:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=644
05/30/2022 11:51:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.49 on epoch=649
05/30/2022 11:51:07 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 11:51:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=654
05/30/2022 11:51:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=659
05/30/2022 11:51:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
05/30/2022 11:51:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=669
05/30/2022 11:51:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=674
05/30/2022 11:51:18 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 11:51:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=679
05/30/2022 11:51:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
05/30/2022 11:51:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=689
05/30/2022 11:51:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=694
05/30/2022 11:51:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=699
05/30/2022 11:51:28 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 11:51:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=704
05/30/2022 11:51:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=709
05/30/2022 11:51:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
05/30/2022 11:51:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=719
05/30/2022 11:51:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
05/30/2022 11:51:38 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 11:51:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=729
05/30/2022 11:51:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=734
05/30/2022 11:51:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/30/2022 11:51:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
05/30/2022 11:51:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=749
05/30/2022 11:51:49 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 11:51:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=754
05/30/2022 11:51:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=759
05/30/2022 11:51:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=764
05/30/2022 11:51:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=769
05/30/2022 11:51:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=774
05/30/2022 11:51:59 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 11:52:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=779
05/30/2022 11:52:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=784
05/30/2022 11:52:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
05/30/2022 11:52:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=794
05/30/2022 11:52:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=799
05/30/2022 11:52:09 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 11:52:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=804
05/30/2022 11:52:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=809
05/30/2022 11:52:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=814
05/30/2022 11:52:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
05/30/2022 11:52:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=824
05/30/2022 11:52:20 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 11:52:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
05/30/2022 11:52:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=834
05/30/2022 11:52:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
05/30/2022 11:52:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
05/30/2022 11:52:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=849
05/30/2022 11:52:30 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.4589371980676329 on epoch=849
05/30/2022 11:52:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4589371980676329 on epoch=849, global_step=1700
05/30/2022 11:52:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/30/2022 11:52:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=859
05/30/2022 11:52:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=864
05/30/2022 11:52:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=869
05/30/2022 11:52:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
05/30/2022 11:52:40 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.4181818181818182 on epoch=874
05/30/2022 11:52:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=879
05/30/2022 11:52:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=884
05/30/2022 11:52:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=889
05/30/2022 11:52:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
05/30/2022 11:52:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=899
05/30/2022 11:52:50 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.3816425120772947 on epoch=899
05/30/2022 11:52:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=904
05/30/2022 11:52:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=909
05/30/2022 11:52:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=914
05/30/2022 11:52:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=919
05/30/2022 11:53:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
05/30/2022 11:53:01 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.4385964912280702 on epoch=924
05/30/2022 11:53:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=929
05/30/2022 11:53:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=934
05/30/2022 11:53:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=939
05/30/2022 11:53:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
05/30/2022 11:53:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=949
05/30/2022 11:53:11 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.46843853820598 on epoch=949
05/30/2022 11:53:11 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.46843853820598 on epoch=949, global_step=1900
05/30/2022 11:53:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
05/30/2022 11:53:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=959
05/30/2022 11:53:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
05/30/2022 11:53:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
05/30/2022 11:53:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/30/2022 11:53:21 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 11:53:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=979
05/30/2022 11:53:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=984
05/30/2022 11:53:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=989
05/30/2022 11:53:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.34 on epoch=994
05/30/2022 11:53:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=999
05/30/2022 11:53:32 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 11:53:32 - INFO - __main__ - save last model!
05/30/2022 11:53:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 11:53:32 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 11:53:32 - INFO - __main__ - Printing 3 examples
05/30/2022 11:53:32 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:53:32 - INFO - __main__ - ['entailed']
05/30/2022 11:53:32 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:53:32 - INFO - __main__ - ['entailed']
05/30/2022 11:53:32 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:53:32 - INFO - __main__ - ['entailed']
05/30/2022 11:53:32 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:53:32 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:53:32 - INFO - __main__ - Printing 3 examples
05/30/2022 11:53:32 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 11:53:32 - INFO - __main__ - ['refuted']
05/30/2022 11:53:32 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 11:53:32 - INFO - __main__ - ['refuted']
05/30/2022 11:53:32 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 11:53:32 - INFO - __main__ - ['refuted']
05/30/2022 11:53:32 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:53:32 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:53:32 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 11:53:32 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:53:32 - INFO - __main__ - Printing 3 examples
05/30/2022 11:53:32 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/30/2022 11:53:32 - INFO - __main__ - ['refuted']
05/30/2022 11:53:32 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/30/2022 11:53:32 - INFO - __main__ - ['refuted']
05/30/2022 11:53:32 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/30/2022 11:53:32 - INFO - __main__ - ['refuted']
05/30/2022 11:53:32 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:53:32 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:53:32 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 11:53:38 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 11:53:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 11:53:38 - INFO - __main__ - Starting training!
05/30/2022 11:53:56 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:54:09 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 11:58:18 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.3_8_predictions.txt
05/30/2022 11:58:18 - INFO - __main__ - Classification-F1 on test data: 0.3312
05/30/2022 11:58:19 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.3, bsz=8, dev_performance=0.46843853820598, test_performance=0.3311539798652483
05/30/2022 11:58:19 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.2, bsz=8 ...
05/30/2022 11:58:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:58:20 - INFO - __main__ - Printing 3 examples
05/30/2022 11:58:20 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 11:58:20 - INFO - __main__ - ['refuted']
05/30/2022 11:58:20 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 11:58:20 - INFO - __main__ - ['refuted']
05/30/2022 11:58:20 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 11:58:20 - INFO - __main__ - ['refuted']
05/30/2022 11:58:20 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:58:20 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:58:20 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 11:58:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 11:58:20 - INFO - __main__ - Printing 3 examples
05/30/2022 11:58:20 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/30/2022 11:58:20 - INFO - __main__ - ['refuted']
05/30/2022 11:58:20 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/30/2022 11:58:20 - INFO - __main__ - ['refuted']
05/30/2022 11:58:20 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/30/2022 11:58:20 - INFO - __main__ - ['refuted']
05/30/2022 11:58:20 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:58:20 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:58:20 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 11:58:26 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 11:58:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 11:58:26 - INFO - __main__ - Starting training!
05/30/2022 11:58:28 - INFO - __main__ - Step 10 Global step 10 Train loss 5.08 on epoch=4
05/30/2022 11:58:30 - INFO - __main__ - Step 20 Global step 20 Train loss 4.89 on epoch=9
05/30/2022 11:58:32 - INFO - __main__ - Step 30 Global step 30 Train loss 4.90 on epoch=14
05/30/2022 11:58:34 - INFO - __main__ - Step 40 Global step 40 Train loss 4.80 on epoch=19
05/30/2022 11:58:36 - INFO - __main__ - Step 50 Global step 50 Train loss 4.85 on epoch=24
05/30/2022 11:58:37 - INFO - __main__ - Global step 50 Train loss 4.90 Classification-F1 0.0 on epoch=24
05/30/2022 11:58:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 11:58:39 - INFO - __main__ - Step 60 Global step 60 Train loss 4.85 on epoch=29
05/30/2022 11:58:41 - INFO - __main__ - Step 70 Global step 70 Train loss 4.71 on epoch=34
05/30/2022 11:58:43 - INFO - __main__ - Step 80 Global step 80 Train loss 4.64 on epoch=39
05/30/2022 11:58:44 - INFO - __main__ - Step 90 Global step 90 Train loss 4.51 on epoch=44
05/30/2022 11:58:46 - INFO - __main__ - Step 100 Global step 100 Train loss 4.61 on epoch=49
05/30/2022 11:58:47 - INFO - __main__ - Global step 100 Train loss 4.66 Classification-F1 0.0 on epoch=49
05/30/2022 11:58:49 - INFO - __main__ - Step 110 Global step 110 Train loss 4.45 on epoch=54
05/30/2022 11:58:51 - INFO - __main__ - Step 120 Global step 120 Train loss 4.38 on epoch=59
05/30/2022 11:58:53 - INFO - __main__ - Step 130 Global step 130 Train loss 4.29 on epoch=64
05/30/2022 11:58:55 - INFO - __main__ - Step 140 Global step 140 Train loss 4.30 on epoch=69
05/30/2022 11:58:57 - INFO - __main__ - Step 150 Global step 150 Train loss 4.19 on epoch=74
05/30/2022 11:59:00 - INFO - __main__ - Global step 150 Train loss 4.32 Classification-F1 0.0 on epoch=74
05/30/2022 11:59:02 - INFO - __main__ - Step 160 Global step 160 Train loss 4.07 on epoch=79
05/30/2022 11:59:04 - INFO - __main__ - Step 170 Global step 170 Train loss 4.13 on epoch=84
05/30/2022 11:59:06 - INFO - __main__ - Step 180 Global step 180 Train loss 4.10 on epoch=89
05/30/2022 11:59:08 - INFO - __main__ - Step 190 Global step 190 Train loss 4.02 on epoch=94
05/30/2022 11:59:10 - INFO - __main__ - Step 200 Global step 200 Train loss 4.03 on epoch=99
05/30/2022 11:59:11 - INFO - __main__ - Global step 200 Train loss 4.07 Classification-F1 0.0 on epoch=99
05/30/2022 11:59:13 - INFO - __main__ - Step 210 Global step 210 Train loss 3.87 on epoch=104
05/30/2022 11:59:15 - INFO - __main__ - Step 220 Global step 220 Train loss 3.78 on epoch=109
05/30/2022 11:59:17 - INFO - __main__ - Step 230 Global step 230 Train loss 3.77 on epoch=114
05/30/2022 11:59:19 - INFO - __main__ - Step 240 Global step 240 Train loss 3.81 on epoch=119
05/30/2022 11:59:21 - INFO - __main__ - Step 250 Global step 250 Train loss 3.79 on epoch=124
05/30/2022 11:59:22 - INFO - __main__ - Global step 250 Train loss 3.80 Classification-F1 0.0 on epoch=124
05/30/2022 11:59:24 - INFO - __main__ - Step 260 Global step 260 Train loss 3.74 on epoch=129
05/30/2022 11:59:26 - INFO - __main__ - Step 270 Global step 270 Train loss 3.69 on epoch=134
05/30/2022 11:59:28 - INFO - __main__ - Step 280 Global step 280 Train loss 3.61 on epoch=139
05/30/2022 11:59:30 - INFO - __main__ - Step 290 Global step 290 Train loss 3.55 on epoch=144
05/30/2022 11:59:31 - INFO - __main__ - Step 300 Global step 300 Train loss 3.56 on epoch=149
05/30/2022 11:59:33 - INFO - __main__ - Global step 300 Train loss 3.63 Classification-F1 0.0 on epoch=149
05/30/2022 11:59:35 - INFO - __main__ - Step 310 Global step 310 Train loss 3.53 on epoch=154
05/30/2022 11:59:37 - INFO - __main__ - Step 320 Global step 320 Train loss 3.50 on epoch=159
05/30/2022 11:59:39 - INFO - __main__ - Step 330 Global step 330 Train loss 3.40 on epoch=164
05/30/2022 11:59:40 - INFO - __main__ - Step 340 Global step 340 Train loss 3.39 on epoch=169
05/30/2022 11:59:42 - INFO - __main__ - Step 350 Global step 350 Train loss 3.28 on epoch=174
05/30/2022 11:59:54 - INFO - __main__ - Global step 350 Train loss 3.42 Classification-F1 0.019047619047619046 on epoch=174
05/30/2022 11:59:54 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.019047619047619046 on epoch=174, global_step=350
05/30/2022 11:59:55 - INFO - __main__ - Step 360 Global step 360 Train loss 3.39 on epoch=179
05/30/2022 11:59:57 - INFO - __main__ - Step 370 Global step 370 Train loss 3.30 on epoch=184
05/30/2022 11:59:59 - INFO - __main__ - Step 380 Global step 380 Train loss 3.25 on epoch=189
05/30/2022 12:00:01 - INFO - __main__ - Step 390 Global step 390 Train loss 3.12 on epoch=194
05/30/2022 12:00:03 - INFO - __main__ - Step 400 Global step 400 Train loss 3.18 on epoch=199
05/30/2022 12:00:10 - INFO - __main__ - Global step 400 Train loss 3.25 Classification-F1 0.10077519379844961 on epoch=199
05/30/2022 12:00:10 - INFO - __main__ - Saving model with best Classification-F1: 0.019047619047619046 -> 0.10077519379844961 on epoch=199, global_step=400
05/30/2022 12:00:12 - INFO - __main__ - Step 410 Global step 410 Train loss 3.02 on epoch=204
05/30/2022 12:00:14 - INFO - __main__ - Step 420 Global step 420 Train loss 3.11 on epoch=209
05/30/2022 12:00:16 - INFO - __main__ - Step 430 Global step 430 Train loss 2.97 on epoch=214
05/30/2022 12:00:18 - INFO - __main__ - Step 440 Global step 440 Train loss 2.92 on epoch=219
05/30/2022 12:00:20 - INFO - __main__ - Step 450 Global step 450 Train loss 2.92 on epoch=224
05/30/2022 12:00:23 - INFO - __main__ - Global step 450 Train loss 2.99 Classification-F1 0.09677419354838708 on epoch=224
05/30/2022 12:00:25 - INFO - __main__ - Step 460 Global step 460 Train loss 2.84 on epoch=229
05/30/2022 12:00:27 - INFO - __main__ - Step 470 Global step 470 Train loss 2.84 on epoch=234
05/30/2022 12:00:29 - INFO - __main__ - Step 480 Global step 480 Train loss 2.77 on epoch=239
05/30/2022 12:00:31 - INFO - __main__ - Step 490 Global step 490 Train loss 2.70 on epoch=244
05/30/2022 12:00:32 - INFO - __main__ - Step 500 Global step 500 Train loss 2.73 on epoch=249
05/30/2022 12:00:36 - INFO - __main__ - Global step 500 Train loss 2.78 Classification-F1 0.1627906976744186 on epoch=249
05/30/2022 12:00:36 - INFO - __main__ - Saving model with best Classification-F1: 0.10077519379844961 -> 0.1627906976744186 on epoch=249, global_step=500
05/30/2022 12:00:38 - INFO - __main__ - Step 510 Global step 510 Train loss 2.57 on epoch=254
05/30/2022 12:00:40 - INFO - __main__ - Step 520 Global step 520 Train loss 2.52 on epoch=259
05/30/2022 12:00:41 - INFO - __main__ - Step 530 Global step 530 Train loss 2.47 on epoch=264
05/30/2022 12:00:43 - INFO - __main__ - Step 540 Global step 540 Train loss 2.53 on epoch=269
05/30/2022 12:00:45 - INFO - __main__ - Step 550 Global step 550 Train loss 2.37 on epoch=274
05/30/2022 12:00:48 - INFO - __main__ - Global step 550 Train loss 2.49 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 12:00:48 - INFO - __main__ - Saving model with best Classification-F1: 0.1627906976744186 -> 0.3333333333333333 on epoch=274, global_step=550
05/30/2022 12:00:50 - INFO - __main__ - Step 560 Global step 560 Train loss 2.39 on epoch=279
05/30/2022 12:00:52 - INFO - __main__ - Step 570 Global step 570 Train loss 2.23 on epoch=284
05/30/2022 12:00:54 - INFO - __main__ - Step 580 Global step 580 Train loss 2.35 on epoch=289
05/30/2022 12:00:56 - INFO - __main__ - Step 590 Global step 590 Train loss 2.20 on epoch=294
05/30/2022 12:00:58 - INFO - __main__ - Step 600 Global step 600 Train loss 2.25 on epoch=299
05/30/2022 12:01:00 - INFO - __main__ - Global step 600 Train loss 2.28 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 12:01:02 - INFO - __main__ - Step 610 Global step 610 Train loss 2.21 on epoch=304
05/30/2022 12:01:04 - INFO - __main__ - Step 620 Global step 620 Train loss 2.17 on epoch=309
05/30/2022 12:01:06 - INFO - __main__ - Step 630 Global step 630 Train loss 2.05 on epoch=314
05/30/2022 12:01:08 - INFO - __main__ - Step 640 Global step 640 Train loss 2.15 on epoch=319
05/30/2022 12:01:10 - INFO - __main__ - Step 650 Global step 650 Train loss 1.94 on epoch=324
05/30/2022 12:01:13 - INFO - __main__ - Global step 650 Train loss 2.10 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 12:01:15 - INFO - __main__ - Step 660 Global step 660 Train loss 1.86 on epoch=329
05/30/2022 12:01:17 - INFO - __main__ - Step 670 Global step 670 Train loss 1.91 on epoch=334
05/30/2022 12:01:19 - INFO - __main__ - Step 680 Global step 680 Train loss 1.90 on epoch=339
05/30/2022 12:01:20 - INFO - __main__ - Step 690 Global step 690 Train loss 1.85 on epoch=344
05/30/2022 12:01:22 - INFO - __main__ - Step 700 Global step 700 Train loss 1.86 on epoch=349
05/30/2022 12:01:24 - INFO - __main__ - Global step 700 Train loss 1.88 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 12:01:26 - INFO - __main__ - Step 710 Global step 710 Train loss 1.78 on epoch=354
05/30/2022 12:01:28 - INFO - __main__ - Step 720 Global step 720 Train loss 1.74 on epoch=359
05/30/2022 12:01:30 - INFO - __main__ - Step 730 Global step 730 Train loss 1.64 on epoch=364
05/30/2022 12:01:32 - INFO - __main__ - Step 740 Global step 740 Train loss 1.68 on epoch=369
05/30/2022 12:01:34 - INFO - __main__ - Step 750 Global step 750 Train loss 1.70 on epoch=374
05/30/2022 12:01:36 - INFO - __main__ - Global step 750 Train loss 1.71 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 12:01:38 - INFO - __main__ - Step 760 Global step 760 Train loss 1.58 on epoch=379
05/30/2022 12:01:40 - INFO - __main__ - Step 770 Global step 770 Train loss 1.58 on epoch=384
05/30/2022 12:01:41 - INFO - __main__ - Step 780 Global step 780 Train loss 1.53 on epoch=389
05/30/2022 12:01:43 - INFO - __main__ - Step 790 Global step 790 Train loss 1.54 on epoch=394
05/30/2022 12:01:45 - INFO - __main__ - Step 800 Global step 800 Train loss 1.53 on epoch=399
05/30/2022 12:01:46 - INFO - __main__ - Global step 800 Train loss 1.55 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 12:01:48 - INFO - __main__ - Step 810 Global step 810 Train loss 1.34 on epoch=404
05/30/2022 12:01:50 - INFO - __main__ - Step 820 Global step 820 Train loss 1.51 on epoch=409
05/30/2022 12:01:52 - INFO - __main__ - Step 830 Global step 830 Train loss 1.44 on epoch=414
05/30/2022 12:01:54 - INFO - __main__ - Step 840 Global step 840 Train loss 1.47 on epoch=419
05/30/2022 12:01:56 - INFO - __main__ - Step 850 Global step 850 Train loss 1.53 on epoch=424
05/30/2022 12:01:57 - INFO - __main__ - Global step 850 Train loss 1.46 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 12:01:58 - INFO - __main__ - Step 860 Global step 860 Train loss 1.37 on epoch=429
05/30/2022 12:02:00 - INFO - __main__ - Step 870 Global step 870 Train loss 1.39 on epoch=434
05/30/2022 12:02:02 - INFO - __main__ - Step 880 Global step 880 Train loss 1.44 on epoch=439
05/30/2022 12:02:04 - INFO - __main__ - Step 890 Global step 890 Train loss 1.29 on epoch=444
05/30/2022 12:02:06 - INFO - __main__ - Step 900 Global step 900 Train loss 1.30 on epoch=449
05/30/2022 12:02:07 - INFO - __main__ - Global step 900 Train loss 1.36 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 12:02:09 - INFO - __main__ - Step 910 Global step 910 Train loss 1.47 on epoch=454
05/30/2022 12:02:11 - INFO - __main__ - Step 920 Global step 920 Train loss 1.41 on epoch=459
05/30/2022 12:02:13 - INFO - __main__ - Step 930 Global step 930 Train loss 1.41 on epoch=464
05/30/2022 12:02:15 - INFO - __main__ - Step 940 Global step 940 Train loss 1.38 on epoch=469
05/30/2022 12:02:17 - INFO - __main__ - Step 950 Global step 950 Train loss 1.31 on epoch=474
05/30/2022 12:02:17 - INFO - __main__ - Global step 950 Train loss 1.40 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 12:02:19 - INFO - __main__ - Step 960 Global step 960 Train loss 1.34 on epoch=479
05/30/2022 12:02:21 - INFO - __main__ - Step 970 Global step 970 Train loss 1.22 on epoch=484
05/30/2022 12:02:23 - INFO - __main__ - Step 980 Global step 980 Train loss 1.37 on epoch=489
05/30/2022 12:02:25 - INFO - __main__ - Step 990 Global step 990 Train loss 1.30 on epoch=494
05/30/2022 12:02:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.21 on epoch=499
05/30/2022 12:02:29 - INFO - __main__ - Global step 1000 Train loss 1.29 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 12:02:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.26 on epoch=504
05/30/2022 12:02:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.11 on epoch=509
05/30/2022 12:02:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.13 on epoch=514
05/30/2022 12:02:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.15 on epoch=519
05/30/2022 12:02:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.04 on epoch=524
05/30/2022 12:02:40 - INFO - __main__ - Global step 1050 Train loss 1.14 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 12:02:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.10 on epoch=529
05/30/2022 12:02:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.01 on epoch=534
05/30/2022 12:02:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.10 on epoch=539
05/30/2022 12:02:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.09 on epoch=544
05/30/2022 12:02:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.05 on epoch=549
05/30/2022 12:02:50 - INFO - __main__ - Global step 1100 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 12:02:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.17 on epoch=554
05/30/2022 12:02:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.16 on epoch=559
05/30/2022 12:02:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.12 on epoch=564
05/30/2022 12:02:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.98 on epoch=569
05/30/2022 12:03:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.09 on epoch=574
05/30/2022 12:03:00 - INFO - __main__ - Global step 1150 Train loss 1.10 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 12:03:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.02 on epoch=579
05/30/2022 12:03:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.16 on epoch=584
05/30/2022 12:03:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.99 on epoch=589
05/30/2022 12:03:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.92 on epoch=594
05/30/2022 12:03:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.94 on epoch=599
05/30/2022 12:03:11 - INFO - __main__ - Global step 1200 Train loss 1.01 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 12:03:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.97 on epoch=604
05/30/2022 12:03:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.92 on epoch=609
05/30/2022 12:03:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.98 on epoch=614
05/30/2022 12:03:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.92 on epoch=619
05/30/2022 12:03:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.01 on epoch=624
05/30/2022 12:03:21 - INFO - __main__ - Global step 1250 Train loss 0.96 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 12:03:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.93 on epoch=629
05/30/2022 12:03:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.96 on epoch=634
05/30/2022 12:03:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.88 on epoch=639
05/30/2022 12:03:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.87 on epoch=644
05/30/2022 12:03:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.81 on epoch=649
05/30/2022 12:03:32 - INFO - __main__ - Global step 1300 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 12:03:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.84 on epoch=654
05/30/2022 12:03:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.79 on epoch=659
05/30/2022 12:03:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.77 on epoch=664
05/30/2022 12:03:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.82 on epoch=669
05/30/2022 12:03:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.90 on epoch=674
05/30/2022 12:03:42 - INFO - __main__ - Global step 1350 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 12:03:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.83 on epoch=679
05/30/2022 12:03:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.81 on epoch=684
05/30/2022 12:03:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.81 on epoch=689
05/30/2022 12:03:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.78 on epoch=694
05/30/2022 12:03:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.78 on epoch=699
05/30/2022 12:03:53 - INFO - __main__ - Global step 1400 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 12:03:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.73 on epoch=704
05/30/2022 12:03:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.74 on epoch=709
05/30/2022 12:03:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.84 on epoch=714
05/30/2022 12:04:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.83 on epoch=719
05/30/2022 12:04:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.82 on epoch=724
05/30/2022 12:04:03 - INFO - __main__ - Global step 1450 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 12:04:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.73 on epoch=729
05/30/2022 12:04:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.71 on epoch=734
05/30/2022 12:04:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.69 on epoch=739
05/30/2022 12:04:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.77 on epoch=744
05/30/2022 12:04:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.68 on epoch=749
05/30/2022 12:04:13 - INFO - __main__ - Global step 1500 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 12:04:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.69 on epoch=754
05/30/2022 12:04:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.71 on epoch=759
05/30/2022 12:04:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.65 on epoch=764
05/30/2022 12:04:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.72 on epoch=769
05/30/2022 12:04:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.62 on epoch=774
05/30/2022 12:04:24 - INFO - __main__ - Global step 1550 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 12:04:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.73 on epoch=779
05/30/2022 12:04:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.63 on epoch=784
05/30/2022 12:04:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.63 on epoch=789
05/30/2022 12:04:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.61 on epoch=794
05/30/2022 12:04:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.60 on epoch=799
05/30/2022 12:04:34 - INFO - __main__ - Global step 1600 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 12:04:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.65 on epoch=804
05/30/2022 12:04:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.64 on epoch=809
05/30/2022 12:04:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.58 on epoch=814
05/30/2022 12:04:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.65 on epoch=819
05/30/2022 12:04:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.63 on epoch=824
05/30/2022 12:04:45 - INFO - __main__ - Global step 1650 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 12:04:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.61 on epoch=829
05/30/2022 12:04:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.57 on epoch=834
05/30/2022 12:04:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.62 on epoch=839
05/30/2022 12:04:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.62 on epoch=844
05/30/2022 12:04:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.59 on epoch=849
05/30/2022 12:04:55 - INFO - __main__ - Global step 1700 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 12:04:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.54 on epoch=854
05/30/2022 12:04:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.62 on epoch=859
05/30/2022 12:05:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.57 on epoch=864
05/30/2022 12:05:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=869
05/30/2022 12:05:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.54 on epoch=874
05/30/2022 12:05:06 - INFO - __main__ - Global step 1750 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 12:05:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.51 on epoch=879
05/30/2022 12:05:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.59 on epoch=884
05/30/2022 12:05:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.54 on epoch=889
05/30/2022 12:05:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.51 on epoch=894
05/30/2022 12:05:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.50 on epoch=899
05/30/2022 12:05:16 - INFO - __main__ - Global step 1800 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 12:05:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.47 on epoch=904
05/30/2022 12:05:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.53 on epoch=909
05/30/2022 12:05:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.49 on epoch=914
05/30/2022 12:05:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.48 on epoch=919
05/30/2022 12:05:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.56 on epoch=924
05/30/2022 12:05:27 - INFO - __main__ - Global step 1850 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 12:05:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.51 on epoch=929
05/30/2022 12:05:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.52 on epoch=934
05/30/2022 12:05:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=939
05/30/2022 12:05:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
05/30/2022 12:05:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=949
05/30/2022 12:05:37 - INFO - __main__ - Global step 1900 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 12:05:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.57 on epoch=954
05/30/2022 12:05:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=959
05/30/2022 12:05:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.48 on epoch=964
05/30/2022 12:05:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.54 on epoch=969
05/30/2022 12:05:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=974
05/30/2022 12:05:47 - INFO - __main__ - Global step 1950 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 12:05:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=979
05/30/2022 12:05:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
05/30/2022 12:05:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=989
05/30/2022 12:05:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=994
05/30/2022 12:05:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=999
05/30/2022 12:05:58 - INFO - __main__ - Global step 2000 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 12:05:58 - INFO - __main__ - save last model!
05/30/2022 12:05:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 12:05:58 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 12:05:58 - INFO - __main__ - Printing 3 examples
05/30/2022 12:05:58 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:05:58 - INFO - __main__ - ['entailed']
05/30/2022 12:05:58 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:05:58 - INFO - __main__ - ['entailed']
05/30/2022 12:05:58 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:05:58 - INFO - __main__ - ['entailed']
05/30/2022 12:05:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:05:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:05:58 - INFO - __main__ - Printing 3 examples
05/30/2022 12:05:58 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 12:05:58 - INFO - __main__ - ['refuted']
05/30/2022 12:05:58 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 12:05:58 - INFO - __main__ - ['refuted']
05/30/2022 12:05:58 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 12:05:58 - INFO - __main__ - ['refuted']
05/30/2022 12:05:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:05:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:05:58 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 12:05:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:05:58 - INFO - __main__ - Printing 3 examples
05/30/2022 12:05:58 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/30/2022 12:05:58 - INFO - __main__ - ['refuted']
05/30/2022 12:05:58 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/30/2022 12:05:58 - INFO - __main__ - ['refuted']
05/30/2022 12:05:58 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/30/2022 12:05:58 - INFO - __main__ - ['refuted']
05/30/2022 12:05:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:05:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:05:58 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 12:06:04 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:06:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 12:06:04 - INFO - __main__ - Starting training!
05/30/2022 12:06:22 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:06:35 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 12:11:53 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.2_8_predictions.txt
05/30/2022 12:11:53 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 12:11:53 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.2, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 12:11:53 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.5, bsz=8 ...
05/30/2022 12:11:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:11:54 - INFO - __main__ - Printing 3 examples
05/30/2022 12:11:54 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 12:11:54 - INFO - __main__ - ['refuted']
05/30/2022 12:11:54 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 12:11:54 - INFO - __main__ - ['refuted']
05/30/2022 12:11:54 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 12:11:54 - INFO - __main__ - ['refuted']
05/30/2022 12:11:54 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:11:54 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:11:54 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 12:11:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:11:54 - INFO - __main__ - Printing 3 examples
05/30/2022 12:11:54 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/30/2022 12:11:54 - INFO - __main__ - ['refuted']
05/30/2022 12:11:54 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/30/2022 12:11:54 - INFO - __main__ - ['refuted']
05/30/2022 12:11:54 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/30/2022 12:11:54 - INFO - __main__ - ['refuted']
05/30/2022 12:11:54 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:11:54 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:11:54 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 12:12:00 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:12:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 12:12:00 - INFO - __main__ - Starting training!
05/30/2022 12:12:02 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/30/2022 12:12:04 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/30/2022 12:12:06 - INFO - __main__ - Step 30 Global step 30 Train loss 4.85 on epoch=14
05/30/2022 12:12:08 - INFO - __main__ - Step 40 Global step 40 Train loss 4.77 on epoch=19
05/30/2022 12:12:10 - INFO - __main__ - Step 50 Global step 50 Train loss 4.65 on epoch=24
05/30/2022 12:12:11 - INFO - __main__ - Global step 50 Train loss 4.85 Classification-F1 0.0 on epoch=24
05/30/2022 12:12:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 12:12:13 - INFO - __main__ - Step 60 Global step 60 Train loss 4.53 on epoch=29
05/30/2022 12:12:15 - INFO - __main__ - Step 70 Global step 70 Train loss 4.49 on epoch=34
05/30/2022 12:12:17 - INFO - __main__ - Step 80 Global step 80 Train loss 4.27 on epoch=39
05/30/2022 12:12:19 - INFO - __main__ - Step 90 Global step 90 Train loss 4.13 on epoch=44
05/30/2022 12:12:21 - INFO - __main__ - Step 100 Global step 100 Train loss 3.96 on epoch=49
05/30/2022 12:12:24 - INFO - __main__ - Global step 100 Train loss 4.28 Classification-F1 0.0 on epoch=49
05/30/2022 12:12:26 - INFO - __main__ - Step 110 Global step 110 Train loss 3.87 on epoch=54
05/30/2022 12:12:28 - INFO - __main__ - Step 120 Global step 120 Train loss 3.84 on epoch=59
05/30/2022 12:12:29 - INFO - __main__ - Step 130 Global step 130 Train loss 3.74 on epoch=64
05/30/2022 12:12:31 - INFO - __main__ - Step 140 Global step 140 Train loss 3.65 on epoch=69
05/30/2022 12:12:33 - INFO - __main__ - Step 150 Global step 150 Train loss 3.45 on epoch=74
05/30/2022 12:12:43 - INFO - __main__ - Global step 150 Train loss 3.71 Classification-F1 0.0 on epoch=74
05/30/2022 12:12:44 - INFO - __main__ - Step 160 Global step 160 Train loss 3.42 on epoch=79
05/30/2022 12:12:46 - INFO - __main__ - Step 170 Global step 170 Train loss 3.31 on epoch=84
05/30/2022 12:12:48 - INFO - __main__ - Step 180 Global step 180 Train loss 3.06 on epoch=89
05/30/2022 12:12:50 - INFO - __main__ - Step 190 Global step 190 Train loss 2.98 on epoch=94
05/30/2022 12:12:52 - INFO - __main__ - Step 200 Global step 200 Train loss 2.67 on epoch=99
05/30/2022 12:12:54 - INFO - __main__ - Global step 200 Train loss 3.09 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 12:12:54 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=99, global_step=200
05/30/2022 12:12:56 - INFO - __main__ - Step 210 Global step 210 Train loss 2.55 on epoch=104
05/30/2022 12:12:58 - INFO - __main__ - Step 220 Global step 220 Train loss 2.43 on epoch=109
05/30/2022 12:13:00 - INFO - __main__ - Step 230 Global step 230 Train loss 2.14 on epoch=114
05/30/2022 12:13:02 - INFO - __main__ - Step 240 Global step 240 Train loss 2.14 on epoch=119
05/30/2022 12:13:04 - INFO - __main__ - Step 250 Global step 250 Train loss 1.96 on epoch=124
05/30/2022 12:13:06 - INFO - __main__ - Global step 250 Train loss 2.24 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 12:13:08 - INFO - __main__ - Step 260 Global step 260 Train loss 1.86 on epoch=129
05/30/2022 12:13:10 - INFO - __main__ - Step 270 Global step 270 Train loss 1.60 on epoch=134
05/30/2022 12:13:12 - INFO - __main__ - Step 280 Global step 280 Train loss 1.55 on epoch=139
05/30/2022 12:13:13 - INFO - __main__ - Step 290 Global step 290 Train loss 1.55 on epoch=144
05/30/2022 12:13:15 - INFO - __main__ - Step 300 Global step 300 Train loss 1.52 on epoch=149
05/30/2022 12:13:18 - INFO - __main__ - Global step 300 Train loss 1.62 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 12:13:20 - INFO - __main__ - Step 310 Global step 310 Train loss 1.33 on epoch=154
05/30/2022 12:13:21 - INFO - __main__ - Step 320 Global step 320 Train loss 1.41 on epoch=159
05/30/2022 12:13:23 - INFO - __main__ - Step 330 Global step 330 Train loss 1.30 on epoch=164
05/30/2022 12:13:25 - INFO - __main__ - Step 340 Global step 340 Train loss 1.25 on epoch=169
05/30/2022 12:13:27 - INFO - __main__ - Step 350 Global step 350 Train loss 1.22 on epoch=174
05/30/2022 12:13:28 - INFO - __main__ - Global step 350 Train loss 1.30 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 12:13:30 - INFO - __main__ - Step 360 Global step 360 Train loss 1.21 on epoch=179
05/30/2022 12:13:32 - INFO - __main__ - Step 370 Global step 370 Train loss 1.16 on epoch=184
05/30/2022 12:13:34 - INFO - __main__ - Step 380 Global step 380 Train loss 1.19 on epoch=189
05/30/2022 12:13:36 - INFO - __main__ - Step 390 Global step 390 Train loss 1.19 on epoch=194
05/30/2022 12:13:38 - INFO - __main__ - Step 400 Global step 400 Train loss 1.01 on epoch=199
05/30/2022 12:13:39 - INFO - __main__ - Global step 400 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 12:13:41 - INFO - __main__ - Step 410 Global step 410 Train loss 1.00 on epoch=204
05/30/2022 12:13:43 - INFO - __main__ - Step 420 Global step 420 Train loss 1.01 on epoch=209
05/30/2022 12:13:45 - INFO - __main__ - Step 430 Global step 430 Train loss 1.01 on epoch=214
05/30/2022 12:13:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.85 on epoch=219
05/30/2022 12:13:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.86 on epoch=224
05/30/2022 12:13:49 - INFO - __main__ - Global step 450 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 12:13:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=229
05/30/2022 12:13:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.85 on epoch=234
05/30/2022 12:13:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.78 on epoch=239
05/30/2022 12:13:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.73 on epoch=244
05/30/2022 12:13:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.79 on epoch=249
05/30/2022 12:14:00 - INFO - __main__ - Global step 500 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 12:14:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.67 on epoch=254
05/30/2022 12:14:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.69 on epoch=259
05/30/2022 12:14:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.71 on epoch=264
05/30/2022 12:14:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=269
05/30/2022 12:14:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.68 on epoch=274
05/30/2022 12:14:10 - INFO - __main__ - Global step 550 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 12:14:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=279
05/30/2022 12:14:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.71 on epoch=284
05/30/2022 12:14:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.63 on epoch=289
05/30/2022 12:14:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.65 on epoch=294
05/30/2022 12:14:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.62 on epoch=299
05/30/2022 12:14:21 - INFO - __main__ - Global step 600 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 12:14:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.64 on epoch=304
05/30/2022 12:14:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.63 on epoch=309
05/30/2022 12:14:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.61 on epoch=314
05/30/2022 12:14:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.55 on epoch=319
05/30/2022 12:14:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.52 on epoch=324
05/30/2022 12:14:31 - INFO - __main__ - Global step 650 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 12:14:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=329
05/30/2022 12:14:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=334
05/30/2022 12:14:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.57 on epoch=339
05/30/2022 12:14:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.54 on epoch=344
05/30/2022 12:14:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=349
05/30/2022 12:14:41 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 12:14:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=354
05/30/2022 12:14:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.50 on epoch=359
05/30/2022 12:14:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=364
05/30/2022 12:14:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=369
05/30/2022 12:14:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=374
05/30/2022 12:14:51 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 12:14:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=379
05/30/2022 12:14:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.51 on epoch=384
05/30/2022 12:14:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.54 on epoch=389
05/30/2022 12:14:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=394
05/30/2022 12:15:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.53 on epoch=399
05/30/2022 12:15:02 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 12:15:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=404
05/30/2022 12:15:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=409
05/30/2022 12:15:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
05/30/2022 12:15:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=419
05/30/2022 12:15:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=424
05/30/2022 12:15:12 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 12:15:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=429
05/30/2022 12:15:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
05/30/2022 12:15:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=439
05/30/2022 12:15:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
05/30/2022 12:15:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=449
05/30/2022 12:15:22 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 12:15:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.49 on epoch=454
05/30/2022 12:15:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=459
05/30/2022 12:15:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=464
05/30/2022 12:15:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=469
05/30/2022 12:15:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
05/30/2022 12:15:32 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 12:15:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=479
05/30/2022 12:15:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=484
05/30/2022 12:15:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=489
05/30/2022 12:15:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=494
05/30/2022 12:15:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=499
05/30/2022 12:15:43 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 12:15:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
05/30/2022 12:15:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=509
05/30/2022 12:15:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=514
05/30/2022 12:15:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=519
05/30/2022 12:15:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=524
05/30/2022 12:15:53 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 12:15:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=529
05/30/2022 12:15:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=534
05/30/2022 12:15:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=539
05/30/2022 12:16:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=544
05/30/2022 12:16:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=549
05/30/2022 12:16:03 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 12:16:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=554
05/30/2022 12:16:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=559
05/30/2022 12:16:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=564
05/30/2022 12:16:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=569
05/30/2022 12:16:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=574
05/30/2022 12:16:14 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 12:16:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=579
05/30/2022 12:16:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=584
05/30/2022 12:16:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=589
05/30/2022 12:16:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=594
05/30/2022 12:16:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=599
05/30/2022 12:16:24 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 12:16:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=604
05/30/2022 12:16:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=609
05/30/2022 12:16:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=614
05/30/2022 12:16:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=619
05/30/2022 12:16:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=624
05/30/2022 12:16:36 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 12:16:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=629
05/30/2022 12:16:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=634
05/30/2022 12:16:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=639
05/30/2022 12:16:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=644
05/30/2022 12:16:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=649
05/30/2022 12:16:46 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 12:16:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=654
05/30/2022 12:16:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=659
05/30/2022 12:16:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
05/30/2022 12:16:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=669
05/30/2022 12:16:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=674
05/30/2022 12:16:56 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 12:16:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=679
05/30/2022 12:17:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=684
05/30/2022 12:17:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=689
05/30/2022 12:17:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=694
05/30/2022 12:17:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=699
05/30/2022 12:17:06 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 12:17:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=704
05/30/2022 12:17:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=709
05/30/2022 12:17:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=714
05/30/2022 12:17:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=719
05/30/2022 12:17:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=724
05/30/2022 12:17:17 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 12:17:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=729
05/30/2022 12:17:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=734
05/30/2022 12:17:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=739
05/30/2022 12:17:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=744
05/30/2022 12:17:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=749
05/30/2022 12:17:27 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 12:17:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=754
05/30/2022 12:17:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=759
05/30/2022 12:17:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=764
05/30/2022 12:17:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=769
05/30/2022 12:17:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=774
05/30/2022 12:17:37 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 12:17:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=779
05/30/2022 12:17:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=784
05/30/2022 12:17:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
05/30/2022 12:17:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=794
05/30/2022 12:17:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=799
05/30/2022 12:17:48 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 12:17:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=804
05/30/2022 12:17:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=809
05/30/2022 12:17:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=814
05/30/2022 12:17:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=819
05/30/2022 12:17:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=824
05/30/2022 12:17:58 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 12:18:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=829
05/30/2022 12:18:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=834
05/30/2022 12:18:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=839
05/30/2022 12:18:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=844
05/30/2022 12:18:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=849
05/30/2022 12:18:08 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 12:18:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/30/2022 12:18:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=859
05/30/2022 12:18:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/30/2022 12:18:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=869
05/30/2022 12:18:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/30/2022 12:18:18 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 12:18:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=879
05/30/2022 12:18:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
05/30/2022 12:18:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.31 on epoch=889
05/30/2022 12:18:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=894
05/30/2022 12:18:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=899
05/30/2022 12:18:28 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 12:18:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=904
05/30/2022 12:18:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/30/2022 12:18:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=914
05/30/2022 12:18:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=919
05/30/2022 12:18:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=924
05/30/2022 12:18:39 - INFO - __main__ - Global step 1850 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 12:18:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=929
05/30/2022 12:18:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=934
05/30/2022 12:18:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=939
05/30/2022 12:18:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=944
05/30/2022 12:18:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/30/2022 12:18:49 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 12:18:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
05/30/2022 12:18:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=959
05/30/2022 12:18:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/30/2022 12:18:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=969
05/30/2022 12:18:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=974
05/30/2022 12:18:59 - INFO - __main__ - Global step 1950 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 12:19:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.33 on epoch=979
05/30/2022 12:19:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=984
05/30/2022 12:19:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/30/2022 12:19:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=994
05/30/2022 12:19:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
05/30/2022 12:19:10 - INFO - __main__ - Global step 2000 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 12:19:10 - INFO - __main__ - save last model!
05/30/2022 12:19:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 12:19:10 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 12:19:10 - INFO - __main__ - Printing 3 examples
05/30/2022 12:19:10 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:19:10 - INFO - __main__ - ['entailed']
05/30/2022 12:19:10 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:19:10 - INFO - __main__ - ['entailed']
05/30/2022 12:19:10 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:19:10 - INFO - __main__ - ['entailed']
05/30/2022 12:19:10 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:19:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:19:10 - INFO - __main__ - Printing 3 examples
05/30/2022 12:19:10 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 12:19:10 - INFO - __main__ - ['refuted']
05/30/2022 12:19:10 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 12:19:10 - INFO - __main__ - ['refuted']
05/30/2022 12:19:10 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 12:19:10 - INFO - __main__ - ['refuted']
05/30/2022 12:19:10 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:19:10 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:19:10 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 12:19:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:19:10 - INFO - __main__ - Printing 3 examples
05/30/2022 12:19:10 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/30/2022 12:19:10 - INFO - __main__ - ['refuted']
05/30/2022 12:19:10 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/30/2022 12:19:10 - INFO - __main__ - ['refuted']
05/30/2022 12:19:10 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/30/2022 12:19:10 - INFO - __main__ - ['refuted']
05/30/2022 12:19:10 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:19:10 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:19:10 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 12:19:16 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:19:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 12:19:16 - INFO - __main__ - Starting training!
05/30/2022 12:19:34 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:19:47 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 12:23:59 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.5_8_predictions.txt
05/30/2022 12:23:59 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 12:24:00 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 12:24:00 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.4, bsz=8 ...
05/30/2022 12:24:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:24:00 - INFO - __main__ - Printing 3 examples
05/30/2022 12:24:00 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 12:24:00 - INFO - __main__ - ['refuted']
05/30/2022 12:24:00 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 12:24:00 - INFO - __main__ - ['refuted']
05/30/2022 12:24:00 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 12:24:00 - INFO - __main__ - ['refuted']
05/30/2022 12:24:00 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:24:00 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:24:01 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 12:24:01 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:24:01 - INFO - __main__ - Printing 3 examples
05/30/2022 12:24:01 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/30/2022 12:24:01 - INFO - __main__ - ['refuted']
05/30/2022 12:24:01 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/30/2022 12:24:01 - INFO - __main__ - ['refuted']
05/30/2022 12:24:01 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/30/2022 12:24:01 - INFO - __main__ - ['refuted']
05/30/2022 12:24:01 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:24:01 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:24:01 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 12:24:06 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:24:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 12:24:07 - INFO - __main__ - Starting training!
05/30/2022 12:24:09 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/30/2022 12:24:11 - INFO - __main__ - Step 20 Global step 20 Train loss 4.91 on epoch=9
05/30/2022 12:24:13 - INFO - __main__ - Step 30 Global step 30 Train loss 4.75 on epoch=14
05/30/2022 12:24:15 - INFO - __main__ - Step 40 Global step 40 Train loss 4.66 on epoch=19
05/30/2022 12:24:17 - INFO - __main__ - Step 50 Global step 50 Train loss 4.53 on epoch=24
05/30/2022 12:24:18 - INFO - __main__ - Global step 50 Train loss 4.78 Classification-F1 0.0 on epoch=24
05/30/2022 12:24:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 12:24:20 - INFO - __main__ - Step 60 Global step 60 Train loss 4.33 on epoch=29
05/30/2022 12:24:22 - INFO - __main__ - Step 70 Global step 70 Train loss 4.24 on epoch=34
05/30/2022 12:24:24 - INFO - __main__ - Step 80 Global step 80 Train loss 4.09 on epoch=39
05/30/2022 12:24:26 - INFO - __main__ - Step 90 Global step 90 Train loss 3.92 on epoch=44
05/30/2022 12:24:28 - INFO - __main__ - Step 100 Global step 100 Train loss 3.90 on epoch=49
05/30/2022 12:24:29 - INFO - __main__ - Global step 100 Train loss 4.10 Classification-F1 0.0 on epoch=49
05/30/2022 12:24:31 - INFO - __main__ - Step 110 Global step 110 Train loss 3.89 on epoch=54
05/30/2022 12:24:33 - INFO - __main__ - Step 120 Global step 120 Train loss 3.71 on epoch=59
05/30/2022 12:24:35 - INFO - __main__ - Step 130 Global step 130 Train loss 3.64 on epoch=64
05/30/2022 12:24:37 - INFO - __main__ - Step 140 Global step 140 Train loss 3.52 on epoch=69
05/30/2022 12:24:39 - INFO - __main__ - Step 150 Global step 150 Train loss 3.43 on epoch=74
05/30/2022 12:24:42 - INFO - __main__ - Global step 150 Train loss 3.64 Classification-F1 0.02 on epoch=74
05/30/2022 12:24:42 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.02 on epoch=74, global_step=150
05/30/2022 12:24:44 - INFO - __main__ - Step 160 Global step 160 Train loss 3.32 on epoch=79
05/30/2022 12:24:46 - INFO - __main__ - Step 170 Global step 170 Train loss 3.19 on epoch=84
05/30/2022 12:24:48 - INFO - __main__ - Step 180 Global step 180 Train loss 3.01 on epoch=89
05/30/2022 12:24:50 - INFO - __main__ - Step 190 Global step 190 Train loss 2.93 on epoch=94
05/30/2022 12:24:52 - INFO - __main__ - Step 200 Global step 200 Train loss 2.92 on epoch=99
05/30/2022 12:24:53 - INFO - __main__ - Global step 200 Train loss 3.07 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 12:24:53 - INFO - __main__ - Saving model with best Classification-F1: 0.02 -> 0.3333333333333333 on epoch=99, global_step=200
05/30/2022 12:24:55 - INFO - __main__ - Step 210 Global step 210 Train loss 2.77 on epoch=104
05/30/2022 12:24:57 - INFO - __main__ - Step 220 Global step 220 Train loss 2.58 on epoch=109
05/30/2022 12:24:59 - INFO - __main__ - Step 230 Global step 230 Train loss 2.53 on epoch=114
05/30/2022 12:25:01 - INFO - __main__ - Step 240 Global step 240 Train loss 2.47 on epoch=119
05/30/2022 12:25:03 - INFO - __main__ - Step 250 Global step 250 Train loss 2.33 on epoch=124
05/30/2022 12:25:05 - INFO - __main__ - Global step 250 Train loss 2.54 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 12:25:07 - INFO - __main__ - Step 260 Global step 260 Train loss 2.39 on epoch=129
05/30/2022 12:25:09 - INFO - __main__ - Step 270 Global step 270 Train loss 2.31 on epoch=134
05/30/2022 12:25:11 - INFO - __main__ - Step 280 Global step 280 Train loss 2.09 on epoch=139
05/30/2022 12:25:13 - INFO - __main__ - Step 290 Global step 290 Train loss 2.08 on epoch=144
05/30/2022 12:25:15 - INFO - __main__ - Step 300 Global step 300 Train loss 2.05 on epoch=149
05/30/2022 12:25:16 - INFO - __main__ - Global step 300 Train loss 2.18 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 12:25:18 - INFO - __main__ - Step 310 Global step 310 Train loss 1.99 on epoch=154
05/30/2022 12:25:20 - INFO - __main__ - Step 320 Global step 320 Train loss 1.95 on epoch=159
05/30/2022 12:25:22 - INFO - __main__ - Step 330 Global step 330 Train loss 1.73 on epoch=164
05/30/2022 12:25:24 - INFO - __main__ - Step 340 Global step 340 Train loss 1.70 on epoch=169
05/30/2022 12:25:26 - INFO - __main__ - Step 350 Global step 350 Train loss 1.65 on epoch=174
05/30/2022 12:25:27 - INFO - __main__ - Global step 350 Train loss 1.80 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 12:25:29 - INFO - __main__ - Step 360 Global step 360 Train loss 1.68 on epoch=179
05/30/2022 12:25:31 - INFO - __main__ - Step 370 Global step 370 Train loss 1.65 on epoch=184
05/30/2022 12:25:33 - INFO - __main__ - Step 380 Global step 380 Train loss 1.53 on epoch=189
05/30/2022 12:25:35 - INFO - __main__ - Step 390 Global step 390 Train loss 1.47 on epoch=194
05/30/2022 12:25:37 - INFO - __main__ - Step 400 Global step 400 Train loss 1.40 on epoch=199
05/30/2022 12:25:39 - INFO - __main__ - Global step 400 Train loss 1.55 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 12:25:41 - INFO - __main__ - Step 410 Global step 410 Train loss 1.40 on epoch=204
05/30/2022 12:25:43 - INFO - __main__ - Step 420 Global step 420 Train loss 1.36 on epoch=209
05/30/2022 12:25:45 - INFO - __main__ - Step 430 Global step 430 Train loss 1.24 on epoch=214
05/30/2022 12:25:47 - INFO - __main__ - Step 440 Global step 440 Train loss 1.19 on epoch=219
05/30/2022 12:25:49 - INFO - __main__ - Step 450 Global step 450 Train loss 1.18 on epoch=224
05/30/2022 12:25:50 - INFO - __main__ - Global step 450 Train loss 1.27 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 12:25:52 - INFO - __main__ - Step 460 Global step 460 Train loss 1.21 on epoch=229
05/30/2022 12:25:54 - INFO - __main__ - Step 470 Global step 470 Train loss 1.17 on epoch=234
05/30/2022 12:25:56 - INFO - __main__ - Step 480 Global step 480 Train loss 1.03 on epoch=239
05/30/2022 12:25:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.99 on epoch=244
05/30/2022 12:25:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.98 on epoch=249
05/30/2022 12:26:00 - INFO - __main__ - Global step 500 Train loss 1.08 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 12:26:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.87 on epoch=254
05/30/2022 12:26:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.85 on epoch=259
05/30/2022 12:26:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.91 on epoch=264
05/30/2022 12:26:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=269
05/30/2022 12:26:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.71 on epoch=274
05/30/2022 12:26:11 - INFO - __main__ - Global step 550 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 12:26:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=279
05/30/2022 12:26:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.78 on epoch=284
05/30/2022 12:26:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.87 on epoch=289
05/30/2022 12:26:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.75 on epoch=294
05/30/2022 12:26:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.72 on epoch=299
05/30/2022 12:26:21 - INFO - __main__ - Global step 600 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 12:26:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.71 on epoch=304
05/30/2022 12:26:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.68 on epoch=309
05/30/2022 12:26:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.79 on epoch=314
05/30/2022 12:26:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.80 on epoch=319
05/30/2022 12:26:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.70 on epoch=324
05/30/2022 12:26:32 - INFO - __main__ - Global step 650 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 12:26:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.65 on epoch=329
05/30/2022 12:26:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.59 on epoch=334
05/30/2022 12:26:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.66 on epoch=339
05/30/2022 12:26:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=344
05/30/2022 12:26:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.60 on epoch=349
05/30/2022 12:26:42 - INFO - __main__ - Global step 700 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 12:26:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.66 on epoch=354
05/30/2022 12:26:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=359
05/30/2022 12:26:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=364
05/30/2022 12:26:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.52 on epoch=369
05/30/2022 12:26:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.56 on epoch=374
05/30/2022 12:26:53 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 12:26:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=379
05/30/2022 12:26:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.54 on epoch=384
05/30/2022 12:26:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=389
05/30/2022 12:27:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.49 on epoch=394
05/30/2022 12:27:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=399
05/30/2022 12:27:03 - INFO - __main__ - Global step 800 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 12:27:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.50 on epoch=404
05/30/2022 12:27:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.54 on epoch=409
05/30/2022 12:27:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.58 on epoch=414
05/30/2022 12:27:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=419
05/30/2022 12:27:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=424
05/30/2022 12:27:13 - INFO - __main__ - Global step 850 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 12:27:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=429
05/30/2022 12:27:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=434
05/30/2022 12:27:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=439
05/30/2022 12:27:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
05/30/2022 12:27:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=449
05/30/2022 12:27:24 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 12:27:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=454
05/30/2022 12:27:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.52 on epoch=459
05/30/2022 12:27:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=464
05/30/2022 12:27:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=469
05/30/2022 12:27:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=474
05/30/2022 12:27:34 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 12:27:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=479
05/30/2022 12:27:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=484
05/30/2022 12:27:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=489
05/30/2022 12:27:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=494
05/30/2022 12:27:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=499
05/30/2022 12:27:44 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 12:27:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=504
05/30/2022 12:27:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.51 on epoch=509
05/30/2022 12:27:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=514
05/30/2022 12:27:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=519
05/30/2022 12:27:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=524
05/30/2022 12:27:54 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 12:27:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=529
05/30/2022 12:27:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=534
05/30/2022 12:28:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=539
05/30/2022 12:28:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=544
05/30/2022 12:28:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=549
05/30/2022 12:28:05 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 12:28:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=554
05/30/2022 12:28:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=559
05/30/2022 12:28:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=564
05/30/2022 12:28:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=569
05/30/2022 12:28:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=574
05/30/2022 12:28:15 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 12:28:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=579
05/30/2022 12:28:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=584
05/30/2022 12:28:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
05/30/2022 12:28:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.29 on epoch=594
05/30/2022 12:28:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.47 on epoch=599
05/30/2022 12:28:25 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 12:28:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=604
05/30/2022 12:28:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=609
05/30/2022 12:28:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.87 on epoch=614
05/30/2022 12:28:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.80 on epoch=619
05/30/2022 12:28:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=624
05/30/2022 12:28:35 - INFO - __main__ - Global step 1250 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 12:28:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=629
05/30/2022 12:28:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.52 on epoch=634
05/30/2022 12:28:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
05/30/2022 12:28:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=644
05/30/2022 12:28:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=649
05/30/2022 12:28:46 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 12:28:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=654
05/30/2022 12:28:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=659
05/30/2022 12:28:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=664
05/30/2022 12:28:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=669
05/30/2022 12:28:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=674
05/30/2022 12:28:56 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 12:28:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=679
05/30/2022 12:29:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=684
05/30/2022 12:29:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=689
05/30/2022 12:29:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=694
05/30/2022 12:29:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=699
05/30/2022 12:29:06 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 12:29:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=704
05/30/2022 12:29:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=709
05/30/2022 12:29:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
05/30/2022 12:29:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=719
05/30/2022 12:29:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=724
05/30/2022 12:29:16 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 12:29:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=729
05/30/2022 12:29:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=734
05/30/2022 12:29:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=739
05/30/2022 12:29:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=744
05/30/2022 12:29:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=749
05/30/2022 12:29:27 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 12:29:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=754
05/30/2022 12:29:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=759
05/30/2022 12:29:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.29 on epoch=764
05/30/2022 12:29:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=769
05/30/2022 12:29:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=774
05/30/2022 12:29:37 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 12:29:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=779
05/30/2022 12:29:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=784
05/30/2022 12:29:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=789
05/30/2022 12:29:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=794
05/30/2022 12:29:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=799
05/30/2022 12:29:47 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 12:29:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.28 on epoch=804
05/30/2022 12:29:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=809
05/30/2022 12:29:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=814
05/30/2022 12:29:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=819
05/30/2022 12:29:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=824
05/30/2022 12:29:57 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 12:29:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=829
05/30/2022 12:30:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=834
05/30/2022 12:30:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=839
05/30/2022 12:30:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=844
05/30/2022 12:30:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=849
05/30/2022 12:30:08 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 12:30:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=854
05/30/2022 12:30:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=859
05/30/2022 12:30:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=864
05/30/2022 12:30:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=869
05/30/2022 12:30:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/30/2022 12:30:18 - INFO - __main__ - Global step 1750 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 12:30:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.30 on epoch=879
05/30/2022 12:30:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=884
05/30/2022 12:30:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=889
05/30/2022 12:30:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=894
05/30/2022 12:30:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=899
05/30/2022 12:30:28 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 12:30:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=904
05/30/2022 12:30:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/30/2022 12:30:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=914
05/30/2022 12:30:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=919
05/30/2022 12:30:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=924
05/30/2022 12:30:38 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 12:30:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=929
05/30/2022 12:30:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=934
05/30/2022 12:30:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=939
05/30/2022 12:30:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=944
05/30/2022 12:30:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/30/2022 12:30:49 - INFO - __main__ - Global step 1900 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 12:30:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=954
05/30/2022 12:30:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=959
05/30/2022 12:30:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/30/2022 12:30:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
05/30/2022 12:30:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=974
05/30/2022 12:30:59 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 12:31:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=979
05/30/2022 12:31:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=984
05/30/2022 12:31:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
05/30/2022 12:31:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
05/30/2022 12:31:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=999
05/30/2022 12:31:09 - INFO - __main__ - Global step 2000 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 12:31:09 - INFO - __main__ - save last model!
05/30/2022 12:31:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 12:31:09 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 12:31:09 - INFO - __main__ - Printing 3 examples
05/30/2022 12:31:09 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:31:09 - INFO - __main__ - ['entailed']
05/30/2022 12:31:09 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:31:09 - INFO - __main__ - ['entailed']
05/30/2022 12:31:09 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:31:09 - INFO - __main__ - ['entailed']
05/30/2022 12:31:09 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:31:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:31:10 - INFO - __main__ - Printing 3 examples
05/30/2022 12:31:10 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 12:31:10 - INFO - __main__ - ['refuted']
05/30/2022 12:31:10 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 12:31:10 - INFO - __main__ - ['refuted']
05/30/2022 12:31:10 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 12:31:10 - INFO - __main__ - ['refuted']
05/30/2022 12:31:10 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:31:10 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:31:10 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 12:31:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:31:10 - INFO - __main__ - Printing 3 examples
05/30/2022 12:31:10 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/30/2022 12:31:10 - INFO - __main__ - ['refuted']
05/30/2022 12:31:10 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/30/2022 12:31:10 - INFO - __main__ - ['refuted']
05/30/2022 12:31:10 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/30/2022 12:31:10 - INFO - __main__ - ['refuted']
05/30/2022 12:31:10 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:31:10 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:31:10 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 12:31:16 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:31:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 12:31:16 - INFO - __main__ - Starting training!
05/30/2022 12:31:34 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:31:46 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 12:35:56 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.4_8_predictions.txt
05/30/2022 12:35:56 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 12:35:57 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 12:35:57 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.3, bsz=8 ...
05/30/2022 12:35:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:35:58 - INFO - __main__ - Printing 3 examples
05/30/2022 12:35:58 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 12:35:58 - INFO - __main__ - ['refuted']
05/30/2022 12:35:58 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 12:35:58 - INFO - __main__ - ['refuted']
05/30/2022 12:35:58 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 12:35:58 - INFO - __main__ - ['refuted']
05/30/2022 12:35:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:35:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:35:58 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 12:35:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:35:58 - INFO - __main__ - Printing 3 examples
05/30/2022 12:35:58 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/30/2022 12:35:58 - INFO - __main__ - ['refuted']
05/30/2022 12:35:58 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/30/2022 12:35:58 - INFO - __main__ - ['refuted']
05/30/2022 12:35:58 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/30/2022 12:35:58 - INFO - __main__ - ['refuted']
05/30/2022 12:35:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:35:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:35:58 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 12:36:03 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:36:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 12:36:03 - INFO - __main__ - Starting training!
05/30/2022 12:36:06 - INFO - __main__ - Step 10 Global step 10 Train loss 4.98 on epoch=4
05/30/2022 12:36:07 - INFO - __main__ - Step 20 Global step 20 Train loss 4.94 on epoch=9
05/30/2022 12:36:09 - INFO - __main__ - Step 30 Global step 30 Train loss 4.93 on epoch=14
05/30/2022 12:36:11 - INFO - __main__ - Step 40 Global step 40 Train loss 4.88 on epoch=19
05/30/2022 12:36:13 - INFO - __main__ - Step 50 Global step 50 Train loss 4.83 on epoch=24
05/30/2022 12:36:15 - INFO - __main__ - Global step 50 Train loss 4.91 Classification-F1 0.0 on epoch=24
05/30/2022 12:36:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 12:36:17 - INFO - __main__ - Step 60 Global step 60 Train loss 4.75 on epoch=29
05/30/2022 12:36:19 - INFO - __main__ - Step 70 Global step 70 Train loss 4.59 on epoch=34
05/30/2022 12:36:20 - INFO - __main__ - Step 80 Global step 80 Train loss 4.61 on epoch=39
05/30/2022 12:36:22 - INFO - __main__ - Step 90 Global step 90 Train loss 4.43 on epoch=44
05/30/2022 12:36:24 - INFO - __main__ - Step 100 Global step 100 Train loss 4.46 on epoch=49
05/30/2022 12:36:25 - INFO - __main__ - Global step 100 Train loss 4.57 Classification-F1 0.0 on epoch=49
05/30/2022 12:36:27 - INFO - __main__ - Step 110 Global step 110 Train loss 4.39 on epoch=54
05/30/2022 12:36:29 - INFO - __main__ - Step 120 Global step 120 Train loss 4.11 on epoch=59
05/30/2022 12:36:31 - INFO - __main__ - Step 130 Global step 130 Train loss 4.18 on epoch=64
05/30/2022 12:36:33 - INFO - __main__ - Step 140 Global step 140 Train loss 3.98 on epoch=69
05/30/2022 12:36:35 - INFO - __main__ - Step 150 Global step 150 Train loss 3.96 on epoch=74
05/30/2022 12:36:37 - INFO - __main__ - Global step 150 Train loss 4.12 Classification-F1 0.0 on epoch=74
05/30/2022 12:36:39 - INFO - __main__ - Step 160 Global step 160 Train loss 3.85 on epoch=79
05/30/2022 12:36:41 - INFO - __main__ - Step 170 Global step 170 Train loss 3.87 on epoch=84
05/30/2022 12:36:43 - INFO - __main__ - Step 180 Global step 180 Train loss 3.88 on epoch=89
05/30/2022 12:36:45 - INFO - __main__ - Step 190 Global step 190 Train loss 3.78 on epoch=94
05/30/2022 12:36:47 - INFO - __main__ - Step 200 Global step 200 Train loss 3.64 on epoch=99
05/30/2022 12:36:50 - INFO - __main__ - Global step 200 Train loss 3.80 Classification-F1 0.038461538461538464 on epoch=99
05/30/2022 12:36:50 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.038461538461538464 on epoch=99, global_step=200
05/30/2022 12:36:52 - INFO - __main__ - Step 210 Global step 210 Train loss 3.56 on epoch=104
05/30/2022 12:36:54 - INFO - __main__ - Step 220 Global step 220 Train loss 3.37 on epoch=109
05/30/2022 12:36:56 - INFO - __main__ - Step 230 Global step 230 Train loss 3.41 on epoch=114
05/30/2022 12:36:57 - INFO - __main__ - Step 240 Global step 240 Train loss 3.28 on epoch=119
05/30/2022 12:36:59 - INFO - __main__ - Step 250 Global step 250 Train loss 3.26 on epoch=124
05/30/2022 12:37:01 - INFO - __main__ - Global step 250 Train loss 3.38 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 12:37:01 - INFO - __main__ - Saving model with best Classification-F1: 0.038461538461538464 -> 0.3333333333333333 on epoch=124, global_step=250
05/30/2022 12:37:03 - INFO - __main__ - Step 260 Global step 260 Train loss 3.09 on epoch=129
05/30/2022 12:37:05 - INFO - __main__ - Step 270 Global step 270 Train loss 3.06 on epoch=134
05/30/2022 12:37:07 - INFO - __main__ - Step 280 Global step 280 Train loss 3.08 on epoch=139
05/30/2022 12:37:09 - INFO - __main__ - Step 290 Global step 290 Train loss 2.93 on epoch=144
05/30/2022 12:37:11 - INFO - __main__ - Step 300 Global step 300 Train loss 2.87 on epoch=149
05/30/2022 12:37:17 - INFO - __main__ - Global step 300 Train loss 3.01 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 12:37:19 - INFO - __main__ - Step 310 Global step 310 Train loss 2.63 on epoch=154
05/30/2022 12:37:21 - INFO - __main__ - Step 320 Global step 320 Train loss 2.63 on epoch=159
05/30/2022 12:37:23 - INFO - __main__ - Step 330 Global step 330 Train loss 2.56 on epoch=164
05/30/2022 12:37:25 - INFO - __main__ - Step 340 Global step 340 Train loss 2.44 on epoch=169
05/30/2022 12:37:27 - INFO - __main__ - Step 350 Global step 350 Train loss 2.40 on epoch=174
05/30/2022 12:37:29 - INFO - __main__ - Global step 350 Train loss 2.53 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 12:37:31 - INFO - __main__ - Step 360 Global step 360 Train loss 2.41 on epoch=179
05/30/2022 12:37:33 - INFO - __main__ - Step 370 Global step 370 Train loss 2.35 on epoch=184
05/30/2022 12:37:35 - INFO - __main__ - Step 380 Global step 380 Train loss 2.19 on epoch=189
05/30/2022 12:37:37 - INFO - __main__ - Step 390 Global step 390 Train loss 2.18 on epoch=194
05/30/2022 12:37:38 - INFO - __main__ - Step 400 Global step 400 Train loss 2.07 on epoch=199
05/30/2022 12:37:41 - INFO - __main__ - Global step 400 Train loss 2.24 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 12:37:42 - INFO - __main__ - Step 410 Global step 410 Train loss 2.03 on epoch=204
05/30/2022 12:37:44 - INFO - __main__ - Step 420 Global step 420 Train loss 2.06 on epoch=209
05/30/2022 12:37:46 - INFO - __main__ - Step 430 Global step 430 Train loss 2.07 on epoch=214
05/30/2022 12:37:48 - INFO - __main__ - Step 440 Global step 440 Train loss 1.92 on epoch=219
05/30/2022 12:37:50 - INFO - __main__ - Step 450 Global step 450 Train loss 2.05 on epoch=224
05/30/2022 12:37:56 - INFO - __main__ - Global step 450 Train loss 2.03 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 12:37:58 - INFO - __main__ - Step 460 Global step 460 Train loss 2.05 on epoch=229
05/30/2022 12:38:00 - INFO - __main__ - Step 470 Global step 470 Train loss 1.83 on epoch=234
05/30/2022 12:38:02 - INFO - __main__ - Step 480 Global step 480 Train loss 1.72 on epoch=239
05/30/2022 12:38:04 - INFO - __main__ - Step 490 Global step 490 Train loss 1.77 on epoch=244
05/30/2022 12:38:06 - INFO - __main__ - Step 500 Global step 500 Train loss 1.70 on epoch=249
05/30/2022 12:38:07 - INFO - __main__ - Global step 500 Train loss 1.81 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 12:38:09 - INFO - __main__ - Step 510 Global step 510 Train loss 1.59 on epoch=254
05/30/2022 12:38:11 - INFO - __main__ - Step 520 Global step 520 Train loss 1.62 on epoch=259
05/30/2022 12:38:12 - INFO - __main__ - Step 530 Global step 530 Train loss 1.78 on epoch=264
05/30/2022 12:38:14 - INFO - __main__ - Step 540 Global step 540 Train loss 1.49 on epoch=269
05/30/2022 12:38:16 - INFO - __main__ - Step 550 Global step 550 Train loss 1.53 on epoch=274
05/30/2022 12:38:19 - INFO - __main__ - Global step 550 Train loss 1.60 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 12:38:21 - INFO - __main__ - Step 560 Global step 560 Train loss 1.45 on epoch=279
05/30/2022 12:38:23 - INFO - __main__ - Step 570 Global step 570 Train loss 1.48 on epoch=284
05/30/2022 12:38:25 - INFO - __main__ - Step 580 Global step 580 Train loss 1.20 on epoch=289
05/30/2022 12:38:27 - INFO - __main__ - Step 590 Global step 590 Train loss 1.32 on epoch=294
05/30/2022 12:38:28 - INFO - __main__ - Step 600 Global step 600 Train loss 1.30 on epoch=299
05/30/2022 12:38:32 - INFO - __main__ - Global step 600 Train loss 1.35 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 12:38:34 - INFO - __main__ - Step 610 Global step 610 Train loss 1.17 on epoch=304
05/30/2022 12:38:35 - INFO - __main__ - Step 620 Global step 620 Train loss 1.25 on epoch=309
05/30/2022 12:38:37 - INFO - __main__ - Step 630 Global step 630 Train loss 1.18 on epoch=314
05/30/2022 12:38:39 - INFO - __main__ - Step 640 Global step 640 Train loss 1.17 on epoch=319
05/30/2022 12:38:41 - INFO - __main__ - Step 650 Global step 650 Train loss 1.21 on epoch=324
05/30/2022 12:38:43 - INFO - __main__ - Global step 650 Train loss 1.20 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 12:38:45 - INFO - __main__ - Step 660 Global step 660 Train loss 1.13 on epoch=329
05/30/2022 12:38:47 - INFO - __main__ - Step 670 Global step 670 Train loss 1.01 on epoch=334
05/30/2022 12:38:49 - INFO - __main__ - Step 680 Global step 680 Train loss 1.03 on epoch=339
05/30/2022 12:38:51 - INFO - __main__ - Step 690 Global step 690 Train loss 1.02 on epoch=344
05/30/2022 12:38:53 - INFO - __main__ - Step 700 Global step 700 Train loss 1.02 on epoch=349
05/30/2022 12:38:54 - INFO - __main__ - Global step 700 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 12:38:56 - INFO - __main__ - Step 710 Global step 710 Train loss 1.08 on epoch=354
05/30/2022 12:38:57 - INFO - __main__ - Step 720 Global step 720 Train loss 1.00 on epoch=359
05/30/2022 12:38:59 - INFO - __main__ - Step 730 Global step 730 Train loss 1.16 on epoch=364
05/30/2022 12:39:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.96 on epoch=369
05/30/2022 12:39:03 - INFO - __main__ - Step 750 Global step 750 Train loss 1.01 on epoch=374
05/30/2022 12:39:04 - INFO - __main__ - Global step 750 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 12:39:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.90 on epoch=379
05/30/2022 12:39:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.91 on epoch=384
05/30/2022 12:39:10 - INFO - __main__ - Step 780 Global step 780 Train loss 1.03 on epoch=389
05/30/2022 12:39:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.89 on epoch=394
05/30/2022 12:39:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.87 on epoch=399
05/30/2022 12:39:19 - INFO - __main__ - Global step 800 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 12:39:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.92 on epoch=404
05/30/2022 12:39:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.90 on epoch=409
05/30/2022 12:39:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.80 on epoch=414
05/30/2022 12:39:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.84 on epoch=419
05/30/2022 12:39:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=424
05/30/2022 12:39:30 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 12:39:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.79 on epoch=429
05/30/2022 12:39:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.83 on epoch=434
05/30/2022 12:39:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.74 on epoch=439
05/30/2022 12:39:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.85 on epoch=444
05/30/2022 12:39:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.76 on epoch=449
05/30/2022 12:39:40 - INFO - __main__ - Global step 900 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 12:39:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.80 on epoch=454
05/30/2022 12:39:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.73 on epoch=459
05/30/2022 12:39:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.78 on epoch=464
05/30/2022 12:39:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.79 on epoch=469
05/30/2022 12:39:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.78 on epoch=474
05/30/2022 12:39:51 - INFO - __main__ - Global step 950 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 12:39:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.71 on epoch=479
05/30/2022 12:39:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.82 on epoch=484
05/30/2022 12:39:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.84 on epoch=489
05/30/2022 12:39:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=494
05/30/2022 12:40:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.77 on epoch=499
05/30/2022 12:40:01 - INFO - __main__ - Global step 1000 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 12:40:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.76 on epoch=504
05/30/2022 12:40:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.71 on epoch=509
05/30/2022 12:40:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.78 on epoch=514
05/30/2022 12:40:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.68 on epoch=519
05/30/2022 12:40:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.72 on epoch=524
05/30/2022 12:40:12 - INFO - __main__ - Global step 1050 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 12:40:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.73 on epoch=529
05/30/2022 12:40:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=534
05/30/2022 12:40:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.66 on epoch=539
05/30/2022 12:40:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.62 on epoch=544
05/30/2022 12:40:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.74 on epoch=549
05/30/2022 12:40:22 - INFO - __main__ - Global step 1100 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 12:40:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.71 on epoch=554
05/30/2022 12:40:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.78 on epoch=559
05/30/2022 12:40:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.70 on epoch=564
05/30/2022 12:40:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.72 on epoch=569
05/30/2022 12:40:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.71 on epoch=574
05/30/2022 12:40:33 - INFO - __main__ - Global step 1150 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 12:40:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.67 on epoch=579
05/30/2022 12:40:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.72 on epoch=584
05/30/2022 12:40:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.74 on epoch=589
05/30/2022 12:40:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.70 on epoch=594
05/30/2022 12:40:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.61 on epoch=599
05/30/2022 12:40:43 - INFO - __main__ - Global step 1200 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 12:40:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.66 on epoch=604
05/30/2022 12:40:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.63 on epoch=609
05/30/2022 12:40:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.66 on epoch=614
05/30/2022 12:40:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.66 on epoch=619
05/30/2022 12:40:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.59 on epoch=624
05/30/2022 12:40:54 - INFO - __main__ - Global step 1250 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 12:40:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.60 on epoch=629
05/30/2022 12:40:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.62 on epoch=634
05/30/2022 12:40:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.70 on epoch=639
05/30/2022 12:41:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.59 on epoch=644
05/30/2022 12:41:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.61 on epoch=649
05/30/2022 12:41:04 - INFO - __main__ - Global step 1300 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 12:41:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.54 on epoch=654
05/30/2022 12:41:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.54 on epoch=659
05/30/2022 12:41:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.55 on epoch=664
05/30/2022 12:41:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.54 on epoch=669
05/30/2022 12:41:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.62 on epoch=674
05/30/2022 12:41:14 - INFO - __main__ - Global step 1350 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 12:41:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.60 on epoch=679
05/30/2022 12:41:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.54 on epoch=684
05/30/2022 12:41:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
05/30/2022 12:41:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.53 on epoch=694
05/30/2022 12:41:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.58 on epoch=699
05/30/2022 12:41:25 - INFO - __main__ - Global step 1400 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 12:41:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.50 on epoch=704
05/30/2022 12:41:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=709
05/30/2022 12:41:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=714
05/30/2022 12:41:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.55 on epoch=719
05/30/2022 12:41:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.54 on epoch=724
05/30/2022 12:41:35 - INFO - __main__ - Global step 1450 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 12:41:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=729
05/30/2022 12:41:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.57 on epoch=734
05/30/2022 12:41:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.50 on epoch=739
05/30/2022 12:41:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.53 on epoch=744
05/30/2022 12:41:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.51 on epoch=749
05/30/2022 12:41:46 - INFO - __main__ - Global step 1500 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 12:41:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.50 on epoch=754
05/30/2022 12:41:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=759
05/30/2022 12:41:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.49 on epoch=764
05/30/2022 12:41:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.52 on epoch=769
05/30/2022 12:41:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.50 on epoch=774
05/30/2022 12:41:56 - INFO - __main__ - Global step 1550 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 12:41:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=779
05/30/2022 12:42:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.52 on epoch=784
05/30/2022 12:42:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
05/30/2022 12:42:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.52 on epoch=794
05/30/2022 12:42:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.46 on epoch=799
05/30/2022 12:42:07 - INFO - __main__ - Global step 1600 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 12:42:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
05/30/2022 12:42:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=809
05/30/2022 12:42:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=814
05/30/2022 12:42:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=819
05/30/2022 12:42:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=824
05/30/2022 12:42:17 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 12:42:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=829
05/30/2022 12:42:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=834
05/30/2022 12:42:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.48 on epoch=839
05/30/2022 12:42:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
05/30/2022 12:42:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=849
05/30/2022 12:42:27 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 12:42:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=854
05/30/2022 12:42:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=859
05/30/2022 12:42:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=864
05/30/2022 12:42:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=869
05/30/2022 12:42:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=874
05/30/2022 12:42:38 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 12:42:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=879
05/30/2022 12:42:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
05/30/2022 12:42:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=889
05/30/2022 12:42:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
05/30/2022 12:42:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
05/30/2022 12:42:48 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 12:42:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=904
05/30/2022 12:42:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
05/30/2022 12:42:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=914
05/30/2022 12:42:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=919
05/30/2022 12:42:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=924
05/30/2022 12:42:58 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 12:43:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
05/30/2022 12:43:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=934
05/30/2022 12:43:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.44 on epoch=939
05/30/2022 12:43:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.48 on epoch=944
05/30/2022 12:43:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=949
05/30/2022 12:43:09 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 12:43:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=954
05/30/2022 12:43:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.46 on epoch=959
05/30/2022 12:43:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.43 on epoch=964
05/30/2022 12:43:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
05/30/2022 12:43:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/30/2022 12:43:19 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 12:43:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=979
05/30/2022 12:43:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=984
05/30/2022 12:43:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=989
05/30/2022 12:43:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/30/2022 12:43:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=999
05/30/2022 12:43:29 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 12:43:29 - INFO - __main__ - save last model!
05/30/2022 12:43:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 12:43:29 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 12:43:29 - INFO - __main__ - Printing 3 examples
05/30/2022 12:43:29 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:43:29 - INFO - __main__ - ['entailed']
05/30/2022 12:43:29 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:43:29 - INFO - __main__ - ['entailed']
05/30/2022 12:43:29 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:43:29 - INFO - __main__ - ['entailed']
05/30/2022 12:43:29 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:43:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:43:30 - INFO - __main__ - Printing 3 examples
05/30/2022 12:43:30 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 12:43:30 - INFO - __main__ - ['refuted']
05/30/2022 12:43:30 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 12:43:30 - INFO - __main__ - ['refuted']
05/30/2022 12:43:30 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 12:43:30 - INFO - __main__ - ['refuted']
05/30/2022 12:43:30 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:43:30 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:43:30 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 12:43:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:43:30 - INFO - __main__ - Printing 3 examples
05/30/2022 12:43:30 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/30/2022 12:43:30 - INFO - __main__ - ['refuted']
05/30/2022 12:43:30 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/30/2022 12:43:30 - INFO - __main__ - ['refuted']
05/30/2022 12:43:30 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/30/2022 12:43:30 - INFO - __main__ - ['refuted']
05/30/2022 12:43:30 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:43:30 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:43:30 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 12:43:36 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:43:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 12:43:36 - INFO - __main__ - Starting training!
05/30/2022 12:43:54 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:44:06 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 12:48:12 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.3_8_predictions.txt
05/30/2022 12:48:12 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 12:48:12 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 12:48:12 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.2, bsz=8 ...
05/30/2022 12:48:13 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:48:13 - INFO - __main__ - Printing 3 examples
05/30/2022 12:48:13 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 12:48:13 - INFO - __main__ - ['refuted']
05/30/2022 12:48:13 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 12:48:13 - INFO - __main__ - ['refuted']
05/30/2022 12:48:13 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 12:48:13 - INFO - __main__ - ['refuted']
05/30/2022 12:48:13 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:48:13 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:48:13 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 12:48:13 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:48:13 - INFO - __main__ - Printing 3 examples
05/30/2022 12:48:13 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/30/2022 12:48:13 - INFO - __main__ - ['refuted']
05/30/2022 12:48:13 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/30/2022 12:48:13 - INFO - __main__ - ['refuted']
05/30/2022 12:48:13 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/30/2022 12:48:13 - INFO - __main__ - ['refuted']
05/30/2022 12:48:13 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:48:13 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:48:13 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 12:48:19 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:48:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 12:48:19 - INFO - __main__ - Starting training!
05/30/2022 12:48:21 - INFO - __main__ - Step 10 Global step 10 Train loss 5.02 on epoch=4
05/30/2022 12:48:23 - INFO - __main__ - Step 20 Global step 20 Train loss 4.94 on epoch=9
05/30/2022 12:48:25 - INFO - __main__ - Step 30 Global step 30 Train loss 5.06 on epoch=14
05/30/2022 12:48:27 - INFO - __main__ - Step 40 Global step 40 Train loss 4.93 on epoch=19
05/30/2022 12:48:29 - INFO - __main__ - Step 50 Global step 50 Train loss 4.78 on epoch=24
05/30/2022 12:48:30 - INFO - __main__ - Global step 50 Train loss 4.95 Classification-F1 0.0 on epoch=24
05/30/2022 12:48:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 12:48:32 - INFO - __main__ - Step 60 Global step 60 Train loss 4.64 on epoch=29
05/30/2022 12:48:34 - INFO - __main__ - Step 70 Global step 70 Train loss 4.87 on epoch=34
05/30/2022 12:48:36 - INFO - __main__ - Step 80 Global step 80 Train loss 4.69 on epoch=39
05/30/2022 12:48:38 - INFO - __main__ - Step 90 Global step 90 Train loss 4.57 on epoch=44
05/30/2022 12:48:39 - INFO - __main__ - Step 100 Global step 100 Train loss 4.59 on epoch=49
05/30/2022 12:48:40 - INFO - __main__ - Global step 100 Train loss 4.67 Classification-F1 0.0 on epoch=49
05/30/2022 12:48:42 - INFO - __main__ - Step 110 Global step 110 Train loss 4.47 on epoch=54
05/30/2022 12:48:44 - INFO - __main__ - Step 120 Global step 120 Train loss 4.57 on epoch=59
05/30/2022 12:48:46 - INFO - __main__ - Step 130 Global step 130 Train loss 4.49 on epoch=64
05/30/2022 12:48:48 - INFO - __main__ - Step 140 Global step 140 Train loss 4.45 on epoch=69
05/30/2022 12:48:50 - INFO - __main__ - Step 150 Global step 150 Train loss 4.39 on epoch=74
05/30/2022 12:48:51 - INFO - __main__ - Global step 150 Train loss 4.47 Classification-F1 0.0 on epoch=74
05/30/2022 12:48:53 - INFO - __main__ - Step 160 Global step 160 Train loss 4.35 on epoch=79
05/30/2022 12:48:55 - INFO - __main__ - Step 170 Global step 170 Train loss 4.32 on epoch=84
05/30/2022 12:48:57 - INFO - __main__ - Step 180 Global step 180 Train loss 4.30 on epoch=89
05/30/2022 12:48:59 - INFO - __main__ - Step 190 Global step 190 Train loss 4.26 on epoch=94
05/30/2022 12:49:01 - INFO - __main__ - Step 200 Global step 200 Train loss 4.22 on epoch=99
05/30/2022 12:49:02 - INFO - __main__ - Global step 200 Train loss 4.29 Classification-F1 0.0 on epoch=99
05/30/2022 12:49:04 - INFO - __main__ - Step 210 Global step 210 Train loss 4.14 on epoch=104
05/30/2022 12:49:06 - INFO - __main__ - Step 220 Global step 220 Train loss 4.06 on epoch=109
05/30/2022 12:49:08 - INFO - __main__ - Step 230 Global step 230 Train loss 4.18 on epoch=114
05/30/2022 12:49:10 - INFO - __main__ - Step 240 Global step 240 Train loss 4.07 on epoch=119
05/30/2022 12:49:11 - INFO - __main__ - Step 250 Global step 250 Train loss 3.99 on epoch=124
05/30/2022 12:49:13 - INFO - __main__ - Global step 250 Train loss 4.09 Classification-F1 0.0 on epoch=124
05/30/2022 12:49:15 - INFO - __main__ - Step 260 Global step 260 Train loss 3.92 on epoch=129
05/30/2022 12:49:17 - INFO - __main__ - Step 270 Global step 270 Train loss 3.87 on epoch=134
05/30/2022 12:49:19 - INFO - __main__ - Step 280 Global step 280 Train loss 3.95 on epoch=139
05/30/2022 12:49:21 - INFO - __main__ - Step 290 Global step 290 Train loss 3.87 on epoch=144
05/30/2022 12:49:22 - INFO - __main__ - Step 300 Global step 300 Train loss 3.83 on epoch=149
05/30/2022 12:49:24 - INFO - __main__ - Global step 300 Train loss 3.89 Classification-F1 0.0 on epoch=149
05/30/2022 12:49:26 - INFO - __main__ - Step 310 Global step 310 Train loss 3.68 on epoch=154
05/30/2022 12:49:28 - INFO - __main__ - Step 320 Global step 320 Train loss 3.76 on epoch=159
05/30/2022 12:49:30 - INFO - __main__ - Step 330 Global step 330 Train loss 3.62 on epoch=164
05/30/2022 12:49:32 - INFO - __main__ - Step 340 Global step 340 Train loss 3.68 on epoch=169
05/30/2022 12:49:33 - INFO - __main__ - Step 350 Global step 350 Train loss 3.69 on epoch=174
05/30/2022 12:49:35 - INFO - __main__ - Global step 350 Train loss 3.69 Classification-F1 0.0 on epoch=174
05/30/2022 12:49:37 - INFO - __main__ - Step 360 Global step 360 Train loss 3.58 on epoch=179
05/30/2022 12:49:39 - INFO - __main__ - Step 370 Global step 370 Train loss 3.53 on epoch=184
05/30/2022 12:49:41 - INFO - __main__ - Step 380 Global step 380 Train loss 3.50 on epoch=189
05/30/2022 12:49:43 - INFO - __main__ - Step 390 Global step 390 Train loss 3.41 on epoch=194
05/30/2022 12:49:45 - INFO - __main__ - Step 400 Global step 400 Train loss 3.41 on epoch=199
05/30/2022 12:49:46 - INFO - __main__ - Global step 400 Train loss 3.48 Classification-F1 0.07777777777777778 on epoch=199
05/30/2022 12:49:46 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.07777777777777778 on epoch=199, global_step=400
05/30/2022 12:49:48 - INFO - __main__ - Step 410 Global step 410 Train loss 3.31 on epoch=204
05/30/2022 12:49:50 - INFO - __main__ - Step 420 Global step 420 Train loss 3.31 on epoch=209
05/30/2022 12:49:52 - INFO - __main__ - Step 430 Global step 430 Train loss 3.24 on epoch=214
05/30/2022 12:49:54 - INFO - __main__ - Step 440 Global step 440 Train loss 3.21 on epoch=219
05/30/2022 12:49:56 - INFO - __main__ - Step 450 Global step 450 Train loss 3.19 on epoch=224
05/30/2022 12:49:57 - INFO - __main__ - Global step 450 Train loss 3.25 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 12:49:58 - INFO - __main__ - Saving model with best Classification-F1: 0.07777777777777778 -> 0.3333333333333333 on epoch=224, global_step=450
05/30/2022 12:49:59 - INFO - __main__ - Step 460 Global step 460 Train loss 3.14 on epoch=229
05/30/2022 12:50:01 - INFO - __main__ - Step 470 Global step 470 Train loss 3.10 on epoch=234
05/30/2022 12:50:03 - INFO - __main__ - Step 480 Global step 480 Train loss 2.98 on epoch=239
05/30/2022 12:50:05 - INFO - __main__ - Step 490 Global step 490 Train loss 3.04 on epoch=244
05/30/2022 12:50:07 - INFO - __main__ - Step 500 Global step 500 Train loss 2.94 on epoch=249
05/30/2022 12:50:11 - INFO - __main__ - Global step 500 Train loss 3.04 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 12:50:13 - INFO - __main__ - Step 510 Global step 510 Train loss 2.93 on epoch=254
05/30/2022 12:50:15 - INFO - __main__ - Step 520 Global step 520 Train loss 2.88 on epoch=259
05/30/2022 12:50:17 - INFO - __main__ - Step 530 Global step 530 Train loss 2.80 on epoch=264
05/30/2022 12:50:19 - INFO - __main__ - Step 540 Global step 540 Train loss 2.80 on epoch=269
05/30/2022 12:50:21 - INFO - __main__ - Step 550 Global step 550 Train loss 2.69 on epoch=274
05/30/2022 12:50:24 - INFO - __main__ - Global step 550 Train loss 2.82 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 12:50:26 - INFO - __main__ - Step 560 Global step 560 Train loss 2.67 on epoch=279
05/30/2022 12:50:28 - INFO - __main__ - Step 570 Global step 570 Train loss 2.59 on epoch=284
05/30/2022 12:50:30 - INFO - __main__ - Step 580 Global step 580 Train loss 2.49 on epoch=289
05/30/2022 12:50:32 - INFO - __main__ - Step 590 Global step 590 Train loss 2.65 on epoch=294
05/30/2022 12:50:34 - INFO - __main__ - Step 600 Global step 600 Train loss 2.58 on epoch=299
05/30/2022 12:50:38 - INFO - __main__ - Global step 600 Train loss 2.60 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 12:50:40 - INFO - __main__ - Step 610 Global step 610 Train loss 2.54 on epoch=304
05/30/2022 12:50:41 - INFO - __main__ - Step 620 Global step 620 Train loss 2.37 on epoch=309
05/30/2022 12:50:43 - INFO - __main__ - Step 630 Global step 630 Train loss 2.29 on epoch=314
05/30/2022 12:50:45 - INFO - __main__ - Step 640 Global step 640 Train loss 2.32 on epoch=319
05/30/2022 12:50:47 - INFO - __main__ - Step 650 Global step 650 Train loss 2.40 on epoch=324
05/30/2022 12:50:51 - INFO - __main__ - Global step 650 Train loss 2.38 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 12:50:53 - INFO - __main__ - Step 660 Global step 660 Train loss 2.33 on epoch=329
05/30/2022 12:50:55 - INFO - __main__ - Step 670 Global step 670 Train loss 2.19 on epoch=334
05/30/2022 12:50:56 - INFO - __main__ - Step 680 Global step 680 Train loss 2.20 on epoch=339
05/30/2022 12:50:58 - INFO - __main__ - Step 690 Global step 690 Train loss 2.23 on epoch=344
05/30/2022 12:51:00 - INFO - __main__ - Step 700 Global step 700 Train loss 2.26 on epoch=349
05/30/2022 12:51:04 - INFO - __main__ - Global step 700 Train loss 2.24 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 12:51:06 - INFO - __main__ - Step 710 Global step 710 Train loss 2.04 on epoch=354
05/30/2022 12:51:07 - INFO - __main__ - Step 720 Global step 720 Train loss 2.10 on epoch=359
05/30/2022 12:51:09 - INFO - __main__ - Step 730 Global step 730 Train loss 2.03 on epoch=364
05/30/2022 12:51:11 - INFO - __main__ - Step 740 Global step 740 Train loss 2.04 on epoch=369
05/30/2022 12:51:13 - INFO - __main__ - Step 750 Global step 750 Train loss 2.06 on epoch=374
05/30/2022 12:51:16 - INFO - __main__ - Global step 750 Train loss 2.06 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 12:51:18 - INFO - __main__ - Step 760 Global step 760 Train loss 2.04 on epoch=379
05/30/2022 12:51:20 - INFO - __main__ - Step 770 Global step 770 Train loss 1.92 on epoch=384
05/30/2022 12:51:22 - INFO - __main__ - Step 780 Global step 780 Train loss 1.88 on epoch=389
05/30/2022 12:51:24 - INFO - __main__ - Step 790 Global step 790 Train loss 1.88 on epoch=394
05/30/2022 12:51:26 - INFO - __main__ - Step 800 Global step 800 Train loss 1.80 on epoch=399
05/30/2022 12:51:27 - INFO - __main__ - Global step 800 Train loss 1.91 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 12:51:29 - INFO - __main__ - Step 810 Global step 810 Train loss 1.77 on epoch=404
05/30/2022 12:51:31 - INFO - __main__ - Step 820 Global step 820 Train loss 1.66 on epoch=409
05/30/2022 12:51:33 - INFO - __main__ - Step 830 Global step 830 Train loss 1.76 on epoch=414
05/30/2022 12:51:34 - INFO - __main__ - Step 840 Global step 840 Train loss 1.59 on epoch=419
05/30/2022 12:51:36 - INFO - __main__ - Step 850 Global step 850 Train loss 1.57 on epoch=424
05/30/2022 12:51:38 - INFO - __main__ - Global step 850 Train loss 1.67 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 12:51:40 - INFO - __main__ - Step 860 Global step 860 Train loss 1.65 on epoch=429
05/30/2022 12:51:42 - INFO - __main__ - Step 870 Global step 870 Train loss 1.80 on epoch=434
05/30/2022 12:51:44 - INFO - __main__ - Step 880 Global step 880 Train loss 1.54 on epoch=439
05/30/2022 12:51:45 - INFO - __main__ - Step 890 Global step 890 Train loss 1.56 on epoch=444
05/30/2022 12:51:47 - INFO - __main__ - Step 900 Global step 900 Train loss 1.53 on epoch=449
05/30/2022 12:51:50 - INFO - __main__ - Global step 900 Train loss 1.62 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 12:51:52 - INFO - __main__ - Step 910 Global step 910 Train loss 1.41 on epoch=454
05/30/2022 12:51:54 - INFO - __main__ - Step 920 Global step 920 Train loss 1.49 on epoch=459
05/30/2022 12:51:56 - INFO - __main__ - Step 930 Global step 930 Train loss 1.50 on epoch=464
05/30/2022 12:51:58 - INFO - __main__ - Step 940 Global step 940 Train loss 1.32 on epoch=469
05/30/2022 12:52:00 - INFO - __main__ - Step 950 Global step 950 Train loss 1.31 on epoch=474
05/30/2022 12:52:02 - INFO - __main__ - Global step 950 Train loss 1.40 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 12:52:04 - INFO - __main__ - Step 960 Global step 960 Train loss 1.35 on epoch=479
05/30/2022 12:52:06 - INFO - __main__ - Step 970 Global step 970 Train loss 1.42 on epoch=484
05/30/2022 12:52:08 - INFO - __main__ - Step 980 Global step 980 Train loss 1.28 on epoch=489
05/30/2022 12:52:10 - INFO - __main__ - Step 990 Global step 990 Train loss 1.29 on epoch=494
05/30/2022 12:52:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.29 on epoch=499
05/30/2022 12:52:15 - INFO - __main__ - Global step 1000 Train loss 1.33 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 12:52:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.32 on epoch=504
05/30/2022 12:52:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.29 on epoch=509
05/30/2022 12:52:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.15 on epoch=514
05/30/2022 12:52:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.21 on epoch=519
05/30/2022 12:52:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.14 on epoch=524
05/30/2022 12:52:27 - INFO - __main__ - Global step 1050 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 12:52:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.13 on epoch=529
05/30/2022 12:52:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.14 on epoch=534
05/30/2022 12:52:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.12 on epoch=539
05/30/2022 12:52:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.16 on epoch=544
05/30/2022 12:52:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.12 on epoch=549
05/30/2022 12:52:39 - INFO - __main__ - Global step 1100 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 12:52:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.11 on epoch=554
05/30/2022 12:52:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.20 on epoch=559
05/30/2022 12:52:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.03 on epoch=564
05/30/2022 12:52:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.09 on epoch=569
05/30/2022 12:52:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.08 on epoch=574
05/30/2022 12:52:50 - INFO - __main__ - Global step 1150 Train loss 1.10 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 12:52:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.01 on epoch=579
05/30/2022 12:52:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.02 on epoch=584
05/30/2022 12:52:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.97 on epoch=589
05/30/2022 12:52:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.93 on epoch=594
05/30/2022 12:53:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.97 on epoch=599
05/30/2022 12:53:02 - INFO - __main__ - Global step 1200 Train loss 0.98 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 12:53:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.96 on epoch=604
05/30/2022 12:53:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.94 on epoch=609
05/30/2022 12:53:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.88 on epoch=614
05/30/2022 12:53:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.87 on epoch=619
05/30/2022 12:53:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.91 on epoch=624
05/30/2022 12:53:12 - INFO - __main__ - Global step 1250 Train loss 0.91 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 12:53:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.00 on epoch=629
05/30/2022 12:53:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.98 on epoch=634
05/30/2022 12:53:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.82 on epoch=639
05/30/2022 12:53:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.94 on epoch=644
05/30/2022 12:53:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.88 on epoch=649
05/30/2022 12:53:23 - INFO - __main__ - Global step 1300 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 12:53:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.82 on epoch=654
05/30/2022 12:53:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.88 on epoch=659
05/30/2022 12:53:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.83 on epoch=664
05/30/2022 12:53:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.97 on epoch=669
05/30/2022 12:53:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=674
05/30/2022 12:53:33 - INFO - __main__ - Global step 1350 Train loss 0.87 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 12:53:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.80 on epoch=679
05/30/2022 12:53:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.73 on epoch=684
05/30/2022 12:53:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.78 on epoch=689
05/30/2022 12:53:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.78 on epoch=694
05/30/2022 12:53:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.69 on epoch=699
05/30/2022 12:53:44 - INFO - __main__ - Global step 1400 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 12:53:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.81 on epoch=704
05/30/2022 12:53:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.73 on epoch=709
05/30/2022 12:53:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.69 on epoch=714
05/30/2022 12:53:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.73 on epoch=719
05/30/2022 12:53:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.70 on epoch=724
05/30/2022 12:53:54 - INFO - __main__ - Global step 1450 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 12:53:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.70 on epoch=729
05/30/2022 12:53:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.68 on epoch=734
05/30/2022 12:54:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.76 on epoch=739
05/30/2022 12:54:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.70 on epoch=744
05/30/2022 12:54:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.72 on epoch=749
05/30/2022 12:54:04 - INFO - __main__ - Global step 1500 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 12:54:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.72 on epoch=754
05/30/2022 12:54:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.66 on epoch=759
05/30/2022 12:54:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.67 on epoch=764
05/30/2022 12:54:12 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.68 on epoch=769
05/30/2022 12:54:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.68 on epoch=774
05/30/2022 12:54:15 - INFO - __main__ - Global step 1550 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 12:54:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.66 on epoch=779
05/30/2022 12:54:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.73 on epoch=784
05/30/2022 12:54:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.59 on epoch=789
05/30/2022 12:54:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.64 on epoch=794
05/30/2022 12:54:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.71 on epoch=799
05/30/2022 12:54:25 - INFO - __main__ - Global step 1600 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 12:54:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.71 on epoch=804
05/30/2022 12:54:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.64 on epoch=809
05/30/2022 12:54:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.55 on epoch=814
05/30/2022 12:54:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.64 on epoch=819
05/30/2022 12:54:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.62 on epoch=824
05/30/2022 12:54:36 - INFO - __main__ - Global step 1650 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 12:54:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.67 on epoch=829
05/30/2022 12:54:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.63 on epoch=834
05/30/2022 12:54:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.63 on epoch=839
05/30/2022 12:54:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.65 on epoch=844
05/30/2022 12:54:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.57 on epoch=849
05/30/2022 12:54:46 - INFO - __main__ - Global step 1700 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 12:54:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.53 on epoch=854
05/30/2022 12:54:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.59 on epoch=859
05/30/2022 12:54:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.67 on epoch=864
05/30/2022 12:54:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.57 on epoch=869
05/30/2022 12:54:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.57 on epoch=874
05/30/2022 12:54:57 - INFO - __main__ - Global step 1750 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 12:54:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.61 on epoch=879
05/30/2022 12:55:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.58 on epoch=884
05/30/2022 12:55:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.57 on epoch=889
05/30/2022 12:55:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=894
05/30/2022 12:55:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.55 on epoch=899
05/30/2022 12:55:07 - INFO - __main__ - Global step 1800 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 12:55:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.59 on epoch=904
05/30/2022 12:55:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.56 on epoch=909
05/30/2022 12:55:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=914
05/30/2022 12:55:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.54 on epoch=919
05/30/2022 12:55:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.50 on epoch=924
05/30/2022 12:55:18 - INFO - __main__ - Global step 1850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 12:55:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.52 on epoch=929
05/30/2022 12:55:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.49 on epoch=934
05/30/2022 12:55:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=939
05/30/2022 12:55:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
05/30/2022 12:55:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.57 on epoch=949
05/30/2022 12:55:28 - INFO - __main__ - Global step 1900 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 12:55:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.59 on epoch=954
05/30/2022 12:55:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.60 on epoch=959
05/30/2022 12:55:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.53 on epoch=964
05/30/2022 12:55:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.52 on epoch=969
05/30/2022 12:55:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.54 on epoch=974
05/30/2022 12:55:39 - INFO - __main__ - Global step 1950 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 12:55:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.48 on epoch=979
05/30/2022 12:55:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.51 on epoch=984
05/30/2022 12:55:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.56 on epoch=989
05/30/2022 12:55:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.51 on epoch=994
05/30/2022 12:55:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.50 on epoch=999
05/30/2022 12:55:49 - INFO - __main__ - Global step 2000 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 12:55:49 - INFO - __main__ - save last model!
05/30/2022 12:55:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 12:55:49 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 12:55:49 - INFO - __main__ - Printing 3 examples
05/30/2022 12:55:49 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:55:49 - INFO - __main__ - ['entailed']
05/30/2022 12:55:49 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:55:49 - INFO - __main__ - ['entailed']
05/30/2022 12:55:49 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:55:49 - INFO - __main__ - ['entailed']
05/30/2022 12:55:49 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:55:50 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:55:50 - INFO - __main__ - Printing 3 examples
05/30/2022 12:55:50 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 12:55:50 - INFO - __main__ - ['entailed']
05/30/2022 12:55:50 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 12:55:50 - INFO - __main__ - ['entailed']
05/30/2022 12:55:50 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 12:55:50 - INFO - __main__ - ['entailed']
05/30/2022 12:55:50 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:55:50 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:55:50 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 12:55:50 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 12:55:50 - INFO - __main__ - Printing 3 examples
05/30/2022 12:55:50 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/30/2022 12:55:50 - INFO - __main__ - ['entailed']
05/30/2022 12:55:50 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/30/2022 12:55:50 - INFO - __main__ - ['entailed']
05/30/2022 12:55:50 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/30/2022 12:55:50 - INFO - __main__ - ['entailed']
05/30/2022 12:55:50 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:55:50 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:55:50 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 12:55:55 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:55:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 12:55:56 - INFO - __main__ - Starting training!
05/30/2022 12:56:14 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:56:26 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 13:02:08 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.2_8_predictions.txt
05/30/2022 13:02:08 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 13:02:08 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.2, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 13:02:08 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.5, bsz=8 ...
05/30/2022 13:02:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:02:09 - INFO - __main__ - Printing 3 examples
05/30/2022 13:02:09 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 13:02:09 - INFO - __main__ - ['entailed']
05/30/2022 13:02:09 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 13:02:09 - INFO - __main__ - ['entailed']
05/30/2022 13:02:09 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 13:02:09 - INFO - __main__ - ['entailed']
05/30/2022 13:02:09 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:02:09 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:02:09 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 13:02:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:02:09 - INFO - __main__ - Printing 3 examples
05/30/2022 13:02:09 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/30/2022 13:02:09 - INFO - __main__ - ['entailed']
05/30/2022 13:02:09 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/30/2022 13:02:09 - INFO - __main__ - ['entailed']
05/30/2022 13:02:09 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/30/2022 13:02:09 - INFO - __main__ - ['entailed']
05/30/2022 13:02:09 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:02:10 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:02:10 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 13:02:15 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:02:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 13:02:15 - INFO - __main__ - Starting training!
05/30/2022 13:02:17 - INFO - __main__ - Step 10 Global step 10 Train loss 5.10 on epoch=4
05/30/2022 13:02:19 - INFO - __main__ - Step 20 Global step 20 Train loss 4.95 on epoch=9
05/30/2022 13:02:21 - INFO - __main__ - Step 30 Global step 30 Train loss 4.84 on epoch=14
05/30/2022 13:02:23 - INFO - __main__ - Step 40 Global step 40 Train loss 4.85 on epoch=19
05/30/2022 13:02:25 - INFO - __main__ - Step 50 Global step 50 Train loss 4.77 on epoch=24
05/30/2022 13:02:36 - INFO - __main__ - Global step 50 Train loss 4.90 Classification-F1 0.0 on epoch=24
05/30/2022 13:02:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 13:02:38 - INFO - __main__ - Step 60 Global step 60 Train loss 4.63 on epoch=29
05/30/2022 13:02:40 - INFO - __main__ - Step 70 Global step 70 Train loss 4.45 on epoch=34
05/30/2022 13:02:42 - INFO - __main__ - Step 80 Global step 80 Train loss 4.54 on epoch=39
05/30/2022 13:02:44 - INFO - __main__ - Step 90 Global step 90 Train loss 4.29 on epoch=44
05/30/2022 13:02:46 - INFO - __main__ - Step 100 Global step 100 Train loss 4.30 on epoch=49
05/30/2022 13:02:57 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/30/2022 13:02:59 - INFO - __main__ - Step 110 Global step 110 Train loss 4.17 on epoch=54
05/30/2022 13:03:01 - INFO - __main__ - Step 120 Global step 120 Train loss 3.95 on epoch=59
05/30/2022 13:03:03 - INFO - __main__ - Step 130 Global step 130 Train loss 4.03 on epoch=64
05/30/2022 13:03:05 - INFO - __main__ - Step 140 Global step 140 Train loss 3.80 on epoch=69
05/30/2022 13:03:07 - INFO - __main__ - Step 150 Global step 150 Train loss 3.72 on epoch=74
05/30/2022 13:03:13 - INFO - __main__ - Global step 150 Train loss 3.93 Classification-F1 0.04105571847507331 on epoch=74
05/30/2022 13:03:13 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.04105571847507331 on epoch=74, global_step=150
05/30/2022 13:03:15 - INFO - __main__ - Step 160 Global step 160 Train loss 3.54 on epoch=79
05/30/2022 13:03:17 - INFO - __main__ - Step 170 Global step 170 Train loss 3.37 on epoch=84
05/30/2022 13:03:19 - INFO - __main__ - Step 180 Global step 180 Train loss 3.27 on epoch=89
05/30/2022 13:03:21 - INFO - __main__ - Step 190 Global step 190 Train loss 3.11 on epoch=94
05/30/2022 13:03:23 - INFO - __main__ - Step 200 Global step 200 Train loss 2.94 on epoch=99
05/30/2022 13:03:28 - INFO - __main__ - Global step 200 Train loss 3.25 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 13:03:28 - INFO - __main__ - Saving model with best Classification-F1: 0.04105571847507331 -> 0.3333333333333333 on epoch=99, global_step=200
05/30/2022 13:03:29 - INFO - __main__ - Step 210 Global step 210 Train loss 2.80 on epoch=104
05/30/2022 13:03:31 - INFO - __main__ - Step 220 Global step 220 Train loss 2.64 on epoch=109
05/30/2022 13:03:33 - INFO - __main__ - Step 230 Global step 230 Train loss 2.63 on epoch=114
05/30/2022 13:03:35 - INFO - __main__ - Step 240 Global step 240 Train loss 2.53 on epoch=119
05/30/2022 13:03:37 - INFO - __main__ - Step 250 Global step 250 Train loss 2.28 on epoch=124
05/30/2022 13:03:41 - INFO - __main__ - Global step 250 Train loss 2.58 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 13:03:43 - INFO - __main__ - Step 260 Global step 260 Train loss 2.17 on epoch=129
05/30/2022 13:03:45 - INFO - __main__ - Step 270 Global step 270 Train loss 2.15 on epoch=134
05/30/2022 13:03:47 - INFO - __main__ - Step 280 Global step 280 Train loss 1.98 on epoch=139
05/30/2022 13:03:49 - INFO - __main__ - Step 290 Global step 290 Train loss 1.82 on epoch=144
05/30/2022 13:03:51 - INFO - __main__ - Step 300 Global step 300 Train loss 1.75 on epoch=149
05/30/2022 13:03:53 - INFO - __main__ - Global step 300 Train loss 1.97 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 13:03:55 - INFO - __main__ - Step 310 Global step 310 Train loss 1.81 on epoch=154
05/30/2022 13:03:57 - INFO - __main__ - Step 320 Global step 320 Train loss 1.71 on epoch=159
05/30/2022 13:03:59 - INFO - __main__ - Step 330 Global step 330 Train loss 1.73 on epoch=164
05/30/2022 13:04:01 - INFO - __main__ - Step 340 Global step 340 Train loss 1.57 on epoch=169
05/30/2022 13:04:03 - INFO - __main__ - Step 350 Global step 350 Train loss 1.63 on epoch=174
05/30/2022 13:04:06 - INFO - __main__ - Global step 350 Train loss 1.69 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 13:04:07 - INFO - __main__ - Step 360 Global step 360 Train loss 1.44 on epoch=179
05/30/2022 13:04:09 - INFO - __main__ - Step 370 Global step 370 Train loss 1.26 on epoch=184
05/30/2022 13:04:11 - INFO - __main__ - Step 380 Global step 380 Train loss 1.31 on epoch=189
05/30/2022 13:04:13 - INFO - __main__ - Step 390 Global step 390 Train loss 1.25 on epoch=194
05/30/2022 13:04:15 - INFO - __main__ - Step 400 Global step 400 Train loss 1.09 on epoch=199
05/30/2022 13:04:17 - INFO - __main__ - Global step 400 Train loss 1.27 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 13:04:18 - INFO - __main__ - Step 410 Global step 410 Train loss 1.02 on epoch=204
05/30/2022 13:04:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.94 on epoch=209
05/30/2022 13:04:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.97 on epoch=214
05/30/2022 13:04:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=219
05/30/2022 13:04:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=224
05/30/2022 13:04:28 - INFO - __main__ - Global step 450 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 13:04:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.86 on epoch=229
05/30/2022 13:04:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.81 on epoch=234
05/30/2022 13:04:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.83 on epoch=239
05/30/2022 13:04:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.78 on epoch=244
05/30/2022 13:04:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.74 on epoch=249
05/30/2022 13:04:38 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 13:04:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.67 on epoch=254
05/30/2022 13:04:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.76 on epoch=259
05/30/2022 13:04:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.64 on epoch=264
05/30/2022 13:04:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=269
05/30/2022 13:04:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.60 on epoch=274
05/30/2022 13:04:49 - INFO - __main__ - Global step 550 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 13:04:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=279
05/30/2022 13:04:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=284
05/30/2022 13:04:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=289
05/30/2022 13:04:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.57 on epoch=294
05/30/2022 13:04:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=299
05/30/2022 13:04:59 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 13:05:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.50 on epoch=304
05/30/2022 13:05:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=309
05/30/2022 13:05:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.63 on epoch=314
05/30/2022 13:05:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=319
05/30/2022 13:05:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.57 on epoch=324
05/30/2022 13:05:10 - INFO - __main__ - Global step 650 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 13:05:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=329
05/30/2022 13:05:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=334
05/30/2022 13:05:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.53 on epoch=339
05/30/2022 13:05:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=344
05/30/2022 13:05:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.53 on epoch=349
05/30/2022 13:05:20 - INFO - __main__ - Global step 700 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 13:05:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=354
05/30/2022 13:05:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=359
05/30/2022 13:05:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=364
05/30/2022 13:05:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=369
05/30/2022 13:05:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=374
05/30/2022 13:05:30 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 13:05:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=379
05/30/2022 13:05:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=384
05/30/2022 13:05:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=389
05/30/2022 13:05:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=394
05/30/2022 13:05:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=399
05/30/2022 13:05:41 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 13:05:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=404
05/30/2022 13:05:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=409
05/30/2022 13:05:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=414
05/30/2022 13:05:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=419
05/30/2022 13:05:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=424
05/30/2022 13:05:51 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 13:05:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=429
05/30/2022 13:05:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
05/30/2022 13:05:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=439
05/30/2022 13:05:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=444
05/30/2022 13:06:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=449
05/30/2022 13:06:01 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 13:06:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=454
05/30/2022 13:06:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=459
05/30/2022 13:06:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=464
05/30/2022 13:06:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=469
05/30/2022 13:06:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=474
05/30/2022 13:06:11 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 13:06:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=479
05/30/2022 13:06:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=484
05/30/2022 13:06:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=489
05/30/2022 13:06:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=494
05/30/2022 13:06:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=499
05/30/2022 13:06:22 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 13:06:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=504
05/30/2022 13:06:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=509
05/30/2022 13:06:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=514
05/30/2022 13:06:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=519
05/30/2022 13:06:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=524
05/30/2022 13:06:32 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 13:06:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=529
05/30/2022 13:06:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=534
05/30/2022 13:06:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=539
05/30/2022 13:06:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=544
05/30/2022 13:06:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=549
05/30/2022 13:06:42 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 13:06:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=554
05/30/2022 13:06:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=559
05/30/2022 13:06:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=564
05/30/2022 13:06:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=569
05/30/2022 13:06:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=574
05/30/2022 13:06:52 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 13:06:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=579
05/30/2022 13:06:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=584
05/30/2022 13:06:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
05/30/2022 13:07:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=594
05/30/2022 13:07:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=599
05/30/2022 13:07:02 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 13:07:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=604
05/30/2022 13:07:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=609
05/30/2022 13:07:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=614
05/30/2022 13:07:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=619
05/30/2022 13:07:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=624
05/30/2022 13:07:13 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 13:07:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=629
05/30/2022 13:07:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=634
05/30/2022 13:07:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=639
05/30/2022 13:07:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=644
05/30/2022 13:07:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=649
05/30/2022 13:07:23 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 13:07:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=654
05/30/2022 13:07:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=659
05/30/2022 13:07:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=664
05/30/2022 13:07:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=669
05/30/2022 13:07:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=674
05/30/2022 13:07:33 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 13:07:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=679
05/30/2022 13:07:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=684
05/30/2022 13:07:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=689
05/30/2022 13:07:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=694
05/30/2022 13:07:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=699
05/30/2022 13:07:43 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 13:07:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=704
05/30/2022 13:07:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=709
05/30/2022 13:07:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=714
05/30/2022 13:07:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=719
05/30/2022 13:07:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=724
05/30/2022 13:07:53 - INFO - __main__ - Global step 1450 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 13:07:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=729
05/30/2022 13:07:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=734
05/30/2022 13:07:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=739
05/30/2022 13:08:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=744
05/30/2022 13:08:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.28 on epoch=749
05/30/2022 13:08:03 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 13:08:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=754
05/30/2022 13:08:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=759
05/30/2022 13:08:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=764
05/30/2022 13:08:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=769
05/30/2022 13:08:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=774
05/30/2022 13:08:13 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 13:08:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=779
05/30/2022 13:08:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=784
05/30/2022 13:08:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=789
05/30/2022 13:08:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
05/30/2022 13:08:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=799
05/30/2022 13:08:23 - INFO - __main__ - Global step 1600 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 13:08:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=804
05/30/2022 13:08:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=809
05/30/2022 13:08:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.25 on epoch=814
05/30/2022 13:08:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.29 on epoch=819
05/30/2022 13:08:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=824
05/30/2022 13:08:33 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 13:08:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=829
05/30/2022 13:08:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=834
05/30/2022 13:08:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=839
05/30/2022 13:08:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=844
05/30/2022 13:08:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=849
05/30/2022 13:08:43 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 13:08:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.32 on epoch=854
05/30/2022 13:08:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=859
05/30/2022 13:08:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=864
05/30/2022 13:08:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=869
05/30/2022 13:08:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/30/2022 13:08:53 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 13:08:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=879
05/30/2022 13:08:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=884
05/30/2022 13:08:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=889
05/30/2022 13:09:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.29 on epoch=894
05/30/2022 13:09:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=899
05/30/2022 13:09:03 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 13:09:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=904
05/30/2022 13:09:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=909
05/30/2022 13:09:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.28 on epoch=914
05/30/2022 13:09:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=919
05/30/2022 13:09:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=924
05/30/2022 13:09:13 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 13:09:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=929
05/30/2022 13:09:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=934
05/30/2022 13:09:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=939
05/30/2022 13:09:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=944
05/30/2022 13:09:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.28 on epoch=949
05/30/2022 13:09:23 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 13:09:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=954
05/30/2022 13:09:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=959
05/30/2022 13:09:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=964
05/30/2022 13:09:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=969
05/30/2022 13:09:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=974
05/30/2022 13:09:33 - INFO - __main__ - Global step 1950 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 13:09:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=979
05/30/2022 13:09:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=984
05/30/2022 13:09:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.30 on epoch=989
05/30/2022 13:09:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=994
05/30/2022 13:09:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=999
05/30/2022 13:09:43 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 13:09:43 - INFO - __main__ - save last model!
05/30/2022 13:09:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 13:09:43 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 13:09:43 - INFO - __main__ - Printing 3 examples
05/30/2022 13:09:43 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:09:43 - INFO - __main__ - ['entailed']
05/30/2022 13:09:43 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:09:43 - INFO - __main__ - ['entailed']
05/30/2022 13:09:43 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:09:43 - INFO - __main__ - ['entailed']
05/30/2022 13:09:43 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:09:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:09:44 - INFO - __main__ - Printing 3 examples
05/30/2022 13:09:44 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 13:09:44 - INFO - __main__ - ['entailed']
05/30/2022 13:09:44 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 13:09:44 - INFO - __main__ - ['entailed']
05/30/2022 13:09:44 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 13:09:44 - INFO - __main__ - ['entailed']
05/30/2022 13:09:44 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:09:44 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:09:44 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 13:09:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:09:44 - INFO - __main__ - Printing 3 examples
05/30/2022 13:09:44 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/30/2022 13:09:44 - INFO - __main__ - ['entailed']
05/30/2022 13:09:44 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/30/2022 13:09:44 - INFO - __main__ - ['entailed']
05/30/2022 13:09:44 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/30/2022 13:09:44 - INFO - __main__ - ['entailed']
05/30/2022 13:09:44 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:09:44 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:09:44 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 13:09:49 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:09:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 13:09:50 - INFO - __main__ - Starting training!
05/30/2022 13:10:08 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:10:20 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 13:14:29 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.5_8_predictions.txt
05/30/2022 13:14:29 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 13:14:29 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 13:14:29 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.4, bsz=8 ...
05/30/2022 13:14:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:14:30 - INFO - __main__ - Printing 3 examples
05/30/2022 13:14:30 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 13:14:30 - INFO - __main__ - ['entailed']
05/30/2022 13:14:30 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 13:14:30 - INFO - __main__ - ['entailed']
05/30/2022 13:14:30 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 13:14:30 - INFO - __main__ - ['entailed']
05/30/2022 13:14:30 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:14:30 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:14:30 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 13:14:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:14:30 - INFO - __main__ - Printing 3 examples
05/30/2022 13:14:30 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/30/2022 13:14:30 - INFO - __main__ - ['entailed']
05/30/2022 13:14:30 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/30/2022 13:14:30 - INFO - __main__ - ['entailed']
05/30/2022 13:14:30 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/30/2022 13:14:30 - INFO - __main__ - ['entailed']
05/30/2022 13:14:30 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:14:30 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:14:30 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 13:14:36 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:14:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 13:14:36 - INFO - __main__ - Starting training!
05/30/2022 13:14:38 - INFO - __main__ - Step 10 Global step 10 Train loss 5.05 on epoch=4
05/30/2022 13:14:40 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/30/2022 13:14:42 - INFO - __main__ - Step 30 Global step 30 Train loss 4.91 on epoch=14
05/30/2022 13:14:44 - INFO - __main__ - Step 40 Global step 40 Train loss 4.82 on epoch=19
05/30/2022 13:14:46 - INFO - __main__ - Step 50 Global step 50 Train loss 4.74 on epoch=24
05/30/2022 13:14:47 - INFO - __main__ - Global step 50 Train loss 4.91 Classification-F1 0.0 on epoch=24
05/30/2022 13:14:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 13:14:49 - INFO - __main__ - Step 60 Global step 60 Train loss 4.68 on epoch=29
05/30/2022 13:14:51 - INFO - __main__ - Step 70 Global step 70 Train loss 4.55 on epoch=34
05/30/2022 13:14:53 - INFO - __main__ - Step 80 Global step 80 Train loss 4.47 on epoch=39
05/30/2022 13:14:55 - INFO - __main__ - Step 90 Global step 90 Train loss 4.28 on epoch=44
05/30/2022 13:14:57 - INFO - __main__ - Step 100 Global step 100 Train loss 4.26 on epoch=49
05/30/2022 13:14:58 - INFO - __main__ - Global step 100 Train loss 4.45 Classification-F1 0.0 on epoch=49
05/30/2022 13:15:00 - INFO - __main__ - Step 110 Global step 110 Train loss 4.06 on epoch=54
05/30/2022 13:15:01 - INFO - __main__ - Step 120 Global step 120 Train loss 3.95 on epoch=59
05/30/2022 13:15:03 - INFO - __main__ - Step 130 Global step 130 Train loss 3.91 on epoch=64
05/30/2022 13:15:05 - INFO - __main__ - Step 140 Global step 140 Train loss 3.88 on epoch=69
05/30/2022 13:15:07 - INFO - __main__ - Step 150 Global step 150 Train loss 3.79 on epoch=74
05/30/2022 13:15:08 - INFO - __main__ - Global step 150 Train loss 3.92 Classification-F1 0.13333333333333336 on epoch=74
05/30/2022 13:15:08 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.13333333333333336 on epoch=74, global_step=150
05/30/2022 13:15:10 - INFO - __main__ - Step 160 Global step 160 Train loss 3.65 on epoch=79
05/30/2022 13:15:12 - INFO - __main__ - Step 170 Global step 170 Train loss 3.44 on epoch=84
05/30/2022 13:15:14 - INFO - __main__ - Step 180 Global step 180 Train loss 3.48 on epoch=89
05/30/2022 13:15:16 - INFO - __main__ - Step 190 Global step 190 Train loss 3.32 on epoch=94
05/30/2022 13:15:18 - INFO - __main__ - Step 200 Global step 200 Train loss 3.28 on epoch=99
05/30/2022 13:15:19 - INFO - __main__ - Global step 200 Train loss 3.43 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 13:15:19 - INFO - __main__ - Saving model with best Classification-F1: 0.13333333333333336 -> 0.3333333333333333 on epoch=99, global_step=200
05/30/2022 13:15:21 - INFO - __main__ - Step 210 Global step 210 Train loss 3.14 on epoch=104
05/30/2022 13:15:23 - INFO - __main__ - Step 220 Global step 220 Train loss 3.05 on epoch=109
05/30/2022 13:15:25 - INFO - __main__ - Step 230 Global step 230 Train loss 2.92 on epoch=114
05/30/2022 13:15:27 - INFO - __main__ - Step 240 Global step 240 Train loss 2.86 on epoch=119
05/30/2022 13:15:29 - INFO - __main__ - Step 250 Global step 250 Train loss 2.73 on epoch=124
05/30/2022 13:15:31 - INFO - __main__ - Global step 250 Train loss 2.94 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 13:15:33 - INFO - __main__ - Step 260 Global step 260 Train loss 2.49 on epoch=129
05/30/2022 13:15:35 - INFO - __main__ - Step 270 Global step 270 Train loss 2.55 on epoch=134
05/30/2022 13:15:36 - INFO - __main__ - Step 280 Global step 280 Train loss 2.44 on epoch=139
05/30/2022 13:15:38 - INFO - __main__ - Step 290 Global step 290 Train loss 2.42 on epoch=144
05/30/2022 13:15:40 - INFO - __main__ - Step 300 Global step 300 Train loss 2.22 on epoch=149
05/30/2022 13:15:42 - INFO - __main__ - Global step 300 Train loss 2.42 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 13:15:44 - INFO - __main__ - Step 310 Global step 310 Train loss 2.33 on epoch=154
05/30/2022 13:15:46 - INFO - __main__ - Step 320 Global step 320 Train loss 2.20 on epoch=159
05/30/2022 13:15:48 - INFO - __main__ - Step 330 Global step 330 Train loss 2.22 on epoch=164
05/30/2022 13:15:50 - INFO - __main__ - Step 340 Global step 340 Train loss 2.09 on epoch=169
05/30/2022 13:15:52 - INFO - __main__ - Step 350 Global step 350 Train loss 2.01 on epoch=174
05/30/2022 13:15:54 - INFO - __main__ - Global step 350 Train loss 2.17 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 13:15:56 - INFO - __main__ - Step 360 Global step 360 Train loss 1.94 on epoch=179
05/30/2022 13:15:58 - INFO - __main__ - Step 370 Global step 370 Train loss 1.76 on epoch=184
05/30/2022 13:16:00 - INFO - __main__ - Step 380 Global step 380 Train loss 1.66 on epoch=189
05/30/2022 13:16:02 - INFO - __main__ - Step 390 Global step 390 Train loss 1.67 on epoch=194
05/30/2022 13:16:04 - INFO - __main__ - Step 400 Global step 400 Train loss 1.61 on epoch=199
05/30/2022 13:16:06 - INFO - __main__ - Global step 400 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 13:16:08 - INFO - __main__ - Step 410 Global step 410 Train loss 1.52 on epoch=204
05/30/2022 13:16:10 - INFO - __main__ - Step 420 Global step 420 Train loss 1.40 on epoch=209
05/30/2022 13:16:12 - INFO - __main__ - Step 430 Global step 430 Train loss 1.27 on epoch=214
05/30/2022 13:16:13 - INFO - __main__ - Step 440 Global step 440 Train loss 1.21 on epoch=219
05/30/2022 13:16:15 - INFO - __main__ - Step 450 Global step 450 Train loss 1.30 on epoch=224
05/30/2022 13:16:16 - INFO - __main__ - Global step 450 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 13:16:18 - INFO - __main__ - Step 460 Global step 460 Train loss 1.18 on epoch=229
05/30/2022 13:16:20 - INFO - __main__ - Step 470 Global step 470 Train loss 1.13 on epoch=234
05/30/2022 13:16:22 - INFO - __main__ - Step 480 Global step 480 Train loss 1.06 on epoch=239
05/30/2022 13:16:24 - INFO - __main__ - Step 490 Global step 490 Train loss 1.01 on epoch=244
05/30/2022 13:16:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.95 on epoch=249
05/30/2022 13:16:27 - INFO - __main__ - Global step 500 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 13:16:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.94 on epoch=254
05/30/2022 13:16:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=259
05/30/2022 13:16:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.83 on epoch=264
05/30/2022 13:16:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.76 on epoch=269
05/30/2022 13:16:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=274
05/30/2022 13:16:37 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 13:16:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.81 on epoch=279
05/30/2022 13:16:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.93 on epoch=284
05/30/2022 13:16:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.69 on epoch=289
05/30/2022 13:16:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.86 on epoch=294
05/30/2022 13:16:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.77 on epoch=299
05/30/2022 13:16:48 - INFO - __main__ - Global step 600 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 13:16:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=304
05/30/2022 13:16:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.72 on epoch=309
05/30/2022 13:16:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.73 on epoch=314
05/30/2022 13:16:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.72 on epoch=319
05/30/2022 13:16:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.70 on epoch=324
05/30/2022 13:16:58 - INFO - __main__ - Global step 650 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 13:17:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.66 on epoch=329
05/30/2022 13:17:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=334
05/30/2022 13:17:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=339
05/30/2022 13:17:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=344
05/30/2022 13:17:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.66 on epoch=349
05/30/2022 13:17:09 - INFO - __main__ - Global step 700 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 13:17:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.56 on epoch=354
05/30/2022 13:17:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
05/30/2022 13:17:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.69 on epoch=364
05/30/2022 13:17:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.57 on epoch=369
05/30/2022 13:17:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=374
05/30/2022 13:17:19 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 13:17:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.62 on epoch=379
05/30/2022 13:17:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=384
05/30/2022 13:17:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.67 on epoch=389
05/30/2022 13:17:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.54 on epoch=394
05/30/2022 13:17:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.56 on epoch=399
05/30/2022 13:17:30 - INFO - __main__ - Global step 800 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 13:17:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.58 on epoch=404
05/30/2022 13:17:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.56 on epoch=409
05/30/2022 13:17:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.55 on epoch=414
05/30/2022 13:17:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=419
05/30/2022 13:17:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=424
05/30/2022 13:17:40 - INFO - __main__ - Global step 850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 13:17:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=429
05/30/2022 13:17:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=434
05/30/2022 13:17:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=439
05/30/2022 13:17:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
05/30/2022 13:17:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=449
05/30/2022 13:17:50 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 13:17:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=454
05/30/2022 13:17:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.53 on epoch=459
05/30/2022 13:17:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=464
05/30/2022 13:17:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=469
05/30/2022 13:18:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=474
05/30/2022 13:18:01 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 13:18:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.51 on epoch=479
05/30/2022 13:18:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=484
05/30/2022 13:18:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=489
05/30/2022 13:18:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=494
05/30/2022 13:18:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=499
05/30/2022 13:18:11 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 13:18:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=504
05/30/2022 13:18:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=509
05/30/2022 13:18:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=514
05/30/2022 13:18:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=519
05/30/2022 13:18:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=524
05/30/2022 13:18:21 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 13:18:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=529
05/30/2022 13:18:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=534
05/30/2022 13:18:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.51 on epoch=539
05/30/2022 13:18:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=544
05/30/2022 13:18:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=549
05/30/2022 13:18:32 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 13:18:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=554
05/30/2022 13:18:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=559
05/30/2022 13:18:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=564
05/30/2022 13:18:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=569
05/30/2022 13:18:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=574
05/30/2022 13:18:42 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 13:18:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=579
05/30/2022 13:18:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=584
05/30/2022 13:18:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=589
05/30/2022 13:18:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=594
05/30/2022 13:18:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=599
05/30/2022 13:18:52 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=599
05/30/2022 13:18:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=599, global_step=1200
05/30/2022 13:18:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=604
05/30/2022 13:18:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=609
05/30/2022 13:18:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=614
05/30/2022 13:19:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=619
05/30/2022 13:19:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=624
05/30/2022 13:19:02 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 13:19:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=629
05/30/2022 13:19:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=634
05/30/2022 13:19:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=639
05/30/2022 13:19:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=644
05/30/2022 13:19:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=649
05/30/2022 13:19:12 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.3191489361702127 on epoch=649
05/30/2022 13:19:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=654
05/30/2022 13:19:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=659
05/30/2022 13:19:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=664
05/30/2022 13:19:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=669
05/30/2022 13:19:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=674
05/30/2022 13:19:22 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 13:19:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=679
05/30/2022 13:19:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=684
05/30/2022 13:19:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=689
05/30/2022 13:19:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=694
05/30/2022 13:19:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=699
05/30/2022 13:19:32 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 13:19:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=704
05/30/2022 13:19:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=709
05/30/2022 13:19:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=714
05/30/2022 13:19:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=719
05/30/2022 13:19:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=724
05/30/2022 13:19:43 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 13:19:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=729
05/30/2022 13:19:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=734
05/30/2022 13:19:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=739
05/30/2022 13:19:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=744
05/30/2022 13:19:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=749
05/30/2022 13:19:53 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 13:19:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=754
05/30/2022 13:19:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=759
05/30/2022 13:19:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=764
05/30/2022 13:20:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=769
05/30/2022 13:20:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=774
05/30/2022 13:20:03 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 13:20:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=779
05/30/2022 13:20:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
05/30/2022 13:20:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
05/30/2022 13:20:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=794
05/30/2022 13:20:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=799
05/30/2022 13:20:13 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 13:20:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=804
05/30/2022 13:20:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.33 on epoch=809
05/30/2022 13:20:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=814
05/30/2022 13:20:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=819
05/30/2022 13:20:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=824
05/30/2022 13:20:23 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 13:20:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=829
05/30/2022 13:20:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=834
05/30/2022 13:20:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
05/30/2022 13:20:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=844
05/30/2022 13:20:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=849
05/30/2022 13:20:33 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 13:20:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=854
05/30/2022 13:20:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.31 on epoch=859
05/30/2022 13:20:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/30/2022 13:20:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=869
05/30/2022 13:20:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=874
05/30/2022 13:20:43 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 13:20:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=879
05/30/2022 13:20:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=884
05/30/2022 13:20:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=889
05/30/2022 13:20:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=894
05/30/2022 13:20:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
05/30/2022 13:20:53 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 13:20:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=904
05/30/2022 13:20:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/30/2022 13:20:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=914
05/30/2022 13:21:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=919
05/30/2022 13:21:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
05/30/2022 13:21:03 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 13:21:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
05/30/2022 13:21:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
05/30/2022 13:21:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.32 on epoch=939
05/30/2022 13:21:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=944
05/30/2022 13:21:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
05/30/2022 13:21:13 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 13:21:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
05/30/2022 13:21:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=959
05/30/2022 13:21:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=964
05/30/2022 13:21:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
05/30/2022 13:21:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
05/30/2022 13:21:23 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 13:21:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
05/30/2022 13:21:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=984
05/30/2022 13:21:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.27 on epoch=989
05/30/2022 13:21:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/30/2022 13:21:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=999
05/30/2022 13:21:33 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 13:21:33 - INFO - __main__ - save last model!
05/30/2022 13:21:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 13:21:33 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 13:21:33 - INFO - __main__ - Printing 3 examples
05/30/2022 13:21:33 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:21:33 - INFO - __main__ - ['entailed']
05/30/2022 13:21:33 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:21:33 - INFO - __main__ - ['entailed']
05/30/2022 13:21:33 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:21:33 - INFO - __main__ - ['entailed']
05/30/2022 13:21:33 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:21:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:21:34 - INFO - __main__ - Printing 3 examples
05/30/2022 13:21:34 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 13:21:34 - INFO - __main__ - ['entailed']
05/30/2022 13:21:34 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 13:21:34 - INFO - __main__ - ['entailed']
05/30/2022 13:21:34 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 13:21:34 - INFO - __main__ - ['entailed']
05/30/2022 13:21:34 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:21:34 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:21:34 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 13:21:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:21:34 - INFO - __main__ - Printing 3 examples
05/30/2022 13:21:34 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/30/2022 13:21:34 - INFO - __main__ - ['entailed']
05/30/2022 13:21:34 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/30/2022 13:21:34 - INFO - __main__ - ['entailed']
05/30/2022 13:21:34 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/30/2022 13:21:34 - INFO - __main__ - ['entailed']
05/30/2022 13:21:34 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:21:34 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:21:34 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 13:21:39 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:21:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 13:21:39 - INFO - __main__ - Starting training!
05/30/2022 13:21:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:22:10 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 13:26:20 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.4_8_predictions.txt
05/30/2022 13:26:20 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 13:26:20 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.4, bsz=8, dev_performance=0.3992490613266583, test_performance=0.33047210300429186
05/30/2022 13:26:20 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.3, bsz=8 ...
05/30/2022 13:26:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:26:21 - INFO - __main__ - Printing 3 examples
05/30/2022 13:26:21 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 13:26:21 - INFO - __main__ - ['entailed']
05/30/2022 13:26:21 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 13:26:21 - INFO - __main__ - ['entailed']
05/30/2022 13:26:21 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 13:26:21 - INFO - __main__ - ['entailed']
05/30/2022 13:26:21 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:26:21 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:26:21 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 13:26:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:26:21 - INFO - __main__ - Printing 3 examples
05/30/2022 13:26:21 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/30/2022 13:26:21 - INFO - __main__ - ['entailed']
05/30/2022 13:26:21 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/30/2022 13:26:21 - INFO - __main__ - ['entailed']
05/30/2022 13:26:21 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/30/2022 13:26:21 - INFO - __main__ - ['entailed']
05/30/2022 13:26:21 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:26:21 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:26:21 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 13:26:26 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:26:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 13:26:27 - INFO - __main__ - Starting training!
05/30/2022 13:26:29 - INFO - __main__ - Step 10 Global step 10 Train loss 5.11 on epoch=4
05/30/2022 13:26:31 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/30/2022 13:26:33 - INFO - __main__ - Step 30 Global step 30 Train loss 4.91 on epoch=14
05/30/2022 13:26:34 - INFO - __main__ - Step 40 Global step 40 Train loss 4.90 on epoch=19
05/30/2022 13:26:36 - INFO - __main__ - Step 50 Global step 50 Train loss 4.86 on epoch=24
05/30/2022 13:26:37 - INFO - __main__ - Global step 50 Train loss 4.95 Classification-F1 0.0 on epoch=24
05/30/2022 13:26:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 13:26:39 - INFO - __main__ - Step 60 Global step 60 Train loss 4.75 on epoch=29
05/30/2022 13:26:41 - INFO - __main__ - Step 70 Global step 70 Train loss 4.66 on epoch=34
05/30/2022 13:26:43 - INFO - __main__ - Step 80 Global step 80 Train loss 4.62 on epoch=39
05/30/2022 13:26:45 - INFO - __main__ - Step 90 Global step 90 Train loss 4.59 on epoch=44
05/30/2022 13:26:47 - INFO - __main__ - Step 100 Global step 100 Train loss 4.54 on epoch=49
05/30/2022 13:26:58 - INFO - __main__ - Global step 100 Train loss 4.63 Classification-F1 0.0 on epoch=49
05/30/2022 13:27:00 - INFO - __main__ - Step 110 Global step 110 Train loss 4.44 on epoch=54
05/30/2022 13:27:02 - INFO - __main__ - Step 120 Global step 120 Train loss 4.30 on epoch=59
05/30/2022 13:27:03 - INFO - __main__ - Step 130 Global step 130 Train loss 4.27 on epoch=64
05/30/2022 13:27:05 - INFO - __main__ - Step 140 Global step 140 Train loss 4.18 on epoch=69
05/30/2022 13:27:07 - INFO - __main__ - Step 150 Global step 150 Train loss 4.14 on epoch=74
05/30/2022 13:27:13 - INFO - __main__ - Global step 150 Train loss 4.27 Classification-F1 0.0 on epoch=74
05/30/2022 13:27:15 - INFO - __main__ - Step 160 Global step 160 Train loss 4.08 on epoch=79
05/30/2022 13:27:17 - INFO - __main__ - Step 170 Global step 170 Train loss 4.13 on epoch=84
05/30/2022 13:27:19 - INFO - __main__ - Step 180 Global step 180 Train loss 3.95 on epoch=89
05/30/2022 13:27:21 - INFO - __main__ - Step 190 Global step 190 Train loss 3.97 on epoch=94
05/30/2022 13:27:23 - INFO - __main__ - Step 200 Global step 200 Train loss 3.73 on epoch=99
05/30/2022 13:27:29 - INFO - __main__ - Global step 200 Train loss 3.97 Classification-F1 0.041666666666666664 on epoch=99
05/30/2022 13:27:29 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.041666666666666664 on epoch=99, global_step=200
05/30/2022 13:27:31 - INFO - __main__ - Step 210 Global step 210 Train loss 3.72 on epoch=104
05/30/2022 13:27:33 - INFO - __main__ - Step 220 Global step 220 Train loss 3.63 on epoch=109
05/30/2022 13:27:35 - INFO - __main__ - Step 230 Global step 230 Train loss 3.66 on epoch=114
05/30/2022 13:27:37 - INFO - __main__ - Step 240 Global step 240 Train loss 3.57 on epoch=119
05/30/2022 13:27:39 - INFO - __main__ - Step 250 Global step 250 Train loss 3.38 on epoch=124
05/30/2022 13:27:41 - INFO - __main__ - Global step 250 Train loss 3.59 Classification-F1 0.17045454545454544 on epoch=124
05/30/2022 13:27:41 - INFO - __main__ - Saving model with best Classification-F1: 0.041666666666666664 -> 0.17045454545454544 on epoch=124, global_step=250
05/30/2022 13:27:43 - INFO - __main__ - Step 260 Global step 260 Train loss 3.43 on epoch=129
05/30/2022 13:27:45 - INFO - __main__ - Step 270 Global step 270 Train loss 3.31 on epoch=134
05/30/2022 13:27:47 - INFO - __main__ - Step 280 Global step 280 Train loss 3.24 on epoch=139
05/30/2022 13:27:49 - INFO - __main__ - Step 290 Global step 290 Train loss 3.25 on epoch=144
05/30/2022 13:27:51 - INFO - __main__ - Step 300 Global step 300 Train loss 3.07 on epoch=149
05/30/2022 13:27:54 - INFO - __main__ - Global step 300 Train loss 3.26 Classification-F1 0.12727272727272726 on epoch=149
05/30/2022 13:27:56 - INFO - __main__ - Step 310 Global step 310 Train loss 3.09 on epoch=154
05/30/2022 13:27:58 - INFO - __main__ - Step 320 Global step 320 Train loss 3.08 on epoch=159
05/30/2022 13:28:00 - INFO - __main__ - Step 330 Global step 330 Train loss 2.98 on epoch=164
05/30/2022 13:28:02 - INFO - __main__ - Step 340 Global step 340 Train loss 2.89 on epoch=169
05/30/2022 13:28:04 - INFO - __main__ - Step 350 Global step 350 Train loss 2.79 on epoch=174
05/30/2022 13:28:07 - INFO - __main__ - Global step 350 Train loss 2.96 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 13:28:07 - INFO - __main__ - Saving model with best Classification-F1: 0.17045454545454544 -> 0.3333333333333333 on epoch=174, global_step=350
05/30/2022 13:28:09 - INFO - __main__ - Step 360 Global step 360 Train loss 2.87 on epoch=179
05/30/2022 13:28:11 - INFO - __main__ - Step 370 Global step 370 Train loss 2.70 on epoch=184
05/30/2022 13:28:13 - INFO - __main__ - Step 380 Global step 380 Train loss 2.64 on epoch=189
05/30/2022 13:28:15 - INFO - __main__ - Step 390 Global step 390 Train loss 2.52 on epoch=194
05/30/2022 13:28:16 - INFO - __main__ - Step 400 Global step 400 Train loss 2.54 on epoch=199
05/30/2022 13:28:22 - INFO - __main__ - Global step 400 Train loss 2.66 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 13:28:24 - INFO - __main__ - Step 410 Global step 410 Train loss 2.58 on epoch=204
05/30/2022 13:28:26 - INFO - __main__ - Step 420 Global step 420 Train loss 2.52 on epoch=209
05/30/2022 13:28:27 - INFO - __main__ - Step 430 Global step 430 Train loss 2.52 on epoch=214
05/30/2022 13:28:29 - INFO - __main__ - Step 440 Global step 440 Train loss 2.37 on epoch=219
05/30/2022 13:28:31 - INFO - __main__ - Step 450 Global step 450 Train loss 2.34 on epoch=224
05/30/2022 13:28:34 - INFO - __main__ - Global step 450 Train loss 2.47 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 13:28:36 - INFO - __main__ - Step 460 Global step 460 Train loss 2.21 on epoch=229
05/30/2022 13:28:38 - INFO - __main__ - Step 470 Global step 470 Train loss 2.20 on epoch=234
05/30/2022 13:28:40 - INFO - __main__ - Step 480 Global step 480 Train loss 2.15 on epoch=239
05/30/2022 13:28:42 - INFO - __main__ - Step 490 Global step 490 Train loss 2.12 on epoch=244
05/30/2022 13:28:44 - INFO - __main__ - Step 500 Global step 500 Train loss 2.07 on epoch=249
05/30/2022 13:28:46 - INFO - __main__ - Global step 500 Train loss 2.15 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 13:28:48 - INFO - __main__ - Step 510 Global step 510 Train loss 2.11 on epoch=254
05/30/2022 13:28:50 - INFO - __main__ - Step 520 Global step 520 Train loss 1.92 on epoch=259
05/30/2022 13:28:52 - INFO - __main__ - Step 530 Global step 530 Train loss 1.90 on epoch=264
05/30/2022 13:28:54 - INFO - __main__ - Step 540 Global step 540 Train loss 1.90 on epoch=269
05/30/2022 13:28:56 - INFO - __main__ - Step 550 Global step 550 Train loss 1.80 on epoch=274
05/30/2022 13:28:58 - INFO - __main__ - Global step 550 Train loss 1.93 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 13:28:59 - INFO - __main__ - Step 560 Global step 560 Train loss 1.68 on epoch=279
05/30/2022 13:29:01 - INFO - __main__ - Step 570 Global step 570 Train loss 1.80 on epoch=284
05/30/2022 13:29:03 - INFO - __main__ - Step 580 Global step 580 Train loss 1.83 on epoch=289
05/30/2022 13:29:05 - INFO - __main__ - Step 590 Global step 590 Train loss 1.61 on epoch=294
05/30/2022 13:29:07 - INFO - __main__ - Step 600 Global step 600 Train loss 1.72 on epoch=299
05/30/2022 13:29:08 - INFO - __main__ - Global step 600 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 13:29:10 - INFO - __main__ - Step 610 Global step 610 Train loss 1.61 on epoch=304
05/30/2022 13:29:11 - INFO - __main__ - Step 620 Global step 620 Train loss 1.70 on epoch=309
05/30/2022 13:29:13 - INFO - __main__ - Step 630 Global step 630 Train loss 1.57 on epoch=314
05/30/2022 13:29:15 - INFO - __main__ - Step 640 Global step 640 Train loss 1.45 on epoch=319
05/30/2022 13:29:17 - INFO - __main__ - Step 650 Global step 650 Train loss 1.45 on epoch=324
05/30/2022 13:29:18 - INFO - __main__ - Global step 650 Train loss 1.56 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 13:29:20 - INFO - __main__ - Step 660 Global step 660 Train loss 1.59 on epoch=329
05/30/2022 13:29:22 - INFO - __main__ - Step 670 Global step 670 Train loss 1.38 on epoch=334
05/30/2022 13:29:24 - INFO - __main__ - Step 680 Global step 680 Train loss 1.45 on epoch=339
05/30/2022 13:29:25 - INFO - __main__ - Step 690 Global step 690 Train loss 1.30 on epoch=344
05/30/2022 13:29:27 - INFO - __main__ - Step 700 Global step 700 Train loss 1.25 on epoch=349
05/30/2022 13:29:28 - INFO - __main__ - Global step 700 Train loss 1.39 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 13:29:30 - INFO - __main__ - Step 710 Global step 710 Train loss 1.25 on epoch=354
05/30/2022 13:29:32 - INFO - __main__ - Step 720 Global step 720 Train loss 1.28 on epoch=359
05/30/2022 13:29:34 - INFO - __main__ - Step 730 Global step 730 Train loss 1.19 on epoch=364
05/30/2022 13:29:36 - INFO - __main__ - Step 740 Global step 740 Train loss 1.16 on epoch=369
05/30/2022 13:29:37 - INFO - __main__ - Step 750 Global step 750 Train loss 1.25 on epoch=374
05/30/2022 13:29:38 - INFO - __main__ - Global step 750 Train loss 1.23 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 13:29:40 - INFO - __main__ - Step 760 Global step 760 Train loss 1.29 on epoch=379
05/30/2022 13:29:42 - INFO - __main__ - Step 770 Global step 770 Train loss 1.14 on epoch=384
05/30/2022 13:29:44 - INFO - __main__ - Step 780 Global step 780 Train loss 1.11 on epoch=389
05/30/2022 13:29:46 - INFO - __main__ - Step 790 Global step 790 Train loss 1.13 on epoch=394
05/30/2022 13:29:47 - INFO - __main__ - Step 800 Global step 800 Train loss 1.07 on epoch=399
05/30/2022 13:29:48 - INFO - __main__ - Global step 800 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 13:29:50 - INFO - __main__ - Step 810 Global step 810 Train loss 1.11 on epoch=404
05/30/2022 13:29:52 - INFO - __main__ - Step 820 Global step 820 Train loss 1.11 on epoch=409
05/30/2022 13:29:54 - INFO - __main__ - Step 830 Global step 830 Train loss 1.04 on epoch=414
05/30/2022 13:29:56 - INFO - __main__ - Step 840 Global step 840 Train loss 1.05 on epoch=419
05/30/2022 13:29:58 - INFO - __main__ - Step 850 Global step 850 Train loss 1.06 on epoch=424
05/30/2022 13:29:58 - INFO - __main__ - Global step 850 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 13:30:00 - INFO - __main__ - Step 860 Global step 860 Train loss 1.04 on epoch=429
05/30/2022 13:30:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.95 on epoch=434
05/30/2022 13:30:04 - INFO - __main__ - Step 880 Global step 880 Train loss 1.02 on epoch=439
05/30/2022 13:30:06 - INFO - __main__ - Step 890 Global step 890 Train loss 1.05 on epoch=444
05/30/2022 13:30:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.89 on epoch=449
05/30/2022 13:30:09 - INFO - __main__ - Global step 900 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 13:30:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.97 on epoch=454
05/30/2022 13:30:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.91 on epoch=459
05/30/2022 13:30:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.79 on epoch=464
05/30/2022 13:30:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.95 on epoch=469
05/30/2022 13:30:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.89 on epoch=474
05/30/2022 13:30:19 - INFO - __main__ - Global step 950 Train loss 0.90 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 13:30:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.86 on epoch=479
05/30/2022 13:30:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.86 on epoch=484
05/30/2022 13:30:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.78 on epoch=489
05/30/2022 13:30:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.93 on epoch=494
05/30/2022 13:30:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.84 on epoch=499
05/30/2022 13:30:29 - INFO - __main__ - Global step 1000 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 13:30:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.80 on epoch=504
05/30/2022 13:30:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.79 on epoch=509
05/30/2022 13:30:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.83 on epoch=514
05/30/2022 13:30:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.83 on epoch=519
05/30/2022 13:30:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.71 on epoch=524
05/30/2022 13:30:39 - INFO - __main__ - Global step 1050 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 13:30:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.79 on epoch=529
05/30/2022 13:30:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.75 on epoch=534
05/30/2022 13:30:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.73 on epoch=539
05/30/2022 13:30:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.75 on epoch=544
05/30/2022 13:30:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.72 on epoch=549
05/30/2022 13:30:50 - INFO - __main__ - Global step 1100 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 13:30:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.67 on epoch=554
05/30/2022 13:30:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.74 on epoch=559
05/30/2022 13:30:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.72 on epoch=564
05/30/2022 13:30:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.70 on epoch=569
05/30/2022 13:30:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.67 on epoch=574
05/30/2022 13:31:00 - INFO - __main__ - Global step 1150 Train loss 0.70 Classification-F1 0.3191489361702127 on epoch=574
05/30/2022 13:31:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.76 on epoch=579
05/30/2022 13:31:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.79 on epoch=584
05/30/2022 13:31:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.74 on epoch=589
05/30/2022 13:31:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.69 on epoch=594
05/30/2022 13:31:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.70 on epoch=599
05/30/2022 13:31:10 - INFO - __main__ - Global step 1200 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 13:31:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.67 on epoch=604
05/30/2022 13:31:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.71 on epoch=609
05/30/2022 13:31:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.66 on epoch=614
05/30/2022 13:31:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.68 on epoch=619
05/30/2022 13:31:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=624
05/30/2022 13:31:20 - INFO - __main__ - Global step 1250 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 13:31:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.67 on epoch=629
05/30/2022 13:31:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.64 on epoch=634
05/30/2022 13:31:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.68 on epoch=639
05/30/2022 13:31:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.61 on epoch=644
05/30/2022 13:31:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.60 on epoch=649
05/30/2022 13:31:31 - INFO - __main__ - Global step 1300 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 13:31:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.66 on epoch=654
05/30/2022 13:31:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.56 on epoch=659
05/30/2022 13:31:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.53 on epoch=664
05/30/2022 13:31:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.59 on epoch=669
05/30/2022 13:31:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.62 on epoch=674
05/30/2022 13:31:41 - INFO - __main__ - Global step 1350 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 13:31:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.57 on epoch=679
05/30/2022 13:31:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.55 on epoch=684
05/30/2022 13:31:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
05/30/2022 13:31:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.55 on epoch=694
05/30/2022 13:31:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.51 on epoch=699
05/30/2022 13:31:52 - INFO - __main__ - Global step 1400 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 13:31:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.62 on epoch=704
05/30/2022 13:31:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.59 on epoch=709
05/30/2022 13:31:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.63 on epoch=714
05/30/2022 13:32:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.56 on epoch=719
05/30/2022 13:32:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.55 on epoch=724
05/30/2022 13:32:03 - INFO - __main__ - Global step 1450 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 13:32:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
05/30/2022 13:32:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
05/30/2022 13:32:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=739
05/30/2022 13:32:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.53 on epoch=744
05/30/2022 13:32:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.58 on epoch=749
05/30/2022 13:32:13 - INFO - __main__ - Global step 1500 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 13:32:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.55 on epoch=754
05/30/2022 13:32:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.50 on epoch=759
05/30/2022 13:32:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.60 on epoch=764
05/30/2022 13:32:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.51 on epoch=769
05/30/2022 13:32:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.53 on epoch=774
05/30/2022 13:32:23 - INFO - __main__ - Global step 1550 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 13:32:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.50 on epoch=779
05/30/2022 13:32:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
05/30/2022 13:32:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.63 on epoch=789
05/30/2022 13:32:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.54 on epoch=794
05/30/2022 13:32:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=799
05/30/2022 13:32:33 - INFO - __main__ - Global step 1600 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 13:32:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=804
05/30/2022 13:32:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.58 on epoch=809
05/30/2022 13:32:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
05/30/2022 13:32:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=819
05/30/2022 13:32:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.48 on epoch=824
05/30/2022 13:32:43 - INFO - __main__ - Global step 1650 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 13:32:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=829
05/30/2022 13:32:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.48 on epoch=834
05/30/2022 13:32:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.53 on epoch=839
05/30/2022 13:32:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=844
05/30/2022 13:32:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=849
05/30/2022 13:32:53 - INFO - __main__ - Global step 1700 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 13:32:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=854
05/30/2022 13:32:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=859
05/30/2022 13:32:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
05/30/2022 13:33:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=869
05/30/2022 13:33:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.48 on epoch=874
05/30/2022 13:33:03 - INFO - __main__ - Global step 1750 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 13:33:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=879
05/30/2022 13:33:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.49 on epoch=884
05/30/2022 13:33:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=889
05/30/2022 13:33:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
05/30/2022 13:33:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=899
05/30/2022 13:33:13 - INFO - __main__ - Global step 1800 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 13:33:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.56 on epoch=904
05/30/2022 13:33:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
05/30/2022 13:33:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=914
05/30/2022 13:33:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=919
05/30/2022 13:33:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=924
05/30/2022 13:33:23 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 13:33:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
05/30/2022 13:33:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.48 on epoch=934
05/30/2022 13:33:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
05/30/2022 13:33:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=944
05/30/2022 13:33:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.48 on epoch=949
05/30/2022 13:33:34 - INFO - __main__ - Global step 1900 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 13:33:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
05/30/2022 13:33:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
05/30/2022 13:33:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=964
05/30/2022 13:33:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=969
05/30/2022 13:33:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
05/30/2022 13:33:44 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 13:33:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
05/30/2022 13:33:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
05/30/2022 13:33:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=989
05/30/2022 13:33:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=994
05/30/2022 13:33:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=999
05/30/2022 13:33:55 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 13:33:55 - INFO - __main__ - save last model!
05/30/2022 13:33:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 13:33:55 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 13:33:55 - INFO - __main__ - Printing 3 examples
05/30/2022 13:33:55 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:33:55 - INFO - __main__ - ['entailed']
05/30/2022 13:33:55 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:33:55 - INFO - __main__ - ['entailed']
05/30/2022 13:33:55 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:33:55 - INFO - __main__ - ['entailed']
05/30/2022 13:33:55 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:33:55 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:33:55 - INFO - __main__ - Printing 3 examples
05/30/2022 13:33:55 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 13:33:55 - INFO - __main__ - ['entailed']
05/30/2022 13:33:55 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 13:33:55 - INFO - __main__ - ['entailed']
05/30/2022 13:33:55 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 13:33:55 - INFO - __main__ - ['entailed']
05/30/2022 13:33:55 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:33:55 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:33:55 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 13:33:55 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:33:55 - INFO - __main__ - Printing 3 examples
05/30/2022 13:33:55 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/30/2022 13:33:55 - INFO - __main__ - ['entailed']
05/30/2022 13:33:55 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/30/2022 13:33:55 - INFO - __main__ - ['entailed']
05/30/2022 13:33:55 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/30/2022 13:33:55 - INFO - __main__ - ['entailed']
05/30/2022 13:33:55 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:33:55 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:33:55 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 13:34:01 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:34:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 13:34:01 - INFO - __main__ - Starting training!
05/30/2022 13:34:19 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:34:32 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 13:38:39 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.3_8_predictions.txt
05/30/2022 13:38:39 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 13:38:40 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 13:38:40 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.2, bsz=8 ...
05/30/2022 13:38:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:38:40 - INFO - __main__ - Printing 3 examples
05/30/2022 13:38:40 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 13:38:40 - INFO - __main__ - ['entailed']
05/30/2022 13:38:40 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 13:38:40 - INFO - __main__ - ['entailed']
05/30/2022 13:38:40 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 13:38:40 - INFO - __main__ - ['entailed']
05/30/2022 13:38:40 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:38:40 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:38:41 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 13:38:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:38:41 - INFO - __main__ - Printing 3 examples
05/30/2022 13:38:41 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/30/2022 13:38:41 - INFO - __main__ - ['entailed']
05/30/2022 13:38:41 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/30/2022 13:38:41 - INFO - __main__ - ['entailed']
05/30/2022 13:38:41 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/30/2022 13:38:41 - INFO - __main__ - ['entailed']
05/30/2022 13:38:41 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:38:41 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:38:41 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 13:38:46 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:38:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 13:38:46 - INFO - __main__ - Starting training!
05/30/2022 13:38:49 - INFO - __main__ - Step 10 Global step 10 Train loss 5.05 on epoch=4
05/30/2022 13:38:51 - INFO - __main__ - Step 20 Global step 20 Train loss 5.04 on epoch=9
05/30/2022 13:38:52 - INFO - __main__ - Step 30 Global step 30 Train loss 4.99 on epoch=14
05/30/2022 13:38:54 - INFO - __main__ - Step 40 Global step 40 Train loss 4.92 on epoch=19
05/30/2022 13:38:56 - INFO - __main__ - Step 50 Global step 50 Train loss 4.83 on epoch=24
05/30/2022 13:38:57 - INFO - __main__ - Global step 50 Train loss 4.97 Classification-F1 0.0 on epoch=24
05/30/2022 13:38:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 13:38:59 - INFO - __main__ - Step 60 Global step 60 Train loss 4.92 on epoch=29
05/30/2022 13:39:01 - INFO - __main__ - Step 70 Global step 70 Train loss 4.83 on epoch=34
05/30/2022 13:39:03 - INFO - __main__ - Step 80 Global step 80 Train loss 4.73 on epoch=39
05/30/2022 13:39:05 - INFO - __main__ - Step 90 Global step 90 Train loss 4.64 on epoch=44
05/30/2022 13:39:07 - INFO - __main__ - Step 100 Global step 100 Train loss 4.64 on epoch=49
05/30/2022 13:39:08 - INFO - __main__ - Global step 100 Train loss 4.75 Classification-F1 0.0 on epoch=49
05/30/2022 13:39:10 - INFO - __main__ - Step 110 Global step 110 Train loss 4.56 on epoch=54
05/30/2022 13:39:12 - INFO - __main__ - Step 120 Global step 120 Train loss 4.47 on epoch=59
05/30/2022 13:39:14 - INFO - __main__ - Step 130 Global step 130 Train loss 4.45 on epoch=64
05/30/2022 13:39:16 - INFO - __main__ - Step 140 Global step 140 Train loss 4.42 on epoch=69
05/30/2022 13:39:17 - INFO - __main__ - Step 150 Global step 150 Train loss 4.32 on epoch=74
05/30/2022 13:39:19 - INFO - __main__ - Global step 150 Train loss 4.45 Classification-F1 0.0 on epoch=74
05/30/2022 13:39:21 - INFO - __main__ - Step 160 Global step 160 Train loss 4.29 on epoch=79
05/30/2022 13:39:23 - INFO - __main__ - Step 170 Global step 170 Train loss 4.24 on epoch=84
05/30/2022 13:39:24 - INFO - __main__ - Step 180 Global step 180 Train loss 4.11 on epoch=89
05/30/2022 13:39:26 - INFO - __main__ - Step 190 Global step 190 Train loss 4.08 on epoch=94
05/30/2022 13:39:28 - INFO - __main__ - Step 200 Global step 200 Train loss 3.98 on epoch=99
05/30/2022 13:39:29 - INFO - __main__ - Global step 200 Train loss 4.14 Classification-F1 0.0 on epoch=99
05/30/2022 13:39:31 - INFO - __main__ - Step 210 Global step 210 Train loss 3.86 on epoch=104
05/30/2022 13:39:33 - INFO - __main__ - Step 220 Global step 220 Train loss 3.86 on epoch=109
05/30/2022 13:39:35 - INFO - __main__ - Step 230 Global step 230 Train loss 3.75 on epoch=114
05/30/2022 13:39:37 - INFO - __main__ - Step 240 Global step 240 Train loss 3.74 on epoch=119
05/30/2022 13:39:39 - INFO - __main__ - Step 250 Global step 250 Train loss 3.72 on epoch=124
05/30/2022 13:39:41 - INFO - __main__ - Global step 250 Train loss 3.78 Classification-F1 0.0 on epoch=124
05/30/2022 13:39:43 - INFO - __main__ - Step 260 Global step 260 Train loss 3.59 on epoch=129
05/30/2022 13:39:45 - INFO - __main__ - Step 270 Global step 270 Train loss 3.54 on epoch=134
05/30/2022 13:39:46 - INFO - __main__ - Step 280 Global step 280 Train loss 3.55 on epoch=139
05/30/2022 13:39:48 - INFO - __main__ - Step 290 Global step 290 Train loss 3.45 on epoch=144
05/30/2022 13:39:50 - INFO - __main__ - Step 300 Global step 300 Train loss 3.45 on epoch=149
05/30/2022 13:39:52 - INFO - __main__ - Global step 300 Train loss 3.51 Classification-F1 0.11666666666666668 on epoch=149
05/30/2022 13:39:52 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.11666666666666668 on epoch=149, global_step=300
05/30/2022 13:39:54 - INFO - __main__ - Step 310 Global step 310 Train loss 3.39 on epoch=154
05/30/2022 13:39:56 - INFO - __main__ - Step 320 Global step 320 Train loss 3.32 on epoch=159
05/30/2022 13:39:58 - INFO - __main__ - Step 330 Global step 330 Train loss 3.37 on epoch=164
05/30/2022 13:39:59 - INFO - __main__ - Step 340 Global step 340 Train loss 3.34 on epoch=169
05/30/2022 13:40:01 - INFO - __main__ - Step 350 Global step 350 Train loss 3.16 on epoch=174
05/30/2022 13:40:04 - INFO - __main__ - Global step 350 Train loss 3.32 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 13:40:04 - INFO - __main__ - Saving model with best Classification-F1: 0.11666666666666668 -> 0.3333333333333333 on epoch=174, global_step=350
05/30/2022 13:40:06 - INFO - __main__ - Step 360 Global step 360 Train loss 3.21 on epoch=179
05/30/2022 13:40:08 - INFO - __main__ - Step 370 Global step 370 Train loss 3.11 on epoch=184
05/30/2022 13:40:10 - INFO - __main__ - Step 380 Global step 380 Train loss 3.11 on epoch=189
05/30/2022 13:40:12 - INFO - __main__ - Step 390 Global step 390 Train loss 3.14 on epoch=194
05/30/2022 13:40:14 - INFO - __main__ - Step 400 Global step 400 Train loss 2.96 on epoch=199
05/30/2022 13:40:15 - INFO - __main__ - Global step 400 Train loss 3.11 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 13:40:17 - INFO - __main__ - Step 410 Global step 410 Train loss 2.97 on epoch=204
05/30/2022 13:40:19 - INFO - __main__ - Step 420 Global step 420 Train loss 2.98 on epoch=209
05/30/2022 13:40:21 - INFO - __main__ - Step 430 Global step 430 Train loss 2.88 on epoch=214
05/30/2022 13:40:23 - INFO - __main__ - Step 440 Global step 440 Train loss 2.76 on epoch=219
05/30/2022 13:40:25 - INFO - __main__ - Step 450 Global step 450 Train loss 2.77 on epoch=224
05/30/2022 13:40:27 - INFO - __main__ - Global step 450 Train loss 2.88 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 13:40:29 - INFO - __main__ - Step 460 Global step 460 Train loss 2.71 on epoch=229
05/30/2022 13:40:31 - INFO - __main__ - Step 470 Global step 470 Train loss 2.64 on epoch=234
05/30/2022 13:40:33 - INFO - __main__ - Step 480 Global step 480 Train loss 2.58 on epoch=239
05/30/2022 13:40:35 - INFO - __main__ - Step 490 Global step 490 Train loss 2.51 on epoch=244
05/30/2022 13:40:36 - INFO - __main__ - Step 500 Global step 500 Train loss 2.51 on epoch=249
05/30/2022 13:40:38 - INFO - __main__ - Global step 500 Train loss 2.59 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 13:40:40 - INFO - __main__ - Step 510 Global step 510 Train loss 2.37 on epoch=254
05/30/2022 13:40:42 - INFO - __main__ - Step 520 Global step 520 Train loss 2.36 on epoch=259
05/30/2022 13:40:44 - INFO - __main__ - Step 530 Global step 530 Train loss 2.40 on epoch=264
05/30/2022 13:40:46 - INFO - __main__ - Step 540 Global step 540 Train loss 2.21 on epoch=269
05/30/2022 13:40:48 - INFO - __main__ - Step 550 Global step 550 Train loss 2.17 on epoch=274
05/30/2022 13:40:50 - INFO - __main__ - Global step 550 Train loss 2.30 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 13:40:52 - INFO - __main__ - Step 560 Global step 560 Train loss 2.21 on epoch=279
05/30/2022 13:40:54 - INFO - __main__ - Step 570 Global step 570 Train loss 2.24 on epoch=284
05/30/2022 13:40:56 - INFO - __main__ - Step 580 Global step 580 Train loss 2.23 on epoch=289
05/30/2022 13:40:57 - INFO - __main__ - Step 590 Global step 590 Train loss 2.12 on epoch=294
05/30/2022 13:40:59 - INFO - __main__ - Step 600 Global step 600 Train loss 2.04 on epoch=299
05/30/2022 13:41:00 - INFO - __main__ - Global step 600 Train loss 2.17 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 13:41:02 - INFO - __main__ - Step 610 Global step 610 Train loss 2.15 on epoch=304
05/30/2022 13:41:04 - INFO - __main__ - Step 620 Global step 620 Train loss 2.10 on epoch=309
05/30/2022 13:41:06 - INFO - __main__ - Step 630 Global step 630 Train loss 2.05 on epoch=314
05/30/2022 13:41:08 - INFO - __main__ - Step 640 Global step 640 Train loss 1.92 on epoch=319
05/30/2022 13:41:10 - INFO - __main__ - Step 650 Global step 650 Train loss 1.89 on epoch=324
05/30/2022 13:41:11 - INFO - __main__ - Global step 650 Train loss 2.02 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 13:41:12 - INFO - __main__ - Step 660 Global step 660 Train loss 1.94 on epoch=329
05/30/2022 13:41:14 - INFO - __main__ - Step 670 Global step 670 Train loss 1.96 on epoch=334
05/30/2022 13:41:16 - INFO - __main__ - Step 680 Global step 680 Train loss 1.95 on epoch=339
05/30/2022 13:41:18 - INFO - __main__ - Step 690 Global step 690 Train loss 1.83 on epoch=344
05/30/2022 13:41:20 - INFO - __main__ - Step 700 Global step 700 Train loss 1.90 on epoch=349
05/30/2022 13:41:21 - INFO - __main__ - Global step 700 Train loss 1.92 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 13:41:23 - INFO - __main__ - Step 710 Global step 710 Train loss 1.81 on epoch=354
05/30/2022 13:41:25 - INFO - __main__ - Step 720 Global step 720 Train loss 1.69 on epoch=359
05/30/2022 13:41:26 - INFO - __main__ - Step 730 Global step 730 Train loss 1.81 on epoch=364
05/30/2022 13:41:28 - INFO - __main__ - Step 740 Global step 740 Train loss 1.58 on epoch=369
05/30/2022 13:41:30 - INFO - __main__ - Step 750 Global step 750 Train loss 1.62 on epoch=374
05/30/2022 13:41:32 - INFO - __main__ - Global step 750 Train loss 1.70 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 13:41:34 - INFO - __main__ - Step 760 Global step 760 Train loss 1.72 on epoch=379
05/30/2022 13:41:36 - INFO - __main__ - Step 770 Global step 770 Train loss 1.56 on epoch=384
05/30/2022 13:41:38 - INFO - __main__ - Step 780 Global step 780 Train loss 1.70 on epoch=389
05/30/2022 13:41:40 - INFO - __main__ - Step 790 Global step 790 Train loss 1.53 on epoch=394
05/30/2022 13:41:42 - INFO - __main__ - Step 800 Global step 800 Train loss 1.63 on epoch=399
05/30/2022 13:41:44 - INFO - __main__ - Global step 800 Train loss 1.63 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 13:41:46 - INFO - __main__ - Step 810 Global step 810 Train loss 1.70 on epoch=404
05/30/2022 13:41:48 - INFO - __main__ - Step 820 Global step 820 Train loss 1.58 on epoch=409
05/30/2022 13:41:50 - INFO - __main__ - Step 830 Global step 830 Train loss 1.50 on epoch=414
05/30/2022 13:41:52 - INFO - __main__ - Step 840 Global step 840 Train loss 1.55 on epoch=419
05/30/2022 13:41:53 - INFO - __main__ - Step 850 Global step 850 Train loss 1.60 on epoch=424
05/30/2022 13:41:56 - INFO - __main__ - Global step 850 Train loss 1.59 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 13:41:57 - INFO - __main__ - Step 860 Global step 860 Train loss 1.42 on epoch=429
05/30/2022 13:41:59 - INFO - __main__ - Step 870 Global step 870 Train loss 1.46 on epoch=434
05/30/2022 13:42:01 - INFO - __main__ - Step 880 Global step 880 Train loss 1.43 on epoch=439
05/30/2022 13:42:03 - INFO - __main__ - Step 890 Global step 890 Train loss 1.40 on epoch=444
05/30/2022 13:42:05 - INFO - __main__ - Step 900 Global step 900 Train loss 1.42 on epoch=449
05/30/2022 13:42:08 - INFO - __main__ - Global step 900 Train loss 1.43 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 13:42:10 - INFO - __main__ - Step 910 Global step 910 Train loss 1.36 on epoch=454
05/30/2022 13:42:12 - INFO - __main__ - Step 920 Global step 920 Train loss 1.39 on epoch=459
05/30/2022 13:42:13 - INFO - __main__ - Step 930 Global step 930 Train loss 1.49 on epoch=464
05/30/2022 13:42:15 - INFO - __main__ - Step 940 Global step 940 Train loss 1.42 on epoch=469
05/30/2022 13:42:17 - INFO - __main__ - Step 950 Global step 950 Train loss 1.41 on epoch=474
05/30/2022 13:42:20 - INFO - __main__ - Global step 950 Train loss 1.41 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 13:42:22 - INFO - __main__ - Step 960 Global step 960 Train loss 1.25 on epoch=479
05/30/2022 13:42:24 - INFO - __main__ - Step 970 Global step 970 Train loss 1.30 on epoch=484
05/30/2022 13:42:26 - INFO - __main__ - Step 980 Global step 980 Train loss 1.24 on epoch=489
05/30/2022 13:42:28 - INFO - __main__ - Step 990 Global step 990 Train loss 1.25 on epoch=494
05/30/2022 13:42:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.20 on epoch=499
05/30/2022 13:42:31 - INFO - __main__ - Global step 1000 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 13:42:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.18 on epoch=504
05/30/2022 13:42:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.18 on epoch=509
05/30/2022 13:42:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.22 on epoch=514
05/30/2022 13:42:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.27 on epoch=519
05/30/2022 13:42:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.21 on epoch=524
05/30/2022 13:42:41 - INFO - __main__ - Global step 1050 Train loss 1.21 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 13:42:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.17 on epoch=529
05/30/2022 13:42:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.16 on epoch=534
05/30/2022 13:42:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.19 on epoch=539
05/30/2022 13:42:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.30 on epoch=544
05/30/2022 13:42:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.07 on epoch=549
05/30/2022 13:42:51 - INFO - __main__ - Global step 1100 Train loss 1.18 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 13:42:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.05 on epoch=554
05/30/2022 13:42:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.15 on epoch=559
05/30/2022 13:42:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.98 on epoch=564
05/30/2022 13:42:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.08 on epoch=569
05/30/2022 13:43:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.02 on epoch=574
05/30/2022 13:43:02 - INFO - __main__ - Global step 1150 Train loss 1.06 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 13:43:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.98 on epoch=579
05/30/2022 13:43:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.04 on epoch=584
05/30/2022 13:43:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.07 on epoch=589
05/30/2022 13:43:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.04 on epoch=594
05/30/2022 13:43:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.05 on epoch=599
05/30/2022 13:43:12 - INFO - __main__ - Global step 1200 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 13:43:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.94 on epoch=604
05/30/2022 13:43:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.95 on epoch=609
05/30/2022 13:43:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.13 on epoch=614
05/30/2022 13:43:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.01 on epoch=619
05/30/2022 13:43:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.93 on epoch=624
05/30/2022 13:43:22 - INFO - __main__ - Global step 1250 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 13:43:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.88 on epoch=629
05/30/2022 13:43:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.91 on epoch=634
05/30/2022 13:43:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.87 on epoch=639
05/30/2022 13:43:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.90 on epoch=644
05/30/2022 13:43:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.89 on epoch=649
05/30/2022 13:43:33 - INFO - __main__ - Global step 1300 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 13:43:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.91 on epoch=654
05/30/2022 13:43:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.97 on epoch=659
05/30/2022 13:43:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.95 on epoch=664
05/30/2022 13:43:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.93 on epoch=669
05/30/2022 13:43:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=674
05/30/2022 13:43:43 - INFO - __main__ - Global step 1350 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 13:43:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.88 on epoch=679
05/30/2022 13:43:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.86 on epoch=684
05/30/2022 13:43:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.84 on epoch=689
05/30/2022 13:43:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.75 on epoch=694
05/30/2022 13:43:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.81 on epoch=699
05/30/2022 13:43:54 - INFO - __main__ - Global step 1400 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 13:43:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.79 on epoch=704
05/30/2022 13:43:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.80 on epoch=709
05/30/2022 13:43:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.81 on epoch=714
05/30/2022 13:44:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.84 on epoch=719
05/30/2022 13:44:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=724
05/30/2022 13:44:04 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 13:44:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.83 on epoch=729
05/30/2022 13:44:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.79 on epoch=734
05/30/2022 13:44:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.83 on epoch=739
05/30/2022 13:44:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.71 on epoch=744
05/30/2022 13:44:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.73 on epoch=749
05/30/2022 13:44:14 - INFO - __main__ - Global step 1500 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 13:44:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.71 on epoch=754
05/30/2022 13:44:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.79 on epoch=759
05/30/2022 13:44:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.69 on epoch=764
05/30/2022 13:44:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.64 on epoch=769
05/30/2022 13:44:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.65 on epoch=774
05/30/2022 13:44:25 - INFO - __main__ - Global step 1550 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 13:44:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.59 on epoch=779
05/30/2022 13:44:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.67 on epoch=784
05/30/2022 13:44:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.75 on epoch=789
05/30/2022 13:44:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.69 on epoch=794
05/30/2022 13:44:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.64 on epoch=799
05/30/2022 13:44:35 - INFO - __main__ - Global step 1600 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 13:44:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.70 on epoch=804
05/30/2022 13:44:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.68 on epoch=809
05/30/2022 13:44:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.66 on epoch=814
05/30/2022 13:44:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.56 on epoch=819
05/30/2022 13:44:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.60 on epoch=824
05/30/2022 13:44:45 - INFO - __main__ - Global step 1650 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 13:44:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.60 on epoch=829
05/30/2022 13:44:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.64 on epoch=834
05/30/2022 13:44:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.68 on epoch=839
05/30/2022 13:44:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.60 on epoch=844
05/30/2022 13:44:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.65 on epoch=849
05/30/2022 13:44:56 - INFO - __main__ - Global step 1700 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 13:44:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.53 on epoch=854
05/30/2022 13:45:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.69 on epoch=859
05/30/2022 13:45:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.65 on epoch=864
05/30/2022 13:45:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.55 on epoch=869
05/30/2022 13:45:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.57 on epoch=874
05/30/2022 13:45:06 - INFO - __main__ - Global step 1750 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 13:45:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.59 on epoch=879
05/30/2022 13:45:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.58 on epoch=884
05/30/2022 13:45:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
05/30/2022 13:45:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.56 on epoch=894
05/30/2022 13:45:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.64 on epoch=899
05/30/2022 13:45:17 - INFO - __main__ - Global step 1800 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 13:45:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.56 on epoch=904
05/30/2022 13:45:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.58 on epoch=909
05/30/2022 13:45:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.55 on epoch=914
05/30/2022 13:45:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.52 on epoch=919
05/30/2022 13:45:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.51 on epoch=924
05/30/2022 13:45:27 - INFO - __main__ - Global step 1850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 13:45:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.61 on epoch=929
05/30/2022 13:45:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.59 on epoch=934
05/30/2022 13:45:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.50 on epoch=939
05/30/2022 13:45:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.62 on epoch=944
05/30/2022 13:45:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=949
05/30/2022 13:45:37 - INFO - __main__ - Global step 1900 Train loss 0.55 Classification-F1 0.3191489361702127 on epoch=949
05/30/2022 13:45:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.55 on epoch=954
05/30/2022 13:45:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.49 on epoch=959
05/30/2022 13:45:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.50 on epoch=964
05/30/2022 13:45:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.51 on epoch=969
05/30/2022 13:45:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=974
05/30/2022 13:45:48 - INFO - __main__ - Global step 1950 Train loss 0.51 Classification-F1 0.5151515151515151 on epoch=974
05/30/2022 13:45:48 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5151515151515151 on epoch=974, global_step=1950
05/30/2022 13:45:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
05/30/2022 13:45:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=984
05/30/2022 13:45:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=989
05/30/2022 13:45:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.50 on epoch=994
05/30/2022 13:45:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=999
05/30/2022 13:45:58 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 13:45:58 - INFO - __main__ - save last model!
05/30/2022 13:45:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 13:45:58 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 13:45:58 - INFO - __main__ - Printing 3 examples
05/30/2022 13:45:58 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:45:58 - INFO - __main__ - ['entailed']
05/30/2022 13:45:58 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:45:58 - INFO - __main__ - ['entailed']
05/30/2022 13:45:58 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:45:58 - INFO - __main__ - ['entailed']
05/30/2022 13:45:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:45:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:45:58 - INFO - __main__ - Printing 3 examples
05/30/2022 13:45:58 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 13:45:58 - INFO - __main__ - ['refuted']
05/30/2022 13:45:58 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 13:45:58 - INFO - __main__ - ['refuted']
05/30/2022 13:45:58 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 13:45:58 - INFO - __main__ - ['refuted']
05/30/2022 13:45:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:45:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:45:58 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 13:45:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:45:58 - INFO - __main__ - Printing 3 examples
05/30/2022 13:45:58 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/30/2022 13:45:58 - INFO - __main__ - ['refuted']
05/30/2022 13:45:58 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/30/2022 13:45:58 - INFO - __main__ - ['refuted']
05/30/2022 13:45:58 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/30/2022 13:45:58 - INFO - __main__ - ['refuted']
05/30/2022 13:45:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:45:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:45:58 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 13:46:04 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:46:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 13:46:05 - INFO - __main__ - Starting training!
05/30/2022 13:46:22 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:46:35 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 13:52:24 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.2_8_predictions.txt
05/30/2022 13:52:24 - INFO - __main__ - Classification-F1 on test data: 0.3350
05/30/2022 13:52:24 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.2, bsz=8, dev_performance=0.5151515151515151, test_performance=0.3349734394132839
05/30/2022 13:52:24 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.5, bsz=8 ...
05/30/2022 13:52:25 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:52:25 - INFO - __main__ - Printing 3 examples
05/30/2022 13:52:25 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 13:52:25 - INFO - __main__ - ['refuted']
05/30/2022 13:52:25 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 13:52:25 - INFO - __main__ - ['refuted']
05/30/2022 13:52:25 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 13:52:25 - INFO - __main__ - ['refuted']
05/30/2022 13:52:25 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:52:25 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:52:25 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 13:52:25 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:52:25 - INFO - __main__ - Printing 3 examples
05/30/2022 13:52:25 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/30/2022 13:52:25 - INFO - __main__ - ['refuted']
05/30/2022 13:52:25 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/30/2022 13:52:25 - INFO - __main__ - ['refuted']
05/30/2022 13:52:25 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/30/2022 13:52:25 - INFO - __main__ - ['refuted']
05/30/2022 13:52:25 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:52:25 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:52:25 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 13:52:31 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:52:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 13:52:31 - INFO - __main__ - Starting training!
05/30/2022 13:52:33 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/30/2022 13:52:35 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/30/2022 13:52:37 - INFO - __main__ - Step 30 Global step 30 Train loss 4.83 on epoch=14
05/30/2022 13:52:39 - INFO - __main__ - Step 40 Global step 40 Train loss 4.83 on epoch=19
05/30/2022 13:52:41 - INFO - __main__ - Step 50 Global step 50 Train loss 4.62 on epoch=24
05/30/2022 13:52:42 - INFO - __main__ - Global step 50 Train loss 4.86 Classification-F1 0.0 on epoch=24
05/30/2022 13:52:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 13:52:44 - INFO - __main__ - Step 60 Global step 60 Train loss 4.56 on epoch=29
05/30/2022 13:52:46 - INFO - __main__ - Step 70 Global step 70 Train loss 4.38 on epoch=34
05/30/2022 13:52:48 - INFO - __main__ - Step 80 Global step 80 Train loss 4.20 on epoch=39
05/30/2022 13:52:50 - INFO - __main__ - Step 90 Global step 90 Train loss 4.13 on epoch=44
05/30/2022 13:52:52 - INFO - __main__ - Step 100 Global step 100 Train loss 4.00 on epoch=49
05/30/2022 13:52:53 - INFO - __main__ - Global step 100 Train loss 4.25 Classification-F1 0.0 on epoch=49
05/30/2022 13:52:55 - INFO - __main__ - Step 110 Global step 110 Train loss 3.94 on epoch=54
05/30/2022 13:52:57 - INFO - __main__ - Step 120 Global step 120 Train loss 3.78 on epoch=59
05/30/2022 13:52:59 - INFO - __main__ - Step 130 Global step 130 Train loss 3.62 on epoch=64
05/30/2022 13:53:01 - INFO - __main__ - Step 140 Global step 140 Train loss 3.48 on epoch=69
05/30/2022 13:53:03 - INFO - __main__ - Step 150 Global step 150 Train loss 3.34 on epoch=74
05/30/2022 13:53:06 - INFO - __main__ - Global step 150 Train loss 3.63 Classification-F1 0.10526315789473684 on epoch=74
05/30/2022 13:53:06 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.10526315789473684 on epoch=74, global_step=150
05/30/2022 13:53:07 - INFO - __main__ - Step 160 Global step 160 Train loss 3.24 on epoch=79
05/30/2022 13:53:09 - INFO - __main__ - Step 170 Global step 170 Train loss 3.04 on epoch=84
05/30/2022 13:53:11 - INFO - __main__ - Step 180 Global step 180 Train loss 2.89 on epoch=89
05/30/2022 13:53:13 - INFO - __main__ - Step 190 Global step 190 Train loss 2.63 on epoch=94
05/30/2022 13:53:15 - INFO - __main__ - Step 200 Global step 200 Train loss 2.74 on epoch=99
05/30/2022 13:53:16 - INFO - __main__ - Global step 200 Train loss 2.91 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 13:53:16 - INFO - __main__ - Saving model with best Classification-F1: 0.10526315789473684 -> 0.3333333333333333 on epoch=99, global_step=200
05/30/2022 13:53:18 - INFO - __main__ - Step 210 Global step 210 Train loss 2.49 on epoch=104
05/30/2022 13:53:20 - INFO - __main__ - Step 220 Global step 220 Train loss 2.25 on epoch=109
05/30/2022 13:53:22 - INFO - __main__ - Step 230 Global step 230 Train loss 2.25 on epoch=114
05/30/2022 13:53:24 - INFO - __main__ - Step 240 Global step 240 Train loss 2.11 on epoch=119
05/30/2022 13:53:26 - INFO - __main__ - Step 250 Global step 250 Train loss 1.89 on epoch=124
05/30/2022 13:53:28 - INFO - __main__ - Global step 250 Train loss 2.20 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 13:53:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.77 on epoch=129
05/30/2022 13:53:32 - INFO - __main__ - Step 270 Global step 270 Train loss 1.76 on epoch=134
05/30/2022 13:53:34 - INFO - __main__ - Step 280 Global step 280 Train loss 1.68 on epoch=139
05/30/2022 13:53:36 - INFO - __main__ - Step 290 Global step 290 Train loss 1.60 on epoch=144
05/30/2022 13:53:38 - INFO - __main__ - Step 300 Global step 300 Train loss 1.60 on epoch=149
05/30/2022 13:53:39 - INFO - __main__ - Global step 300 Train loss 1.68 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 13:53:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.46 on epoch=154
05/30/2022 13:53:43 - INFO - __main__ - Step 320 Global step 320 Train loss 1.54 on epoch=159
05/30/2022 13:53:45 - INFO - __main__ - Step 330 Global step 330 Train loss 1.45 on epoch=164
05/30/2022 13:53:46 - INFO - __main__ - Step 340 Global step 340 Train loss 1.50 on epoch=169
05/30/2022 13:53:48 - INFO - __main__ - Step 350 Global step 350 Train loss 1.38 on epoch=174
05/30/2022 13:53:51 - INFO - __main__ - Global step 350 Train loss 1.47 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 13:53:53 - INFO - __main__ - Step 360 Global step 360 Train loss 1.30 on epoch=179
05/30/2022 13:53:55 - INFO - __main__ - Step 370 Global step 370 Train loss 1.31 on epoch=184
05/30/2022 13:53:57 - INFO - __main__ - Step 380 Global step 380 Train loss 1.16 on epoch=189
05/30/2022 13:53:59 - INFO - __main__ - Step 390 Global step 390 Train loss 1.28 on epoch=194
05/30/2022 13:54:01 - INFO - __main__ - Step 400 Global step 400 Train loss 1.20 on epoch=199
05/30/2022 13:54:03 - INFO - __main__ - Global step 400 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 13:54:05 - INFO - __main__ - Step 410 Global step 410 Train loss 1.14 on epoch=204
05/30/2022 13:54:07 - INFO - __main__ - Step 420 Global step 420 Train loss 1.20 on epoch=209
05/30/2022 13:54:09 - INFO - __main__ - Step 430 Global step 430 Train loss 1.10 on epoch=214
05/30/2022 13:54:11 - INFO - __main__ - Step 440 Global step 440 Train loss 1.02 on epoch=219
05/30/2022 13:54:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.99 on epoch=224
05/30/2022 13:54:13 - INFO - __main__ - Global step 450 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 13:54:15 - INFO - __main__ - Step 460 Global step 460 Train loss 1.07 on epoch=229
05/30/2022 13:54:17 - INFO - __main__ - Step 470 Global step 470 Train loss 1.01 on epoch=234
05/30/2022 13:54:19 - INFO - __main__ - Step 480 Global step 480 Train loss 1.03 on epoch=239
05/30/2022 13:54:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.93 on epoch=244
05/30/2022 13:54:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.92 on epoch=249
05/30/2022 13:54:24 - INFO - __main__ - Global step 500 Train loss 0.99 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 13:54:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.86 on epoch=254
05/30/2022 13:54:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=259
05/30/2022 13:54:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.85 on epoch=264
05/30/2022 13:54:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.82 on epoch=269
05/30/2022 13:54:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=274
05/30/2022 13:54:34 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 13:54:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.91 on epoch=279
05/30/2022 13:54:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.73 on epoch=284
05/30/2022 13:54:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.71 on epoch=289
05/30/2022 13:54:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.68 on epoch=294
05/30/2022 13:54:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.75 on epoch=299
05/30/2022 13:54:45 - INFO - __main__ - Global step 600 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 13:54:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.81 on epoch=304
05/30/2022 13:54:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.75 on epoch=309
05/30/2022 13:54:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.77 on epoch=314
05/30/2022 13:54:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.66 on epoch=319
05/30/2022 13:54:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.63 on epoch=324
05/30/2022 13:54:55 - INFO - __main__ - Global step 650 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 13:54:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.70 on epoch=329
05/30/2022 13:54:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.65 on epoch=334
05/30/2022 13:55:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=339
05/30/2022 13:55:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.65 on epoch=344
05/30/2022 13:55:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.75 on epoch=349
05/30/2022 13:55:06 - INFO - __main__ - Global step 700 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 13:55:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.61 on epoch=354
05/30/2022 13:55:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
05/30/2022 13:55:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.61 on epoch=364
05/30/2022 13:55:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.59 on epoch=369
05/30/2022 13:55:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=374
05/30/2022 13:55:16 - INFO - __main__ - Global step 750 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 13:55:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.58 on epoch=379
05/30/2022 13:55:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.67 on epoch=384
05/30/2022 13:55:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.56 on epoch=389
05/30/2022 13:55:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.59 on epoch=394
05/30/2022 13:55:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.62 on epoch=399
05/30/2022 13:55:27 - INFO - __main__ - Global step 800 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 13:55:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.54 on epoch=404
05/30/2022 13:55:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.59 on epoch=409
05/30/2022 13:55:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.61 on epoch=414
05/30/2022 13:55:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.53 on epoch=419
05/30/2022 13:55:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=424
05/30/2022 13:55:37 - INFO - __main__ - Global step 850 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 13:55:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=429
05/30/2022 13:55:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=434
05/30/2022 13:55:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=439
05/30/2022 13:55:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=444
05/30/2022 13:55:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=449
05/30/2022 13:55:48 - INFO - __main__ - Global step 900 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 13:55:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=454
05/30/2022 13:55:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.55 on epoch=459
05/30/2022 13:55:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.49 on epoch=464
05/30/2022 13:55:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=469
05/30/2022 13:55:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=474
05/30/2022 13:55:58 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 13:56:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.52 on epoch=479
05/30/2022 13:56:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=484
05/30/2022 13:56:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=489
05/30/2022 13:56:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=494
05/30/2022 13:56:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=499
05/30/2022 13:56:09 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 13:56:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=504
05/30/2022 13:56:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=509
05/30/2022 13:56:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.55 on epoch=514
05/30/2022 13:56:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=519
05/30/2022 13:56:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=524
05/30/2022 13:56:19 - INFO - __main__ - Global step 1050 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 13:56:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=529
05/30/2022 13:56:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=534
05/30/2022 13:56:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=539
05/30/2022 13:56:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
05/30/2022 13:56:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=549
05/30/2022 13:56:29 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 13:56:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=554
05/30/2022 13:56:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=559
05/30/2022 13:56:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=564
05/30/2022 13:56:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.49 on epoch=569
05/30/2022 13:56:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=574
05/30/2022 13:56:40 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 13:56:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=579
05/30/2022 13:56:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=584
05/30/2022 13:56:46 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=589
05/30/2022 13:56:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=594
05/30/2022 13:56:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=599
05/30/2022 13:56:50 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 13:56:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=604
05/30/2022 13:56:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.46 on epoch=609
05/30/2022 13:56:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=614
05/30/2022 13:56:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=619
05/30/2022 13:57:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=624
05/30/2022 13:57:01 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 13:57:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=629
05/30/2022 13:57:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=634
05/30/2022 13:57:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=639
05/30/2022 13:57:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=644
05/30/2022 13:57:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=649
05/30/2022 13:57:12 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.3191489361702127 on epoch=649
05/30/2022 13:57:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=654
05/30/2022 13:57:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=659
05/30/2022 13:57:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=664
05/30/2022 13:57:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=669
05/30/2022 13:57:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=674
05/30/2022 13:57:22 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 13:57:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=679
05/30/2022 13:57:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=684
05/30/2022 13:57:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=689
05/30/2022 13:57:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=694
05/30/2022 13:57:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=699
05/30/2022 13:57:33 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 13:57:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=704
05/30/2022 13:57:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=709
05/30/2022 13:57:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=714
05/30/2022 13:57:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=719
05/30/2022 13:57:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=724
05/30/2022 13:57:43 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 13:57:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=729
05/30/2022 13:57:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=734
05/30/2022 13:57:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=739
05/30/2022 13:57:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=744
05/30/2022 13:57:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=749
05/30/2022 13:57:54 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 13:57:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=754
05/30/2022 13:57:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=759
05/30/2022 13:58:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=764
05/30/2022 13:58:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=769
05/30/2022 13:58:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=774
05/30/2022 13:58:04 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 13:58:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=779
05/30/2022 13:58:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=784
05/30/2022 13:58:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
05/30/2022 13:58:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
05/30/2022 13:58:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=799
05/30/2022 13:58:15 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 13:58:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=804
05/30/2022 13:58:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=809
05/30/2022 13:58:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=814
05/30/2022 13:58:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=819
05/30/2022 13:58:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=824
05/30/2022 13:58:26 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 13:58:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=829
05/30/2022 13:58:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=834
05/30/2022 13:58:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.36 on epoch=839
05/30/2022 13:58:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=844
05/30/2022 13:58:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=849
05/30/2022 13:58:36 - INFO - __main__ - Global step 1700 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 13:58:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/30/2022 13:58:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=859
05/30/2022 13:58:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=864
05/30/2022 13:58:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=869
05/30/2022 13:58:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
05/30/2022 13:58:46 - INFO - __main__ - Global step 1750 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 13:58:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=879
05/30/2022 13:58:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
05/30/2022 13:58:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.31 on epoch=889
05/30/2022 13:58:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=894
05/30/2022 13:58:56 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
05/30/2022 13:58:57 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 13:58:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=904
05/30/2022 13:59:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/30/2022 13:59:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=914
05/30/2022 13:59:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.29 on epoch=919
05/30/2022 13:59:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
05/30/2022 13:59:07 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 13:59:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=929
05/30/2022 13:59:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=934
05/30/2022 13:59:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=939
05/30/2022 13:59:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=944
05/30/2022 13:59:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=949
05/30/2022 13:59:17 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 13:59:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.30 on epoch=954
05/30/2022 13:59:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.34 on epoch=959
05/30/2022 13:59:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/30/2022 13:59:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
05/30/2022 13:59:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=974
05/30/2022 13:59:28 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 13:59:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.33 on epoch=979
05/30/2022 13:59:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=984
05/30/2022 13:59:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.25 on epoch=989
05/30/2022 13:59:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/30/2022 13:59:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=999
05/30/2022 13:59:38 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 13:59:38 - INFO - __main__ - save last model!
05/30/2022 13:59:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 13:59:38 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 13:59:38 - INFO - __main__ - Printing 3 examples
05/30/2022 13:59:38 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:59:38 - INFO - __main__ - ['entailed']
05/30/2022 13:59:38 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:59:38 - INFO - __main__ - ['entailed']
05/30/2022 13:59:38 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:59:38 - INFO - __main__ - ['entailed']
05/30/2022 13:59:38 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:59:39 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:59:39 - INFO - __main__ - Printing 3 examples
05/30/2022 13:59:39 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 13:59:39 - INFO - __main__ - ['refuted']
05/30/2022 13:59:39 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 13:59:39 - INFO - __main__ - ['refuted']
05/30/2022 13:59:39 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 13:59:39 - INFO - __main__ - ['refuted']
05/30/2022 13:59:39 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:59:39 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:59:39 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 13:59:39 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 13:59:39 - INFO - __main__ - Printing 3 examples
05/30/2022 13:59:39 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/30/2022 13:59:39 - INFO - __main__ - ['refuted']
05/30/2022 13:59:39 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/30/2022 13:59:39 - INFO - __main__ - ['refuted']
05/30/2022 13:59:39 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/30/2022 13:59:39 - INFO - __main__ - ['refuted']
05/30/2022 13:59:39 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:59:39 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:59:39 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 13:59:45 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:59:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 13:59:45 - INFO - __main__ - Starting training!
05/30/2022 14:00:02 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:00:15 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 14:04:45 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.5_8_predictions.txt
05/30/2022 14:04:45 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 14:04:46 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 14:04:46 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.4, bsz=8 ...
05/30/2022 14:04:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:04:47 - INFO - __main__ - Printing 3 examples
05/30/2022 14:04:47 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 14:04:47 - INFO - __main__ - ['refuted']
05/30/2022 14:04:47 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 14:04:47 - INFO - __main__ - ['refuted']
05/30/2022 14:04:47 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 14:04:47 - INFO - __main__ - ['refuted']
05/30/2022 14:04:47 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:04:47 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:04:47 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 14:04:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:04:47 - INFO - __main__ - Printing 3 examples
05/30/2022 14:04:47 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/30/2022 14:04:47 - INFO - __main__ - ['refuted']
05/30/2022 14:04:47 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/30/2022 14:04:47 - INFO - __main__ - ['refuted']
05/30/2022 14:04:47 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/30/2022 14:04:47 - INFO - __main__ - ['refuted']
05/30/2022 14:04:47 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:04:47 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:04:47 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 14:04:52 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 14:04:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 14:04:52 - INFO - __main__ - Starting training!
05/30/2022 14:04:55 - INFO - __main__ - Step 10 Global step 10 Train loss 4.97 on epoch=4
05/30/2022 14:04:56 - INFO - __main__ - Step 20 Global step 20 Train loss 5.00 on epoch=9
05/30/2022 14:04:58 - INFO - __main__ - Step 30 Global step 30 Train loss 4.73 on epoch=14
05/30/2022 14:05:00 - INFO - __main__ - Step 40 Global step 40 Train loss 4.75 on epoch=19
05/30/2022 14:05:02 - INFO - __main__ - Step 50 Global step 50 Train loss 4.65 on epoch=24
05/30/2022 14:05:08 - INFO - __main__ - Global step 50 Train loss 4.82 Classification-F1 0.0 on epoch=24
05/30/2022 14:05:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 14:05:10 - INFO - __main__ - Step 60 Global step 60 Train loss 4.51 on epoch=29
05/30/2022 14:05:12 - INFO - __main__ - Step 70 Global step 70 Train loss 4.44 on epoch=34
05/30/2022 14:05:14 - INFO - __main__ - Step 80 Global step 80 Train loss 4.27 on epoch=39
05/30/2022 14:05:16 - INFO - __main__ - Step 90 Global step 90 Train loss 4.15 on epoch=44
05/30/2022 14:05:18 - INFO - __main__ - Step 100 Global step 100 Train loss 4.08 on epoch=49
05/30/2022 14:05:24 - INFO - __main__ - Global step 100 Train loss 4.29 Classification-F1 0.0 on epoch=49
05/30/2022 14:05:26 - INFO - __main__ - Step 110 Global step 110 Train loss 3.96 on epoch=54
05/30/2022 14:05:28 - INFO - __main__ - Step 120 Global step 120 Train loss 3.76 on epoch=59
05/30/2022 14:05:30 - INFO - __main__ - Step 130 Global step 130 Train loss 3.70 on epoch=64
05/30/2022 14:05:32 - INFO - __main__ - Step 140 Global step 140 Train loss 3.53 on epoch=69
05/30/2022 14:05:34 - INFO - __main__ - Step 150 Global step 150 Train loss 3.63 on epoch=74
05/30/2022 14:05:38 - INFO - __main__ - Global step 150 Train loss 3.72 Classification-F1 0.016304347826086953 on epoch=74
05/30/2022 14:05:38 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.016304347826086953 on epoch=74, global_step=150
05/30/2022 14:05:40 - INFO - __main__ - Step 160 Global step 160 Train loss 3.35 on epoch=79
05/30/2022 14:05:42 - INFO - __main__ - Step 170 Global step 170 Train loss 3.28 on epoch=84
05/30/2022 14:05:44 - INFO - __main__ - Step 180 Global step 180 Train loss 3.14 on epoch=89
05/30/2022 14:05:46 - INFO - __main__ - Step 190 Global step 190 Train loss 3.07 on epoch=94
05/30/2022 14:05:47 - INFO - __main__ - Step 200 Global step 200 Train loss 3.01 on epoch=99
05/30/2022 14:05:51 - INFO - __main__ - Global step 200 Train loss 3.17 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 14:05:52 - INFO - __main__ - Saving model with best Classification-F1: 0.016304347826086953 -> 0.3333333333333333 on epoch=99, global_step=200
05/30/2022 14:05:53 - INFO - __main__ - Step 210 Global step 210 Train loss 2.93 on epoch=104
05/30/2022 14:05:55 - INFO - __main__ - Step 220 Global step 220 Train loss 2.89 on epoch=109
05/30/2022 14:05:57 - INFO - __main__ - Step 230 Global step 230 Train loss 2.64 on epoch=114
05/30/2022 14:05:59 - INFO - __main__ - Step 240 Global step 240 Train loss 2.50 on epoch=119
05/30/2022 14:06:01 - INFO - __main__ - Step 250 Global step 250 Train loss 2.54 on epoch=124
05/30/2022 14:06:03 - INFO - __main__ - Global step 250 Train loss 2.70 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 14:06:05 - INFO - __main__ - Step 260 Global step 260 Train loss 2.38 on epoch=129
05/30/2022 14:06:07 - INFO - __main__ - Step 270 Global step 270 Train loss 2.28 on epoch=134
05/30/2022 14:06:09 - INFO - __main__ - Step 280 Global step 280 Train loss 2.24 on epoch=139
05/30/2022 14:06:11 - INFO - __main__ - Step 290 Global step 290 Train loss 2.22 on epoch=144
05/30/2022 14:06:13 - INFO - __main__ - Step 300 Global step 300 Train loss 2.20 on epoch=149
05/30/2022 14:06:24 - INFO - __main__ - Global step 300 Train loss 2.26 Classification-F1 0.10852713178294572 on epoch=149
05/30/2022 14:06:26 - INFO - __main__ - Step 310 Global step 310 Train loss 2.17 on epoch=154
05/30/2022 14:06:28 - INFO - __main__ - Step 320 Global step 320 Train loss 2.00 on epoch=159
05/30/2022 14:06:30 - INFO - __main__ - Step 330 Global step 330 Train loss 2.00 on epoch=164
05/30/2022 14:06:32 - INFO - __main__ - Step 340 Global step 340 Train loss 1.97 on epoch=169
05/30/2022 14:06:34 - INFO - __main__ - Step 350 Global step 350 Train loss 1.86 on epoch=174
05/30/2022 14:06:45 - INFO - __main__ - Global step 350 Train loss 2.00 Classification-F1 0.22695035460992907 on epoch=174
05/30/2022 14:06:46 - INFO - __main__ - Step 360 Global step 360 Train loss 1.90 on epoch=179
05/30/2022 14:06:48 - INFO - __main__ - Step 370 Global step 370 Train loss 1.89 on epoch=184
05/30/2022 14:06:50 - INFO - __main__ - Step 380 Global step 380 Train loss 1.74 on epoch=189
05/30/2022 14:06:52 - INFO - __main__ - Step 390 Global step 390 Train loss 1.78 on epoch=194
05/30/2022 14:06:54 - INFO - __main__ - Step 400 Global step 400 Train loss 1.75 on epoch=199
05/30/2022 14:07:05 - INFO - __main__ - Global step 400 Train loss 1.81 Classification-F1 0.21276595744680848 on epoch=199
05/30/2022 14:07:07 - INFO - __main__ - Step 410 Global step 410 Train loss 1.72 on epoch=204
05/30/2022 14:07:09 - INFO - __main__ - Step 420 Global step 420 Train loss 1.73 on epoch=209
05/30/2022 14:07:10 - INFO - __main__ - Step 430 Global step 430 Train loss 1.65 on epoch=214
05/30/2022 14:07:12 - INFO - __main__ - Step 440 Global step 440 Train loss 1.60 on epoch=219
05/30/2022 14:07:14 - INFO - __main__ - Step 450 Global step 450 Train loss 1.50 on epoch=224
05/30/2022 14:07:16 - INFO - __main__ - Global step 450 Train loss 1.64 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 14:07:18 - INFO - __main__ - Step 460 Global step 460 Train loss 1.40 on epoch=229
05/30/2022 14:07:20 - INFO - __main__ - Step 470 Global step 470 Train loss 1.41 on epoch=234
05/30/2022 14:07:22 - INFO - __main__ - Step 480 Global step 480 Train loss 1.41 on epoch=239
05/30/2022 14:07:24 - INFO - __main__ - Step 490 Global step 490 Train loss 1.25 on epoch=244
05/30/2022 14:07:26 - INFO - __main__ - Step 500 Global step 500 Train loss 1.29 on epoch=249
05/30/2022 14:07:27 - INFO - __main__ - Global step 500 Train loss 1.35 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 14:07:28 - INFO - __main__ - Step 510 Global step 510 Train loss 1.32 on epoch=254
05/30/2022 14:07:30 - INFO - __main__ - Step 520 Global step 520 Train loss 1.34 on epoch=259
05/30/2022 14:07:32 - INFO - __main__ - Step 530 Global step 530 Train loss 1.21 on epoch=264
05/30/2022 14:07:34 - INFO - __main__ - Step 540 Global step 540 Train loss 1.16 on epoch=269
05/30/2022 14:07:36 - INFO - __main__ - Step 550 Global step 550 Train loss 1.08 on epoch=274
05/30/2022 14:07:37 - INFO - __main__ - Global step 550 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 14:07:39 - INFO - __main__ - Step 560 Global step 560 Train loss 1.09 on epoch=279
05/30/2022 14:07:41 - INFO - __main__ - Step 570 Global step 570 Train loss 1.17 on epoch=284
05/30/2022 14:07:43 - INFO - __main__ - Step 580 Global step 580 Train loss 1.16 on epoch=289
05/30/2022 14:07:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.97 on epoch=294
05/30/2022 14:07:46 - INFO - __main__ - Step 600 Global step 600 Train loss 1.04 on epoch=299
05/30/2022 14:07:47 - INFO - __main__ - Global step 600 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 14:07:49 - INFO - __main__ - Step 610 Global step 610 Train loss 1.00 on epoch=304
05/30/2022 14:07:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.98 on epoch=309
05/30/2022 14:07:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.95 on epoch=314
05/30/2022 14:07:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.90 on epoch=319
05/30/2022 14:07:57 - INFO - __main__ - Step 650 Global step 650 Train loss 1.05 on epoch=324
05/30/2022 14:07:57 - INFO - __main__ - Global step 650 Train loss 0.97 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 14:07:59 - INFO - __main__ - Step 660 Global step 660 Train loss 1.03 on epoch=329
05/30/2022 14:08:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.87 on epoch=334
05/30/2022 14:08:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.98 on epoch=339
05/30/2022 14:08:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.91 on epoch=344
05/30/2022 14:08:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.96 on epoch=349
05/30/2022 14:08:08 - INFO - __main__ - Global step 700 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 14:08:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.90 on epoch=354
05/30/2022 14:08:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=359
05/30/2022 14:08:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.81 on epoch=364
05/30/2022 14:08:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.81 on epoch=369
05/30/2022 14:08:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.77 on epoch=374
05/30/2022 14:08:18 - INFO - __main__ - Global step 750 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 14:08:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.73 on epoch=379
05/30/2022 14:08:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=384
05/30/2022 14:08:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.72 on epoch=389
05/30/2022 14:08:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.66 on epoch=394
05/30/2022 14:08:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.75 on epoch=399
05/30/2022 14:08:29 - INFO - __main__ - Global step 800 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 14:08:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.66 on epoch=404
05/30/2022 14:08:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.64 on epoch=409
05/30/2022 14:08:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.76 on epoch=414
05/30/2022 14:08:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.68 on epoch=419
05/30/2022 14:08:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.68 on epoch=424
05/30/2022 14:08:39 - INFO - __main__ - Global step 850 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 14:08:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.77 on epoch=429
05/30/2022 14:08:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.70 on epoch=434
05/30/2022 14:08:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.58 on epoch=439
05/30/2022 14:08:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.60 on epoch=444
05/30/2022 14:08:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.58 on epoch=449
05/30/2022 14:08:49 - INFO - __main__ - Global step 900 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 14:08:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.59 on epoch=454
05/30/2022 14:08:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.64 on epoch=459
05/30/2022 14:08:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.61 on epoch=464
05/30/2022 14:08:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.66 on epoch=469
05/30/2022 14:08:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.62 on epoch=474
05/30/2022 14:09:00 - INFO - __main__ - Global step 950 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 14:09:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.64 on epoch=479
05/30/2022 14:09:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.67 on epoch=484
05/30/2022 14:09:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.55 on epoch=489
05/30/2022 14:09:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.60 on epoch=494
05/30/2022 14:09:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.58 on epoch=499
05/30/2022 14:09:10 - INFO - __main__ - Global step 1000 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 14:09:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=504
05/30/2022 14:09:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=509
05/30/2022 14:09:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=514
05/30/2022 14:09:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=519
05/30/2022 14:09:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=524
05/30/2022 14:09:21 - INFO - __main__ - Global step 1050 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 14:09:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.53 on epoch=529
05/30/2022 14:09:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=534
05/30/2022 14:09:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.55 on epoch=539
05/30/2022 14:09:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.60 on epoch=544
05/30/2022 14:09:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=549
05/30/2022 14:09:31 - INFO - __main__ - Global step 1100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 14:09:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=554
05/30/2022 14:09:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.53 on epoch=559
05/30/2022 14:09:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.55 on epoch=564
05/30/2022 14:09:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=569
05/30/2022 14:09:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=574
05/30/2022 14:09:41 - INFO - __main__ - Global step 1150 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 14:09:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=579
05/30/2022 14:09:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=584
05/30/2022 14:09:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=589
05/30/2022 14:09:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=594
05/30/2022 14:09:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=599
05/30/2022 14:09:52 - INFO - __main__ - Global step 1200 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 14:09:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=604
05/30/2022 14:09:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
05/30/2022 14:09:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=614
05/30/2022 14:09:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=619
05/30/2022 14:10:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=624
05/30/2022 14:10:02 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 14:10:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=629
05/30/2022 14:10:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=634
05/30/2022 14:10:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=639
05/30/2022 14:10:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.51 on epoch=644
05/30/2022 14:10:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=649
05/30/2022 14:10:12 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 14:10:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=654
05/30/2022 14:10:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
05/30/2022 14:10:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=664
05/30/2022 14:10:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=669
05/30/2022 14:10:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=674
05/30/2022 14:10:22 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 14:10:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=679
05/30/2022 14:10:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=684
05/30/2022 14:10:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=689
05/30/2022 14:10:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.54 on epoch=694
05/30/2022 14:10:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.55 on epoch=699
05/30/2022 14:10:33 - INFO - __main__ - Global step 1400 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 14:10:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=704
05/30/2022 14:10:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=709
05/30/2022 14:10:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=714
05/30/2022 14:10:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=719
05/30/2022 14:10:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=724
05/30/2022 14:10:43 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 14:10:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=729
05/30/2022 14:10:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=734
05/30/2022 14:10:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/30/2022 14:10:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=744
05/30/2022 14:10:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=749
05/30/2022 14:10:54 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 14:10:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.45 on epoch=754
05/30/2022 14:10:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=759
05/30/2022 14:10:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=764
05/30/2022 14:11:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=769
05/30/2022 14:11:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=774
05/30/2022 14:11:04 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 14:11:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=779
05/30/2022 14:11:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=784
05/30/2022 14:11:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=789
05/30/2022 14:11:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=794
05/30/2022 14:11:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=799
05/30/2022 14:11:14 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 14:11:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=804
05/30/2022 14:11:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=809
05/30/2022 14:11:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=814
05/30/2022 14:11:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
05/30/2022 14:11:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=824
05/30/2022 14:11:25 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 14:11:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=829
05/30/2022 14:11:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=834
05/30/2022 14:11:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=839
05/30/2022 14:11:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=844
05/30/2022 14:11:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.52 on epoch=849
05/30/2022 14:11:35 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 14:11:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/30/2022 14:11:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=859
05/30/2022 14:11:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=864
05/30/2022 14:11:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=869
05/30/2022 14:11:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
05/30/2022 14:11:45 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 14:11:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=879
05/30/2022 14:11:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
05/30/2022 14:11:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=889
05/30/2022 14:11:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
05/30/2022 14:11:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
05/30/2022 14:11:56 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 14:11:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=904
05/30/2022 14:11:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
05/30/2022 14:12:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=914
05/30/2022 14:12:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=919
05/30/2022 14:12:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
05/30/2022 14:12:06 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 14:12:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=929
05/30/2022 14:12:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=934
05/30/2022 14:12:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.52 on epoch=939
05/30/2022 14:12:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=944
05/30/2022 14:12:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=949
05/30/2022 14:12:16 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 14:12:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=954
05/30/2022 14:12:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
05/30/2022 14:12:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=964
05/30/2022 14:12:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.37 on epoch=969
05/30/2022 14:12:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=974
05/30/2022 14:12:26 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 14:12:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
05/30/2022 14:12:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/30/2022 14:12:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
05/30/2022 14:12:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/30/2022 14:12:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=999
05/30/2022 14:12:36 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 14:12:36 - INFO - __main__ - save last model!
05/30/2022 14:12:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 14:12:36 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 14:12:36 - INFO - __main__ - Printing 3 examples
05/30/2022 14:12:36 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:12:36 - INFO - __main__ - ['entailed']
05/30/2022 14:12:36 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:12:36 - INFO - __main__ - ['entailed']
05/30/2022 14:12:36 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:12:36 - INFO - __main__ - ['entailed']
05/30/2022 14:12:36 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:12:37 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:12:37 - INFO - __main__ - Printing 3 examples
05/30/2022 14:12:37 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 14:12:37 - INFO - __main__ - ['refuted']
05/30/2022 14:12:37 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 14:12:37 - INFO - __main__ - ['refuted']
05/30/2022 14:12:37 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 14:12:37 - INFO - __main__ - ['refuted']
05/30/2022 14:12:37 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:12:37 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:12:37 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 14:12:37 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:12:37 - INFO - __main__ - Printing 3 examples
05/30/2022 14:12:37 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/30/2022 14:12:37 - INFO - __main__ - ['refuted']
05/30/2022 14:12:37 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/30/2022 14:12:37 - INFO - __main__ - ['refuted']
05/30/2022 14:12:37 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/30/2022 14:12:37 - INFO - __main__ - ['refuted']
05/30/2022 14:12:37 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:12:37 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:12:37 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 14:12:42 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 14:12:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 14:12:42 - INFO - __main__ - Starting training!
05/30/2022 14:13:01 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:13:13 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 14:17:21 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.4_8_predictions.txt
05/30/2022 14:17:21 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 14:17:21 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 14:17:21 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.3, bsz=8 ...
05/30/2022 14:17:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:17:22 - INFO - __main__ - Printing 3 examples
05/30/2022 14:17:22 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 14:17:22 - INFO - __main__ - ['refuted']
05/30/2022 14:17:22 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 14:17:22 - INFO - __main__ - ['refuted']
05/30/2022 14:17:22 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 14:17:22 - INFO - __main__ - ['refuted']
05/30/2022 14:17:22 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:17:22 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:17:22 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 14:17:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:17:22 - INFO - __main__ - Printing 3 examples
05/30/2022 14:17:22 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/30/2022 14:17:22 - INFO - __main__ - ['refuted']
05/30/2022 14:17:22 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/30/2022 14:17:22 - INFO - __main__ - ['refuted']
05/30/2022 14:17:22 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/30/2022 14:17:22 - INFO - __main__ - ['refuted']
05/30/2022 14:17:22 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:17:22 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:17:22 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 14:17:28 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 14:17:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 14:17:28 - INFO - __main__ - Starting training!
05/30/2022 14:17:30 - INFO - __main__ - Step 10 Global step 10 Train loss 4.93 on epoch=4
05/30/2022 14:17:32 - INFO - __main__ - Step 20 Global step 20 Train loss 4.96 on epoch=9
05/30/2022 14:17:34 - INFO - __main__ - Step 30 Global step 30 Train loss 4.83 on epoch=14
05/30/2022 14:17:36 - INFO - __main__ - Step 40 Global step 40 Train loss 4.79 on epoch=19
05/30/2022 14:17:38 - INFO - __main__ - Step 50 Global step 50 Train loss 4.76 on epoch=24
05/30/2022 14:17:44 - INFO - __main__ - Global step 50 Train loss 4.85 Classification-F1 0.0 on epoch=24
05/30/2022 14:17:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 14:17:46 - INFO - __main__ - Step 60 Global step 60 Train loss 4.55 on epoch=29
05/30/2022 14:17:48 - INFO - __main__ - Step 70 Global step 70 Train loss 4.49 on epoch=34
05/30/2022 14:17:50 - INFO - __main__ - Step 80 Global step 80 Train loss 4.46 on epoch=39
05/30/2022 14:17:52 - INFO - __main__ - Step 90 Global step 90 Train loss 4.41 on epoch=44
05/30/2022 14:17:54 - INFO - __main__ - Step 100 Global step 100 Train loss 4.27 on epoch=49
05/30/2022 14:17:55 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/30/2022 14:17:57 - INFO - __main__ - Step 110 Global step 110 Train loss 4.04 on epoch=54
05/30/2022 14:17:59 - INFO - __main__ - Step 120 Global step 120 Train loss 3.99 on epoch=59
05/30/2022 14:18:01 - INFO - __main__ - Step 130 Global step 130 Train loss 3.90 on epoch=64
05/30/2022 14:18:03 - INFO - __main__ - Step 140 Global step 140 Train loss 3.86 on epoch=69
05/30/2022 14:18:05 - INFO - __main__ - Step 150 Global step 150 Train loss 3.85 on epoch=74
05/30/2022 14:18:06 - INFO - __main__ - Global step 150 Train loss 3.93 Classification-F1 0.0 on epoch=74
05/30/2022 14:18:08 - INFO - __main__ - Step 160 Global step 160 Train loss 3.69 on epoch=79
05/30/2022 14:18:10 - INFO - __main__ - Step 170 Global step 170 Train loss 3.57 on epoch=84
05/30/2022 14:18:12 - INFO - __main__ - Step 180 Global step 180 Train loss 3.63 on epoch=89
05/30/2022 14:18:13 - INFO - __main__ - Step 190 Global step 190 Train loss 3.55 on epoch=94
05/30/2022 14:18:15 - INFO - __main__ - Step 200 Global step 200 Train loss 3.35 on epoch=99
05/30/2022 14:18:17 - INFO - __main__ - Global step 200 Train loss 3.56 Classification-F1 0.0 on epoch=99
05/30/2022 14:18:18 - INFO - __main__ - Step 210 Global step 210 Train loss 3.37 on epoch=104
05/30/2022 14:18:20 - INFO - __main__ - Step 220 Global step 220 Train loss 3.39 on epoch=109
05/30/2022 14:18:22 - INFO - __main__ - Step 230 Global step 230 Train loss 3.29 on epoch=114
05/30/2022 14:18:24 - INFO - __main__ - Step 240 Global step 240 Train loss 3.10 on epoch=119
05/30/2022 14:18:26 - INFO - __main__ - Step 250 Global step 250 Train loss 3.17 on epoch=124
05/30/2022 14:18:27 - INFO - __main__ - Global step 250 Train loss 3.26 Classification-F1 0.14285714285714285 on epoch=124
05/30/2022 14:18:27 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.14285714285714285 on epoch=124, global_step=250
05/30/2022 14:18:29 - INFO - __main__ - Step 260 Global step 260 Train loss 2.88 on epoch=129
05/30/2022 14:18:31 - INFO - __main__ - Step 270 Global step 270 Train loss 2.99 on epoch=134
05/30/2022 14:18:33 - INFO - __main__ - Step 280 Global step 280 Train loss 2.91 on epoch=139
05/30/2022 14:18:35 - INFO - __main__ - Step 290 Global step 290 Train loss 2.79 on epoch=144
05/30/2022 14:18:37 - INFO - __main__ - Step 300 Global step 300 Train loss 2.69 on epoch=149
05/30/2022 14:18:39 - INFO - __main__ - Global step 300 Train loss 2.85 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 14:18:39 - INFO - __main__ - Saving model with best Classification-F1: 0.14285714285714285 -> 0.3333333333333333 on epoch=149, global_step=300
05/30/2022 14:18:40 - INFO - __main__ - Step 310 Global step 310 Train loss 2.65 on epoch=154
05/30/2022 14:18:42 - INFO - __main__ - Step 320 Global step 320 Train loss 2.49 on epoch=159
05/30/2022 14:18:44 - INFO - __main__ - Step 330 Global step 330 Train loss 2.46 on epoch=164
05/30/2022 14:18:46 - INFO - __main__ - Step 340 Global step 340 Train loss 2.35 on epoch=169
05/30/2022 14:18:48 - INFO - __main__ - Step 350 Global step 350 Train loss 2.37 on epoch=174
05/30/2022 14:18:50 - INFO - __main__ - Global step 350 Train loss 2.46 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 14:18:52 - INFO - __main__ - Step 360 Global step 360 Train loss 2.20 on epoch=179
05/30/2022 14:18:54 - INFO - __main__ - Step 370 Global step 370 Train loss 2.00 on epoch=184
05/30/2022 14:18:56 - INFO - __main__ - Step 380 Global step 380 Train loss 1.99 on epoch=189
05/30/2022 14:18:57 - INFO - __main__ - Step 390 Global step 390 Train loss 1.93 on epoch=194
05/30/2022 14:18:59 - INFO - __main__ - Step 400 Global step 400 Train loss 1.79 on epoch=199
05/30/2022 14:19:02 - INFO - __main__ - Global step 400 Train loss 1.98 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 14:19:04 - INFO - __main__ - Step 410 Global step 410 Train loss 1.78 on epoch=204
05/30/2022 14:19:06 - INFO - __main__ - Step 420 Global step 420 Train loss 1.79 on epoch=209
05/30/2022 14:19:08 - INFO - __main__ - Step 430 Global step 430 Train loss 1.69 on epoch=214
05/30/2022 14:19:09 - INFO - __main__ - Step 440 Global step 440 Train loss 1.56 on epoch=219
05/30/2022 14:19:11 - INFO - __main__ - Step 450 Global step 450 Train loss 1.54 on epoch=224
05/30/2022 14:19:14 - INFO - __main__ - Global step 450 Train loss 1.67 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 14:19:16 - INFO - __main__ - Step 460 Global step 460 Train loss 1.67 on epoch=229
05/30/2022 14:19:18 - INFO - __main__ - Step 470 Global step 470 Train loss 1.39 on epoch=234
05/30/2022 14:19:20 - INFO - __main__ - Step 480 Global step 480 Train loss 1.46 on epoch=239
05/30/2022 14:19:22 - INFO - __main__ - Step 490 Global step 490 Train loss 1.44 on epoch=244
05/30/2022 14:19:23 - INFO - __main__ - Step 500 Global step 500 Train loss 1.30 on epoch=249
05/30/2022 14:19:30 - INFO - __main__ - Global step 500 Train loss 1.45 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 14:19:32 - INFO - __main__ - Step 510 Global step 510 Train loss 1.29 on epoch=254
05/30/2022 14:19:34 - INFO - __main__ - Step 520 Global step 520 Train loss 1.30 on epoch=259
05/30/2022 14:19:35 - INFO - __main__ - Step 530 Global step 530 Train loss 1.34 on epoch=264
05/30/2022 14:19:37 - INFO - __main__ - Step 540 Global step 540 Train loss 1.18 on epoch=269
05/30/2022 14:19:39 - INFO - __main__ - Step 550 Global step 550 Train loss 1.17 on epoch=274
05/30/2022 14:19:46 - INFO - __main__ - Global step 550 Train loss 1.26 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 14:19:48 - INFO - __main__ - Step 560 Global step 560 Train loss 1.27 on epoch=279
05/30/2022 14:19:49 - INFO - __main__ - Step 570 Global step 570 Train loss 1.20 on epoch=284
05/30/2022 14:19:51 - INFO - __main__ - Step 580 Global step 580 Train loss 1.12 on epoch=289
05/30/2022 14:19:53 - INFO - __main__ - Step 590 Global step 590 Train loss 1.11 on epoch=294
05/30/2022 14:19:55 - INFO - __main__ - Step 600 Global step 600 Train loss 1.02 on epoch=299
05/30/2022 14:19:58 - INFO - __main__ - Global step 600 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 14:20:00 - INFO - __main__ - Step 610 Global step 610 Train loss 1.07 on epoch=304
05/30/2022 14:20:02 - INFO - __main__ - Step 620 Global step 620 Train loss 1.07 on epoch=309
05/30/2022 14:20:03 - INFO - __main__ - Step 630 Global step 630 Train loss 1.07 on epoch=314
05/30/2022 14:20:05 - INFO - __main__ - Step 640 Global step 640 Train loss 1.04 on epoch=319
05/30/2022 14:20:07 - INFO - __main__ - Step 650 Global step 650 Train loss 1.07 on epoch=324
05/30/2022 14:20:08 - INFO - __main__ - Global step 650 Train loss 1.06 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 14:20:10 - INFO - __main__ - Step 660 Global step 660 Train loss 1.06 on epoch=329
05/30/2022 14:20:12 - INFO - __main__ - Step 670 Global step 670 Train loss 1.01 on epoch=334
05/30/2022 14:20:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.98 on epoch=339
05/30/2022 14:20:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.98 on epoch=344
05/30/2022 14:20:18 - INFO - __main__ - Step 700 Global step 700 Train loss 1.04 on epoch=349
05/30/2022 14:20:18 - INFO - __main__ - Global step 700 Train loss 1.01 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 14:20:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.89 on epoch=354
05/30/2022 14:20:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.95 on epoch=359
05/30/2022 14:20:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.95 on epoch=364
05/30/2022 14:20:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.92 on epoch=369
05/30/2022 14:20:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.92 on epoch=374
05/30/2022 14:20:29 - INFO - __main__ - Global step 750 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 14:20:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=379
05/30/2022 14:20:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.92 on epoch=384
05/30/2022 14:20:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.81 on epoch=389
05/30/2022 14:20:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=394
05/30/2022 14:20:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.87 on epoch=399
05/30/2022 14:20:39 - INFO - __main__ - Global step 800 Train loss 0.86 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 14:20:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=404
05/30/2022 14:20:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.75 on epoch=409
05/30/2022 14:20:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=414
05/30/2022 14:20:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.78 on epoch=419
05/30/2022 14:20:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.85 on epoch=424
05/30/2022 14:20:49 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 14:20:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.75 on epoch=429
05/30/2022 14:20:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.81 on epoch=434
05/30/2022 14:20:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.68 on epoch=439
05/30/2022 14:20:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.68 on epoch=444
05/30/2022 14:20:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.68 on epoch=449
05/30/2022 14:21:00 - INFO - __main__ - Global step 900 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 14:21:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.68 on epoch=454
05/30/2022 14:21:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.71 on epoch=459
05/30/2022 14:21:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.63 on epoch=464
05/30/2022 14:21:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.70 on epoch=469
05/30/2022 14:21:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.75 on epoch=474
05/30/2022 14:21:10 - INFO - __main__ - Global step 950 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 14:21:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.63 on epoch=479
05/30/2022 14:21:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.64 on epoch=484
05/30/2022 14:21:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.62 on epoch=489
05/30/2022 14:21:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.66 on epoch=494
05/30/2022 14:21:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.76 on epoch=499
05/30/2022 14:21:20 - INFO - __main__ - Global step 1000 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 14:21:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.63 on epoch=504
05/30/2022 14:21:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.71 on epoch=509
05/30/2022 14:21:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=514
05/30/2022 14:21:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.68 on epoch=519
05/30/2022 14:21:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=524
05/30/2022 14:21:30 - INFO - __main__ - Global step 1050 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 14:21:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.65 on epoch=529
05/30/2022 14:21:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.65 on epoch=534
05/30/2022 14:21:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.65 on epoch=539
05/30/2022 14:21:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.57 on epoch=544
05/30/2022 14:21:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=549
05/30/2022 14:21:41 - INFO - __main__ - Global step 1100 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 14:21:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.61 on epoch=554
05/30/2022 14:21:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.55 on epoch=559
05/30/2022 14:21:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.56 on epoch=564
05/30/2022 14:21:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.56 on epoch=569
05/30/2022 14:21:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.57 on epoch=574
05/30/2022 14:21:51 - INFO - __main__ - Global step 1150 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 14:21:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.53 on epoch=579
05/30/2022 14:21:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=584
05/30/2022 14:21:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=589
05/30/2022 14:21:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.51 on epoch=594
05/30/2022 14:22:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.50 on epoch=599
05/30/2022 14:22:01 - INFO - __main__ - Global step 1200 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 14:22:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.48 on epoch=604
05/30/2022 14:22:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.52 on epoch=609
05/30/2022 14:22:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=614
05/30/2022 14:22:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.53 on epoch=619
05/30/2022 14:22:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=624
05/30/2022 14:22:12 - INFO - __main__ - Global step 1250 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 14:22:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=629
05/30/2022 14:22:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=634
05/30/2022 14:22:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=639
05/30/2022 14:22:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=644
05/30/2022 14:22:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=649
05/30/2022 14:22:22 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 14:22:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=654
05/30/2022 14:22:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.52 on epoch=659
05/30/2022 14:22:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=664
05/30/2022 14:22:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=669
05/30/2022 14:22:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=674
05/30/2022 14:22:32 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 14:22:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=679
05/30/2022 14:22:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
05/30/2022 14:22:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=689
05/30/2022 14:22:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=694
05/30/2022 14:22:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=699
05/30/2022 14:22:42 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 14:22:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.47 on epoch=704
05/30/2022 14:22:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
05/30/2022 14:22:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
05/30/2022 14:22:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
05/30/2022 14:22:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=724
05/30/2022 14:22:52 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 14:22:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=729
05/30/2022 14:22:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=734
05/30/2022 14:22:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
05/30/2022 14:23:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=744
05/30/2022 14:23:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=749
05/30/2022 14:23:02 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 14:23:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=754
05/30/2022 14:23:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=759
05/30/2022 14:23:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=764
05/30/2022 14:23:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=769
05/30/2022 14:23:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=774
05/30/2022 14:23:12 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 14:23:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=779
05/30/2022 14:23:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
05/30/2022 14:23:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
05/30/2022 14:23:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=794
05/30/2022 14:23:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
05/30/2022 14:23:23 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 14:23:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=804
05/30/2022 14:23:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=809
05/30/2022 14:23:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=814
05/30/2022 14:23:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=819
05/30/2022 14:23:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=824
05/30/2022 14:23:33 - INFO - __main__ - Global step 1650 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 14:23:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=829
05/30/2022 14:23:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
05/30/2022 14:23:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
05/30/2022 14:23:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=844
05/30/2022 14:23:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=849
05/30/2022 14:23:43 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 14:23:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
05/30/2022 14:23:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=859
05/30/2022 14:23:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=864
05/30/2022 14:23:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
05/30/2022 14:23:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
05/30/2022 14:23:53 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 14:23:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=879
05/30/2022 14:23:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=884
05/30/2022 14:23:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=889
05/30/2022 14:24:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=894
05/30/2022 14:24:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=899
05/30/2022 14:24:03 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 14:24:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=904
05/30/2022 14:24:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
05/30/2022 14:24:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=914
05/30/2022 14:24:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=919
05/30/2022 14:24:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
05/30/2022 14:24:13 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 14:24:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=929
05/30/2022 14:24:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
05/30/2022 14:24:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=939
05/30/2022 14:24:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=944
05/30/2022 14:24:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
05/30/2022 14:24:24 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 14:24:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=954
05/30/2022 14:24:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=959
05/30/2022 14:24:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
05/30/2022 14:24:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=969
05/30/2022 14:24:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/30/2022 14:24:34 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 14:24:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
05/30/2022 14:24:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=984
05/30/2022 14:24:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
05/30/2022 14:24:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
05/30/2022 14:24:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=999
05/30/2022 14:24:44 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 14:24:44 - INFO - __main__ - save last model!
05/30/2022 14:24:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 14:24:44 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 14:24:44 - INFO - __main__ - Printing 3 examples
05/30/2022 14:24:44 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:24:44 - INFO - __main__ - ['entailed']
05/30/2022 14:24:44 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:24:44 - INFO - __main__ - ['entailed']
05/30/2022 14:24:44 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:24:44 - INFO - __main__ - ['entailed']
05/30/2022 14:24:44 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:24:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:24:44 - INFO - __main__ - Printing 3 examples
05/30/2022 14:24:44 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 14:24:44 - INFO - __main__ - ['refuted']
05/30/2022 14:24:44 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 14:24:44 - INFO - __main__ - ['refuted']
05/30/2022 14:24:44 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 14:24:44 - INFO - __main__ - ['refuted']
05/30/2022 14:24:44 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:24:44 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:24:44 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 14:24:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:24:44 - INFO - __main__ - Printing 3 examples
05/30/2022 14:24:44 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/30/2022 14:24:44 - INFO - __main__ - ['refuted']
05/30/2022 14:24:44 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/30/2022 14:24:44 - INFO - __main__ - ['refuted']
05/30/2022 14:24:44 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/30/2022 14:24:44 - INFO - __main__ - ['refuted']
05/30/2022 14:24:44 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:24:44 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:24:45 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 14:24:51 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 14:24:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 14:24:51 - INFO - __main__ - Starting training!
05/30/2022 14:25:08 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:25:21 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 14:29:30 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.3_8_predictions.txt
05/30/2022 14:29:30 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 14:29:30 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 14:29:30 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.2, bsz=8 ...
05/30/2022 14:29:31 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:29:31 - INFO - __main__ - Printing 3 examples
05/30/2022 14:29:31 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 14:29:31 - INFO - __main__ - ['refuted']
05/30/2022 14:29:31 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 14:29:31 - INFO - __main__ - ['refuted']
05/30/2022 14:29:31 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 14:29:31 - INFO - __main__ - ['refuted']
05/30/2022 14:29:31 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:29:31 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:29:31 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 14:29:31 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:29:31 - INFO - __main__ - Printing 3 examples
05/30/2022 14:29:31 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/30/2022 14:29:31 - INFO - __main__ - ['refuted']
05/30/2022 14:29:31 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/30/2022 14:29:31 - INFO - __main__ - ['refuted']
05/30/2022 14:29:31 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/30/2022 14:29:31 - INFO - __main__ - ['refuted']
05/30/2022 14:29:31 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:29:31 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:29:31 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 14:29:37 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 14:29:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 14:29:37 - INFO - __main__ - Starting training!
05/30/2022 14:29:40 - INFO - __main__ - Step 10 Global step 10 Train loss 4.93 on epoch=4
05/30/2022 14:29:41 - INFO - __main__ - Step 20 Global step 20 Train loss 4.93 on epoch=9
05/30/2022 14:29:43 - INFO - __main__ - Step 30 Global step 30 Train loss 4.94 on epoch=14
05/30/2022 14:29:45 - INFO - __main__ - Step 40 Global step 40 Train loss 4.89 on epoch=19
05/30/2022 14:29:47 - INFO - __main__ - Step 50 Global step 50 Train loss 4.86 on epoch=24
05/30/2022 14:29:49 - INFO - __main__ - Global step 50 Train loss 4.91 Classification-F1 0.0 on epoch=24
05/30/2022 14:29:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 14:29:51 - INFO - __main__ - Step 60 Global step 60 Train loss 4.71 on epoch=29
05/30/2022 14:29:53 - INFO - __main__ - Step 70 Global step 70 Train loss 4.81 on epoch=34
05/30/2022 14:29:54 - INFO - __main__ - Step 80 Global step 80 Train loss 4.81 on epoch=39
05/30/2022 14:29:56 - INFO - __main__ - Step 90 Global step 90 Train loss 4.82 on epoch=44
05/30/2022 14:29:58 - INFO - __main__ - Step 100 Global step 100 Train loss 4.63 on epoch=49
05/30/2022 14:29:59 - INFO - __main__ - Global step 100 Train loss 4.76 Classification-F1 0.0 on epoch=49
05/30/2022 14:30:01 - INFO - __main__ - Step 110 Global step 110 Train loss 4.68 on epoch=54
05/30/2022 14:30:03 - INFO - __main__ - Step 120 Global step 120 Train loss 4.58 on epoch=59
05/30/2022 14:30:05 - INFO - __main__ - Step 130 Global step 130 Train loss 4.53 on epoch=64
05/30/2022 14:30:07 - INFO - __main__ - Step 140 Global step 140 Train loss 4.56 on epoch=69
05/30/2022 14:30:09 - INFO - __main__ - Step 150 Global step 150 Train loss 4.47 on epoch=74
05/30/2022 14:30:11 - INFO - __main__ - Global step 150 Train loss 4.57 Classification-F1 0.0 on epoch=74
05/30/2022 14:30:13 - INFO - __main__ - Step 160 Global step 160 Train loss 4.39 on epoch=79
05/30/2022 14:30:15 - INFO - __main__ - Step 170 Global step 170 Train loss 4.40 on epoch=84
05/30/2022 14:30:17 - INFO - __main__ - Step 180 Global step 180 Train loss 4.32 on epoch=89
05/30/2022 14:30:19 - INFO - __main__ - Step 190 Global step 190 Train loss 4.13 on epoch=94
05/30/2022 14:30:21 - INFO - __main__ - Step 200 Global step 200 Train loss 4.18 on epoch=99
05/30/2022 14:30:23 - INFO - __main__ - Global step 200 Train loss 4.28 Classification-F1 0.0 on epoch=99
05/30/2022 14:30:25 - INFO - __main__ - Step 210 Global step 210 Train loss 4.16 on epoch=104
05/30/2022 14:30:27 - INFO - __main__ - Step 220 Global step 220 Train loss 4.06 on epoch=109
05/30/2022 14:30:29 - INFO - __main__ - Step 230 Global step 230 Train loss 3.93 on epoch=114
05/30/2022 14:30:31 - INFO - __main__ - Step 240 Global step 240 Train loss 4.01 on epoch=119
05/30/2022 14:30:33 - INFO - __main__ - Step 250 Global step 250 Train loss 3.95 on epoch=124
05/30/2022 14:30:35 - INFO - __main__ - Global step 250 Train loss 4.02 Classification-F1 0.0 on epoch=124
05/30/2022 14:30:36 - INFO - __main__ - Step 260 Global step 260 Train loss 3.93 on epoch=129
05/30/2022 14:30:38 - INFO - __main__ - Step 270 Global step 270 Train loss 3.83 on epoch=134
05/30/2022 14:30:40 - INFO - __main__ - Step 280 Global step 280 Train loss 3.80 on epoch=139
05/30/2022 14:30:42 - INFO - __main__ - Step 290 Global step 290 Train loss 3.77 on epoch=144
05/30/2022 14:30:44 - INFO - __main__ - Step 300 Global step 300 Train loss 3.70 on epoch=149
05/30/2022 14:30:46 - INFO - __main__ - Global step 300 Train loss 3.81 Classification-F1 0.0 on epoch=149
05/30/2022 14:30:48 - INFO - __main__ - Step 310 Global step 310 Train loss 3.56 on epoch=154
05/30/2022 14:30:50 - INFO - __main__ - Step 320 Global step 320 Train loss 3.63 on epoch=159
05/30/2022 14:30:51 - INFO - __main__ - Step 330 Global step 330 Train loss 3.55 on epoch=164
05/30/2022 14:30:53 - INFO - __main__ - Step 340 Global step 340 Train loss 3.53 on epoch=169
05/30/2022 14:30:55 - INFO - __main__ - Step 350 Global step 350 Train loss 3.49 on epoch=174
05/30/2022 14:30:56 - INFO - __main__ - Global step 350 Train loss 3.55 Classification-F1 0.08637873754152824 on epoch=174
05/30/2022 14:30:57 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.08637873754152824 on epoch=174, global_step=350
05/30/2022 14:30:58 - INFO - __main__ - Step 360 Global step 360 Train loss 3.48 on epoch=179
05/30/2022 14:31:00 - INFO - __main__ - Step 370 Global step 370 Train loss 3.44 on epoch=184
05/30/2022 14:31:02 - INFO - __main__ - Step 380 Global step 380 Train loss 3.40 on epoch=189
05/30/2022 14:31:04 - INFO - __main__ - Step 390 Global step 390 Train loss 3.21 on epoch=194
05/30/2022 14:31:06 - INFO - __main__ - Step 400 Global step 400 Train loss 3.29 on epoch=199
05/30/2022 14:31:07 - INFO - __main__ - Global step 400 Train loss 3.36 Classification-F1 0.21276595744680848 on epoch=199
05/30/2022 14:31:07 - INFO - __main__ - Saving model with best Classification-F1: 0.08637873754152824 -> 0.21276595744680848 on epoch=199, global_step=400
05/30/2022 14:31:09 - INFO - __main__ - Step 410 Global step 410 Train loss 3.18 on epoch=204
05/30/2022 14:31:11 - INFO - __main__ - Step 420 Global step 420 Train loss 3.16 on epoch=209
05/30/2022 14:31:13 - INFO - __main__ - Step 430 Global step 430 Train loss 3.07 on epoch=214
05/30/2022 14:31:15 - INFO - __main__ - Step 440 Global step 440 Train loss 3.14 on epoch=219
05/30/2022 14:31:17 - INFO - __main__ - Step 450 Global step 450 Train loss 2.96 on epoch=224
05/30/2022 14:31:20 - INFO - __main__ - Global step 450 Train loss 3.10 Classification-F1 0.1590909090909091 on epoch=224
05/30/2022 14:31:22 - INFO - __main__ - Step 460 Global step 460 Train loss 2.87 on epoch=229
05/30/2022 14:31:24 - INFO - __main__ - Step 470 Global step 470 Train loss 3.00 on epoch=234
05/30/2022 14:31:26 - INFO - __main__ - Step 480 Global step 480 Train loss 2.81 on epoch=239
05/30/2022 14:31:28 - INFO - __main__ - Step 490 Global step 490 Train loss 2.82 on epoch=244
05/30/2022 14:31:30 - INFO - __main__ - Step 500 Global step 500 Train loss 2.80 on epoch=249
05/30/2022 14:31:33 - INFO - __main__ - Global step 500 Train loss 2.86 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 14:31:33 - INFO - __main__ - Saving model with best Classification-F1: 0.21276595744680848 -> 0.3333333333333333 on epoch=249, global_step=500
05/30/2022 14:31:35 - INFO - __main__ - Step 510 Global step 510 Train loss 2.76 on epoch=254
05/30/2022 14:31:37 - INFO - __main__ - Step 520 Global step 520 Train loss 2.67 on epoch=259
05/30/2022 14:31:38 - INFO - __main__ - Step 530 Global step 530 Train loss 2.52 on epoch=264
05/30/2022 14:31:40 - INFO - __main__ - Step 540 Global step 540 Train loss 2.53 on epoch=269
05/30/2022 14:31:42 - INFO - __main__ - Step 550 Global step 550 Train loss 2.52 on epoch=274
05/30/2022 14:31:46 - INFO - __main__ - Global step 550 Train loss 2.60 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 14:31:48 - INFO - __main__ - Step 560 Global step 560 Train loss 2.54 on epoch=279
05/30/2022 14:31:49 - INFO - __main__ - Step 570 Global step 570 Train loss 2.45 on epoch=284
05/30/2022 14:31:51 - INFO - __main__ - Step 580 Global step 580 Train loss 2.34 on epoch=289
05/30/2022 14:31:53 - INFO - __main__ - Step 590 Global step 590 Train loss 2.43 on epoch=294
05/30/2022 14:31:55 - INFO - __main__ - Step 600 Global step 600 Train loss 2.28 on epoch=299
05/30/2022 14:31:59 - INFO - __main__ - Global step 600 Train loss 2.41 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 14:32:01 - INFO - __main__ - Step 610 Global step 610 Train loss 2.26 on epoch=304
05/30/2022 14:32:03 - INFO - __main__ - Step 620 Global step 620 Train loss 2.27 on epoch=309
05/30/2022 14:32:05 - INFO - __main__ - Step 630 Global step 630 Train loss 2.15 on epoch=314
05/30/2022 14:32:07 - INFO - __main__ - Step 640 Global step 640 Train loss 2.18 on epoch=319
05/30/2022 14:32:09 - INFO - __main__ - Step 650 Global step 650 Train loss 2.09 on epoch=324
05/30/2022 14:32:13 - INFO - __main__ - Global step 650 Train loss 2.19 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 14:32:14 - INFO - __main__ - Step 660 Global step 660 Train loss 2.01 on epoch=329
05/30/2022 14:32:16 - INFO - __main__ - Step 670 Global step 670 Train loss 2.02 on epoch=334
05/30/2022 14:32:18 - INFO - __main__ - Step 680 Global step 680 Train loss 2.06 on epoch=339
05/30/2022 14:32:20 - INFO - __main__ - Step 690 Global step 690 Train loss 2.03 on epoch=344
05/30/2022 14:32:22 - INFO - __main__ - Step 700 Global step 700 Train loss 1.95 on epoch=349
05/30/2022 14:32:26 - INFO - __main__ - Global step 700 Train loss 2.02 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 14:32:28 - INFO - __main__ - Step 710 Global step 710 Train loss 1.86 on epoch=354
05/30/2022 14:32:30 - INFO - __main__ - Step 720 Global step 720 Train loss 1.92 on epoch=359
05/30/2022 14:32:32 - INFO - __main__ - Step 730 Global step 730 Train loss 1.83 on epoch=364
05/30/2022 14:32:34 - INFO - __main__ - Step 740 Global step 740 Train loss 1.80 on epoch=369
05/30/2022 14:32:36 - INFO - __main__ - Step 750 Global step 750 Train loss 1.82 on epoch=374
05/30/2022 14:32:39 - INFO - __main__ - Global step 750 Train loss 1.85 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 14:32:41 - INFO - __main__ - Step 760 Global step 760 Train loss 1.65 on epoch=379
05/30/2022 14:32:43 - INFO - __main__ - Step 770 Global step 770 Train loss 1.83 on epoch=384
05/30/2022 14:32:45 - INFO - __main__ - Step 780 Global step 780 Train loss 1.74 on epoch=389
05/30/2022 14:32:47 - INFO - __main__ - Step 790 Global step 790 Train loss 1.65 on epoch=394
05/30/2022 14:32:49 - INFO - __main__ - Step 800 Global step 800 Train loss 1.57 on epoch=399
05/30/2022 14:32:52 - INFO - __main__ - Global step 800 Train loss 1.69 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 14:32:54 - INFO - __main__ - Step 810 Global step 810 Train loss 1.61 on epoch=404
05/30/2022 14:32:56 - INFO - __main__ - Step 820 Global step 820 Train loss 1.57 on epoch=409
05/30/2022 14:32:57 - INFO - __main__ - Step 830 Global step 830 Train loss 1.54 on epoch=414
05/30/2022 14:32:59 - INFO - __main__ - Step 840 Global step 840 Train loss 1.48 on epoch=419
05/30/2022 14:33:01 - INFO - __main__ - Step 850 Global step 850 Train loss 1.46 on epoch=424
05/30/2022 14:33:04 - INFO - __main__ - Global step 850 Train loss 1.53 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 14:33:06 - INFO - __main__ - Step 860 Global step 860 Train loss 1.38 on epoch=429
05/30/2022 14:33:08 - INFO - __main__ - Step 870 Global step 870 Train loss 1.30 on epoch=434
05/30/2022 14:33:10 - INFO - __main__ - Step 880 Global step 880 Train loss 1.33 on epoch=439
05/30/2022 14:33:12 - INFO - __main__ - Step 890 Global step 890 Train loss 1.44 on epoch=444
05/30/2022 14:33:14 - INFO - __main__ - Step 900 Global step 900 Train loss 1.25 on epoch=449
05/30/2022 14:33:17 - INFO - __main__ - Global step 900 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 14:33:19 - INFO - __main__ - Step 910 Global step 910 Train loss 1.29 on epoch=454
05/30/2022 14:33:21 - INFO - __main__ - Step 920 Global step 920 Train loss 1.28 on epoch=459
05/30/2022 14:33:23 - INFO - __main__ - Step 930 Global step 930 Train loss 1.16 on epoch=464
05/30/2022 14:33:25 - INFO - __main__ - Step 940 Global step 940 Train loss 1.19 on epoch=469
05/30/2022 14:33:27 - INFO - __main__ - Step 950 Global step 950 Train loss 1.10 on epoch=474
05/30/2022 14:33:29 - INFO - __main__ - Global step 950 Train loss 1.20 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 14:33:31 - INFO - __main__ - Step 960 Global step 960 Train loss 1.16 on epoch=479
05/30/2022 14:33:33 - INFO - __main__ - Step 970 Global step 970 Train loss 1.20 on epoch=484
05/30/2022 14:33:35 - INFO - __main__ - Step 980 Global step 980 Train loss 1.08 on epoch=489
05/30/2022 14:33:37 - INFO - __main__ - Step 990 Global step 990 Train loss 1.10 on epoch=494
05/30/2022 14:33:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.14 on epoch=499
05/30/2022 14:33:40 - INFO - __main__ - Global step 1000 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 14:33:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.09 on epoch=504
05/30/2022 14:33:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.15 on epoch=509
05/30/2022 14:33:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.12 on epoch=514
05/30/2022 14:33:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.03 on epoch=519
05/30/2022 14:33:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.01 on epoch=524
05/30/2022 14:33:50 - INFO - __main__ - Global step 1050 Train loss 1.08 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 14:33:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.99 on epoch=529
05/30/2022 14:33:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.09 on epoch=534
05/30/2022 14:33:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.92 on epoch=539
05/30/2022 14:33:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.11 on epoch=544
05/30/2022 14:34:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.02 on epoch=549
05/30/2022 14:34:00 - INFO - __main__ - Global step 1100 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 14:34:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.88 on epoch=554
05/30/2022 14:34:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.02 on epoch=559
05/30/2022 14:34:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.96 on epoch=564
05/30/2022 14:34:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.90 on epoch=569
05/30/2022 14:34:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.86 on epoch=574
05/30/2022 14:34:11 - INFO - __main__ - Global step 1150 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 14:34:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.92 on epoch=579
05/30/2022 14:34:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=584
05/30/2022 14:34:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.94 on epoch=589
05/30/2022 14:34:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.89 on epoch=594
05/30/2022 14:34:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.85 on epoch=599
05/30/2022 14:34:21 - INFO - __main__ - Global step 1200 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 14:34:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.88 on epoch=604
05/30/2022 14:34:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.76 on epoch=609
05/30/2022 14:34:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.77 on epoch=614
05/30/2022 14:34:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=619
05/30/2022 14:34:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.84 on epoch=624
05/30/2022 14:34:32 - INFO - __main__ - Global step 1250 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 14:34:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.81 on epoch=629
05/30/2022 14:34:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.80 on epoch=634
05/30/2022 14:34:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.71 on epoch=639
05/30/2022 14:34:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.78 on epoch=644
05/30/2022 14:34:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.85 on epoch=649
05/30/2022 14:34:42 - INFO - __main__ - Global step 1300 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 14:34:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=654
05/30/2022 14:34:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.73 on epoch=659
05/30/2022 14:34:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.71 on epoch=664
05/30/2022 14:34:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=669
05/30/2022 14:34:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.73 on epoch=674
05/30/2022 14:34:53 - INFO - __main__ - Global step 1350 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 14:34:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.76 on epoch=679
05/30/2022 14:34:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.77 on epoch=684
05/30/2022 14:34:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.72 on epoch=689
05/30/2022 14:35:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.67 on epoch=694
05/30/2022 14:35:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.69 on epoch=699
05/30/2022 14:35:03 - INFO - __main__ - Global step 1400 Train loss 0.72 Classification-F1 0.3191489361702127 on epoch=699
05/30/2022 14:35:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.68 on epoch=704
05/30/2022 14:35:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.72 on epoch=709
05/30/2022 14:35:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.76 on epoch=714
05/30/2022 14:35:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.63 on epoch=719
05/30/2022 14:35:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.59 on epoch=724
05/30/2022 14:35:14 - INFO - __main__ - Global step 1450 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 14:35:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.73 on epoch=729
05/30/2022 14:35:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.63 on epoch=734
05/30/2022 14:35:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.69 on epoch=739
05/30/2022 14:35:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.65 on epoch=744
05/30/2022 14:35:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.63 on epoch=749
05/30/2022 14:35:24 - INFO - __main__ - Global step 1500 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 14:35:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.72 on epoch=754
05/30/2022 14:35:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.64 on epoch=759
05/30/2022 14:35:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.62 on epoch=764
05/30/2022 14:35:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=769
05/30/2022 14:35:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.53 on epoch=774
05/30/2022 14:35:35 - INFO - __main__ - Global step 1550 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 14:35:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=779
05/30/2022 14:35:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=784
05/30/2022 14:35:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.69 on epoch=789
05/30/2022 14:35:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.58 on epoch=794
05/30/2022 14:35:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.54 on epoch=799
05/30/2022 14:35:45 - INFO - __main__ - Global step 1600 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 14:35:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.59 on epoch=804
05/30/2022 14:35:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.55 on epoch=809
05/30/2022 14:35:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=814
05/30/2022 14:35:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.59 on epoch=819
05/30/2022 14:35:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.57 on epoch=824
05/30/2022 14:35:56 - INFO - __main__ - Global step 1650 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 14:35:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.53 on epoch=829
05/30/2022 14:36:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.61 on epoch=834
05/30/2022 14:36:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.59 on epoch=839
05/30/2022 14:36:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
05/30/2022 14:36:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.59 on epoch=849
05/30/2022 14:36:06 - INFO - __main__ - Global step 1700 Train loss 0.56 Classification-F1 0.3043478260869565 on epoch=849
05/30/2022 14:36:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.58 on epoch=854
05/30/2022 14:36:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.53 on epoch=859
05/30/2022 14:36:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.52 on epoch=864
05/30/2022 14:36:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=869
05/30/2022 14:36:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.53 on epoch=874
05/30/2022 14:36:17 - INFO - __main__ - Global step 1750 Train loss 0.52 Classification-F1 0.39139139139139134 on epoch=874
05/30/2022 14:36:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39139139139139134 on epoch=874, global_step=1750
05/30/2022 14:36:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.51 on epoch=879
05/30/2022 14:36:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.51 on epoch=884
05/30/2022 14:36:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.57 on epoch=889
05/30/2022 14:36:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.55 on epoch=894
05/30/2022 14:36:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
05/30/2022 14:36:27 - INFO - __main__ - Global step 1800 Train loss 0.51 Classification-F1 0.3043478260869565 on epoch=899
05/30/2022 14:36:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=904
05/30/2022 14:36:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.47 on epoch=909
05/30/2022 14:36:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.55 on epoch=914
05/30/2022 14:36:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=919
05/30/2022 14:36:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.62 on epoch=924
05/30/2022 14:36:38 - INFO - __main__ - Global step 1850 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 14:36:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
05/30/2022 14:36:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.53 on epoch=934
05/30/2022 14:36:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.53 on epoch=939
05/30/2022 14:36:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.48 on epoch=944
05/30/2022 14:36:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.58 on epoch=949
05/30/2022 14:36:48 - INFO - __main__ - Global step 1900 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 14:36:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=954
05/30/2022 14:36:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=959
05/30/2022 14:36:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=964
05/30/2022 14:36:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=969
05/30/2022 14:36:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=974
05/30/2022 14:36:59 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 14:37:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=979
05/30/2022 14:37:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.56 on epoch=984
05/30/2022 14:37:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.49 on epoch=989
05/30/2022 14:37:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=994
05/30/2022 14:37:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.50 on epoch=999
05/30/2022 14:37:09 - INFO - __main__ - Global step 2000 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 14:37:09 - INFO - __main__ - save last model!
05/30/2022 14:37:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 14:37:09 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 14:37:09 - INFO - __main__ - Printing 3 examples
05/30/2022 14:37:09 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:37:09 - INFO - __main__ - ['entailed']
05/30/2022 14:37:09 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:37:09 - INFO - __main__ - ['entailed']
05/30/2022 14:37:09 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:37:09 - INFO - __main__ - ['entailed']
05/30/2022 14:37:09 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:37:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:37:10 - INFO - __main__ - Printing 3 examples
05/30/2022 14:37:10 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 14:37:10 - INFO - __main__ - ['entailed']
05/30/2022 14:37:10 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 14:37:10 - INFO - __main__ - ['entailed']
05/30/2022 14:37:10 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 14:37:10 - INFO - __main__ - ['entailed']
05/30/2022 14:37:10 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:37:10 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:37:10 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 14:37:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:37:10 - INFO - __main__ - Printing 3 examples
05/30/2022 14:37:10 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/30/2022 14:37:10 - INFO - __main__ - ['entailed']
05/30/2022 14:37:10 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/30/2022 14:37:10 - INFO - __main__ - ['entailed']
05/30/2022 14:37:10 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/30/2022 14:37:10 - INFO - __main__ - ['entailed']
05/30/2022 14:37:10 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:37:10 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:37:10 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 14:37:16 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 14:37:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 14:37:16 - INFO - __main__ - Starting training!
05/30/2022 14:37:34 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:37:46 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 14:42:06 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.2_8_predictions.txt
05/30/2022 14:42:06 - INFO - __main__ - Classification-F1 on test data: 0.3373
05/30/2022 14:42:06 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.2, bsz=8, dev_performance=0.39139139139139134, test_performance=0.3373174086208301
05/30/2022 14:42:06 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.5, bsz=8 ...
05/30/2022 14:42:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:42:07 - INFO - __main__ - Printing 3 examples
05/30/2022 14:42:07 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 14:42:07 - INFO - __main__ - ['entailed']
05/30/2022 14:42:07 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 14:42:07 - INFO - __main__ - ['entailed']
05/30/2022 14:42:07 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 14:42:07 - INFO - __main__ - ['entailed']
05/30/2022 14:42:07 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:42:07 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:42:07 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 14:42:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:42:07 - INFO - __main__ - Printing 3 examples
05/30/2022 14:42:07 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/30/2022 14:42:07 - INFO - __main__ - ['entailed']
05/30/2022 14:42:07 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/30/2022 14:42:07 - INFO - __main__ - ['entailed']
05/30/2022 14:42:07 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/30/2022 14:42:07 - INFO - __main__ - ['entailed']
05/30/2022 14:42:07 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:42:07 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:42:07 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 14:42:13 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 14:42:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 14:42:13 - INFO - __main__ - Starting training!
05/30/2022 14:42:15 - INFO - __main__ - Step 10 Global step 10 Train loss 5.10 on epoch=4
05/30/2022 14:42:17 - INFO - __main__ - Step 20 Global step 20 Train loss 4.90 on epoch=9
05/30/2022 14:42:19 - INFO - __main__ - Step 30 Global step 30 Train loss 4.87 on epoch=14
05/30/2022 14:42:21 - INFO - __main__ - Step 40 Global step 40 Train loss 4.81 on epoch=19
05/30/2022 14:42:23 - INFO - __main__ - Step 50 Global step 50 Train loss 4.67 on epoch=24
05/30/2022 14:42:30 - INFO - __main__ - Global step 50 Train loss 4.87 Classification-F1 0.0 on epoch=24
05/30/2022 14:42:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 14:42:32 - INFO - __main__ - Step 60 Global step 60 Train loss 4.56 on epoch=29
05/30/2022 14:42:33 - INFO - __main__ - Step 70 Global step 70 Train loss 4.45 on epoch=34
05/30/2022 14:42:35 - INFO - __main__ - Step 80 Global step 80 Train loss 4.29 on epoch=39
05/30/2022 14:42:37 - INFO - __main__ - Step 90 Global step 90 Train loss 4.23 on epoch=44
05/30/2022 14:42:39 - INFO - __main__ - Step 100 Global step 100 Train loss 4.07 on epoch=49
05/30/2022 14:42:46 - INFO - __main__ - Global step 100 Train loss 4.32 Classification-F1 0.0 on epoch=49
05/30/2022 14:42:48 - INFO - __main__ - Step 110 Global step 110 Train loss 3.94 on epoch=54
05/30/2022 14:42:50 - INFO - __main__ - Step 120 Global step 120 Train loss 3.75 on epoch=59
05/30/2022 14:42:52 - INFO - __main__ - Step 130 Global step 130 Train loss 3.72 on epoch=64
05/30/2022 14:42:54 - INFO - __main__ - Step 140 Global step 140 Train loss 3.47 on epoch=69
05/30/2022 14:42:56 - INFO - __main__ - Step 150 Global step 150 Train loss 3.33 on epoch=74
05/30/2022 14:42:59 - INFO - __main__ - Global step 150 Train loss 3.64 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 14:42:59 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3333333333333333 on epoch=74, global_step=150
05/30/2022 14:43:01 - INFO - __main__ - Step 160 Global step 160 Train loss 3.34 on epoch=79
05/30/2022 14:43:03 - INFO - __main__ - Step 170 Global step 170 Train loss 3.11 on epoch=84
05/30/2022 14:43:05 - INFO - __main__ - Step 180 Global step 180 Train loss 2.85 on epoch=89
05/30/2022 14:43:07 - INFO - __main__ - Step 190 Global step 190 Train loss 2.89 on epoch=94
05/30/2022 14:43:09 - INFO - __main__ - Step 200 Global step 200 Train loss 2.64 on epoch=99
05/30/2022 14:43:13 - INFO - __main__ - Global step 200 Train loss 2.97 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 14:43:15 - INFO - __main__ - Step 210 Global step 210 Train loss 2.48 on epoch=104
05/30/2022 14:43:17 - INFO - __main__ - Step 220 Global step 220 Train loss 2.30 on epoch=109
05/30/2022 14:43:19 - INFO - __main__ - Step 230 Global step 230 Train loss 2.37 on epoch=114
05/30/2022 14:43:21 - INFO - __main__ - Step 240 Global step 240 Train loss 2.18 on epoch=119
05/30/2022 14:43:23 - INFO - __main__ - Step 250 Global step 250 Train loss 2.12 on epoch=124
05/30/2022 14:43:26 - INFO - __main__ - Global step 250 Train loss 2.29 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 14:43:28 - INFO - __main__ - Step 260 Global step 260 Train loss 2.00 on epoch=129
05/30/2022 14:43:30 - INFO - __main__ - Step 270 Global step 270 Train loss 1.89 on epoch=134
05/30/2022 14:43:32 - INFO - __main__ - Step 280 Global step 280 Train loss 1.66 on epoch=139
05/30/2022 14:43:34 - INFO - __main__ - Step 290 Global step 290 Train loss 1.62 on epoch=144
05/30/2022 14:43:36 - INFO - __main__ - Step 300 Global step 300 Train loss 1.49 on epoch=149
05/30/2022 14:43:42 - INFO - __main__ - Global step 300 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 14:43:44 - INFO - __main__ - Step 310 Global step 310 Train loss 1.41 on epoch=154
05/30/2022 14:43:45 - INFO - __main__ - Step 320 Global step 320 Train loss 1.34 on epoch=159
05/30/2022 14:43:47 - INFO - __main__ - Step 330 Global step 330 Train loss 1.17 on epoch=164
05/30/2022 14:43:49 - INFO - __main__ - Step 340 Global step 340 Train loss 1.19 on epoch=169
05/30/2022 14:43:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.17 on epoch=174
05/30/2022 14:43:52 - INFO - __main__ - Global step 350 Train loss 1.26 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 14:43:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.23 on epoch=179
05/30/2022 14:43:56 - INFO - __main__ - Step 370 Global step 370 Train loss 1.04 on epoch=184
05/30/2022 14:43:58 - INFO - __main__ - Step 380 Global step 380 Train loss 1.10 on epoch=189
05/30/2022 14:44:00 - INFO - __main__ - Step 390 Global step 390 Train loss 1.01 on epoch=194
05/30/2022 14:44:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.95 on epoch=199
05/30/2022 14:44:03 - INFO - __main__ - Global step 400 Train loss 1.07 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 14:44:05 - INFO - __main__ - Step 410 Global step 410 Train loss 1.10 on epoch=204
05/30/2022 14:44:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.89 on epoch=209
05/30/2022 14:44:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.91 on epoch=214
05/30/2022 14:44:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.87 on epoch=219
05/30/2022 14:44:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=224
05/30/2022 14:44:13 - INFO - __main__ - Global step 450 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 14:44:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.92 on epoch=229
05/30/2022 14:44:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.89 on epoch=234
05/30/2022 14:44:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=239
05/30/2022 14:44:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.68 on epoch=244
05/30/2022 14:44:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.74 on epoch=249
05/30/2022 14:44:24 - INFO - __main__ - Global step 500 Train loss 0.79 Classification-F1 0.3191489361702127 on epoch=249
05/30/2022 14:44:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.64 on epoch=254
05/30/2022 14:44:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.71 on epoch=259
05/30/2022 14:44:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.74 on epoch=264
05/30/2022 14:44:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.61 on epoch=269
05/30/2022 14:44:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.61 on epoch=274
05/30/2022 14:44:34 - INFO - __main__ - Global step 550 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 14:44:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=279
05/30/2022 14:44:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=284
05/30/2022 14:44:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=289
05/30/2022 14:44:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=294
05/30/2022 14:44:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=299
05/30/2022 14:44:45 - INFO - __main__ - Global step 600 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 14:44:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.62 on epoch=304
05/30/2022 14:44:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.55 on epoch=309
05/30/2022 14:44:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=314
05/30/2022 14:44:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=319
05/30/2022 14:44:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=324
05/30/2022 14:44:55 - INFO - __main__ - Global step 650 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 14:44:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=329
05/30/2022 14:44:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.56 on epoch=334
05/30/2022 14:45:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=339
05/30/2022 14:45:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=344
05/30/2022 14:45:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=349
05/30/2022 14:45:06 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.4458874458874459 on epoch=349
05/30/2022 14:45:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4458874458874459 on epoch=349, global_step=700
05/30/2022 14:45:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=354
05/30/2022 14:45:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=359
05/30/2022 14:45:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=364
05/30/2022 14:45:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=369
05/30/2022 14:45:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.51 on epoch=374
05/30/2022 14:45:17 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.4181818181818182 on epoch=374
05/30/2022 14:45:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=379
05/30/2022 14:45:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=384
05/30/2022 14:45:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=389
05/30/2022 14:45:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=394
05/30/2022 14:45:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=399
05/30/2022 14:45:27 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.3816425120772947 on epoch=399
05/30/2022 14:45:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=404
05/30/2022 14:45:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=409
05/30/2022 14:45:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=414
05/30/2022 14:45:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=419
05/30/2022 14:45:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=424
05/30/2022 14:45:38 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 14:45:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=429
05/30/2022 14:45:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=434
05/30/2022 14:45:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=439
05/30/2022 14:45:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=444
05/30/2022 14:45:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=449
05/30/2022 14:45:48 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 14:45:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=454
05/30/2022 14:45:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=459
05/30/2022 14:45:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=464
05/30/2022 14:45:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=469
05/30/2022 14:45:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=474
05/30/2022 14:45:59 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 14:46:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=479
05/30/2022 14:46:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=484
05/30/2022 14:46:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=489
05/30/2022 14:46:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=494
05/30/2022 14:46:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
05/30/2022 14:46:09 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 14:46:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=504
05/30/2022 14:46:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=509
05/30/2022 14:46:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=514
05/30/2022 14:46:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=519
05/30/2022 14:46:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=524
05/30/2022 14:46:19 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.3191489361702127 on epoch=524
05/30/2022 14:46:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=529
05/30/2022 14:46:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=534
05/30/2022 14:46:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=539
05/30/2022 14:46:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=544
05/30/2022 14:46:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=549
05/30/2022 14:46:30 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.3992490613266583 on epoch=549
05/30/2022 14:46:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=554
05/30/2022 14:46:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=559
05/30/2022 14:46:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=564
05/30/2022 14:46:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=569
05/30/2022 14:46:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=574
05/30/2022 14:46:40 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 14:46:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=579
05/30/2022 14:46:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=584
05/30/2022 14:46:46 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
05/30/2022 14:46:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=594
05/30/2022 14:46:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=599
05/30/2022 14:46:50 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 14:46:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
05/30/2022 14:46:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=609
05/30/2022 14:46:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=614
05/30/2022 14:46:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=619
05/30/2022 14:47:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=624
05/30/2022 14:47:01 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 14:47:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=629
05/30/2022 14:47:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=634
05/30/2022 14:47:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=639
05/30/2022 14:47:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=644
05/30/2022 14:47:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=649
05/30/2022 14:47:11 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 14:47:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=654
05/30/2022 14:47:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=659
05/30/2022 14:47:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=664
05/30/2022 14:47:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.28 on epoch=669
05/30/2022 14:47:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=674
05/30/2022 14:47:21 - INFO - __main__ - Global step 1350 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 14:47:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=679
05/30/2022 14:47:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=684
05/30/2022 14:47:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=689
05/30/2022 14:47:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.32 on epoch=694
05/30/2022 14:47:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=699
05/30/2022 14:47:32 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 14:47:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=704
05/30/2022 14:47:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=709
05/30/2022 14:47:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=714
05/30/2022 14:47:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.26 on epoch=719
05/30/2022 14:47:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=724
05/30/2022 14:47:42 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 14:47:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
05/30/2022 14:47:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=734
05/30/2022 14:47:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=739
05/30/2022 14:47:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=744
05/30/2022 14:47:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=749
05/30/2022 14:47:52 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 14:47:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=754
05/30/2022 14:47:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=759
05/30/2022 14:47:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=764
05/30/2022 14:48:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=769
05/30/2022 14:48:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=774
05/30/2022 14:48:02 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 14:48:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=779
05/30/2022 14:48:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=784
05/30/2022 14:48:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=789
05/30/2022 14:48:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=794
05/30/2022 14:48:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=799
05/30/2022 14:48:13 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 14:48:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=804
05/30/2022 14:48:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=809
05/30/2022 14:48:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=814
05/30/2022 14:48:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=819
05/30/2022 14:48:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=824
05/30/2022 14:48:23 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 14:48:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=829
05/30/2022 14:48:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=834
05/30/2022 14:48:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.29 on epoch=839
05/30/2022 14:48:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=844
05/30/2022 14:48:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.27 on epoch=849
05/30/2022 14:48:33 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 14:48:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=854
05/30/2022 14:48:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=859
05/30/2022 14:48:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/30/2022 14:48:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=869
05/30/2022 14:48:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
05/30/2022 14:48:43 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 14:48:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.31 on epoch=879
05/30/2022 14:48:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=884
05/30/2022 14:48:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
05/30/2022 14:48:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.30 on epoch=894
05/30/2022 14:48:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=899
05/30/2022 14:48:54 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 14:48:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=904
05/30/2022 14:48:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=909
05/30/2022 14:48:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=914
05/30/2022 14:49:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.29 on epoch=919
05/30/2022 14:49:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
05/30/2022 14:49:04 - INFO - __main__ - Global step 1850 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 14:49:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.31 on epoch=929
05/30/2022 14:49:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=934
05/30/2022 14:49:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=939
05/30/2022 14:49:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=944
05/30/2022 14:49:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.29 on epoch=949
05/30/2022 14:49:14 - INFO - __main__ - Global step 1900 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 14:49:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=954
05/30/2022 14:49:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=959
05/30/2022 14:49:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=964
05/30/2022 14:49:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=969
05/30/2022 14:49:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=974
05/30/2022 14:49:24 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 14:49:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=979
05/30/2022 14:49:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=984
05/30/2022 14:49:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/30/2022 14:49:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=994
05/30/2022 14:49:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=999
05/30/2022 14:49:35 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 14:49:35 - INFO - __main__ - save last model!
05/30/2022 14:49:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 14:49:35 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 14:49:35 - INFO - __main__ - Printing 3 examples
05/30/2022 14:49:35 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:49:35 - INFO - __main__ - ['entailed']
05/30/2022 14:49:35 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:49:35 - INFO - __main__ - ['entailed']
05/30/2022 14:49:35 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:49:35 - INFO - __main__ - ['entailed']
05/30/2022 14:49:35 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:49:35 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:49:35 - INFO - __main__ - Printing 3 examples
05/30/2022 14:49:35 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 14:49:35 - INFO - __main__ - ['entailed']
05/30/2022 14:49:35 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 14:49:35 - INFO - __main__ - ['entailed']
05/30/2022 14:49:35 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 14:49:35 - INFO - __main__ - ['entailed']
05/30/2022 14:49:35 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:49:35 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:49:35 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 14:49:35 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:49:35 - INFO - __main__ - Printing 3 examples
05/30/2022 14:49:35 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/30/2022 14:49:35 - INFO - __main__ - ['entailed']
05/30/2022 14:49:35 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/30/2022 14:49:35 - INFO - __main__ - ['entailed']
05/30/2022 14:49:35 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/30/2022 14:49:35 - INFO - __main__ - ['entailed']
05/30/2022 14:49:35 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:49:35 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:49:35 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 14:49:41 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 14:49:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 14:49:41 - INFO - __main__ - Starting training!
05/30/2022 14:49:59 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:50:12 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 14:54:22 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.5_8_predictions.txt
05/30/2022 14:54:22 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 14:54:22 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.5, bsz=8, dev_performance=0.4458874458874459, test_performance=0.33047210300429186
05/30/2022 14:54:22 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.4, bsz=8 ...
05/30/2022 14:54:23 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:54:23 - INFO - __main__ - Printing 3 examples
05/30/2022 14:54:23 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 14:54:23 - INFO - __main__ - ['entailed']
05/30/2022 14:54:23 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 14:54:23 - INFO - __main__ - ['entailed']
05/30/2022 14:54:23 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 14:54:23 - INFO - __main__ - ['entailed']
05/30/2022 14:54:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:54:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:54:23 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 14:54:23 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 14:54:23 - INFO - __main__ - Printing 3 examples
05/30/2022 14:54:23 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/30/2022 14:54:23 - INFO - __main__ - ['entailed']
05/30/2022 14:54:23 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/30/2022 14:54:23 - INFO - __main__ - ['entailed']
05/30/2022 14:54:23 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/30/2022 14:54:23 - INFO - __main__ - ['entailed']
05/30/2022 14:54:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:54:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:54:23 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 14:54:29 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 14:54:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 14:54:29 - INFO - __main__ - Starting training!
05/30/2022 14:54:31 - INFO - __main__ - Step 10 Global step 10 Train loss 4.97 on epoch=4
05/30/2022 14:54:33 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/30/2022 14:54:35 - INFO - __main__ - Step 30 Global step 30 Train loss 4.80 on epoch=14
05/30/2022 14:54:37 - INFO - __main__ - Step 40 Global step 40 Train loss 4.77 on epoch=19
05/30/2022 14:54:39 - INFO - __main__ - Step 50 Global step 50 Train loss 4.72 on epoch=24
05/30/2022 14:54:40 - INFO - __main__ - Global step 50 Train loss 4.86 Classification-F1 0.0 on epoch=24
05/30/2022 14:54:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 14:54:42 - INFO - __main__ - Step 60 Global step 60 Train loss 4.71 on epoch=29
05/30/2022 14:54:44 - INFO - __main__ - Step 70 Global step 70 Train loss 4.70 on epoch=34
05/30/2022 14:54:46 - INFO - __main__ - Step 80 Global step 80 Train loss 4.60 on epoch=39
05/30/2022 14:54:48 - INFO - __main__ - Step 90 Global step 90 Train loss 4.56 on epoch=44
05/30/2022 14:54:50 - INFO - __main__ - Step 100 Global step 100 Train loss 4.38 on epoch=49
05/30/2022 14:54:51 - INFO - __main__ - Global step 100 Train loss 4.59 Classification-F1 0.0 on epoch=49
05/30/2022 14:54:53 - INFO - __main__ - Step 110 Global step 110 Train loss 4.34 on epoch=54
05/30/2022 14:54:55 - INFO - __main__ - Step 120 Global step 120 Train loss 4.35 on epoch=59
05/30/2022 14:54:56 - INFO - __main__ - Step 130 Global step 130 Train loss 4.22 on epoch=64
05/30/2022 14:54:58 - INFO - __main__ - Step 140 Global step 140 Train loss 4.29 on epoch=69
05/30/2022 14:55:00 - INFO - __main__ - Step 150 Global step 150 Train loss 4.15 on epoch=74
05/30/2022 14:55:02 - INFO - __main__ - Global step 150 Train loss 4.27 Classification-F1 0.0 on epoch=74
05/30/2022 14:55:04 - INFO - __main__ - Step 160 Global step 160 Train loss 4.11 on epoch=79
05/30/2022 14:55:06 - INFO - __main__ - Step 170 Global step 170 Train loss 3.98 on epoch=84
05/30/2022 14:55:08 - INFO - __main__ - Step 180 Global step 180 Train loss 3.89 on epoch=89
05/30/2022 14:55:10 - INFO - __main__ - Step 190 Global step 190 Train loss 3.76 on epoch=94
05/30/2022 14:55:12 - INFO - __main__ - Step 200 Global step 200 Train loss 3.77 on epoch=99
05/30/2022 14:55:19 - INFO - __main__ - Global step 200 Train loss 3.90 Classification-F1 0.0058823529411764705 on epoch=99
05/30/2022 14:55:19 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0058823529411764705 on epoch=99, global_step=200
05/30/2022 14:55:21 - INFO - __main__ - Step 210 Global step 210 Train loss 3.70 on epoch=104
05/30/2022 14:55:23 - INFO - __main__ - Step 220 Global step 220 Train loss 3.61 on epoch=109
05/30/2022 14:55:24 - INFO - __main__ - Step 230 Global step 230 Train loss 3.59 on epoch=114
05/30/2022 14:55:26 - INFO - __main__ - Step 240 Global step 240 Train loss 3.50 on epoch=119
05/30/2022 14:55:28 - INFO - __main__ - Step 250 Global step 250 Train loss 3.44 on epoch=124
05/30/2022 14:55:35 - INFO - __main__ - Global step 250 Train loss 3.57 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 14:55:35 - INFO - __main__ - Saving model with best Classification-F1: 0.0058823529411764705 -> 0.3333333333333333 on epoch=124, global_step=250
05/30/2022 14:55:37 - INFO - __main__ - Step 260 Global step 260 Train loss 3.33 on epoch=129
05/30/2022 14:55:39 - INFO - __main__ - Step 270 Global step 270 Train loss 3.18 on epoch=134
05/30/2022 14:55:41 - INFO - __main__ - Step 280 Global step 280 Train loss 3.07 on epoch=139
05/30/2022 14:55:43 - INFO - __main__ - Step 290 Global step 290 Train loss 2.91 on epoch=144
05/30/2022 14:55:45 - INFO - __main__ - Step 300 Global step 300 Train loss 2.78 on epoch=149
05/30/2022 14:55:48 - INFO - __main__ - Global step 300 Train loss 3.05 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 14:55:50 - INFO - __main__ - Step 310 Global step 310 Train loss 2.87 on epoch=154
05/30/2022 14:55:52 - INFO - __main__ - Step 320 Global step 320 Train loss 2.75 on epoch=159
05/30/2022 14:55:54 - INFO - __main__ - Step 330 Global step 330 Train loss 2.62 on epoch=164
05/30/2022 14:55:56 - INFO - __main__ - Step 340 Global step 340 Train loss 2.57 on epoch=169
05/30/2022 14:55:58 - INFO - __main__ - Step 350 Global step 350 Train loss 2.50 on epoch=174
05/30/2022 14:56:01 - INFO - __main__ - Global step 350 Train loss 2.66 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 14:56:03 - INFO - __main__ - Step 360 Global step 360 Train loss 2.47 on epoch=179
05/30/2022 14:56:05 - INFO - __main__ - Step 370 Global step 370 Train loss 2.33 on epoch=184
05/30/2022 14:56:07 - INFO - __main__ - Step 380 Global step 380 Train loss 2.28 on epoch=189
05/30/2022 14:56:09 - INFO - __main__ - Step 390 Global step 390 Train loss 2.15 on epoch=194
05/30/2022 14:56:11 - INFO - __main__ - Step 400 Global step 400 Train loss 2.11 on epoch=199
05/30/2022 14:56:14 - INFO - __main__ - Global step 400 Train loss 2.27 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 14:56:16 - INFO - __main__ - Step 410 Global step 410 Train loss 2.04 on epoch=204
05/30/2022 14:56:18 - INFO - __main__ - Step 420 Global step 420 Train loss 2.01 on epoch=209
05/30/2022 14:56:20 - INFO - __main__ - Step 430 Global step 430 Train loss 1.99 on epoch=214
05/30/2022 14:56:21 - INFO - __main__ - Step 440 Global step 440 Train loss 1.84 on epoch=219
05/30/2022 14:56:23 - INFO - __main__ - Step 450 Global step 450 Train loss 1.71 on epoch=224
05/30/2022 14:56:27 - INFO - __main__ - Global step 450 Train loss 1.91 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 14:56:29 - INFO - __main__ - Step 460 Global step 460 Train loss 1.78 on epoch=229
05/30/2022 14:56:31 - INFO - __main__ - Step 470 Global step 470 Train loss 1.74 on epoch=234
05/30/2022 14:56:33 - INFO - __main__ - Step 480 Global step 480 Train loss 1.74 on epoch=239
05/30/2022 14:56:35 - INFO - __main__ - Step 490 Global step 490 Train loss 1.69 on epoch=244
05/30/2022 14:56:37 - INFO - __main__ - Step 500 Global step 500 Train loss 1.71 on epoch=249
05/30/2022 14:56:43 - INFO - __main__ - Global step 500 Train loss 1.73 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 14:56:45 - INFO - __main__ - Step 510 Global step 510 Train loss 1.66 on epoch=254
05/30/2022 14:56:47 - INFO - __main__ - Step 520 Global step 520 Train loss 1.72 on epoch=259
05/30/2022 14:56:49 - INFO - __main__ - Step 530 Global step 530 Train loss 1.55 on epoch=264
05/30/2022 14:56:51 - INFO - __main__ - Step 540 Global step 540 Train loss 1.54 on epoch=269
05/30/2022 14:56:53 - INFO - __main__ - Step 550 Global step 550 Train loss 1.51 on epoch=274
05/30/2022 14:56:56 - INFO - __main__ - Global step 550 Train loss 1.59 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 14:56:58 - INFO - __main__ - Step 560 Global step 560 Train loss 1.41 on epoch=279
05/30/2022 14:57:00 - INFO - __main__ - Step 570 Global step 570 Train loss 1.49 on epoch=284
05/30/2022 14:57:02 - INFO - __main__ - Step 580 Global step 580 Train loss 1.39 on epoch=289
05/30/2022 14:57:04 - INFO - __main__ - Step 590 Global step 590 Train loss 1.35 on epoch=294
05/30/2022 14:57:05 - INFO - __main__ - Step 600 Global step 600 Train loss 1.29 on epoch=299
05/30/2022 14:57:08 - INFO - __main__ - Global step 600 Train loss 1.39 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 14:57:10 - INFO - __main__ - Step 610 Global step 610 Train loss 1.25 on epoch=304
05/30/2022 14:57:12 - INFO - __main__ - Step 620 Global step 620 Train loss 1.27 on epoch=309
05/30/2022 14:57:14 - INFO - __main__ - Step 630 Global step 630 Train loss 1.24 on epoch=314
05/30/2022 14:57:16 - INFO - __main__ - Step 640 Global step 640 Train loss 1.31 on epoch=319
05/30/2022 14:57:18 - INFO - __main__ - Step 650 Global step 650 Train loss 1.17 on epoch=324
05/30/2022 14:57:19 - INFO - __main__ - Global step 650 Train loss 1.25 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 14:57:21 - INFO - __main__ - Step 660 Global step 660 Train loss 1.34 on epoch=329
05/30/2022 14:57:23 - INFO - __main__ - Step 670 Global step 670 Train loss 1.17 on epoch=334
05/30/2022 14:57:24 - INFO - __main__ - Step 680 Global step 680 Train loss 1.12 on epoch=339
05/30/2022 14:57:26 - INFO - __main__ - Step 690 Global step 690 Train loss 1.13 on epoch=344
05/30/2022 14:57:28 - INFO - __main__ - Step 700 Global step 700 Train loss 1.01 on epoch=349
05/30/2022 14:57:29 - INFO - __main__ - Global step 700 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 14:57:31 - INFO - __main__ - Step 710 Global step 710 Train loss 1.22 on epoch=354
05/30/2022 14:57:33 - INFO - __main__ - Step 720 Global step 720 Train loss 1.05 on epoch=359
05/30/2022 14:57:35 - INFO - __main__ - Step 730 Global step 730 Train loss 1.01 on epoch=364
05/30/2022 14:57:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.91 on epoch=369
05/30/2022 14:57:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.92 on epoch=374
05/30/2022 14:57:40 - INFO - __main__ - Global step 750 Train loss 1.02 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 14:57:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.93 on epoch=379
05/30/2022 14:57:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.96 on epoch=384
05/30/2022 14:57:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.87 on epoch=389
05/30/2022 14:57:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.86 on epoch=394
05/30/2022 14:57:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.80 on epoch=399
05/30/2022 14:57:50 - INFO - __main__ - Global step 800 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 14:57:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.86 on epoch=404
05/30/2022 14:57:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.84 on epoch=409
05/30/2022 14:57:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=414
05/30/2022 14:57:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.83 on epoch=419
05/30/2022 14:58:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.77 on epoch=424
05/30/2022 14:58:01 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 14:58:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.71 on epoch=429
05/30/2022 14:58:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.69 on epoch=434
05/30/2022 14:58:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.77 on epoch=439
05/30/2022 14:58:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.77 on epoch=444
05/30/2022 14:58:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.67 on epoch=449
05/30/2022 14:58:12 - INFO - __main__ - Global step 900 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 14:58:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.73 on epoch=454
05/30/2022 14:58:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.72 on epoch=459
05/30/2022 14:58:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.71 on epoch=464
05/30/2022 14:58:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.65 on epoch=469
05/30/2022 14:58:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.65 on epoch=474
05/30/2022 14:58:23 - INFO - __main__ - Global step 950 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 14:58:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.58 on epoch=479
05/30/2022 14:58:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.62 on epoch=484
05/30/2022 14:58:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.65 on epoch=489
05/30/2022 14:58:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.65 on epoch=494
05/30/2022 14:58:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.66 on epoch=499
05/30/2022 14:58:33 - INFO - __main__ - Global step 1000 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 14:58:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.57 on epoch=504
05/30/2022 14:58:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.68 on epoch=509
05/30/2022 14:58:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.56 on epoch=514
05/30/2022 14:58:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.59 on epoch=519
05/30/2022 14:58:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.58 on epoch=524
05/30/2022 14:58:44 - INFO - __main__ - Global step 1050 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 14:58:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.50 on epoch=529
05/30/2022 14:58:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.55 on epoch=534
05/30/2022 14:58:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.55 on epoch=539
05/30/2022 14:58:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.60 on epoch=544
05/30/2022 14:58:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=549
05/30/2022 14:58:55 - INFO - __main__ - Global step 1100 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 14:58:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.53 on epoch=554
05/30/2022 14:58:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.52 on epoch=559
05/30/2022 14:59:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.54 on epoch=564
05/30/2022 14:59:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.55 on epoch=569
05/30/2022 14:59:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.58 on epoch=574
05/30/2022 14:59:05 - INFO - __main__ - Global step 1150 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 14:59:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.53 on epoch=579
05/30/2022 14:59:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.54 on epoch=584
05/30/2022 14:59:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=589
05/30/2022 14:59:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.53 on epoch=594
05/30/2022 14:59:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=599
05/30/2022 14:59:16 - INFO - __main__ - Global step 1200 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 14:59:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=604
05/30/2022 14:59:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=609
05/30/2022 14:59:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=614
05/30/2022 14:59:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.51 on epoch=619
05/30/2022 14:59:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.54 on epoch=624
05/30/2022 14:59:26 - INFO - __main__ - Global step 1250 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 14:59:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=629
05/30/2022 14:59:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=634
05/30/2022 14:59:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=639
05/30/2022 14:59:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=644
05/30/2022 14:59:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=649
05/30/2022 14:59:37 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 14:59:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=654
05/30/2022 14:59:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
05/30/2022 14:59:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=664
05/30/2022 14:59:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=669
05/30/2022 14:59:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.47 on epoch=674
05/30/2022 14:59:48 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 14:59:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=679
05/30/2022 14:59:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
05/30/2022 14:59:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=689
05/30/2022 14:59:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=694
05/30/2022 14:59:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=699
05/30/2022 14:59:58 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 15:00:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=704
05/30/2022 15:00:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
05/30/2022 15:00:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
05/30/2022 15:00:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=719
05/30/2022 15:00:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=724
05/30/2022 15:00:08 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 15:00:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=729
05/30/2022 15:00:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
05/30/2022 15:00:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=739
05/30/2022 15:00:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
05/30/2022 15:00:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.50 on epoch=749
05/30/2022 15:00:19 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 15:00:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=754
05/30/2022 15:00:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=759
05/30/2022 15:00:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=764
05/30/2022 15:00:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=769
05/30/2022 15:00:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=774
05/30/2022 15:00:29 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 15:00:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=779
05/30/2022 15:00:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=784
05/30/2022 15:00:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=789
05/30/2022 15:00:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=794
05/30/2022 15:00:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
05/30/2022 15:00:39 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 15:00:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=804
05/30/2022 15:00:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=809
05/30/2022 15:00:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=814
05/30/2022 15:00:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
05/30/2022 15:00:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=824
05/30/2022 15:00:50 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 15:00:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=829
05/30/2022 15:00:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=834
05/30/2022 15:00:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
05/30/2022 15:00:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
05/30/2022 15:00:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=849
05/30/2022 15:01:00 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 15:01:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=854
05/30/2022 15:01:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=859
05/30/2022 15:01:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
05/30/2022 15:01:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=869
05/30/2022 15:01:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=874
05/30/2022 15:01:10 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 15:01:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=879
05/30/2022 15:01:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
05/30/2022 15:01:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=889
05/30/2022 15:01:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
05/30/2022 15:01:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=899
05/30/2022 15:01:21 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 15:01:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=904
05/30/2022 15:01:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=909
05/30/2022 15:01:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=914
05/30/2022 15:01:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=919
05/30/2022 15:01:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=924
05/30/2022 15:01:31 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 15:01:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=929
05/30/2022 15:01:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
05/30/2022 15:01:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=939
05/30/2022 15:01:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
05/30/2022 15:01:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
05/30/2022 15:01:42 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 15:01:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=954
05/30/2022 15:01:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
05/30/2022 15:01:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=964
05/30/2022 15:01:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=969
05/30/2022 15:01:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
05/30/2022 15:01:52 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 15:01:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=979
05/30/2022 15:01:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/30/2022 15:01:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
05/30/2022 15:02:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.34 on epoch=994
05/30/2022 15:02:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=999
05/30/2022 15:02:02 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 15:02:02 - INFO - __main__ - save last model!
05/30/2022 15:02:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 15:02:02 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 15:02:02 - INFO - __main__ - Printing 3 examples
05/30/2022 15:02:02 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:02:02 - INFO - __main__ - ['entailed']
05/30/2022 15:02:02 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:02:02 - INFO - __main__ - ['entailed']
05/30/2022 15:02:02 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:02:02 - INFO - __main__ - ['entailed']
05/30/2022 15:02:03 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:02:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 15:02:03 - INFO - __main__ - Printing 3 examples
05/30/2022 15:02:03 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 15:02:03 - INFO - __main__ - ['entailed']
05/30/2022 15:02:03 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 15:02:03 - INFO - __main__ - ['entailed']
05/30/2022 15:02:03 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 15:02:03 - INFO - __main__ - ['entailed']
05/30/2022 15:02:03 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:02:03 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:02:03 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 15:02:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 15:02:03 - INFO - __main__ - Printing 3 examples
05/30/2022 15:02:03 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/30/2022 15:02:03 - INFO - __main__ - ['entailed']
05/30/2022 15:02:03 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/30/2022 15:02:03 - INFO - __main__ - ['entailed']
05/30/2022 15:02:03 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/30/2022 15:02:03 - INFO - __main__ - ['entailed']
05/30/2022 15:02:03 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:02:03 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:02:03 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 15:02:08 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 15:02:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 15:02:09 - INFO - __main__ - Starting training!
05/30/2022 15:02:27 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:02:40 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 15:06:49 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.4_8_predictions.txt
05/30/2022 15:06:49 - INFO - __main__ - Classification-F1 on test data: 0.3306
05/30/2022 15:06:49 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.3306437454867474
05/30/2022 15:06:49 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.3, bsz=8 ...
05/30/2022 15:06:50 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 15:06:50 - INFO - __main__ - Printing 3 examples
05/30/2022 15:06:50 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 15:06:50 - INFO - __main__ - ['entailed']
05/30/2022 15:06:50 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 15:06:50 - INFO - __main__ - ['entailed']
05/30/2022 15:06:50 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 15:06:50 - INFO - __main__ - ['entailed']
05/30/2022 15:06:50 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:06:50 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:06:50 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 15:06:50 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 15:06:50 - INFO - __main__ - Printing 3 examples
05/30/2022 15:06:50 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/30/2022 15:06:50 - INFO - __main__ - ['entailed']
05/30/2022 15:06:50 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/30/2022 15:06:50 - INFO - __main__ - ['entailed']
05/30/2022 15:06:50 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/30/2022 15:06:50 - INFO - __main__ - ['entailed']
05/30/2022 15:06:50 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:06:51 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:06:51 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 15:06:56 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 15:06:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 15:06:56 - INFO - __main__ - Starting training!
05/30/2022 15:06:59 - INFO - __main__ - Step 10 Global step 10 Train loss 5.01 on epoch=4
05/30/2022 15:07:00 - INFO - __main__ - Step 20 Global step 20 Train loss 4.97 on epoch=9
05/30/2022 15:07:02 - INFO - __main__ - Step 30 Global step 30 Train loss 4.96 on epoch=14
05/30/2022 15:07:04 - INFO - __main__ - Step 40 Global step 40 Train loss 4.83 on epoch=19
05/30/2022 15:07:06 - INFO - __main__ - Step 50 Global step 50 Train loss 4.86 on epoch=24
05/30/2022 15:07:07 - INFO - __main__ - Global step 50 Train loss 4.93 Classification-F1 0.0 on epoch=24
05/30/2022 15:07:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 15:07:09 - INFO - __main__ - Step 60 Global step 60 Train loss 4.69 on epoch=29
05/30/2022 15:07:11 - INFO - __main__ - Step 70 Global step 70 Train loss 4.51 on epoch=34
05/30/2022 15:07:13 - INFO - __main__ - Step 80 Global step 80 Train loss 4.46 on epoch=39
05/30/2022 15:07:15 - INFO - __main__ - Step 90 Global step 90 Train loss 4.33 on epoch=44
05/30/2022 15:07:17 - INFO - __main__ - Step 100 Global step 100 Train loss 4.19 on epoch=49
05/30/2022 15:07:18 - INFO - __main__ - Global step 100 Train loss 4.44 Classification-F1 0.0 on epoch=49
05/30/2022 15:07:20 - INFO - __main__ - Step 110 Global step 110 Train loss 4.12 on epoch=54
05/30/2022 15:07:22 - INFO - __main__ - Step 120 Global step 120 Train loss 3.96 on epoch=59
05/30/2022 15:07:24 - INFO - __main__ - Step 130 Global step 130 Train loss 3.95 on epoch=64
05/30/2022 15:07:26 - INFO - __main__ - Step 140 Global step 140 Train loss 3.91 on epoch=69
05/30/2022 15:07:27 - INFO - __main__ - Step 150 Global step 150 Train loss 3.81 on epoch=74
05/30/2022 15:07:29 - INFO - __main__ - Global step 150 Train loss 3.95 Classification-F1 0.0 on epoch=74
05/30/2022 15:07:31 - INFO - __main__ - Step 160 Global step 160 Train loss 3.79 on epoch=79
05/30/2022 15:07:33 - INFO - __main__ - Step 170 Global step 170 Train loss 3.65 on epoch=84
05/30/2022 15:07:35 - INFO - __main__ - Step 180 Global step 180 Train loss 3.62 on epoch=89
05/30/2022 15:07:37 - INFO - __main__ - Step 190 Global step 190 Train loss 3.61 on epoch=94
05/30/2022 15:07:39 - INFO - __main__ - Step 200 Global step 200 Train loss 3.52 on epoch=99
05/30/2022 15:07:41 - INFO - __main__ - Global step 200 Train loss 3.64 Classification-F1 0.13 on epoch=99
05/30/2022 15:07:41 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.13 on epoch=99, global_step=200
05/30/2022 15:07:43 - INFO - __main__ - Step 210 Global step 210 Train loss 3.45 on epoch=104
05/30/2022 15:07:45 - INFO - __main__ - Step 220 Global step 220 Train loss 3.31 on epoch=109
05/30/2022 15:07:47 - INFO - __main__ - Step 230 Global step 230 Train loss 3.34 on epoch=114
05/30/2022 15:07:49 - INFO - __main__ - Step 240 Global step 240 Train loss 3.22 on epoch=119
05/30/2022 15:07:51 - INFO - __main__ - Step 250 Global step 250 Train loss 3.17 on epoch=124
05/30/2022 15:07:52 - INFO - __main__ - Global step 250 Train loss 3.30 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 15:07:53 - INFO - __main__ - Saving model with best Classification-F1: 0.13 -> 0.3333333333333333 on epoch=124, global_step=250
05/30/2022 15:07:54 - INFO - __main__ - Step 260 Global step 260 Train loss 3.02 on epoch=129
05/30/2022 15:07:56 - INFO - __main__ - Step 270 Global step 270 Train loss 3.06 on epoch=134
05/30/2022 15:07:58 - INFO - __main__ - Step 280 Global step 280 Train loss 2.93 on epoch=139
05/30/2022 15:08:00 - INFO - __main__ - Step 290 Global step 290 Train loss 2.82 on epoch=144
05/30/2022 15:08:02 - INFO - __main__ - Step 300 Global step 300 Train loss 2.70 on epoch=149
05/30/2022 15:08:05 - INFO - __main__ - Global step 300 Train loss 2.91 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 15:08:07 - INFO - __main__ - Step 310 Global step 310 Train loss 2.70 on epoch=154
05/30/2022 15:08:09 - INFO - __main__ - Step 320 Global step 320 Train loss 2.72 on epoch=159
05/30/2022 15:08:11 - INFO - __main__ - Step 330 Global step 330 Train loss 2.53 on epoch=164
05/30/2022 15:08:13 - INFO - __main__ - Step 340 Global step 340 Train loss 2.60 on epoch=169
05/30/2022 15:08:15 - INFO - __main__ - Step 350 Global step 350 Train loss 2.45 on epoch=174
05/30/2022 15:08:17 - INFO - __main__ - Global step 350 Train loss 2.60 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 15:08:19 - INFO - __main__ - Step 360 Global step 360 Train loss 2.36 on epoch=179
05/30/2022 15:08:21 - INFO - __main__ - Step 370 Global step 370 Train loss 2.42 on epoch=184
05/30/2022 15:08:23 - INFO - __main__ - Step 380 Global step 380 Train loss 2.39 on epoch=189
05/30/2022 15:08:25 - INFO - __main__ - Step 390 Global step 390 Train loss 2.32 on epoch=194
05/30/2022 15:08:27 - INFO - __main__ - Step 400 Global step 400 Train loss 2.23 on epoch=199
05/30/2022 15:08:29 - INFO - __main__ - Global step 400 Train loss 2.34 Classification-F1 0.3333333333333333 on epoch=199
05/30/2022 15:08:31 - INFO - __main__ - Step 410 Global step 410 Train loss 2.27 on epoch=204
05/30/2022 15:08:32 - INFO - __main__ - Step 420 Global step 420 Train loss 2.29 on epoch=209
05/30/2022 15:08:34 - INFO - __main__ - Step 430 Global step 430 Train loss 2.24 on epoch=214
05/30/2022 15:08:36 - INFO - __main__ - Step 440 Global step 440 Train loss 2.06 on epoch=219
05/30/2022 15:08:38 - INFO - __main__ - Step 450 Global step 450 Train loss 1.93 on epoch=224
05/30/2022 15:08:39 - INFO - __main__ - Global step 450 Train loss 2.16 Classification-F1 0.3333333333333333 on epoch=224
05/30/2022 15:08:41 - INFO - __main__ - Step 460 Global step 460 Train loss 2.06 on epoch=229
05/30/2022 15:08:43 - INFO - __main__ - Step 470 Global step 470 Train loss 1.86 on epoch=234
05/30/2022 15:08:45 - INFO - __main__ - Step 480 Global step 480 Train loss 1.87 on epoch=239
05/30/2022 15:08:47 - INFO - __main__ - Step 490 Global step 490 Train loss 1.88 on epoch=244
05/30/2022 15:08:49 - INFO - __main__ - Step 500 Global step 500 Train loss 1.90 on epoch=249
05/30/2022 15:08:50 - INFO - __main__ - Global step 500 Train loss 1.91 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 15:08:52 - INFO - __main__ - Step 510 Global step 510 Train loss 1.76 on epoch=254
05/30/2022 15:08:54 - INFO - __main__ - Step 520 Global step 520 Train loss 1.78 on epoch=259
05/30/2022 15:08:56 - INFO - __main__ - Step 530 Global step 530 Train loss 1.80 on epoch=264
05/30/2022 15:08:57 - INFO - __main__ - Step 540 Global step 540 Train loss 1.58 on epoch=269
05/30/2022 15:08:59 - INFO - __main__ - Step 550 Global step 550 Train loss 1.61 on epoch=274
05/30/2022 15:09:00 - INFO - __main__ - Global step 550 Train loss 1.71 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 15:09:02 - INFO - __main__ - Step 560 Global step 560 Train loss 1.59 on epoch=279
05/30/2022 15:09:04 - INFO - __main__ - Step 570 Global step 570 Train loss 1.56 on epoch=284
05/30/2022 15:09:06 - INFO - __main__ - Step 580 Global step 580 Train loss 1.54 on epoch=289
05/30/2022 15:09:08 - INFO - __main__ - Step 590 Global step 590 Train loss 1.56 on epoch=294
05/30/2022 15:09:10 - INFO - __main__ - Step 600 Global step 600 Train loss 1.59 on epoch=299
05/30/2022 15:09:11 - INFO - __main__ - Global step 600 Train loss 1.57 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 15:09:13 - INFO - __main__ - Step 610 Global step 610 Train loss 1.50 on epoch=304
05/30/2022 15:09:15 - INFO - __main__ - Step 620 Global step 620 Train loss 1.54 on epoch=309
05/30/2022 15:09:17 - INFO - __main__ - Step 630 Global step 630 Train loss 1.44 on epoch=314
05/30/2022 15:09:19 - INFO - __main__ - Step 640 Global step 640 Train loss 1.54 on epoch=319
05/30/2022 15:09:20 - INFO - __main__ - Step 650 Global step 650 Train loss 1.44 on epoch=324
05/30/2022 15:09:22 - INFO - __main__ - Global step 650 Train loss 1.49 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 15:09:24 - INFO - __main__ - Step 660 Global step 660 Train loss 1.31 on epoch=329
05/30/2022 15:09:26 - INFO - __main__ - Step 670 Global step 670 Train loss 1.28 on epoch=334
05/30/2022 15:09:28 - INFO - __main__ - Step 680 Global step 680 Train loss 1.25 on epoch=339
05/30/2022 15:09:30 - INFO - __main__ - Step 690 Global step 690 Train loss 1.20 on epoch=344
05/30/2022 15:09:32 - INFO - __main__ - Step 700 Global step 700 Train loss 1.30 on epoch=349
05/30/2022 15:09:33 - INFO - __main__ - Global step 700 Train loss 1.27 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 15:09:35 - INFO - __main__ - Step 710 Global step 710 Train loss 1.10 on epoch=354
05/30/2022 15:09:37 - INFO - __main__ - Step 720 Global step 720 Train loss 1.16 on epoch=359
05/30/2022 15:09:39 - INFO - __main__ - Step 730 Global step 730 Train loss 1.15 on epoch=364
05/30/2022 15:09:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.99 on epoch=369
05/30/2022 15:09:43 - INFO - __main__ - Step 750 Global step 750 Train loss 1.04 on epoch=374
05/30/2022 15:09:45 - INFO - __main__ - Global step 750 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 15:09:47 - INFO - __main__ - Step 760 Global step 760 Train loss 1.12 on epoch=379
05/30/2022 15:09:49 - INFO - __main__ - Step 770 Global step 770 Train loss 1.14 on epoch=384
05/30/2022 15:09:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.93 on epoch=389
05/30/2022 15:09:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.97 on epoch=394
05/30/2022 15:09:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.98 on epoch=399
05/30/2022 15:09:56 - INFO - __main__ - Global step 800 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 15:09:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.95 on epoch=404
05/30/2022 15:09:59 - INFO - __main__ - Step 820 Global step 820 Train loss 1.02 on epoch=409
05/30/2022 15:10:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.90 on epoch=414
05/30/2022 15:10:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.92 on epoch=419
05/30/2022 15:10:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.89 on epoch=424
05/30/2022 15:10:06 - INFO - __main__ - Global step 850 Train loss 0.93 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 15:10:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.93 on epoch=429
05/30/2022 15:10:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.90 on epoch=434
05/30/2022 15:10:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=439
05/30/2022 15:10:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.75 on epoch=444
05/30/2022 15:10:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.80 on epoch=449
05/30/2022 15:10:17 - INFO - __main__ - Global step 900 Train loss 0.84 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 15:10:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.73 on epoch=454
05/30/2022 15:10:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.75 on epoch=459
05/30/2022 15:10:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.76 on epoch=464
05/30/2022 15:10:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.75 on epoch=469
05/30/2022 15:10:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.69 on epoch=474
05/30/2022 15:10:27 - INFO - __main__ - Global step 950 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 15:10:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.80 on epoch=479
05/30/2022 15:10:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.66 on epoch=484
05/30/2022 15:10:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.82 on epoch=489
05/30/2022 15:10:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.71 on epoch=494
05/30/2022 15:10:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.69 on epoch=499
05/30/2022 15:10:38 - INFO - __main__ - Global step 1000 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 15:10:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.76 on epoch=504
05/30/2022 15:10:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.73 on epoch=509
05/30/2022 15:10:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.68 on epoch=514
05/30/2022 15:10:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.70 on epoch=519
05/30/2022 15:10:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.69 on epoch=524
05/30/2022 15:10:48 - INFO - __main__ - Global step 1050 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 15:10:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.70 on epoch=529
05/30/2022 15:10:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.69 on epoch=534
05/30/2022 15:10:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.67 on epoch=539
05/30/2022 15:10:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.59 on epoch=544
05/30/2022 15:10:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.59 on epoch=549
05/30/2022 15:10:59 - INFO - __main__ - Global step 1100 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 15:11:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.56 on epoch=554
05/30/2022 15:11:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.62 on epoch=559
05/30/2022 15:11:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.58 on epoch=564
05/30/2022 15:11:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.55 on epoch=569
05/30/2022 15:11:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.61 on epoch=574
05/30/2022 15:11:09 - INFO - __main__ - Global step 1150 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 15:11:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.55 on epoch=579
05/30/2022 15:11:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.61 on epoch=584
05/30/2022 15:11:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.59 on epoch=589
05/30/2022 15:11:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.65 on epoch=594
05/30/2022 15:11:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.54 on epoch=599
05/30/2022 15:11:20 - INFO - __main__ - Global step 1200 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 15:11:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=604
05/30/2022 15:11:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.54 on epoch=609
05/30/2022 15:11:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=614
05/30/2022 15:11:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.60 on epoch=619
05/30/2022 15:11:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=624
05/30/2022 15:11:31 - INFO - __main__ - Global step 1250 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 15:11:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=629
05/30/2022 15:11:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.51 on epoch=634
05/30/2022 15:11:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
05/30/2022 15:11:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.60 on epoch=644
05/30/2022 15:11:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.57 on epoch=649
05/30/2022 15:11:41 - INFO - __main__ - Global step 1300 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 15:11:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.51 on epoch=654
05/30/2022 15:11:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=659
05/30/2022 15:11:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.54 on epoch=664
05/30/2022 15:11:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=669
05/30/2022 15:11:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=674
05/30/2022 15:11:52 - INFO - __main__ - Global step 1350 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 15:11:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.52 on epoch=679
05/30/2022 15:11:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.51 on epoch=684
05/30/2022 15:11:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=689
05/30/2022 15:11:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=694
05/30/2022 15:12:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=699
05/30/2022 15:12:02 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 15:12:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=704
05/30/2022 15:12:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=709
05/30/2022 15:12:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
05/30/2022 15:12:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=719
05/30/2022 15:12:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=724
05/30/2022 15:12:12 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=724
05/30/2022 15:12:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=729
05/30/2022 15:12:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.49 on epoch=734
05/30/2022 15:12:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.50 on epoch=739
05/30/2022 15:12:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=744
05/30/2022 15:12:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=749
05/30/2022 15:12:23 - INFO - __main__ - Global step 1500 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 15:12:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
05/30/2022 15:12:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=759
05/30/2022 15:12:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=764
05/30/2022 15:12:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=769
05/30/2022 15:12:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=774
05/30/2022 15:12:33 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=774
05/30/2022 15:12:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=779
05/30/2022 15:12:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.42 on epoch=784
05/30/2022 15:12:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=789
05/30/2022 15:12:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=794
05/30/2022 15:12:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.41 on epoch=799
05/30/2022 15:12:43 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 15:12:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=804
05/30/2022 15:12:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=809
05/30/2022 15:12:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=814
05/30/2022 15:12:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=819
05/30/2022 15:12:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=824
05/30/2022 15:12:53 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 15:12:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=829
05/30/2022 15:12:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=834
05/30/2022 15:12:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
05/30/2022 15:13:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=844
05/30/2022 15:13:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=849
05/30/2022 15:13:04 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 15:13:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=854
05/30/2022 15:13:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.18 on epoch=859
05/30/2022 15:13:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=864
05/30/2022 15:13:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=869
05/30/2022 15:13:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
05/30/2022 15:13:14 - INFO - __main__ - Global step 1750 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 15:13:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=879
05/30/2022 15:13:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
05/30/2022 15:13:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=889
05/30/2022 15:13:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=894
05/30/2022 15:13:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=899
05/30/2022 15:13:24 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 15:13:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=904
05/30/2022 15:13:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=909
05/30/2022 15:13:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=914
05/30/2022 15:13:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=919
05/30/2022 15:13:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=924
05/30/2022 15:13:35 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 15:13:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=929
05/30/2022 15:13:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
05/30/2022 15:13:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=939
05/30/2022 15:13:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=944
05/30/2022 15:13:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
05/30/2022 15:13:45 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 15:13:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=954
05/30/2022 15:13:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
05/30/2022 15:13:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=964
05/30/2022 15:13:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=969
05/30/2022 15:13:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=974
05/30/2022 15:13:55 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 15:13:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=979
05/30/2022 15:13:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=984
05/30/2022 15:14:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=989
05/30/2022 15:14:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=994
05/30/2022 15:14:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=999
05/30/2022 15:14:05 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=999
05/30/2022 15:14:05 - INFO - __main__ - save last model!
05/30/2022 15:14:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 15:14:05 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 15:14:05 - INFO - __main__ - Printing 3 examples
05/30/2022 15:14:05 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:14:05 - INFO - __main__ - ['entailed']
05/30/2022 15:14:05 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:14:05 - INFO - __main__ - ['entailed']
05/30/2022 15:14:05 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:14:05 - INFO - __main__ - ['entailed']
05/30/2022 15:14:05 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:14:06 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 15:14:06 - INFO - __main__ - Printing 3 examples
05/30/2022 15:14:06 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 15:14:06 - INFO - __main__ - ['entailed']
05/30/2022 15:14:06 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 15:14:06 - INFO - __main__ - ['entailed']
05/30/2022 15:14:06 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 15:14:06 - INFO - __main__ - ['entailed']
05/30/2022 15:14:06 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:14:06 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:14:06 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 15:14:06 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 15:14:06 - INFO - __main__ - Printing 3 examples
05/30/2022 15:14:06 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/30/2022 15:14:06 - INFO - __main__ - ['entailed']
05/30/2022 15:14:06 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/30/2022 15:14:06 - INFO - __main__ - ['entailed']
05/30/2022 15:14:06 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/30/2022 15:14:06 - INFO - __main__ - ['entailed']
05/30/2022 15:14:06 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:14:06 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:14:06 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 15:14:11 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 15:14:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 15:14:12 - INFO - __main__ - Starting training!
05/30/2022 15:14:30 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:14:43 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 15:18:52 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.3_8_predictions.txt
05/30/2022 15:18:52 - INFO - __main__ - Classification-F1 on test data: 0.3305
05/30/2022 15:18:53 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.3, bsz=8, dev_performance=0.3333333333333333, test_performance=0.33047210300429186
05/30/2022 15:18:53 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.2, bsz=8 ...
05/30/2022 15:18:53 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 15:18:53 - INFO - __main__ - Printing 3 examples
05/30/2022 15:18:53 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 15:18:53 - INFO - __main__ - ['entailed']
05/30/2022 15:18:53 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 15:18:53 - INFO - __main__ - ['entailed']
05/30/2022 15:18:53 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 15:18:53 - INFO - __main__ - ['entailed']
05/30/2022 15:18:53 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:18:54 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:18:54 - INFO - __main__ - Loaded 32 examples from train data
05/30/2022 15:18:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/30/2022 15:18:54 - INFO - __main__ - Printing 3 examples
05/30/2022 15:18:54 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
05/30/2022 15:18:54 - INFO - __main__ - ['entailed']
05/30/2022 15:18:54 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
05/30/2022 15:18:54 - INFO - __main__ - ['entailed']
05/30/2022 15:18:54 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
05/30/2022 15:18:54 - INFO - __main__ - ['entailed']
05/30/2022 15:18:54 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:18:54 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:18:54 - INFO - __main__ - Loaded 32 examples from dev data
05/30/2022 15:18:59 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 15:18:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/30/2022 15:18:59 - INFO - __main__ - Starting training!
05/30/2022 15:19:02 - INFO - __main__ - Step 10 Global step 10 Train loss 5.03 on epoch=4
05/30/2022 15:19:03 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=9
05/30/2022 15:19:05 - INFO - __main__ - Step 30 Global step 30 Train loss 4.96 on epoch=14
05/30/2022 15:19:07 - INFO - __main__ - Step 40 Global step 40 Train loss 4.86 on epoch=19
05/30/2022 15:19:09 - INFO - __main__ - Step 50 Global step 50 Train loss 4.80 on epoch=24
05/30/2022 15:19:11 - INFO - __main__ - Global step 50 Train loss 4.93 Classification-F1 0.0 on epoch=24
05/30/2022 15:19:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
05/30/2022 15:19:13 - INFO - __main__ - Step 60 Global step 60 Train loss 4.86 on epoch=29
05/30/2022 15:19:15 - INFO - __main__ - Step 70 Global step 70 Train loss 4.82 on epoch=34
05/30/2022 15:19:17 - INFO - __main__ - Step 80 Global step 80 Train loss 4.67 on epoch=39
05/30/2022 15:19:19 - INFO - __main__ - Step 90 Global step 90 Train loss 4.71 on epoch=44
05/30/2022 15:19:21 - INFO - __main__ - Step 100 Global step 100 Train loss 4.65 on epoch=49
05/30/2022 15:19:22 - INFO - __main__ - Global step 100 Train loss 4.74 Classification-F1 0.0 on epoch=49
05/30/2022 15:19:24 - INFO - __main__ - Step 110 Global step 110 Train loss 4.58 on epoch=54
05/30/2022 15:19:26 - INFO - __main__ - Step 120 Global step 120 Train loss 4.59 on epoch=59
05/30/2022 15:19:28 - INFO - __main__ - Step 130 Global step 130 Train loss 4.59 on epoch=64
05/30/2022 15:19:30 - INFO - __main__ - Step 140 Global step 140 Train loss 4.45 on epoch=69
05/30/2022 15:19:32 - INFO - __main__ - Step 150 Global step 150 Train loss 4.52 on epoch=74
05/30/2022 15:19:33 - INFO - __main__ - Global step 150 Train loss 4.55 Classification-F1 0.0 on epoch=74
05/30/2022 15:19:35 - INFO - __main__ - Step 160 Global step 160 Train loss 4.51 on epoch=79
05/30/2022 15:19:37 - INFO - __main__ - Step 170 Global step 170 Train loss 4.40 on epoch=84
05/30/2022 15:19:39 - INFO - __main__ - Step 180 Global step 180 Train loss 4.29 on epoch=89
05/30/2022 15:19:41 - INFO - __main__ - Step 190 Global step 190 Train loss 4.24 on epoch=94
05/30/2022 15:19:43 - INFO - __main__ - Step 200 Global step 200 Train loss 4.27 on epoch=99
05/30/2022 15:19:44 - INFO - __main__ - Global step 200 Train loss 4.34 Classification-F1 0.0 on epoch=99
05/30/2022 15:19:46 - INFO - __main__ - Step 210 Global step 210 Train loss 4.27 on epoch=104
05/30/2022 15:19:48 - INFO - __main__ - Step 220 Global step 220 Train loss 4.16 on epoch=109
05/30/2022 15:19:50 - INFO - __main__ - Step 230 Global step 230 Train loss 4.01 on epoch=114
05/30/2022 15:19:52 - INFO - __main__ - Step 240 Global step 240 Train loss 4.02 on epoch=119
05/30/2022 15:19:54 - INFO - __main__ - Step 250 Global step 250 Train loss 3.99 on epoch=124
05/30/2022 15:19:57 - INFO - __main__ - Global step 250 Train loss 4.09 Classification-F1 0.0 on epoch=124
05/30/2022 15:19:59 - INFO - __main__ - Step 260 Global step 260 Train loss 3.88 on epoch=129
05/30/2022 15:20:01 - INFO - __main__ - Step 270 Global step 270 Train loss 3.93 on epoch=134
05/30/2022 15:20:02 - INFO - __main__ - Step 280 Global step 280 Train loss 3.83 on epoch=139
05/30/2022 15:20:04 - INFO - __main__ - Step 290 Global step 290 Train loss 3.84 on epoch=144
05/30/2022 15:20:06 - INFO - __main__ - Step 300 Global step 300 Train loss 3.73 on epoch=149
05/30/2022 15:20:08 - INFO - __main__ - Global step 300 Train loss 3.84 Classification-F1 0.0 on epoch=149
05/30/2022 15:20:09 - INFO - __main__ - Step 310 Global step 310 Train loss 3.75 on epoch=154
05/30/2022 15:20:11 - INFO - __main__ - Step 320 Global step 320 Train loss 3.62 on epoch=159
05/30/2022 15:20:13 - INFO - __main__ - Step 330 Global step 330 Train loss 3.63 on epoch=164
05/30/2022 15:20:15 - INFO - __main__ - Step 340 Global step 340 Train loss 3.62 on epoch=169
05/30/2022 15:20:17 - INFO - __main__ - Step 350 Global step 350 Train loss 3.54 on epoch=174
05/30/2022 15:20:19 - INFO - __main__ - Global step 350 Train loss 3.63 Classification-F1 0.0 on epoch=174
05/30/2022 15:20:21 - INFO - __main__ - Step 360 Global step 360 Train loss 3.50 on epoch=179
05/30/2022 15:20:23 - INFO - __main__ - Step 370 Global step 370 Train loss 3.48 on epoch=184
05/30/2022 15:20:25 - INFO - __main__ - Step 380 Global step 380 Train loss 3.51 on epoch=189
05/30/2022 15:20:27 - INFO - __main__ - Step 390 Global step 390 Train loss 3.39 on epoch=194
05/30/2022 15:20:29 - INFO - __main__ - Step 400 Global step 400 Train loss 3.29 on epoch=199
05/30/2022 15:20:32 - INFO - __main__ - Global step 400 Train loss 3.43 Classification-F1 0.0625 on epoch=199
05/30/2022 15:20:32 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0625 on epoch=199, global_step=400
05/30/2022 15:20:34 - INFO - __main__ - Step 410 Global step 410 Train loss 3.35 on epoch=204
05/30/2022 15:20:36 - INFO - __main__ - Step 420 Global step 420 Train loss 3.26 on epoch=209
05/30/2022 15:20:38 - INFO - __main__ - Step 430 Global step 430 Train loss 3.23 on epoch=214
05/30/2022 15:20:39 - INFO - __main__ - Step 440 Global step 440 Train loss 3.30 on epoch=219
05/30/2022 15:20:41 - INFO - __main__ - Step 450 Global step 450 Train loss 3.18 on epoch=224
05/30/2022 15:20:45 - INFO - __main__ - Global step 450 Train loss 3.26 Classification-F1 0.10317460317460318 on epoch=224
05/30/2022 15:20:45 - INFO - __main__ - Saving model with best Classification-F1: 0.0625 -> 0.10317460317460318 on epoch=224, global_step=450
05/30/2022 15:20:47 - INFO - __main__ - Step 460 Global step 460 Train loss 3.17 on epoch=229
05/30/2022 15:20:48 - INFO - __main__ - Step 470 Global step 470 Train loss 3.11 on epoch=234
05/30/2022 15:20:50 - INFO - __main__ - Step 480 Global step 480 Train loss 3.01 on epoch=239
05/30/2022 15:20:52 - INFO - __main__ - Step 490 Global step 490 Train loss 3.09 on epoch=244
05/30/2022 15:20:54 - INFO - __main__ - Step 500 Global step 500 Train loss 2.92 on epoch=249
05/30/2022 15:20:57 - INFO - __main__ - Global step 500 Train loss 3.06 Classification-F1 0.3333333333333333 on epoch=249
05/30/2022 15:20:57 - INFO - __main__ - Saving model with best Classification-F1: 0.10317460317460318 -> 0.3333333333333333 on epoch=249, global_step=500
05/30/2022 15:20:59 - INFO - __main__ - Step 510 Global step 510 Train loss 3.00 on epoch=254
05/30/2022 15:21:01 - INFO - __main__ - Step 520 Global step 520 Train loss 2.88 on epoch=259
05/30/2022 15:21:02 - INFO - __main__ - Step 530 Global step 530 Train loss 2.94 on epoch=264
05/30/2022 15:21:04 - INFO - __main__ - Step 540 Global step 540 Train loss 2.75 on epoch=269
05/30/2022 15:21:06 - INFO - __main__ - Step 550 Global step 550 Train loss 2.89 on epoch=274
05/30/2022 15:21:10 - INFO - __main__ - Global step 550 Train loss 2.89 Classification-F1 0.3333333333333333 on epoch=274
05/30/2022 15:21:12 - INFO - __main__ - Step 560 Global step 560 Train loss 2.72 on epoch=279
05/30/2022 15:21:14 - INFO - __main__ - Step 570 Global step 570 Train loss 2.73 on epoch=284
05/30/2022 15:21:15 - INFO - __main__ - Step 580 Global step 580 Train loss 2.69 on epoch=289
05/30/2022 15:21:17 - INFO - __main__ - Step 590 Global step 590 Train loss 2.71 on epoch=294
05/30/2022 15:21:19 - INFO - __main__ - Step 600 Global step 600 Train loss 2.70 on epoch=299
05/30/2022 15:21:22 - INFO - __main__ - Global step 600 Train loss 2.71 Classification-F1 0.3333333333333333 on epoch=299
05/30/2022 15:21:24 - INFO - __main__ - Step 610 Global step 610 Train loss 2.58 on epoch=304
05/30/2022 15:21:26 - INFO - __main__ - Step 620 Global step 620 Train loss 2.63 on epoch=309
05/30/2022 15:21:28 - INFO - __main__ - Step 630 Global step 630 Train loss 2.50 on epoch=314
05/30/2022 15:21:30 - INFO - __main__ - Step 640 Global step 640 Train loss 2.43 on epoch=319
05/30/2022 15:21:32 - INFO - __main__ - Step 650 Global step 650 Train loss 2.49 on epoch=324
05/30/2022 15:21:35 - INFO - __main__ - Global step 650 Train loss 2.53 Classification-F1 0.3333333333333333 on epoch=324
05/30/2022 15:21:37 - INFO - __main__ - Step 660 Global step 660 Train loss 2.48 on epoch=329
05/30/2022 15:21:39 - INFO - __main__ - Step 670 Global step 670 Train loss 2.36 on epoch=334
05/30/2022 15:21:41 - INFO - __main__ - Step 680 Global step 680 Train loss 2.28 on epoch=339
05/30/2022 15:21:43 - INFO - __main__ - Step 690 Global step 690 Train loss 2.31 on epoch=344
05/30/2022 15:21:45 - INFO - __main__ - Step 700 Global step 700 Train loss 2.34 on epoch=349
05/30/2022 15:21:48 - INFO - __main__ - Global step 700 Train loss 2.35 Classification-F1 0.3333333333333333 on epoch=349
05/30/2022 15:21:50 - INFO - __main__ - Step 710 Global step 710 Train loss 2.20 on epoch=354
05/30/2022 15:21:52 - INFO - __main__ - Step 720 Global step 720 Train loss 2.09 on epoch=359
05/30/2022 15:21:54 - INFO - __main__ - Step 730 Global step 730 Train loss 2.13 on epoch=364
05/30/2022 15:21:56 - INFO - __main__ - Step 740 Global step 740 Train loss 2.08 on epoch=369
05/30/2022 15:21:58 - INFO - __main__ - Step 750 Global step 750 Train loss 2.09 on epoch=374
05/30/2022 15:22:01 - INFO - __main__ - Global step 750 Train loss 2.12 Classification-F1 0.3333333333333333 on epoch=374
05/30/2022 15:22:03 - INFO - __main__ - Step 760 Global step 760 Train loss 2.04 on epoch=379
05/30/2022 15:22:05 - INFO - __main__ - Step 770 Global step 770 Train loss 1.91 on epoch=384
05/30/2022 15:22:07 - INFO - __main__ - Step 780 Global step 780 Train loss 1.88 on epoch=389
05/30/2022 15:22:09 - INFO - __main__ - Step 790 Global step 790 Train loss 1.89 on epoch=394
05/30/2022 15:22:11 - INFO - __main__ - Step 800 Global step 800 Train loss 1.88 on epoch=399
05/30/2022 15:22:14 - INFO - __main__ - Global step 800 Train loss 1.92 Classification-F1 0.3333333333333333 on epoch=399
05/30/2022 15:22:16 - INFO - __main__ - Step 810 Global step 810 Train loss 1.82 on epoch=404
05/30/2022 15:22:18 - INFO - __main__ - Step 820 Global step 820 Train loss 1.90 on epoch=409
05/30/2022 15:22:20 - INFO - __main__ - Step 830 Global step 830 Train loss 1.75 on epoch=414
05/30/2022 15:22:22 - INFO - __main__ - Step 840 Global step 840 Train loss 1.61 on epoch=419
05/30/2022 15:22:23 - INFO - __main__ - Step 850 Global step 850 Train loss 1.66 on epoch=424
05/30/2022 15:22:27 - INFO - __main__ - Global step 850 Train loss 1.75 Classification-F1 0.3333333333333333 on epoch=424
05/30/2022 15:22:29 - INFO - __main__ - Step 860 Global step 860 Train loss 1.55 on epoch=429
05/30/2022 15:22:30 - INFO - __main__ - Step 870 Global step 870 Train loss 1.59 on epoch=434
05/30/2022 15:22:32 - INFO - __main__ - Step 880 Global step 880 Train loss 1.49 on epoch=439
05/30/2022 15:22:34 - INFO - __main__ - Step 890 Global step 890 Train loss 1.53 on epoch=444
05/30/2022 15:22:36 - INFO - __main__ - Step 900 Global step 900 Train loss 1.55 on epoch=449
05/30/2022 15:22:39 - INFO - __main__ - Global step 900 Train loss 1.54 Classification-F1 0.3333333333333333 on epoch=449
05/30/2022 15:22:41 - INFO - __main__ - Step 910 Global step 910 Train loss 1.46 on epoch=454
05/30/2022 15:22:43 - INFO - __main__ - Step 920 Global step 920 Train loss 1.56 on epoch=459
05/30/2022 15:22:45 - INFO - __main__ - Step 930 Global step 930 Train loss 1.42 on epoch=464
05/30/2022 15:22:47 - INFO - __main__ - Step 940 Global step 940 Train loss 1.33 on epoch=469
05/30/2022 15:22:48 - INFO - __main__ - Step 950 Global step 950 Train loss 1.35 on epoch=474
05/30/2022 15:22:50 - INFO - __main__ - Global step 950 Train loss 1.42 Classification-F1 0.3333333333333333 on epoch=474
05/30/2022 15:22:52 - INFO - __main__ - Step 960 Global step 960 Train loss 1.26 on epoch=479
05/30/2022 15:22:54 - INFO - __main__ - Step 970 Global step 970 Train loss 1.34 on epoch=484
05/30/2022 15:22:56 - INFO - __main__ - Step 980 Global step 980 Train loss 1.37 on epoch=489
05/30/2022 15:22:58 - INFO - __main__ - Step 990 Global step 990 Train loss 1.42 on epoch=494
05/30/2022 15:23:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.24 on epoch=499
05/30/2022 15:23:01 - INFO - __main__ - Global step 1000 Train loss 1.33 Classification-F1 0.3333333333333333 on epoch=499
05/30/2022 15:23:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.31 on epoch=504
05/30/2022 15:23:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.19 on epoch=509
05/30/2022 15:23:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.23 on epoch=514
05/30/2022 15:23:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.11 on epoch=519
05/30/2022 15:23:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.16 on epoch=524
05/30/2022 15:23:11 - INFO - __main__ - Global step 1050 Train loss 1.20 Classification-F1 0.3333333333333333 on epoch=524
05/30/2022 15:23:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.14 on epoch=529
05/30/2022 15:23:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.05 on epoch=534
05/30/2022 15:23:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.03 on epoch=539
05/30/2022 15:23:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.03 on epoch=544
05/30/2022 15:23:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.96 on epoch=549
05/30/2022 15:23:21 - INFO - __main__ - Global step 1100 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=549
05/30/2022 15:23:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.05 on epoch=554
05/30/2022 15:23:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.87 on epoch=559
05/30/2022 15:23:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.02 on epoch=564
05/30/2022 15:23:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.94 on epoch=569
05/30/2022 15:23:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.88 on epoch=574
05/30/2022 15:23:32 - INFO - __main__ - Global step 1150 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=574
05/30/2022 15:23:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.87 on epoch=579
05/30/2022 15:23:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.94 on epoch=584
05/30/2022 15:23:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.93 on epoch=589
05/30/2022 15:23:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.94 on epoch=594
05/30/2022 15:23:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=599
05/30/2022 15:23:42 - INFO - __main__ - Global step 1200 Train loss 0.90 Classification-F1 0.3333333333333333 on epoch=599
05/30/2022 15:23:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.86 on epoch=604
05/30/2022 15:23:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.90 on epoch=609
05/30/2022 15:23:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.85 on epoch=614
05/30/2022 15:23:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.85 on epoch=619
05/30/2022 15:23:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.71 on epoch=624
05/30/2022 15:23:52 - INFO - __main__ - Global step 1250 Train loss 0.83 Classification-F1 0.3333333333333333 on epoch=624
05/30/2022 15:23:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.82 on epoch=629
05/30/2022 15:23:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=634
05/30/2022 15:23:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.78 on epoch=639
05/30/2022 15:24:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.79 on epoch=644
05/30/2022 15:24:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.75 on epoch=649
05/30/2022 15:24:03 - INFO - __main__ - Global step 1300 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=649
05/30/2022 15:24:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.67 on epoch=654
05/30/2022 15:24:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.74 on epoch=659
05/30/2022 15:24:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.79 on epoch=664
05/30/2022 15:24:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.67 on epoch=669
05/30/2022 15:24:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.74 on epoch=674
05/30/2022 15:24:13 - INFO - __main__ - Global step 1350 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=674
05/30/2022 15:24:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.72 on epoch=679
05/30/2022 15:24:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.59 on epoch=684
05/30/2022 15:24:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.66 on epoch=689
05/30/2022 15:24:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.63 on epoch=694
05/30/2022 15:24:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.64 on epoch=699
05/30/2022 15:24:24 - INFO - __main__ - Global step 1400 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=699
05/30/2022 15:24:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.72 on epoch=704
05/30/2022 15:24:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.56 on epoch=709
05/30/2022 15:24:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.61 on epoch=714
05/30/2022 15:24:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=719
05/30/2022 15:24:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=724
05/30/2022 15:24:34 - INFO - __main__ - Global step 1450 Train loss 0.59 Classification-F1 0.3992490613266583 on epoch=724
05/30/2022 15:24:34 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=724, global_step=1450
05/30/2022 15:24:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.61 on epoch=729
05/30/2022 15:24:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=734
05/30/2022 15:24:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=739
05/30/2022 15:24:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.60 on epoch=744
05/30/2022 15:24:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.57 on epoch=749
05/30/2022 15:24:44 - INFO - __main__ - Global step 1500 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=749
05/30/2022 15:24:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.56 on epoch=754
05/30/2022 15:24:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.55 on epoch=759
05/30/2022 15:24:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.50 on epoch=764
05/30/2022 15:24:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.52 on epoch=769
05/30/2022 15:24:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.55 on epoch=774
05/30/2022 15:24:55 - INFO - __main__ - Global step 1550 Train loss 0.54 Classification-F1 0.3992490613266583 on epoch=774
05/30/2022 15:24:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=779
05/30/2022 15:24:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.46 on epoch=784
05/30/2022 15:25:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.51 on epoch=789
05/30/2022 15:25:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.61 on epoch=794
05/30/2022 15:25:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=799
05/30/2022 15:25:05 - INFO - __main__ - Global step 1600 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=799
05/30/2022 15:25:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=804
05/30/2022 15:25:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.56 on epoch=809
05/30/2022 15:25:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
05/30/2022 15:25:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.50 on epoch=819
05/30/2022 15:25:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.56 on epoch=824
05/30/2022 15:25:16 - INFO - __main__ - Global step 1650 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=824
05/30/2022 15:25:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=829
05/30/2022 15:25:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=834
05/30/2022 15:25:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=839
05/30/2022 15:25:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=844
05/30/2022 15:25:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=849
05/30/2022 15:25:26 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=849
05/30/2022 15:25:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=854
05/30/2022 15:25:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.46 on epoch=859
05/30/2022 15:25:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=864
05/30/2022 15:25:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=869
05/30/2022 15:25:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.47 on epoch=874
05/30/2022 15:25:36 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=874
05/30/2022 15:25:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=879
05/30/2022 15:25:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=884
05/30/2022 15:25:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=889
05/30/2022 15:25:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=894
05/30/2022 15:25:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=899
05/30/2022 15:25:46 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=899
05/30/2022 15:25:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=904
05/30/2022 15:25:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=909
05/30/2022 15:25:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=914
05/30/2022 15:25:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=919
05/30/2022 15:25:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=924
05/30/2022 15:25:57 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=924
05/30/2022 15:25:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=929
05/30/2022 15:26:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=934
05/30/2022 15:26:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
05/30/2022 15:26:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.50 on epoch=944
05/30/2022 15:26:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=949
05/30/2022 15:26:07 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=949
05/30/2022 15:26:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
05/30/2022 15:26:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
05/30/2022 15:26:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
05/30/2022 15:26:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=969
05/30/2022 15:26:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=974
05/30/2022 15:26:17 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=974
05/30/2022 15:26:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=979
05/30/2022 15:26:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
05/30/2022 15:26:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=989
05/30/2022 15:26:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=994
05/30/2022 15:26:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=999
05/30/2022 15:26:27 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.3191489361702127 on epoch=999
05/30/2022 15:26:27 - INFO - __main__ - save last model!
05/30/2022 15:26:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 15:26:27 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 15:26:27 - INFO - __main__ - Printing 3 examples
05/30/2022 15:26:27 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:26:27 - INFO - __main__ - ['entailed']
05/30/2022 15:26:27 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:26:27 - INFO - __main__ - ['entailed']
05/30/2022 15:26:27 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:26:27 - INFO - __main__ - ['entailed']
05/30/2022 15:26:27 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:26:52 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:27:05 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 15:31:14 - INFO - __main__ - Saved prediction in models/T5-base-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_87_0.2_8_predictions.txt
05/30/2022 15:31:14 - INFO - __main__ - Classification-F1 on test data: 0.3372
05/30/2022 15:31:14 - INFO - __main__ - prefix=tab_fact_16_87, lr=0.2, bsz=8, dev_performance=0.3992490613266583, test_performance=0.33724740154282734
