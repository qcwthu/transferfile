05/21/2022 06:34:54 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 06:34:54 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli
05/21/2022 06:34:54 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 06:34:54 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli
05/21/2022 06:34:56 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 06:34:56 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 06:34:56 - INFO - __main__ - args.device: cuda:0
05/21/2022 06:34:56 - INFO - __main__ - Using 2 gpus
05/21/2022 06:34:56 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
05/21/2022 06:34:56 - INFO - __main__ - args.device: cuda:1
05/21/2022 06:34:56 - INFO - __main__ - Using 2 gpus
05/21/2022 06:34:56 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
05/21/2022 06:35:00 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.5, bsz=8 ...
05/21/2022 06:35:01 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 06:35:01 - INFO - __main__ - Printing 3 examples
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 06:35:01 - INFO - __main__ - Tokenizing Output ...
05/21/2022 06:35:01 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 06:35:01 - INFO - __main__ - Printing 3 examples
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 06:35:01 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 06:35:01 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 06:35:01 - INFO - __main__ - Printing 3 examples
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 06:35:01 - INFO - __main__ - Tokenizing Output ...
05/21/2022 06:35:01 - INFO - __main__ - Tokenizing Output ...
05/21/2022 06:35:01 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 06:35:01 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 06:35:01 - INFO - __main__ - Printing 3 examples
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/21/2022 06:35:01 - INFO - __main__ - ['neutral']
05/21/2022 06:35:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 06:35:01 - INFO - __main__ - Loaded 48 examples from dev data
05/21/2022 06:35:01 - INFO - __main__ - Tokenizing Output ...
05/21/2022 06:35:02 - INFO - __main__ - Loaded 48 examples from dev data
05/21/2022 06:35:19 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 06:35:19 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 04:22:53 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/05/2022 04:22:53 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli
06/05/2022 04:22:53 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1-64shot/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/05/2022 04:22:53 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli
06/05/2022 04:22:54 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/05/2022 04:22:54 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/05/2022 04:22:54 - INFO - __main__ - args.device: cuda:0
06/05/2022 04:22:54 - INFO - __main__ - Using 2 gpus
06/05/2022 04:22:54 - INFO - __main__ - args.device: cuda:1
06/05/2022 04:22:54 - INFO - __main__ - Using 2 gpus
06/05/2022 04:22:54 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
06/05/2022 04:22:54 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
06/05/2022 04:22:59 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.5, bsz=8 ...
06/05/2022 04:22:59 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:22:59 - INFO - __main__ - Printing 3 examples
06/05/2022 04:22:59 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/05/2022 04:22:59 - INFO - __main__ - ['neutral']
06/05/2022 04:22:59 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/05/2022 04:22:59 - INFO - __main__ - ['neutral']
06/05/2022 04:22:59 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/05/2022 04:22:59 - INFO - __main__ - ['neutral']
06/05/2022 04:22:59 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:22:59 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:22:59 - INFO - __main__ - Printing 3 examples
06/05/2022 04:22:59 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/05/2022 04:22:59 - INFO - __main__ - ['neutral']
06/05/2022 04:22:59 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/05/2022 04:22:59 - INFO - __main__ - ['neutral']
06/05/2022 04:22:59 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/05/2022 04:22:59 - INFO - __main__ - ['neutral']
06/05/2022 04:22:59 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:22:59 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:23:00 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:23:00 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 04:23:00 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:23:00 - INFO - __main__ - Printing 3 examples
06/05/2022 04:23:00 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/05/2022 04:23:00 - INFO - __main__ - ['neutral']
06/05/2022 04:23:00 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/05/2022 04:23:00 - INFO - __main__ - ['neutral']
06/05/2022 04:23:00 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/05/2022 04:23:00 - INFO - __main__ - ['neutral']
06/05/2022 04:23:00 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:23:00 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 04:23:00 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:23:00 - INFO - __main__ - Printing 3 examples
06/05/2022 04:23:00 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/05/2022 04:23:00 - INFO - __main__ - ['neutral']
06/05/2022 04:23:00 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/05/2022 04:23:00 - INFO - __main__ - ['neutral']
06/05/2022 04:23:00 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/05/2022 04:23:00 - INFO - __main__ - ['neutral']
06/05/2022 04:23:00 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:23:00 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:23:00 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:23:00 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 04:23:00 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 04:23:17 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 04:23:18 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 04:23:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 04:23:19 - INFO - __main__ - Starting training!
06/05/2022 04:23:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 04:23:23 - INFO - __main__ - Starting training!
06/05/2022 04:23:27 - INFO - __main__ - Step 10 Global step 10 Train loss 0.92 on epoch=3
06/05/2022 04:23:30 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=6
06/05/2022 04:23:33 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=9
06/05/2022 04:23:35 - INFO - __main__ - Step 40 Global step 40 Train loss 0.46 on epoch=13
06/05/2022 04:23:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=16
06/05/2022 04:23:40 - INFO - __main__ - Global step 50 Train loss 0.64 Classification-F1 0.16666666666666666 on epoch=16
06/05/2022 04:23:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/05/2022 04:23:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=19
06/05/2022 04:23:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=23
06/05/2022 04:23:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=26
06/05/2022 04:23:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=29
06/05/2022 04:23:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=33
06/05/2022 04:23:54 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
06/05/2022 04:23:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=36
06/05/2022 04:24:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=39
06/05/2022 04:24:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=43
06/05/2022 04:24:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
06/05/2022 04:24:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=49
06/05/2022 04:24:09 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=49
06/05/2022 04:24:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=53
06/05/2022 04:24:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=56
06/05/2022 04:24:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=59
06/05/2022 04:24:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=63
06/05/2022 04:24:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=66
06/05/2022 04:24:23 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.243859649122807 on epoch=66
06/05/2022 04:24:23 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.243859649122807 on epoch=66, global_step=200
06/05/2022 04:24:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
06/05/2022 04:24:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
06/05/2022 04:24:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=76
06/05/2022 04:24:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
06/05/2022 04:24:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=83
06/05/2022 04:24:38 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=83
06/05/2022 04:24:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=86
06/05/2022 04:24:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=89
06/05/2022 04:24:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=93
06/05/2022 04:24:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=96
06/05/2022 04:24:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=99
06/05/2022 04:24:52 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.20315011250401802 on epoch=99
06/05/2022 04:24:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=103
06/05/2022 04:24:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=106
06/05/2022 04:25:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=109
06/05/2022 04:25:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=113
06/05/2022 04:25:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=116
06/05/2022 04:25:07 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.2722234932255523 on epoch=116
06/05/2022 04:25:07 - INFO - __main__ - Saving model with best Classification-F1: 0.243859649122807 -> 0.2722234932255523 on epoch=116, global_step=350
06/05/2022 04:25:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=119
06/05/2022 04:25:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=123
06/05/2022 04:25:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=126
06/05/2022 04:25:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=129
06/05/2022 04:25:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.32 on epoch=133
06/05/2022 04:25:21 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.16666666666666666 on epoch=133
06/05/2022 04:25:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=136
06/05/2022 04:25:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=139
06/05/2022 04:25:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=143
06/05/2022 04:25:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=146
06/05/2022 04:25:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=149
06/05/2022 04:25:36 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.1581920903954802 on epoch=149
06/05/2022 04:25:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=153
06/05/2022 04:25:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=156
06/05/2022 04:25:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=159
06/05/2022 04:25:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=163
06/05/2022 04:25:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=166
06/05/2022 04:25:51 - INFO - __main__ - Global step 500 Train loss 0.28 Classification-F1 0.24561403508771928 on epoch=166
06/05/2022 04:25:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=169
06/05/2022 04:25:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=173
06/05/2022 04:25:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=176
06/05/2022 04:26:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=179
06/05/2022 04:26:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=183
06/05/2022 04:26:06 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.28563899868247694 on epoch=183
06/05/2022 04:26:06 - INFO - __main__ - Saving model with best Classification-F1: 0.2722234932255523 -> 0.28563899868247694 on epoch=183, global_step=550
06/05/2022 04:26:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=186
06/05/2022 04:26:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=189
06/05/2022 04:26:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=193
06/05/2022 04:26:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=196
06/05/2022 04:26:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=199
06/05/2022 04:26:21 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.1931124106562703 on epoch=199
06/05/2022 04:26:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=203
06/05/2022 04:26:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=206
06/05/2022 04:26:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.14 on epoch=209
06/05/2022 04:26:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=213
06/05/2022 04:26:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=216
06/05/2022 04:26:36 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.21523809523809523 on epoch=216
06/05/2022 04:26:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=219
06/05/2022 04:26:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=223
06/05/2022 04:26:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=226
06/05/2022 04:26:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=229
06/05/2022 04:26:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=233
06/05/2022 04:26:51 - INFO - __main__ - Global step 700 Train loss 0.11 Classification-F1 0.19272819730485635 on epoch=233
06/05/2022 04:26:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=236
06/05/2022 04:26:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=239
06/05/2022 04:26:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=243
06/05/2022 04:27:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=246
06/05/2022 04:27:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=249
06/05/2022 04:27:06 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.09110297110297111 on epoch=249
06/05/2022 04:27:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=253
06/05/2022 04:27:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=256
06/05/2022 04:27:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=259
06/05/2022 04:27:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=263
06/05/2022 04:27:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=266
06/05/2022 04:27:21 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.13707729468599034 on epoch=266
06/05/2022 04:27:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=269
06/05/2022 04:27:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=273
06/05/2022 04:27:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=276
06/05/2022 04:27:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=279
06/05/2022 04:27:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=283
06/05/2022 04:27:37 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.25 on epoch=283
06/05/2022 04:27:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=286
06/05/2022 04:27:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=289
06/05/2022 04:27:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=293
06/05/2022 04:27:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
06/05/2022 04:27:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=299
06/05/2022 04:27:52 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.14794500185804532 on epoch=299
06/05/2022 04:27:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=303
06/05/2022 04:27:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=306
06/05/2022 04:28:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=309
06/05/2022 04:28:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=313
06/05/2022 04:28:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
06/05/2022 04:28:07 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.10255102040816326 on epoch=316
06/05/2022 04:28:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=319
06/05/2022 04:28:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=323
06/05/2022 04:28:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=326
06/05/2022 04:28:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
06/05/2022 04:28:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=333
06/05/2022 04:28:23 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.10814059251559252 on epoch=333
06/05/2022 04:28:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=336
06/05/2022 04:28:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=339
06/05/2022 04:28:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
06/05/2022 04:28:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=346
06/05/2022 04:28:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=349
06/05/2022 04:28:39 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.19404510218463708 on epoch=349
06/05/2022 04:28:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=353
06/05/2022 04:28:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
06/05/2022 04:28:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=359
06/05/2022 04:28:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
06/05/2022 04:28:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=366
06/05/2022 04:28:54 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.18502844950213368 on epoch=366
06/05/2022 04:28:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
06/05/2022 04:29:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/05/2022 04:29:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=376
06/05/2022 04:29:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
06/05/2022 04:29:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=383
06/05/2022 04:29:09 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.21051529116045245 on epoch=383
06/05/2022 04:29:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
06/05/2022 04:29:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=389
06/05/2022 04:29:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=393
06/05/2022 04:29:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
06/05/2022 04:29:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
06/05/2022 04:29:24 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.19095238095238098 on epoch=399
06/05/2022 04:29:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=403
06/05/2022 04:29:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
06/05/2022 04:29:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=409
06/05/2022 04:29:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
06/05/2022 04:29:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
06/05/2022 04:29:40 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.29484029484029484 on epoch=416
06/05/2022 04:29:40 - INFO - __main__ - Saving model with best Classification-F1: 0.28563899868247694 -> 0.29484029484029484 on epoch=416, global_step=1250
06/05/2022 04:29:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=419
06/05/2022 04:29:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=423
06/05/2022 04:29:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
06/05/2022 04:29:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
06/05/2022 04:29:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
06/05/2022 04:29:55 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.2161443494776828 on epoch=433
06/05/2022 04:29:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=436
06/05/2022 04:30:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/05/2022 04:30:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=443
06/05/2022 04:30:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
06/05/2022 04:30:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=449
06/05/2022 04:30:11 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.2911982730531118 on epoch=449
06/05/2022 04:30:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
06/05/2022 04:30:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
06/05/2022 04:30:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
06/05/2022 04:30:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/05/2022 04:30:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
06/05/2022 04:30:26 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.27735084838533114 on epoch=466
06/05/2022 04:30:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=469
06/05/2022 04:30:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=473
06/05/2022 04:30:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
06/05/2022 04:30:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
06/05/2022 04:30:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
06/05/2022 04:30:41 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.3111111111111111 on epoch=483
06/05/2022 04:30:41 - INFO - __main__ - Saving model with best Classification-F1: 0.29484029484029484 -> 0.3111111111111111 on epoch=483, global_step=1450
06/05/2022 04:30:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/05/2022 04:30:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
06/05/2022 04:30:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
06/05/2022 04:30:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
06/05/2022 04:30:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/05/2022 04:30:56 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.30923076923076925 on epoch=499
06/05/2022 04:30:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
06/05/2022 04:31:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
06/05/2022 04:31:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
06/05/2022 04:31:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
06/05/2022 04:31:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=516
06/05/2022 04:31:10 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.27070707070707073 on epoch=516
06/05/2022 04:31:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/05/2022 04:31:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
06/05/2022 04:31:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/05/2022 04:31:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/05/2022 04:31:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/05/2022 04:31:25 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.272454681750034 on epoch=533
06/05/2022 04:31:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
06/05/2022 04:31:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=539
06/05/2022 04:31:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/05/2022 04:31:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
06/05/2022 04:31:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/05/2022 04:31:40 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.28760619690154926 on epoch=549
06/05/2022 04:31:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=553
06/05/2022 04:31:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=556
06/05/2022 04:31:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/05/2022 04:31:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/05/2022 04:31:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
06/05/2022 04:31:56 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.25952380952380955 on epoch=566
06/05/2022 04:31:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/05/2022 04:32:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
06/05/2022 04:32:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/05/2022 04:32:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/05/2022 04:32:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/05/2022 04:32:10 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.24519924519924519 on epoch=583
06/05/2022 04:32:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/05/2022 04:32:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=589
06/05/2022 04:32:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=593
06/05/2022 04:32:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/05/2022 04:32:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
06/05/2022 04:32:25 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.2528260270195754 on epoch=599
06/05/2022 04:32:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/05/2022 04:32:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/05/2022 04:32:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/05/2022 04:32:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/05/2022 04:32:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 04:32:41 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.2675268817204301 on epoch=616
06/05/2022 04:32:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/05/2022 04:32:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/05/2022 04:32:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/05/2022 04:32:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/05/2022 04:32:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/05/2022 04:32:56 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.23119658119658118 on epoch=633
06/05/2022 04:32:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/05/2022 04:33:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/05/2022 04:33:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=643
06/05/2022 04:33:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/05/2022 04:33:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/05/2022 04:33:12 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.27732683982683987 on epoch=649
06/05/2022 04:33:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/05/2022 04:33:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/05/2022 04:33:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/05/2022 04:33:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 04:33:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/05/2022 04:33:27 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.2332383665716999 on epoch=666
06/05/2022 04:33:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/05/2022 04:33:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/05/2022 04:33:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/05/2022 04:33:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/05/2022 04:33:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
06/05/2022 04:33:42 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.201010101010101 on epoch=683
06/05/2022 04:33:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 04:33:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/05/2022 04:33:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/05/2022 04:33:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/05/2022 04:33:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/05/2022 04:33:57 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.2144973544973545 on epoch=699
06/05/2022 04:34:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/05/2022 04:34:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/05/2022 04:34:05 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/05/2022 04:34:08 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=713
06/05/2022 04:34:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/05/2022 04:34:12 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.285220016254499 on epoch=716
06/05/2022 04:34:15 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/05/2022 04:34:18 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
06/05/2022 04:34:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/05/2022 04:34:23 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/05/2022 04:34:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/05/2022 04:34:27 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.2626611573979995 on epoch=733
06/05/2022 04:34:30 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/05/2022 04:34:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/05/2022 04:34:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 04:34:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 04:34:42 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/05/2022 04:34:43 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.2816425120772947 on epoch=749
06/05/2022 04:34:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 04:34:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=756
06/05/2022 04:34:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/05/2022 04:34:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/05/2022 04:34:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/05/2022 04:34:58 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.2984126984126984 on epoch=766
06/05/2022 04:35:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 04:35:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/05/2022 04:35:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/05/2022 04:35:09 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 04:35:12 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/05/2022 04:35:13 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.24720942140296978 on epoch=783
06/05/2022 04:35:16 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
06/05/2022 04:35:18 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 04:35:21 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/05/2022 04:35:24 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 04:35:27 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 04:35:28 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.26461500432579405 on epoch=799
06/05/2022 04:35:31 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 04:35:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=806
06/05/2022 04:35:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 04:35:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 04:35:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/05/2022 04:35:44 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.2628968253968254 on epoch=816
06/05/2022 04:35:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/05/2022 04:35:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 04:35:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 04:35:55 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/05/2022 04:35:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 04:35:59 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.28760619690154926 on epoch=833
06/05/2022 04:36:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 04:36:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 04:36:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/05/2022 04:36:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/05/2022 04:36:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/05/2022 04:36:14 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.21998550274412343 on epoch=849
06/05/2022 04:36:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 04:36:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 04:36:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 04:36:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/05/2022 04:36:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/05/2022 04:36:29 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.30958421423537696 on epoch=866
06/05/2022 04:36:32 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 04:36:35 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=873
06/05/2022 04:36:37 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 04:36:40 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 04:36:43 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 04:36:44 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.2494281045751634 on epoch=883
06/05/2022 04:36:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
06/05/2022 04:36:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 04:36:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 04:36:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/05/2022 04:36:58 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 04:36:59 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.22056878306878305 on epoch=899
06/05/2022 04:37:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 04:37:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 04:37:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 04:37:10 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 04:37:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 04:37:14 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.20746363061353576 on epoch=916
06/05/2022 04:37:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 04:37:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 04:37:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 04:37:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 04:37:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 04:37:29 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.20746363061353576 on epoch=933
06/05/2022 04:37:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 04:37:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 04:37:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 04:37:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 04:37:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 04:37:44 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.26147783251231527 on epoch=949
06/05/2022 04:37:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=953
06/05/2022 04:37:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 04:37:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 04:37:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 04:37:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/05/2022 04:37:59 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.29556106975461816 on epoch=966
06/05/2022 04:38:02 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 04:38:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=973
06/05/2022 04:38:07 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 04:38:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 04:38:12 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 04:38:14 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.2332383665716999 on epoch=983
06/05/2022 04:38:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 04:38:19 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 04:38:22 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 04:38:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 04:38:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
06/05/2022 04:38:29 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.18127782955369162 on epoch=999
06/05/2022 04:38:29 - INFO - __main__ - save last model!
06/05/2022 04:38:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 04:38:29 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 04:38:29 - INFO - __main__ - Printing 3 examples
06/05/2022 04:38:29 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 04:38:29 - INFO - __main__ - ['contradiction']
06/05/2022 04:38:29 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 04:38:29 - INFO - __main__ - ['entailment']
06/05/2022 04:38:29 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 04:38:29 - INFO - __main__ - ['contradiction']
06/05/2022 04:38:29 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:38:29 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:38:29 - INFO - __main__ - Printing 3 examples
06/05/2022 04:38:29 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/05/2022 04:38:29 - INFO - __main__ - ['neutral']
06/05/2022 04:38:29 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/05/2022 04:38:29 - INFO - __main__ - ['neutral']
06/05/2022 04:38:29 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/05/2022 04:38:29 - INFO - __main__ - ['neutral']
06/05/2022 04:38:29 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:38:29 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:38:29 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 04:38:29 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:38:29 - INFO - __main__ - Printing 3 examples
06/05/2022 04:38:29 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/05/2022 04:38:29 - INFO - __main__ - ['neutral']
06/05/2022 04:38:29 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/05/2022 04:38:29 - INFO - __main__ - ['neutral']
06/05/2022 04:38:29 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/05/2022 04:38:29 - INFO - __main__ - ['neutral']
06/05/2022 04:38:29 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:38:29 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:38:29 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 04:38:30 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:38:31 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 04:38:49 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 04:38:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 04:38:50 - INFO - __main__ - Starting training!
06/05/2022 04:39:00 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_100_0.5_8_predictions.txt
06/05/2022 04:39:00 - INFO - __main__ - Classification-F1 on test data: 0.3285
06/05/2022 04:39:01 - INFO - __main__ - prefix=anli_16_100, lr=0.5, bsz=8, dev_performance=0.3111111111111111, test_performance=0.3284980965289102
06/05/2022 04:39:01 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.4, bsz=8 ...
06/05/2022 04:39:02 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:39:02 - INFO - __main__ - Printing 3 examples
06/05/2022 04:39:02 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/05/2022 04:39:02 - INFO - __main__ - ['neutral']
06/05/2022 04:39:02 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/05/2022 04:39:02 - INFO - __main__ - ['neutral']
06/05/2022 04:39:02 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/05/2022 04:39:02 - INFO - __main__ - ['neutral']
06/05/2022 04:39:02 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:39:02 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:39:02 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 04:39:02 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:39:02 - INFO - __main__ - Printing 3 examples
06/05/2022 04:39:02 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/05/2022 04:39:02 - INFO - __main__ - ['neutral']
06/05/2022 04:39:02 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/05/2022 04:39:02 - INFO - __main__ - ['neutral']
06/05/2022 04:39:02 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/05/2022 04:39:02 - INFO - __main__ - ['neutral']
06/05/2022 04:39:02 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:39:02 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:39:02 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 04:39:16 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 04:39:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 04:39:17 - INFO - __main__ - Starting training!
06/05/2022 04:39:21 - INFO - __main__ - Step 10 Global step 10 Train loss 0.94 on epoch=3
06/05/2022 04:39:23 - INFO - __main__ - Step 20 Global step 20 Train loss 0.66 on epoch=6
06/05/2022 04:39:26 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=9
06/05/2022 04:39:29 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=13
06/05/2022 04:39:31 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=16
06/05/2022 04:39:32 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.1693121693121693 on epoch=16
06/05/2022 04:39:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1693121693121693 on epoch=16, global_step=50
06/05/2022 04:39:35 - INFO - __main__ - Step 60 Global step 60 Train loss 0.52 on epoch=19
06/05/2022 04:39:38 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=23
06/05/2022 04:39:41 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=26
06/05/2022 04:39:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=29
06/05/2022 04:39:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=33
06/05/2022 04:39:47 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
06/05/2022 04:39:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=36
06/05/2022 04:39:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=39
06/05/2022 04:39:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=43
06/05/2022 04:39:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=46
06/05/2022 04:40:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=49
06/05/2022 04:40:02 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=49
06/05/2022 04:40:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
06/05/2022 04:40:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=56
06/05/2022 04:40:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=59
06/05/2022 04:40:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=63
06/05/2022 04:40:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=66
06/05/2022 04:40:17 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
06/05/2022 04:40:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=69
06/05/2022 04:40:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=73
06/05/2022 04:40:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
06/05/2022 04:40:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
06/05/2022 04:40:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=83
06/05/2022 04:40:32 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=83
06/05/2022 04:40:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=86
06/05/2022 04:40:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
06/05/2022 04:40:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
06/05/2022 04:40:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=96
06/05/2022 04:40:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=99
06/05/2022 04:40:47 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.25396825396825395 on epoch=99
06/05/2022 04:40:47 - INFO - __main__ - Saving model with best Classification-F1: 0.1693121693121693 -> 0.25396825396825395 on epoch=99, global_step=300
06/05/2022 04:40:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=103
06/05/2022 04:40:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.32 on epoch=106
06/05/2022 04:40:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=109
06/05/2022 04:40:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=113
06/05/2022 04:41:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=116
06/05/2022 04:41:03 - INFO - __main__ - Global step 350 Train loss 0.32 Classification-F1 0.24126984126984127 on epoch=116
06/05/2022 04:41:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=119
06/05/2022 04:41:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=123
06/05/2022 04:41:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=126
06/05/2022 04:41:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=129
06/05/2022 04:41:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=133
06/05/2022 04:41:17 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.19404572036150983 on epoch=133
06/05/2022 04:41:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=136
06/05/2022 04:41:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=139
06/05/2022 04:41:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=143
06/05/2022 04:41:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=146
06/05/2022 04:41:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=149
06/05/2022 04:41:32 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.2789473684210526 on epoch=149
06/05/2022 04:41:32 - INFO - __main__ - Saving model with best Classification-F1: 0.25396825396825395 -> 0.2789473684210526 on epoch=149, global_step=450
06/05/2022 04:41:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=153
06/05/2022 04:41:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=156
06/05/2022 04:41:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.10 on epoch=159
06/05/2022 04:41:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.19 on epoch=163
06/05/2022 04:41:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=166
06/05/2022 04:41:47 - INFO - __main__ - Global step 500 Train loss 0.16 Classification-F1 0.2981897329723417 on epoch=166
06/05/2022 04:41:47 - INFO - __main__ - Saving model with best Classification-F1: 0.2789473684210526 -> 0.2981897329723417 on epoch=166, global_step=500
06/05/2022 04:41:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.12 on epoch=169
06/05/2022 04:41:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.15 on epoch=173
06/05/2022 04:41:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.14 on epoch=176
06/05/2022 04:41:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=179
06/05/2022 04:42:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=183
06/05/2022 04:42:02 - INFO - __main__ - Global step 550 Train loss 0.14 Classification-F1 0.1822100313479624 on epoch=183
06/05/2022 04:42:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=186
06/05/2022 04:42:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=189
06/05/2022 04:42:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=193
06/05/2022 04:42:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=196
06/05/2022 04:42:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.13 on epoch=199
06/05/2022 04:42:17 - INFO - __main__ - Global step 600 Train loss 0.15 Classification-F1 0.2105263157894737 on epoch=199
06/05/2022 04:42:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.11 on epoch=203
06/05/2022 04:42:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=206
06/05/2022 04:42:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=209
06/05/2022 04:42:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=213
06/05/2022 04:42:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=216
06/05/2022 04:42:32 - INFO - __main__ - Global step 650 Train loss 0.10 Classification-F1 0.2099511072763877 on epoch=216
06/05/2022 04:42:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.10 on epoch=219
06/05/2022 04:42:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=223
06/05/2022 04:42:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=226
06/05/2022 04:42:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=229
06/05/2022 04:42:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=233
06/05/2022 04:42:47 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.33015873015873015 on epoch=233
06/05/2022 04:42:47 - INFO - __main__ - Saving model with best Classification-F1: 0.2981897329723417 -> 0.33015873015873015 on epoch=233, global_step=700
06/05/2022 04:42:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=236
06/05/2022 04:42:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=239
06/05/2022 04:42:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=243
06/05/2022 04:42:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=246
06/05/2022 04:43:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=249
06/05/2022 04:43:02 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.21561302681992334 on epoch=249
06/05/2022 04:43:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=253
06/05/2022 04:43:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=256
06/05/2022 04:43:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=259
06/05/2022 04:43:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=263
06/05/2022 04:43:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=266
06/05/2022 04:43:17 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.16591251885369535 on epoch=266
06/05/2022 04:43:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=269
06/05/2022 04:43:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=273
06/05/2022 04:43:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=276
06/05/2022 04:43:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=279
06/05/2022 04:43:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=283
06/05/2022 04:43:33 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.17357142857142857 on epoch=283
06/05/2022 04:43:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=286
06/05/2022 04:43:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=289
06/05/2022 04:43:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=293
06/05/2022 04:43:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=296
06/05/2022 04:43:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=299
06/05/2022 04:43:48 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.18421052631578946 on epoch=299
06/05/2022 04:43:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=303
06/05/2022 04:43:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=306
06/05/2022 04:43:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=309
06/05/2022 04:43:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=313
06/05/2022 04:44:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
06/05/2022 04:44:03 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.21623262958596792 on epoch=316
06/05/2022 04:44:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=319
06/05/2022 04:44:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=323
06/05/2022 04:44:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
06/05/2022 04:44:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=329
06/05/2022 04:44:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=333
06/05/2022 04:44:19 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.22610837438423645 on epoch=333
06/05/2022 04:44:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
06/05/2022 04:44:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=339
06/05/2022 04:44:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=343
06/05/2022 04:44:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
06/05/2022 04:44:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=349
06/05/2022 04:44:34 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.3289610818780346 on epoch=349
06/05/2022 04:44:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=353
06/05/2022 04:44:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
06/05/2022 04:44:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=359
06/05/2022 04:44:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
06/05/2022 04:44:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
06/05/2022 04:44:49 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.23698800014589486 on epoch=366
06/05/2022 04:44:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
06/05/2022 04:44:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/05/2022 04:44:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=376
06/05/2022 04:45:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=379
06/05/2022 04:45:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=383
06/05/2022 04:45:04 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.2624320124320125 on epoch=383
06/05/2022 04:45:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
06/05/2022 04:45:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
06/05/2022 04:45:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
06/05/2022 04:45:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=396
06/05/2022 04:45:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
06/05/2022 04:45:19 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.18884838565689627 on epoch=399
06/05/2022 04:45:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
06/05/2022 04:45:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
06/05/2022 04:45:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
06/05/2022 04:45:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
06/05/2022 04:45:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=416
06/05/2022 04:45:34 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.3090728715728716 on epoch=416
06/05/2022 04:45:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=419
06/05/2022 04:45:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
06/05/2022 04:45:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
06/05/2022 04:45:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
06/05/2022 04:45:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
06/05/2022 04:45:49 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.3019985502744123 on epoch=433
06/05/2022 04:45:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
06/05/2022 04:45:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/05/2022 04:45:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=443
06/05/2022 04:46:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
06/05/2022 04:46:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
06/05/2022 04:46:04 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.28371173030615754 on epoch=449
06/05/2022 04:46:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=453
06/05/2022 04:46:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=456
06/05/2022 04:46:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=459
06/05/2022 04:46:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/05/2022 04:46:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=466
06/05/2022 04:46:19 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.2180808080808081 on epoch=466
06/05/2022 04:46:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
06/05/2022 04:46:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
06/05/2022 04:46:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
06/05/2022 04:46:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
06/05/2022 04:46:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=483
06/05/2022 04:46:34 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.2245670537051163 on epoch=483
06/05/2022 04:46:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/05/2022 04:46:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
06/05/2022 04:46:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
06/05/2022 04:46:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
06/05/2022 04:46:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/05/2022 04:46:50 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.2844342037890425 on epoch=499
06/05/2022 04:46:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
06/05/2022 04:46:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
06/05/2022 04:46:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
06/05/2022 04:47:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=513
06/05/2022 04:47:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
06/05/2022 04:47:05 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.20057971014492756 on epoch=516
06/05/2022 04:47:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/05/2022 04:47:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
06/05/2022 04:47:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
06/05/2022 04:47:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/05/2022 04:47:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/05/2022 04:47:21 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.2430128205128205 on epoch=533
06/05/2022 04:47:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
06/05/2022 04:47:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
06/05/2022 04:47:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/05/2022 04:47:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
06/05/2022 04:47:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/05/2022 04:47:37 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.2764602547211243 on epoch=549
06/05/2022 04:47:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/05/2022 04:47:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
06/05/2022 04:47:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
06/05/2022 04:47:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/05/2022 04:47:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
06/05/2022 04:47:52 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.28715728715728717 on epoch=566
06/05/2022 04:47:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
06/05/2022 04:47:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/05/2022 04:48:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/05/2022 04:48:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=579
06/05/2022 04:48:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/05/2022 04:48:08 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.27619047619047615 on epoch=583
06/05/2022 04:48:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/05/2022 04:48:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/05/2022 04:48:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/05/2022 04:48:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=596
06/05/2022 04:48:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=599
06/05/2022 04:48:23 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.3081035923141186 on epoch=599
06/05/2022 04:48:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
06/05/2022 04:48:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/05/2022 04:48:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/05/2022 04:48:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/05/2022 04:48:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 04:48:38 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.2522317950087936 on epoch=616
06/05/2022 04:48:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/05/2022 04:48:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/05/2022 04:48:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/05/2022 04:48:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/05/2022 04:48:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=633
06/05/2022 04:48:53 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.24862401100791195 on epoch=633
06/05/2022 04:48:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/05/2022 04:48:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=639
06/05/2022 04:49:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/05/2022 04:49:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/05/2022 04:49:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/05/2022 04:49:09 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.3232774137806897 on epoch=649
06/05/2022 04:49:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/05/2022 04:49:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/05/2022 04:49:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/05/2022 04:49:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 04:49:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/05/2022 04:49:24 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.25958679768203574 on epoch=666
06/05/2022 04:49:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/05/2022 04:49:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/05/2022 04:49:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/05/2022 04:49:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/05/2022 04:49:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/05/2022 04:49:40 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.2611518915866742 on epoch=683
06/05/2022 04:49:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
06/05/2022 04:49:45 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/05/2022 04:49:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/05/2022 04:49:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/05/2022 04:49:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/05/2022 04:49:55 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.28134088134088137 on epoch=699
06/05/2022 04:49:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/05/2022 04:50:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/05/2022 04:50:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/05/2022 04:50:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/05/2022 04:50:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/05/2022 04:50:11 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.26461769115442274 on epoch=716
06/05/2022 04:50:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/05/2022 04:50:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/05/2022 04:50:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/05/2022 04:50:22 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/05/2022 04:50:25 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/05/2022 04:50:26 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.26554001554001555 on epoch=733
06/05/2022 04:50:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/05/2022 04:50:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/05/2022 04:50:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 04:50:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 04:50:40 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/05/2022 04:50:41 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.2226107226107226 on epoch=749
06/05/2022 04:50:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=753
06/05/2022 04:50:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/05/2022 04:50:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/05/2022 04:50:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/05/2022 04:50:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/05/2022 04:50:57 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.3152218152218152 on epoch=766
06/05/2022 04:50:59 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
06/05/2022 04:51:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/05/2022 04:51:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/05/2022 04:51:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 04:51:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/05/2022 04:51:12 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.22442818994543134 on epoch=783
06/05/2022 04:51:15 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=786
06/05/2022 04:51:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 04:51:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
06/05/2022 04:51:23 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 04:51:25 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 04:51:27 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.21132615510426608 on epoch=799
06/05/2022 04:51:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 04:51:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 04:51:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 04:51:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 04:51:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/05/2022 04:51:42 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.301010101010101 on epoch=816
06/05/2022 04:51:45 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/05/2022 04:51:47 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 04:51:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 04:51:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 04:51:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=833
06/05/2022 04:51:58 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.28424076250163205 on epoch=833
06/05/2022 04:52:00 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 04:52:03 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 04:52:06 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/05/2022 04:52:08 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/05/2022 04:52:11 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/05/2022 04:52:13 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.25630441110317276 on epoch=849
06/05/2022 04:52:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
06/05/2022 04:52:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=856
06/05/2022 04:52:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
06/05/2022 04:52:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/05/2022 04:52:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 04:52:28 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.3409130060292851 on epoch=866
06/05/2022 04:52:28 - INFO - __main__ - Saving model with best Classification-F1: 0.33015873015873015 -> 0.3409130060292851 on epoch=866, global_step=2600
06/05/2022 04:52:30 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 04:52:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/05/2022 04:52:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 04:52:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 04:52:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 04:52:43 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.3232774137806897 on epoch=883
06/05/2022 04:52:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 04:52:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 04:52:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 04:52:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
06/05/2022 04:52:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.18 on epoch=899
06/05/2022 04:52:59 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.19221056721056717 on epoch=899
06/05/2022 04:53:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 04:53:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 04:53:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 04:53:10 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 04:53:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 04:53:14 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.35044283413848637 on epoch=916
06/05/2022 04:53:14 - INFO - __main__ - Saving model with best Classification-F1: 0.3409130060292851 -> 0.35044283413848637 on epoch=916, global_step=2750
06/05/2022 04:53:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 04:53:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=923
06/05/2022 04:53:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 04:53:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 04:53:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 04:53:29 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.2183524349987018 on epoch=933
06/05/2022 04:53:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 04:53:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 04:53:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 04:53:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 04:53:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 04:53:44 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.28669601369528 on epoch=949
06/05/2022 04:53:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 04:53:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 04:53:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
06/05/2022 04:53:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 04:53:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=966
06/05/2022 04:53:59 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.3113342596101217 on epoch=966
06/05/2022 04:54:02 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 04:54:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=973
06/05/2022 04:54:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=976
06/05/2022 04:54:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=979
06/05/2022 04:54:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 04:54:14 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.2797906602254428 on epoch=983
06/05/2022 04:54:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 04:54:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 04:54:23 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 04:54:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
06/05/2022 04:54:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 04:54:30 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:54:30 - INFO - __main__ - Printing 3 examples
06/05/2022 04:54:30 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/05/2022 04:54:30 - INFO - __main__ - ['neutral']
06/05/2022 04:54:30 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/05/2022 04:54:30 - INFO - __main__ - ['neutral']
06/05/2022 04:54:30 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/05/2022 04:54:30 - INFO - __main__ - ['neutral']
06/05/2022 04:54:30 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:54:30 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:54:30 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.31816658287246524 on epoch=999
06/05/2022 04:54:30 - INFO - __main__ - save last model!
06/05/2022 04:54:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 04:54:30 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 04:54:30 - INFO - __main__ - Printing 3 examples
06/05/2022 04:54:30 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 04:54:30 - INFO - __main__ - ['contradiction']
06/05/2022 04:54:30 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 04:54:30 - INFO - __main__ - ['entailment']
06/05/2022 04:54:30 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 04:54:30 - INFO - __main__ - ['contradiction']
06/05/2022 04:54:30 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:54:30 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 04:54:30 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:54:30 - INFO - __main__ - Printing 3 examples
06/05/2022 04:54:30 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/05/2022 04:54:30 - INFO - __main__ - ['neutral']
06/05/2022 04:54:30 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/05/2022 04:54:30 - INFO - __main__ - ['neutral']
06/05/2022 04:54:30 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/05/2022 04:54:30 - INFO - __main__ - ['neutral']
06/05/2022 04:54:30 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:54:30 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:54:30 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 04:54:30 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:54:31 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 04:54:46 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 04:54:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 04:54:47 - INFO - __main__ - Starting training!
06/05/2022 04:55:03 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_100_0.4_8_predictions.txt
06/05/2022 04:55:03 - INFO - __main__ - Classification-F1 on test data: 0.3222
06/05/2022 04:55:04 - INFO - __main__ - prefix=anli_16_100, lr=0.4, bsz=8, dev_performance=0.35044283413848637, test_performance=0.3221972537793627
06/05/2022 04:55:04 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.3, bsz=8 ...
06/05/2022 04:55:05 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:55:05 - INFO - __main__ - Printing 3 examples
06/05/2022 04:55:05 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/05/2022 04:55:05 - INFO - __main__ - ['neutral']
06/05/2022 04:55:05 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/05/2022 04:55:05 - INFO - __main__ - ['neutral']
06/05/2022 04:55:05 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/05/2022 04:55:05 - INFO - __main__ - ['neutral']
06/05/2022 04:55:05 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:55:05 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:55:05 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 04:55:05 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 04:55:05 - INFO - __main__ - Printing 3 examples
06/05/2022 04:55:05 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/05/2022 04:55:05 - INFO - __main__ - ['neutral']
06/05/2022 04:55:05 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/05/2022 04:55:05 - INFO - __main__ - ['neutral']
06/05/2022 04:55:05 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/05/2022 04:55:05 - INFO - __main__ - ['neutral']
06/05/2022 04:55:05 - INFO - __main__ - Tokenizing Input ...
06/05/2022 04:55:05 - INFO - __main__ - Tokenizing Output ...
06/05/2022 04:55:05 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 04:55:24 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 04:55:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 04:55:25 - INFO - __main__ - Starting training!
06/05/2022 04:55:28 - INFO - __main__ - Step 10 Global step 10 Train loss 1.05 on epoch=3
06/05/2022 04:55:31 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=6
06/05/2022 04:55:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=9
06/05/2022 04:55:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=13
06/05/2022 04:55:40 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=16
06/05/2022 04:55:41 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.23301985370950887 on epoch=16
06/05/2022 04:55:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23301985370950887 on epoch=16, global_step=50
06/05/2022 04:55:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=19
06/05/2022 04:55:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=23
06/05/2022 04:55:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
06/05/2022 04:55:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=29
06/05/2022 04:55:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=33
06/05/2022 04:55:56 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
06/05/2022 04:55:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
06/05/2022 04:56:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=39
06/05/2022 04:56:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
06/05/2022 04:56:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=46
06/05/2022 04:56:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=49
06/05/2022 04:56:11 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=49
06/05/2022 04:56:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=53
06/05/2022 04:56:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=56
06/05/2022 04:56:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=59
06/05/2022 04:56:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=63
06/05/2022 04:56:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=66
06/05/2022 04:56:25 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=66
06/05/2022 04:56:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=69
06/05/2022 04:56:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=73
06/05/2022 04:56:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=76
06/05/2022 04:56:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
06/05/2022 04:56:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=83
06/05/2022 04:56:40 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=83
06/05/2022 04:56:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=86
06/05/2022 04:56:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=89
06/05/2022 04:56:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=93
06/05/2022 04:56:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=96
06/05/2022 04:56:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=99
06/05/2022 04:56:55 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.2099511072763877 on epoch=99
06/05/2022 04:56:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=103
06/05/2022 04:57:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=106
06/05/2022 04:57:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
06/05/2022 04:57:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=113
06/05/2022 04:57:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=116
06/05/2022 04:57:10 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.1693121693121693 on epoch=116
06/05/2022 04:57:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=119
06/05/2022 04:57:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=123
06/05/2022 04:57:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=126
06/05/2022 04:57:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=129
06/05/2022 04:57:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=133
06/05/2022 04:57:25 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.21031746031746035 on epoch=133
06/05/2022 04:57:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=136
06/05/2022 04:57:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=139
06/05/2022 04:57:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=143
06/05/2022 04:57:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=146
06/05/2022 04:57:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=149
06/05/2022 04:57:40 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.27261904761904765 on epoch=149
06/05/2022 04:57:40 - INFO - __main__ - Saving model with best Classification-F1: 0.23301985370950887 -> 0.27261904761904765 on epoch=149, global_step=450
06/05/2022 04:57:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=153
06/05/2022 04:57:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=156
06/05/2022 04:57:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=159
06/05/2022 04:57:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=163
06/05/2022 04:57:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=166
06/05/2022 04:57:55 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.19318181818181815 on epoch=166
06/05/2022 04:57:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=169
06/05/2022 04:58:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=173
06/05/2022 04:58:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.28 on epoch=176
06/05/2022 04:58:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=179
06/05/2022 04:58:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=183
06/05/2022 04:58:10 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.17993897787948132 on epoch=183
06/05/2022 04:58:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=186
06/05/2022 04:58:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=189
06/05/2022 04:58:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=193
06/05/2022 04:58:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=196
06/05/2022 04:58:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=199
06/05/2022 04:58:25 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.1484021164021164 on epoch=199
06/05/2022 04:58:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=203
06/05/2022 04:58:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=206
06/05/2022 04:58:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=209
06/05/2022 04:58:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=213
06/05/2022 04:58:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=216
06/05/2022 04:58:40 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.0775630703166935 on epoch=216
06/05/2022 04:58:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=219
06/05/2022 04:58:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=223
06/05/2022 04:58:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=226
06/05/2022 04:58:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=229
06/05/2022 04:58:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.16 on epoch=233
06/05/2022 04:58:55 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.16600958565244278 on epoch=233
06/05/2022 04:58:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.15 on epoch=236
06/05/2022 04:59:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=239
06/05/2022 04:59:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=243
06/05/2022 04:59:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.12 on epoch=246
06/05/2022 04:59:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=249
06/05/2022 04:59:11 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.07801472070982052 on epoch=249
06/05/2022 04:59:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=253
06/05/2022 04:59:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=256
06/05/2022 04:59:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=259
06/05/2022 04:59:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=263
06/05/2022 04:59:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=266
06/05/2022 04:59:26 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.1851851851851852 on epoch=266
06/05/2022 04:59:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=269
06/05/2022 04:59:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=273
06/05/2022 04:59:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=276
06/05/2022 04:59:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=279
06/05/2022 04:59:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=283
06/05/2022 04:59:41 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.05718842560947824 on epoch=283
06/05/2022 04:59:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=286
06/05/2022 04:59:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=289
06/05/2022 04:59:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=293
06/05/2022 04:59:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=296
06/05/2022 04:59:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=299
06/05/2022 04:59:56 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.1748400156719342 on epoch=299
06/05/2022 04:59:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=303
06/05/2022 05:00:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=306
06/05/2022 05:00:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=309
06/05/2022 05:00:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=313
06/05/2022 05:00:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=316
06/05/2022 05:00:11 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.17728144811478144 on epoch=316
06/05/2022 05:00:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=319
06/05/2022 05:00:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=323
06/05/2022 05:00:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
06/05/2022 05:00:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
06/05/2022 05:00:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=333
06/05/2022 05:00:26 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.15311653116531163 on epoch=333
06/05/2022 05:00:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=336
06/05/2022 05:00:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=339
06/05/2022 05:00:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
06/05/2022 05:00:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=346
06/05/2022 05:00:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=349
06/05/2022 05:00:41 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.15899122807017543 on epoch=349
06/05/2022 05:00:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=353
06/05/2022 05:00:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
06/05/2022 05:00:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=359
06/05/2022 05:00:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=363
06/05/2022 05:00:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=366
06/05/2022 05:00:57 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.14645604395604397 on epoch=366
06/05/2022 05:00:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
06/05/2022 05:01:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
06/05/2022 05:01:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=376
06/05/2022 05:01:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=379
06/05/2022 05:01:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
06/05/2022 05:01:12 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.1156688839615669 on epoch=383
06/05/2022 05:01:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=386
06/05/2022 05:01:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
06/05/2022 05:01:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=393
06/05/2022 05:01:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
06/05/2022 05:01:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
06/05/2022 05:01:27 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.3047138047138047 on epoch=399
06/05/2022 05:01:27 - INFO - __main__ - Saving model with best Classification-F1: 0.27261904761904765 -> 0.3047138047138047 on epoch=399, global_step=1200
06/05/2022 05:01:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=403
06/05/2022 05:01:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
06/05/2022 05:01:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=409
06/05/2022 05:01:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
06/05/2022 05:01:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=416
06/05/2022 05:01:42 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.21121196290975577 on epoch=416
06/05/2022 05:01:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
06/05/2022 05:01:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=423
06/05/2022 05:01:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=426
06/05/2022 05:01:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=429
06/05/2022 05:01:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=433
06/05/2022 05:01:57 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.2789351851851852 on epoch=433
06/05/2022 05:02:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
06/05/2022 05:02:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/05/2022 05:02:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
06/05/2022 05:02:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=446
06/05/2022 05:02:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=449
06/05/2022 05:02:12 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.24541043797534753 on epoch=449
06/05/2022 05:02:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=453
06/05/2022 05:02:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=456
06/05/2022 05:02:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
06/05/2022 05:02:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
06/05/2022 05:02:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
06/05/2022 05:02:27 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.2931736722059303 on epoch=466
06/05/2022 05:02:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=469
06/05/2022 05:02:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
06/05/2022 05:02:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
06/05/2022 05:02:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/05/2022 05:02:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=483
06/05/2022 05:02:43 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.2525925925925926 on epoch=483
06/05/2022 05:02:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
06/05/2022 05:02:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
06/05/2022 05:02:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
06/05/2022 05:02:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=496
06/05/2022 05:02:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/05/2022 05:02:58 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.20416666666666664 on epoch=499
06/05/2022 05:03:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
06/05/2022 05:03:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/05/2022 05:03:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
06/05/2022 05:03:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=513
06/05/2022 05:03:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
06/05/2022 05:03:13 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.25934305346070047 on epoch=516
06/05/2022 05:03:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/05/2022 05:03:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
06/05/2022 05:03:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
06/05/2022 05:03:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=529
06/05/2022 05:03:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/05/2022 05:03:28 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.2916666666666667 on epoch=533
06/05/2022 05:03:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=536
06/05/2022 05:03:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
06/05/2022 05:03:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/05/2022 05:03:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/05/2022 05:03:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
06/05/2022 05:03:43 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.30947712418300655 on epoch=549
06/05/2022 05:03:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3047138047138047 -> 0.30947712418300655 on epoch=549, global_step=1650
06/05/2022 05:03:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=553
06/05/2022 05:03:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
06/05/2022 05:03:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=559
06/05/2022 05:03:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=563
06/05/2022 05:03:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=566
06/05/2022 05:03:58 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.32043884954978585 on epoch=566
06/05/2022 05:03:58 - INFO - __main__ - Saving model with best Classification-F1: 0.30947712418300655 -> 0.32043884954978585 on epoch=566, global_step=1700
06/05/2022 05:04:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
06/05/2022 05:04:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/05/2022 05:04:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
06/05/2022 05:04:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
06/05/2022 05:04:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=583
06/05/2022 05:04:13 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.3238095238095238 on epoch=583
06/05/2022 05:04:13 - INFO - __main__ - Saving model with best Classification-F1: 0.32043884954978585 -> 0.3238095238095238 on epoch=583, global_step=1750
06/05/2022 05:04:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/05/2022 05:04:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
06/05/2022 05:04:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/05/2022 05:04:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/05/2022 05:04:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
06/05/2022 05:04:29 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.2652236652236652 on epoch=599
06/05/2022 05:04:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/05/2022 05:04:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=606
06/05/2022 05:04:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/05/2022 05:04:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/05/2022 05:04:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 05:04:44 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.2513555221888555 on epoch=616
06/05/2022 05:04:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=619
06/05/2022 05:04:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
06/05/2022 05:04:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/05/2022 05:04:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/05/2022 05:04:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/05/2022 05:04:59 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.31799031629252344 on epoch=633
06/05/2022 05:05:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/05/2022 05:05:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/05/2022 05:05:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/05/2022 05:05:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/05/2022 05:05:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/05/2022 05:05:14 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.26646556058320764 on epoch=649
06/05/2022 05:05:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/05/2022 05:05:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/05/2022 05:05:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/05/2022 05:05:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 05:05:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/05/2022 05:05:29 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.31799031629252344 on epoch=666
06/05/2022 05:05:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/05/2022 05:05:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
06/05/2022 05:05:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/05/2022 05:05:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
06/05/2022 05:05:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/05/2022 05:05:44 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.2744922913343966 on epoch=683
06/05/2022 05:05:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 05:05:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=689
06/05/2022 05:05:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=693
06/05/2022 05:05:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/05/2022 05:05:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/05/2022 05:05:59 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.20753588516746413 on epoch=699
06/05/2022 05:06:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/05/2022 05:06:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/05/2022 05:06:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
06/05/2022 05:06:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/05/2022 05:06:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/05/2022 05:06:15 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.32422805494334245 on epoch=716
06/05/2022 05:06:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3238095238095238 -> 0.32422805494334245 on epoch=716, global_step=2150
06/05/2022 05:06:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/05/2022 05:06:20 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/05/2022 05:06:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/05/2022 05:06:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/05/2022 05:06:28 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/05/2022 05:06:30 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.3133756924079505 on epoch=733
06/05/2022 05:06:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/05/2022 05:06:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/05/2022 05:06:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 05:06:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 05:06:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/05/2022 05:06:45 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.2904761904761905 on epoch=749
06/05/2022 05:06:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 05:06:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/05/2022 05:06:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/05/2022 05:06:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/05/2022 05:06:59 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/05/2022 05:07:00 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.3501831501831502 on epoch=766
06/05/2022 05:07:00 - INFO - __main__ - Saving model with best Classification-F1: 0.32422805494334245 -> 0.3501831501831502 on epoch=766, global_step=2300
06/05/2022 05:07:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 05:07:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/05/2022 05:07:09 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/05/2022 05:07:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 05:07:14 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/05/2022 05:07:16 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.3117144293614882 on epoch=783
06/05/2022 05:07:18 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 05:07:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/05/2022 05:07:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/05/2022 05:07:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 05:07:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
06/05/2022 05:07:31 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.25718725718725716 on epoch=799
06/05/2022 05:07:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/05/2022 05:07:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=806
06/05/2022 05:07:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 05:07:41 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/05/2022 05:07:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/05/2022 05:07:46 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.3391053391053391 on epoch=816
06/05/2022 05:07:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/05/2022 05:07:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 05:07:54 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/05/2022 05:07:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/05/2022 05:07:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 05:08:01 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.25759109311740885 on epoch=833
06/05/2022 05:08:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 05:08:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 05:08:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/05/2022 05:08:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/05/2022 05:08:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=849
06/05/2022 05:08:16 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.2659913107067579 on epoch=849
06/05/2022 05:08:19 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
06/05/2022 05:08:21 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 05:08:24 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 05:08:27 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/05/2022 05:08:29 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=866
06/05/2022 05:08:31 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.3369803511791341 on epoch=866
06/05/2022 05:08:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 05:08:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/05/2022 05:08:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 05:08:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 05:08:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 05:08:46 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.3520730706075534 on epoch=883
06/05/2022 05:08:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3501831501831502 -> 0.3520730706075534 on epoch=883, global_step=2650
06/05/2022 05:08:49 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 05:08:51 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 05:08:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 05:08:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=896
06/05/2022 05:09:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 05:09:01 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.3323784614107195 on epoch=899
06/05/2022 05:09:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 05:09:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 05:09:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 05:09:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 05:09:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 05:09:16 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.3521825396825397 on epoch=916
06/05/2022 05:09:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3520730706075534 -> 0.3521825396825397 on epoch=916, global_step=2750
06/05/2022 05:09:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 05:09:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 05:09:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 05:09:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
06/05/2022 05:09:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 05:09:31 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.3350842499897005 on epoch=933
06/05/2022 05:09:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
06/05/2022 05:09:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 05:09:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 05:09:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 05:09:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 05:09:46 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.3158068783068783 on epoch=949
06/05/2022 05:09:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
06/05/2022 05:09:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=956
06/05/2022 05:09:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 05:09:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 05:10:00 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
06/05/2022 05:10:02 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.3462401795735129 on epoch=966
06/05/2022 05:10:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 05:10:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 05:10:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 05:10:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 05:10:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 05:10:17 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.34342176277660147 on epoch=983
06/05/2022 05:10:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 05:10:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 05:10:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 05:10:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 05:10:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 05:10:32 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:10:32 - INFO - __main__ - Printing 3 examples
06/05/2022 05:10:32 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/05/2022 05:10:32 - INFO - __main__ - ['neutral']
06/05/2022 05:10:32 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/05/2022 05:10:32 - INFO - __main__ - ['neutral']
06/05/2022 05:10:32 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/05/2022 05:10:32 - INFO - __main__ - ['neutral']
06/05/2022 05:10:32 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:10:32 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:10:32 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.35125448028673834 on epoch=999
06/05/2022 05:10:32 - INFO - __main__ - save last model!
06/05/2022 05:10:32 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 05:10:32 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:10:32 - INFO - __main__ - Printing 3 examples
06/05/2022 05:10:32 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/05/2022 05:10:32 - INFO - __main__ - ['neutral']
06/05/2022 05:10:32 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/05/2022 05:10:32 - INFO - __main__ - ['neutral']
06/05/2022 05:10:32 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/05/2022 05:10:32 - INFO - __main__ - ['neutral']
06/05/2022 05:10:32 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:10:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 05:10:32 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 05:10:32 - INFO - __main__ - Printing 3 examples
06/05/2022 05:10:32 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 05:10:32 - INFO - __main__ - ['contradiction']
06/05/2022 05:10:32 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 05:10:32 - INFO - __main__ - ['entailment']
06/05/2022 05:10:32 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 05:10:32 - INFO - __main__ - ['contradiction']
06/05/2022 05:10:32 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:10:33 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:10:33 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 05:10:33 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:10:34 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 05:10:48 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 05:10:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 05:10:49 - INFO - __main__ - Starting training!
06/05/2022 05:11:05 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_100_0.3_8_predictions.txt
06/05/2022 05:11:05 - INFO - __main__ - Classification-F1 on test data: 0.3174
06/05/2022 05:11:05 - INFO - __main__ - prefix=anli_16_100, lr=0.3, bsz=8, dev_performance=0.3521825396825397, test_performance=0.31736483044515124
06/05/2022 05:11:05 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.2, bsz=8 ...
06/05/2022 05:11:06 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:11:06 - INFO - __main__ - Printing 3 examples
06/05/2022 05:11:06 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/05/2022 05:11:06 - INFO - __main__ - ['neutral']
06/05/2022 05:11:06 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/05/2022 05:11:06 - INFO - __main__ - ['neutral']
06/05/2022 05:11:06 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/05/2022 05:11:06 - INFO - __main__ - ['neutral']
06/05/2022 05:11:06 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:11:06 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:11:06 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 05:11:06 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:11:06 - INFO - __main__ - Printing 3 examples
06/05/2022 05:11:06 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
06/05/2022 05:11:06 - INFO - __main__ - ['neutral']
06/05/2022 05:11:06 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
06/05/2022 05:11:06 - INFO - __main__ - ['neutral']
06/05/2022 05:11:06 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
06/05/2022 05:11:06 - INFO - __main__ - ['neutral']
06/05/2022 05:11:06 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:11:06 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:11:06 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 05:11:26 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 05:11:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 05:11:27 - INFO - __main__ - Starting training!
06/05/2022 05:11:30 - INFO - __main__ - Step 10 Global step 10 Train loss 1.03 on epoch=3
06/05/2022 05:11:33 - INFO - __main__ - Step 20 Global step 20 Train loss 0.68 on epoch=6
06/05/2022 05:11:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=9
06/05/2022 05:11:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=13
06/05/2022 05:11:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=16
06/05/2022 05:11:42 - INFO - __main__ - Global step 50 Train loss 0.68 Classification-F1 0.27777777777777773 on epoch=16
06/05/2022 05:11:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.27777777777777773 on epoch=16, global_step=50
06/05/2022 05:11:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
06/05/2022 05:11:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=23
06/05/2022 05:11:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.59 on epoch=26
06/05/2022 05:11:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=29
06/05/2022 05:11:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=33
06/05/2022 05:11:57 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.2085278555866791 on epoch=33
06/05/2022 05:12:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
06/05/2022 05:12:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=39
06/05/2022 05:12:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=43
06/05/2022 05:12:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=46
06/05/2022 05:12:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=49
06/05/2022 05:12:12 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.3181818181818182 on epoch=49
06/05/2022 05:12:12 - INFO - __main__ - Saving model with best Classification-F1: 0.27777777777777773 -> 0.3181818181818182 on epoch=49, global_step=150
06/05/2022 05:12:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=53
06/05/2022 05:12:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=56
06/05/2022 05:12:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=59
06/05/2022 05:12:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
06/05/2022 05:12:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=66
06/05/2022 05:12:26 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.1983273596176822 on epoch=66
06/05/2022 05:12:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=69
06/05/2022 05:12:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=73
06/05/2022 05:12:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=76
06/05/2022 05:12:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=79
06/05/2022 05:12:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=83
06/05/2022 05:12:41 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.2085278555866791 on epoch=83
06/05/2022 05:12:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=86
06/05/2022 05:12:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=89
06/05/2022 05:12:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
06/05/2022 05:12:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=96
06/05/2022 05:12:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
06/05/2022 05:12:56 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.30104463437796775 on epoch=99
06/05/2022 05:12:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=103
06/05/2022 05:13:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=106
06/05/2022 05:13:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=109
06/05/2022 05:13:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=113
06/05/2022 05:13:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.53 on epoch=116
06/05/2022 05:13:11 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.2668334375651449 on epoch=116
06/05/2022 05:13:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=119
06/05/2022 05:13:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=123
06/05/2022 05:13:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=126
06/05/2022 05:13:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=129
06/05/2022 05:13:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=133
06/05/2022 05:13:25 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.1693121693121693 on epoch=133
06/05/2022 05:13:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=136
06/05/2022 05:13:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=139
06/05/2022 05:13:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=143
06/05/2022 05:13:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=146
06/05/2022 05:13:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=149
06/05/2022 05:13:39 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.23298358891579232 on epoch=149
06/05/2022 05:13:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=153
06/05/2022 05:13:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=156
06/05/2022 05:13:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=159
06/05/2022 05:13:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=163
06/05/2022 05:13:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=166
06/05/2022 05:13:54 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=166
06/05/2022 05:13:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=169
06/05/2022 05:13:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=173
06/05/2022 05:14:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=176
06/05/2022 05:14:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=179
06/05/2022 05:14:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=183
06/05/2022 05:14:09 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=183
06/05/2022 05:14:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=186
06/05/2022 05:14:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=189
06/05/2022 05:14:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=193
06/05/2022 05:14:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.31 on epoch=196
06/05/2022 05:14:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=199
06/05/2022 05:14:24 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.18253968253968256 on epoch=199
06/05/2022 05:14:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=203
06/05/2022 05:14:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=206
06/05/2022 05:14:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=209
06/05/2022 05:14:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=213
06/05/2022 05:14:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=216
06/05/2022 05:14:39 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.16949152542372883 on epoch=216
06/05/2022 05:14:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=219
06/05/2022 05:14:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=223
06/05/2022 05:14:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=226
06/05/2022 05:14:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=229
06/05/2022 05:14:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=233
06/05/2022 05:14:54 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.21253699219800914 on epoch=233
06/05/2022 05:14:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=236
06/05/2022 05:14:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=239
06/05/2022 05:15:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=243
06/05/2022 05:15:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=246
06/05/2022 05:15:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=249
06/05/2022 05:15:09 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.24136321195144728 on epoch=249
06/05/2022 05:15:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=253
06/05/2022 05:15:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=256
06/05/2022 05:15:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=259
06/05/2022 05:15:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=263
06/05/2022 05:15:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=266
06/05/2022 05:15:24 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.2510970374060106 on epoch=266
06/05/2022 05:15:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=269
06/05/2022 05:15:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=273
06/05/2022 05:15:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=276
06/05/2022 05:15:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=279
06/05/2022 05:15:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=283
06/05/2022 05:15:39 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.19017094017094016 on epoch=283
06/05/2022 05:15:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=286
06/05/2022 05:15:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=289
06/05/2022 05:15:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=293
06/05/2022 05:15:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=296
06/05/2022 05:15:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=299
06/05/2022 05:15:53 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.20674329501915711 on epoch=299
06/05/2022 05:15:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=303
06/05/2022 05:15:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=306
06/05/2022 05:16:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=309
06/05/2022 05:16:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=313
06/05/2022 05:16:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=316
06/05/2022 05:16:08 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.18526785714285715 on epoch=316
06/05/2022 05:16:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=319
06/05/2022 05:16:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=323
06/05/2022 05:16:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.08 on epoch=326
06/05/2022 05:16:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=329
06/05/2022 05:16:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=333
06/05/2022 05:16:22 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.1277815751499962 on epoch=333
06/05/2022 05:16:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=336
06/05/2022 05:16:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=339
06/05/2022 05:16:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=343
06/05/2022 05:16:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=346
06/05/2022 05:16:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=349
06/05/2022 05:16:37 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.17522349936143042 on epoch=349
06/05/2022 05:16:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=353
06/05/2022 05:16:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=356
06/05/2022 05:16:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=359
06/05/2022 05:16:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=363
06/05/2022 05:16:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=366
06/05/2022 05:16:52 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.11326442721791559 on epoch=366
06/05/2022 05:16:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=369
06/05/2022 05:16:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=373
06/05/2022 05:17:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=376
06/05/2022 05:17:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=379
06/05/2022 05:17:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=383
06/05/2022 05:17:07 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.16328793158061453 on epoch=383
06/05/2022 05:17:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=386
06/05/2022 05:17:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=389
06/05/2022 05:17:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=393
06/05/2022 05:17:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=396
06/05/2022 05:17:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=399
06/05/2022 05:17:22 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.1498612395929695 on epoch=399
06/05/2022 05:17:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=403
06/05/2022 05:17:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
06/05/2022 05:17:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=409
06/05/2022 05:17:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=413
06/05/2022 05:17:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=416
06/05/2022 05:17:37 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.15778677462887986 on epoch=416
06/05/2022 05:17:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
06/05/2022 05:17:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=423
06/05/2022 05:17:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=426
06/05/2022 05:17:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=429
06/05/2022 05:17:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=433
06/05/2022 05:17:52 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.13245261071348027 on epoch=433
06/05/2022 05:17:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=436
06/05/2022 05:17:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=439
06/05/2022 05:18:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=443
06/05/2022 05:18:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=446
06/05/2022 05:18:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
06/05/2022 05:18:07 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.1262062062062062 on epoch=449
06/05/2022 05:18:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=453
06/05/2022 05:18:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=456
06/05/2022 05:18:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
06/05/2022 05:18:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=463
06/05/2022 05:18:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
06/05/2022 05:18:22 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.1555299539170507 on epoch=466
06/05/2022 05:18:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
06/05/2022 05:18:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
06/05/2022 05:18:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=476
06/05/2022 05:18:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
06/05/2022 05:18:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
06/05/2022 05:18:37 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.14615873015873015 on epoch=483
06/05/2022 05:18:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
06/05/2022 05:18:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=489
06/05/2022 05:18:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
06/05/2022 05:18:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=496
06/05/2022 05:18:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
06/05/2022 05:18:52 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.15326797385620913 on epoch=499
06/05/2022 05:18:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
06/05/2022 05:18:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/05/2022 05:19:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=509
06/05/2022 05:19:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=513
06/05/2022 05:19:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
06/05/2022 05:19:08 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.1420890937019969 on epoch=516
06/05/2022 05:19:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
06/05/2022 05:19:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
06/05/2022 05:19:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
06/05/2022 05:19:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=529
06/05/2022 05:19:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
06/05/2022 05:19:23 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.14895104895104896 on epoch=533
06/05/2022 05:19:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
06/05/2022 05:19:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
06/05/2022 05:19:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/05/2022 05:19:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
06/05/2022 05:19:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
06/05/2022 05:19:38 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.18020671834625324 on epoch=549
06/05/2022 05:19:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
06/05/2022 05:19:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
06/05/2022 05:19:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
06/05/2022 05:19:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=563
06/05/2022 05:19:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
06/05/2022 05:19:53 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.1384126984126984 on epoch=566
06/05/2022 05:19:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=569
06/05/2022 05:19:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
06/05/2022 05:20:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=576
06/05/2022 05:20:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/05/2022 05:20:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/05/2022 05:20:08 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.2245911094631896 on epoch=583
06/05/2022 05:20:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/05/2022 05:20:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
06/05/2022 05:20:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/05/2022 05:20:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/05/2022 05:20:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=599
06/05/2022 05:20:23 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.15670498084291187 on epoch=599
06/05/2022 05:20:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
06/05/2022 05:20:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=606
06/05/2022 05:20:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/05/2022 05:20:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
06/05/2022 05:20:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
06/05/2022 05:20:38 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.2896900695762176 on epoch=616
06/05/2022 05:20:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
06/05/2022 05:20:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=623
06/05/2022 05:20:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/05/2022 05:20:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
06/05/2022 05:20:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/05/2022 05:20:53 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.20073426573426578 on epoch=633
06/05/2022 05:20:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=636
06/05/2022 05:20:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
06/05/2022 05:21:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
06/05/2022 05:21:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/05/2022 05:21:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/05/2022 05:21:08 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.1957461440220061 on epoch=649
06/05/2022 05:21:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/05/2022 05:21:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/05/2022 05:21:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/05/2022 05:21:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/05/2022 05:21:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/05/2022 05:21:23 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.15328185328185326 on epoch=666
06/05/2022 05:21:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/05/2022 05:21:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/05/2022 05:21:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/05/2022 05:21:34 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
06/05/2022 05:21:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/05/2022 05:21:38 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.22720588235294117 on epoch=683
06/05/2022 05:21:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
06/05/2022 05:21:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=689
06/05/2022 05:21:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
06/05/2022 05:21:49 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/05/2022 05:21:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
06/05/2022 05:21:53 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.15311653116531165 on epoch=699
06/05/2022 05:21:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/05/2022 05:21:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/05/2022 05:22:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
06/05/2022 05:22:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
06/05/2022 05:22:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=716
06/05/2022 05:22:08 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.3043668668668668 on epoch=716
06/05/2022 05:22:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/05/2022 05:22:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
06/05/2022 05:22:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/05/2022 05:22:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/05/2022 05:22:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/05/2022 05:22:24 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.24928297755883966 on epoch=733
06/05/2022 05:22:26 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/05/2022 05:22:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
06/05/2022 05:22:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 05:22:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/05/2022 05:22:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=749
06/05/2022 05:22:39 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.2485768500948767 on epoch=749
06/05/2022 05:22:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
06/05/2022 05:22:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/05/2022 05:22:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/05/2022 05:22:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/05/2022 05:22:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/05/2022 05:22:54 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.33898448043184887 on epoch=766
06/05/2022 05:22:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3181818181818182 -> 0.33898448043184887 on epoch=766, global_step=2300
06/05/2022 05:22:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
06/05/2022 05:22:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/05/2022 05:23:02 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=776
06/05/2022 05:23:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/05/2022 05:23:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/05/2022 05:23:09 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.22720588235294117 on epoch=783
06/05/2022 05:23:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/05/2022 05:23:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 05:23:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/05/2022 05:23:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
06/05/2022 05:23:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 05:23:24 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.20222526674139576 on epoch=799
06/05/2022 05:23:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
06/05/2022 05:23:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 05:23:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
06/05/2022 05:23:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 05:23:38 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/05/2022 05:23:39 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.1974008296603578 on epoch=816
06/05/2022 05:23:42 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/05/2022 05:23:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
06/05/2022 05:23:47 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/05/2022 05:23:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 05:23:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 05:23:54 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.2941935483870968 on epoch=833
06/05/2022 05:23:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 05:24:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 05:24:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
06/05/2022 05:24:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
06/05/2022 05:24:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=849
06/05/2022 05:24:09 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.20808080808080806 on epoch=849
06/05/2022 05:24:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
06/05/2022 05:24:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 05:24:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 05:24:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=863
06/05/2022 05:24:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 05:24:24 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.27632520735969013 on epoch=866
06/05/2022 05:24:27 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 05:24:29 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/05/2022 05:24:32 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 05:24:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/05/2022 05:24:37 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 05:24:39 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.2468137254901961 on epoch=883
06/05/2022 05:24:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=886
06/05/2022 05:24:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/05/2022 05:24:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 05:24:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/05/2022 05:24:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 05:24:53 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.24374029201615408 on epoch=899
06/05/2022 05:24:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 05:24:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/05/2022 05:25:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 05:25:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
06/05/2022 05:25:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 05:25:08 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.15998624619314275 on epoch=916
06/05/2022 05:25:11 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 05:25:13 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 05:25:16 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 05:25:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 05:25:21 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 05:25:23 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.15998624619314275 on epoch=933
06/05/2022 05:25:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 05:25:28 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 05:25:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 05:25:33 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 05:25:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 05:25:37 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.26658903193806477 on epoch=949
06/05/2022 05:25:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 05:25:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 05:25:45 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 05:25:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 05:25:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
06/05/2022 05:25:52 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.26750700280112044 on epoch=966
06/05/2022 05:25:55 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/05/2022 05:25:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 05:26:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 05:26:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=979
06/05/2022 05:26:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
06/05/2022 05:26:06 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.31722874826323105 on epoch=983
06/05/2022 05:26:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=986
06/05/2022 05:26:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 05:26:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=993
06/05/2022 05:26:17 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 05:26:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 05:26:21 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.22626262626262625 on epoch=999
06/05/2022 05:26:21 - INFO - __main__ - save last model!
06/05/2022 05:26:21 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:26:21 - INFO - __main__ - Printing 3 examples
06/05/2022 05:26:21 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/05/2022 05:26:21 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:21 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/05/2022 05:26:21 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:21 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/05/2022 05:26:21 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:21 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:26:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 05:26:21 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:26:21 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 05:26:21 - INFO - __main__ - Printing 3 examples
06/05/2022 05:26:21 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 05:26:21 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:21 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 05:26:21 - INFO - __main__ - ['entailment']
06/05/2022 05:26:21 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 05:26:21 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:21 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:26:21 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 05:26:21 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:26:21 - INFO - __main__ - Printing 3 examples
06/05/2022 05:26:21 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/05/2022 05:26:21 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:21 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/05/2022 05:26:21 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:21 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/05/2022 05:26:21 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:21 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:26:21 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:26:21 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 05:26:22 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:26:23 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 05:26:40 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 05:26:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 05:26:41 - INFO - __main__ - Starting training!
06/05/2022 05:26:53 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_100_0.2_8_predictions.txt
06/05/2022 05:26:53 - INFO - __main__ - Classification-F1 on test data: 0.1822
06/05/2022 05:26:53 - INFO - __main__ - prefix=anli_16_100, lr=0.2, bsz=8, dev_performance=0.33898448043184887, test_performance=0.18224332543478364
06/05/2022 05:26:53 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.5, bsz=8 ...
06/05/2022 05:26:54 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:26:54 - INFO - __main__ - Printing 3 examples
06/05/2022 05:26:54 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/05/2022 05:26:54 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:54 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/05/2022 05:26:54 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:54 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/05/2022 05:26:54 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:54 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:26:54 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:26:54 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 05:26:54 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:26:54 - INFO - __main__ - Printing 3 examples
06/05/2022 05:26:54 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/05/2022 05:26:54 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:54 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/05/2022 05:26:54 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:54 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/05/2022 05:26:54 - INFO - __main__ - ['contradiction']
06/05/2022 05:26:54 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:26:54 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:26:54 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 05:27:14 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 05:27:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 05:27:15 - INFO - __main__ - Starting training!
06/05/2022 05:27:19 - INFO - __main__ - Step 10 Global step 10 Train loss 0.92 on epoch=3
06/05/2022 05:27:21 - INFO - __main__ - Step 20 Global step 20 Train loss 0.60 on epoch=6
06/05/2022 05:27:24 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=9
06/05/2022 05:27:27 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=13
06/05/2022 05:27:30 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=16
06/05/2022 05:27:31 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.16666666666666666 on epoch=16
06/05/2022 05:27:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/05/2022 05:27:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
06/05/2022 05:27:37 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=23
06/05/2022 05:27:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=26
06/05/2022 05:27:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=29
06/05/2022 05:27:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=33
06/05/2022 05:27:46 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.29647565240785584 on epoch=33
06/05/2022 05:27:46 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.29647565240785584 on epoch=33, global_step=100
06/05/2022 05:27:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=36
06/05/2022 05:27:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=39
06/05/2022 05:27:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
06/05/2022 05:27:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
06/05/2022 05:28:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
06/05/2022 05:28:01 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.1693121693121693 on epoch=49
06/05/2022 05:28:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=53
06/05/2022 05:28:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
06/05/2022 05:28:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=59
06/05/2022 05:28:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=63
06/05/2022 05:28:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=66
06/05/2022 05:28:17 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=66
06/05/2022 05:28:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=69
06/05/2022 05:28:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
06/05/2022 05:28:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=76
06/05/2022 05:28:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=79
06/05/2022 05:28:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=83
06/05/2022 05:28:32 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=83
06/05/2022 05:28:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=86
06/05/2022 05:28:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
06/05/2022 05:28:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
06/05/2022 05:28:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=96
06/05/2022 05:28:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=99
06/05/2022 05:28:46 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=99
06/05/2022 05:28:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=103
06/05/2022 05:28:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=106
06/05/2022 05:28:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
06/05/2022 05:28:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=113
06/05/2022 05:29:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=116
06/05/2022 05:29:01 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.1693121693121693 on epoch=116
06/05/2022 05:29:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=119
06/05/2022 05:29:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=123
06/05/2022 05:29:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=126
06/05/2022 05:29:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=129
06/05/2022 05:29:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=133
06/05/2022 05:29:15 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.30952380952380953 on epoch=133
06/05/2022 05:29:15 - INFO - __main__ - Saving model with best Classification-F1: 0.29647565240785584 -> 0.30952380952380953 on epoch=133, global_step=400
06/05/2022 05:29:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=136
06/05/2022 05:29:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=139
06/05/2022 05:29:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=143
06/05/2022 05:29:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.34 on epoch=146
06/05/2022 05:29:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=149
06/05/2022 05:29:30 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.31213603540647566 on epoch=149
06/05/2022 05:29:30 - INFO - __main__ - Saving model with best Classification-F1: 0.30952380952380953 -> 0.31213603540647566 on epoch=149, global_step=450
06/05/2022 05:29:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=153
06/05/2022 05:29:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=156
06/05/2022 05:29:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=159
06/05/2022 05:29:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=163
06/05/2022 05:29:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=166
06/05/2022 05:29:44 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.3156139253700229 on epoch=166
06/05/2022 05:29:44 - INFO - __main__ - Saving model with best Classification-F1: 0.31213603540647566 -> 0.3156139253700229 on epoch=166, global_step=500
06/05/2022 05:29:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.28 on epoch=169
06/05/2022 05:29:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=173
06/05/2022 05:29:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=176
06/05/2022 05:29:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=179
06/05/2022 05:29:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=183
06/05/2022 05:29:59 - INFO - __main__ - Global step 550 Train loss 0.28 Classification-F1 0.21001779811848462 on epoch=183
06/05/2022 05:30:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=186
06/05/2022 05:30:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=189
06/05/2022 05:30:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=193
06/05/2022 05:30:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=196
06/05/2022 05:30:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=199
06/05/2022 05:30:13 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.2990810359231412 on epoch=199
06/05/2022 05:30:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=203
06/05/2022 05:30:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=206
06/05/2022 05:30:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=209
06/05/2022 05:30:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=213
06/05/2022 05:30:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=216
06/05/2022 05:30:28 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3172009218520846 on epoch=216
06/05/2022 05:30:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3156139253700229 -> 0.3172009218520846 on epoch=216, global_step=650
06/05/2022 05:30:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=219
06/05/2022 05:30:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=223
06/05/2022 05:30:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=226
06/05/2022 05:30:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=229
06/05/2022 05:30:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=233
06/05/2022 05:30:44 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.33842746400885937 on epoch=233
06/05/2022 05:30:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3172009218520846 -> 0.33842746400885937 on epoch=233, global_step=700
06/05/2022 05:30:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=236
06/05/2022 05:30:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=239
06/05/2022 05:30:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=243
06/05/2022 05:30:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=246
06/05/2022 05:30:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=249
06/05/2022 05:30:59 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.2875457875457876 on epoch=249
06/05/2022 05:31:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=253
06/05/2022 05:31:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=256
06/05/2022 05:31:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=259
06/05/2022 05:31:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=263
06/05/2022 05:31:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=266
06/05/2022 05:31:14 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.2099511072763877 on epoch=266
06/05/2022 05:31:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=269
06/05/2022 05:31:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=273
06/05/2022 05:31:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=276
06/05/2022 05:31:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=279
06/05/2022 05:31:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=283
06/05/2022 05:31:29 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.2391156462585034 on epoch=283
06/05/2022 05:31:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=286
06/05/2022 05:31:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=289
06/05/2022 05:31:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=293
06/05/2022 05:31:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
06/05/2022 05:31:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
06/05/2022 05:31:44 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.21764705882352942 on epoch=299
06/05/2022 05:31:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=303
06/05/2022 05:31:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=306
06/05/2022 05:31:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=309
06/05/2022 05:31:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=313
06/05/2022 05:31:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
06/05/2022 05:32:00 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.21919504643962848 on epoch=316
06/05/2022 05:32:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=319
06/05/2022 05:32:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=323
06/05/2022 05:32:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
06/05/2022 05:32:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=329
06/05/2022 05:32:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=333
06/05/2022 05:32:15 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.1740792540792541 on epoch=333
06/05/2022 05:32:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=336
06/05/2022 05:32:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
06/05/2022 05:32:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=343
06/05/2022 05:32:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
06/05/2022 05:32:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=349
06/05/2022 05:32:31 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.23785425101214572 on epoch=349
06/05/2022 05:32:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=353
06/05/2022 05:32:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=356
06/05/2022 05:32:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=359
06/05/2022 05:32:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
06/05/2022 05:32:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=366
06/05/2022 05:32:46 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.16955128205128206 on epoch=366
06/05/2022 05:32:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=369
06/05/2022 05:32:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/05/2022 05:32:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
06/05/2022 05:32:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=379
06/05/2022 05:33:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
06/05/2022 05:33:01 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.20816087138667783 on epoch=383
06/05/2022 05:33:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=386
06/05/2022 05:33:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
06/05/2022 05:33:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=393
06/05/2022 05:33:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
06/05/2022 05:33:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
06/05/2022 05:33:16 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.2380785413744741 on epoch=399
06/05/2022 05:33:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
06/05/2022 05:33:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=406
06/05/2022 05:33:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=409
06/05/2022 05:33:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
06/05/2022 05:33:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
06/05/2022 05:33:31 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.251604621309371 on epoch=416
06/05/2022 05:33:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=419
06/05/2022 05:33:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
06/05/2022 05:33:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
06/05/2022 05:33:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
06/05/2022 05:33:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
06/05/2022 05:33:46 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.22913992297817715 on epoch=433
06/05/2022 05:33:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=436
06/05/2022 05:33:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
06/05/2022 05:33:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
06/05/2022 05:33:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
06/05/2022 05:34:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
06/05/2022 05:34:01 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.16541353383458646 on epoch=449
06/05/2022 05:34:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
06/05/2022 05:34:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
06/05/2022 05:34:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
06/05/2022 05:34:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=463
06/05/2022 05:34:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
06/05/2022 05:34:18 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.15873015873015872 on epoch=466
06/05/2022 05:34:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=469
06/05/2022 05:34:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
06/05/2022 05:34:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
06/05/2022 05:34:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
06/05/2022 05:34:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
06/05/2022 05:34:33 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.17767722473604824 on epoch=483
06/05/2022 05:34:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
06/05/2022 05:34:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=489
06/05/2022 05:34:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/05/2022 05:34:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
06/05/2022 05:34:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/05/2022 05:34:48 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.170303605313093 on epoch=499
06/05/2022 05:34:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
06/05/2022 05:34:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
06/05/2022 05:34:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
06/05/2022 05:34:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=513
06/05/2022 05:35:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
06/05/2022 05:35:03 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.13777089783281735 on epoch=516
06/05/2022 05:35:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
06/05/2022 05:35:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=523
06/05/2022 05:35:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=526
06/05/2022 05:35:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=529
06/05/2022 05:35:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/05/2022 05:35:19 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.22604422604422603 on epoch=533
06/05/2022 05:35:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=536
06/05/2022 05:35:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
06/05/2022 05:35:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
06/05/2022 05:35:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
06/05/2022 05:35:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/05/2022 05:35:34 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.1977006528417819 on epoch=549
06/05/2022 05:35:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
06/05/2022 05:35:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
06/05/2022 05:35:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/05/2022 05:35:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/05/2022 05:35:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
06/05/2022 05:35:50 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.20921658986175112 on epoch=566
06/05/2022 05:35:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
06/05/2022 05:35:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/05/2022 05:35:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/05/2022 05:36:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/05/2022 05:36:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
06/05/2022 05:36:05 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.20129870129870128 on epoch=583
06/05/2022 05:36:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/05/2022 05:36:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
06/05/2022 05:36:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=593
06/05/2022 05:36:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/05/2022 05:36:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/05/2022 05:36:20 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.13409234661606578 on epoch=599
06/05/2022 05:36:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/05/2022 05:36:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/05/2022 05:36:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
06/05/2022 05:36:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=613
06/05/2022 05:36:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 05:36:35 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.12746675246675246 on epoch=616
06/05/2022 05:36:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/05/2022 05:36:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
06/05/2022 05:36:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/05/2022 05:36:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
06/05/2022 05:36:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/05/2022 05:36:51 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.1452593917710197 on epoch=633
06/05/2022 05:36:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/05/2022 05:36:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/05/2022 05:36:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/05/2022 05:37:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/05/2022 05:37:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/05/2022 05:37:06 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.18611111111111112 on epoch=649
06/05/2022 05:37:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/05/2022 05:37:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/05/2022 05:37:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/05/2022 05:37:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/05/2022 05:37:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
06/05/2022 05:37:21 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.187012987012987 on epoch=666
06/05/2022 05:37:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/05/2022 05:37:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/05/2022 05:37:29 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/05/2022 05:37:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/05/2022 05:37:34 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/05/2022 05:37:36 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.17812499999999998 on epoch=683
06/05/2022 05:37:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 05:37:41 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/05/2022 05:37:44 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/05/2022 05:37:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/05/2022 05:37:49 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/05/2022 05:37:51 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.14267241379310344 on epoch=699
06/05/2022 05:37:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/05/2022 05:37:56 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/05/2022 05:37:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/05/2022 05:38:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/05/2022 05:38:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=716
06/05/2022 05:38:06 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.15520833333333334 on epoch=716
06/05/2022 05:38:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/05/2022 05:38:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
06/05/2022 05:38:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/05/2022 05:38:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/05/2022 05:38:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/05/2022 05:38:21 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.18167641325536063 on epoch=733
06/05/2022 05:38:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/05/2022 05:38:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/05/2022 05:38:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
06/05/2022 05:38:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 05:38:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/05/2022 05:38:36 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.16370235934664246 on epoch=749
06/05/2022 05:38:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 05:38:42 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/05/2022 05:38:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/05/2022 05:38:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
06/05/2022 05:38:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/05/2022 05:38:52 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.1531135531135531 on epoch=766
06/05/2022 05:38:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
06/05/2022 05:38:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/05/2022 05:39:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
06/05/2022 05:39:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 05:39:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/05/2022 05:39:07 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.18943498452012383 on epoch=783
06/05/2022 05:39:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 05:39:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 05:39:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/05/2022 05:39:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 05:39:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 05:39:23 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.1902151488358385 on epoch=799
06/05/2022 05:39:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 05:39:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
06/05/2022 05:39:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 05:39:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 05:39:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/05/2022 05:39:38 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.1903118908382066 on epoch=816
06/05/2022 05:39:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/05/2022 05:39:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=823
06/05/2022 05:39:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 05:39:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 05:39:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 05:39:53 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.19927884615384617 on epoch=833
06/05/2022 05:39:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 05:39:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 05:40:01 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/05/2022 05:40:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/05/2022 05:40:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/05/2022 05:40:08 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.18263546798029556 on epoch=849
06/05/2022 05:40:10 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 05:40:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 05:40:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 05:40:18 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/05/2022 05:40:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=866
06/05/2022 05:40:22 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.22393822393822393 on epoch=866
06/05/2022 05:40:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=869
06/05/2022 05:40:28 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=873
06/05/2022 05:40:31 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 05:40:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 05:40:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 05:40:37 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.20207336523125996 on epoch=883
06/05/2022 05:40:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 05:40:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 05:40:45 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 05:40:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/05/2022 05:40:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 05:40:52 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.16103778467908902 on epoch=899
06/05/2022 05:40:55 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 05:40:58 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 05:41:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 05:41:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 05:41:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 05:41:07 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.17352941176470588 on epoch=916
06/05/2022 05:41:10 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 05:41:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=923
06/05/2022 05:41:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 05:41:18 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 05:41:21 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 05:41:22 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.16902899967416096 on epoch=933
06/05/2022 05:41:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 05:41:27 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 05:41:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 05:41:33 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 05:41:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 05:41:37 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.16427015250544658 on epoch=949
06/05/2022 05:41:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 05:41:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 05:41:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 05:41:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 05:41:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/05/2022 05:41:53 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.18238025415444772 on epoch=966
06/05/2022 05:41:55 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/05/2022 05:41:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 05:42:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 05:42:04 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 05:42:06 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 05:42:08 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.18436363636363637 on epoch=983
06/05/2022 05:42:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 05:42:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=989
06/05/2022 05:42:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 05:42:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 05:42:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 05:42:23 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.17253176930596287 on epoch=999
06/05/2022 05:42:23 - INFO - __main__ - save last model!
06/05/2022 05:42:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 05:42:23 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 05:42:23 - INFO - __main__ - Printing 3 examples
06/05/2022 05:42:23 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 05:42:23 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:23 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 05:42:23 - INFO - __main__ - ['entailment']
06/05/2022 05:42:23 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 05:42:23 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:23 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:42:23 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:42:23 - INFO - __main__ - Printing 3 examples
06/05/2022 05:42:23 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/05/2022 05:42:23 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:23 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/05/2022 05:42:23 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:23 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/05/2022 05:42:23 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:23 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:42:23 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:42:23 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 05:42:23 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:42:23 - INFO - __main__ - Printing 3 examples
06/05/2022 05:42:23 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/05/2022 05:42:23 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:23 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/05/2022 05:42:23 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:23 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/05/2022 05:42:23 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:23 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:42:23 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:42:23 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 05:42:24 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:42:25 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 05:42:41 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 05:42:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 05:42:42 - INFO - __main__ - Starting training!
06/05/2022 05:42:55 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_13_0.5_8_predictions.txt
06/05/2022 05:42:55 - INFO - __main__ - Classification-F1 on test data: 0.1245
06/05/2022 05:42:56 - INFO - __main__ - prefix=anli_16_13, lr=0.5, bsz=8, dev_performance=0.33842746400885937, test_performance=0.1245015764092755
06/05/2022 05:42:56 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.4, bsz=8 ...
06/05/2022 05:42:57 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:42:57 - INFO - __main__ - Printing 3 examples
06/05/2022 05:42:57 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/05/2022 05:42:57 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:57 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/05/2022 05:42:57 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:57 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/05/2022 05:42:57 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:57 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:42:57 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:42:57 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 05:42:57 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:42:57 - INFO - __main__ - Printing 3 examples
06/05/2022 05:42:57 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/05/2022 05:42:57 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:57 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/05/2022 05:42:57 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:57 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/05/2022 05:42:57 - INFO - __main__ - ['contradiction']
06/05/2022 05:42:57 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:42:57 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:42:57 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 05:43:17 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 05:43:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 05:43:18 - INFO - __main__ - Starting training!
06/05/2022 05:43:22 - INFO - __main__ - Step 10 Global step 10 Train loss 0.89 on epoch=3
06/05/2022 05:43:25 - INFO - __main__ - Step 20 Global step 20 Train loss 0.66 on epoch=6
06/05/2022 05:43:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=9
06/05/2022 05:43:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=13
06/05/2022 05:43:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=16
06/05/2022 05:43:34 - INFO - __main__ - Global step 50 Train loss 0.64 Classification-F1 0.16666666666666666 on epoch=16
06/05/2022 05:43:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/05/2022 05:43:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=19
06/05/2022 05:43:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=23
06/05/2022 05:43:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
06/05/2022 05:43:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=29
06/05/2022 05:43:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=33
06/05/2022 05:43:48 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.20138888888888892 on epoch=33
06/05/2022 05:43:48 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.20138888888888892 on epoch=33, global_step=100
06/05/2022 05:43:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=36
06/05/2022 05:43:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=39
06/05/2022 05:43:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=43
06/05/2022 05:43:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=46
06/05/2022 05:44:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=49
06/05/2022 05:44:03 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.1983273596176822 on epoch=49
06/05/2022 05:44:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
06/05/2022 05:44:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
06/05/2022 05:44:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
06/05/2022 05:44:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=63
06/05/2022 05:44:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=66
06/05/2022 05:44:17 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
06/05/2022 05:44:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=69
06/05/2022 05:44:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=73
06/05/2022 05:44:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=76
06/05/2022 05:44:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=79
06/05/2022 05:44:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=83
06/05/2022 05:44:32 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=83
06/05/2022 05:44:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=86
06/05/2022 05:44:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=89
06/05/2022 05:44:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
06/05/2022 05:44:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=96
06/05/2022 05:44:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=99
06/05/2022 05:44:46 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.20908004778972522 on epoch=99
06/05/2022 05:44:46 - INFO - __main__ - Saving model with best Classification-F1: 0.20138888888888892 -> 0.20908004778972522 on epoch=99, global_step=300
06/05/2022 05:44:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=103
06/05/2022 05:44:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
06/05/2022 05:44:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
06/05/2022 05:44:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=113
06/05/2022 05:45:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=116
06/05/2022 05:45:01 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.27777777777777773 on epoch=116
06/05/2022 05:45:01 - INFO - __main__ - Saving model with best Classification-F1: 0.20908004778972522 -> 0.27777777777777773 on epoch=116, global_step=350
06/05/2022 05:45:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=119
06/05/2022 05:45:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=123
06/05/2022 05:45:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=126
06/05/2022 05:45:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=129
06/05/2022 05:45:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=133
06/05/2022 05:45:15 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.31213603540647566 on epoch=133
06/05/2022 05:45:15 - INFO - __main__ - Saving model with best Classification-F1: 0.27777777777777773 -> 0.31213603540647566 on epoch=133, global_step=400
06/05/2022 05:45:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=136
06/05/2022 05:45:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=139
06/05/2022 05:45:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=143
06/05/2022 05:45:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=146
06/05/2022 05:45:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=149
06/05/2022 05:45:30 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.32234432234432236 on epoch=149
06/05/2022 05:45:30 - INFO - __main__ - Saving model with best Classification-F1: 0.31213603540647566 -> 0.32234432234432236 on epoch=149, global_step=450
06/05/2022 05:45:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=153
06/05/2022 05:45:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=156
06/05/2022 05:45:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=159
06/05/2022 05:45:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=163
06/05/2022 05:45:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=166
06/05/2022 05:45:45 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.2990810359231412 on epoch=166
06/05/2022 05:45:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=169
06/05/2022 05:45:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=173
06/05/2022 05:45:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.31 on epoch=176
06/05/2022 05:45:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=179
06/05/2022 05:45:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=183
06/05/2022 05:46:00 - INFO - __main__ - Global step 550 Train loss 0.33 Classification-F1 0.3508158508158508 on epoch=183
06/05/2022 05:46:00 - INFO - __main__ - Saving model with best Classification-F1: 0.32234432234432236 -> 0.3508158508158508 on epoch=183, global_step=550
06/05/2022 05:46:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=186
06/05/2022 05:46:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.33 on epoch=189
06/05/2022 05:46:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=193
06/05/2022 05:46:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=196
06/05/2022 05:46:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=199
06/05/2022 05:46:14 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.16666666666666666 on epoch=199
06/05/2022 05:46:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.34 on epoch=203
06/05/2022 05:46:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=206
06/05/2022 05:46:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.29 on epoch=209
06/05/2022 05:46:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=213
06/05/2022 05:46:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=216
06/05/2022 05:46:29 - INFO - __main__ - Global step 650 Train loss 0.28 Classification-F1 0.36746319822544865 on epoch=216
06/05/2022 05:46:29 - INFO - __main__ - Saving model with best Classification-F1: 0.3508158508158508 -> 0.36746319822544865 on epoch=216, global_step=650
06/05/2022 05:46:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=219
06/05/2022 05:46:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=223
06/05/2022 05:46:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=226
06/05/2022 05:46:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=229
06/05/2022 05:46:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=233
06/05/2022 05:46:45 - INFO - __main__ - Global step 700 Train loss 0.24 Classification-F1 0.39994425863991084 on epoch=233
06/05/2022 05:46:45 - INFO - __main__ - Saving model with best Classification-F1: 0.36746319822544865 -> 0.39994425863991084 on epoch=233, global_step=700
06/05/2022 05:46:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=236
06/05/2022 05:46:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=239
06/05/2022 05:46:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=243
06/05/2022 05:46:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=246
06/05/2022 05:46:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=249
06/05/2022 05:47:00 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.2975723622782447 on epoch=249
06/05/2022 05:47:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=253
06/05/2022 05:47:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=256
06/05/2022 05:47:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=259
06/05/2022 05:47:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=263
06/05/2022 05:47:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=266
06/05/2022 05:47:15 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.29650884744141565 on epoch=266
06/05/2022 05:47:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=269
06/05/2022 05:47:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=273
06/05/2022 05:47:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=276
06/05/2022 05:47:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=279
06/05/2022 05:47:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=283
06/05/2022 05:47:29 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.1985294117647059 on epoch=283
06/05/2022 05:47:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=286
06/05/2022 05:47:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=289
06/05/2022 05:47:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=293
06/05/2022 05:47:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=296
06/05/2022 05:47:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=299
06/05/2022 05:47:44 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.2965986394557823 on epoch=299
06/05/2022 05:47:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=303
06/05/2022 05:47:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=306
06/05/2022 05:47:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=309
06/05/2022 05:47:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=313
06/05/2022 05:47:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=316
06/05/2022 05:47:59 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.16969696969696968 on epoch=316
06/05/2022 05:48:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=319
06/05/2022 05:48:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=323
06/05/2022 05:48:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.08 on epoch=326
06/05/2022 05:48:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=329
06/05/2022 05:48:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=333
06/05/2022 05:48:14 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.2785627862760349 on epoch=333
06/05/2022 05:48:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=336
06/05/2022 05:48:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
06/05/2022 05:48:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=343
06/05/2022 05:48:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=346
06/05/2022 05:48:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=349
06/05/2022 05:48:29 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.20089285714285715 on epoch=349
06/05/2022 05:48:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=353
06/05/2022 05:48:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=356
06/05/2022 05:48:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=359
06/05/2022 05:48:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=363
06/05/2022 05:48:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
06/05/2022 05:48:44 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.22881192881192883 on epoch=366
06/05/2022 05:48:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=369
06/05/2022 05:48:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
06/05/2022 05:48:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=376
06/05/2022 05:48:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
06/05/2022 05:48:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
06/05/2022 05:48:59 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.2482638888888889 on epoch=383
06/05/2022 05:49:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
06/05/2022 05:49:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
06/05/2022 05:49:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
06/05/2022 05:49:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
06/05/2022 05:49:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
06/05/2022 05:49:14 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.19526315789473686 on epoch=399
06/05/2022 05:49:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=403
06/05/2022 05:49:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
06/05/2022 05:49:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=409
06/05/2022 05:49:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=413
06/05/2022 05:49:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
06/05/2022 05:49:29 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.22181372549019607 on epoch=416
06/05/2022 05:49:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
06/05/2022 05:49:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=423
06/05/2022 05:49:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
06/05/2022 05:49:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
06/05/2022 05:49:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
06/05/2022 05:49:44 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.20487570028011204 on epoch=433
06/05/2022 05:49:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
06/05/2022 05:49:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/05/2022 05:49:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=443
06/05/2022 05:49:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=446
06/05/2022 05:49:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=449
06/05/2022 05:50:00 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.19124670619235837 on epoch=449
06/05/2022 05:50:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
06/05/2022 05:50:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
06/05/2022 05:50:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
06/05/2022 05:50:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=463
06/05/2022 05:50:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
06/05/2022 05:50:15 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.16656641604010025 on epoch=466
06/05/2022 05:50:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
06/05/2022 05:50:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
06/05/2022 05:50:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
06/05/2022 05:50:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/05/2022 05:50:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
06/05/2022 05:50:30 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.21498599439775912 on epoch=483
06/05/2022 05:50:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
06/05/2022 05:50:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
06/05/2022 05:50:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
06/05/2022 05:50:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/05/2022 05:50:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/05/2022 05:50:45 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.20797720797720798 on epoch=499
06/05/2022 05:50:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
06/05/2022 05:50:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/05/2022 05:50:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
06/05/2022 05:50:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
06/05/2022 05:50:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
06/05/2022 05:51:00 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.19719367588932807 on epoch=516
06/05/2022 05:51:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=519
06/05/2022 05:51:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
06/05/2022 05:51:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
06/05/2022 05:51:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/05/2022 05:51:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=533
06/05/2022 05:51:15 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.1381662149954833 on epoch=533
06/05/2022 05:51:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
06/05/2022 05:51:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/05/2022 05:51:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=543
06/05/2022 05:51:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/05/2022 05:51:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
06/05/2022 05:51:31 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.16158466158466156 on epoch=549
06/05/2022 05:51:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
06/05/2022 05:51:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
06/05/2022 05:51:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
06/05/2022 05:51:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/05/2022 05:51:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
06/05/2022 05:51:46 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.18057142857142855 on epoch=566
06/05/2022 05:51:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
06/05/2022 05:51:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/05/2022 05:51:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
06/05/2022 05:51:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/05/2022 05:52:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/05/2022 05:52:01 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.15473389355742295 on epoch=583
06/05/2022 05:52:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/05/2022 05:52:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/05/2022 05:52:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/05/2022 05:52:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/05/2022 05:52:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/05/2022 05:52:16 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.15664160401002508 on epoch=599
06/05/2022 05:52:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
06/05/2022 05:52:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/05/2022 05:52:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
06/05/2022 05:52:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/05/2022 05:52:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
06/05/2022 05:52:31 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.2107200368069933 on epoch=616
06/05/2022 05:52:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/05/2022 05:52:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=623
06/05/2022 05:52:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/05/2022 05:52:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/05/2022 05:52:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
06/05/2022 05:52:46 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.22364532019704433 on epoch=633
06/05/2022 05:52:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
06/05/2022 05:52:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/05/2022 05:52:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/05/2022 05:52:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/05/2022 05:53:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/05/2022 05:53:01 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.17082352941176468 on epoch=649
06/05/2022 05:53:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/05/2022 05:53:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/05/2022 05:53:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
06/05/2022 05:53:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 05:53:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/05/2022 05:53:16 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.19909502262443438 on epoch=666
06/05/2022 05:53:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=669
06/05/2022 05:53:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/05/2022 05:53:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=676
06/05/2022 05:53:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/05/2022 05:53:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/05/2022 05:53:32 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.22055555555555556 on epoch=683
06/05/2022 05:53:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
06/05/2022 05:53:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=689
06/05/2022 05:53:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/05/2022 05:53:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/05/2022 05:53:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/05/2022 05:53:47 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.2958257713248639 on epoch=699
06/05/2022 05:53:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/05/2022 05:53:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/05/2022 05:53:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=709
06/05/2022 05:53:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/05/2022 05:54:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=716
06/05/2022 05:54:01 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.18779322638146168 on epoch=716
06/05/2022 05:54:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/05/2022 05:54:07 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/05/2022 05:54:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/05/2022 05:54:12 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/05/2022 05:54:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
06/05/2022 05:54:16 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.18484848484848487 on epoch=733
06/05/2022 05:54:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/05/2022 05:54:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/05/2022 05:54:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 05:54:27 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 05:54:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
06/05/2022 05:54:31 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.16769433465085637 on epoch=749
06/05/2022 05:54:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 05:54:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/05/2022 05:54:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=759
06/05/2022 05:54:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/05/2022 05:54:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/05/2022 05:54:46 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.15341880341880343 on epoch=766
06/05/2022 05:54:49 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 05:54:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/05/2022 05:54:54 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/05/2022 05:54:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 05:55:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/05/2022 05:55:01 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.14141307658069202 on epoch=783
06/05/2022 05:55:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 05:55:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 05:55:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
06/05/2022 05:55:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 05:55:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 05:55:16 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.24166666666666667 on epoch=799
06/05/2022 05:55:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/05/2022 05:55:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 05:55:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 05:55:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 05:55:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/05/2022 05:55:31 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.202650290885585 on epoch=816
06/05/2022 05:55:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/05/2022 05:55:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 05:55:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/05/2022 05:55:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 05:55:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 05:55:47 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.3318903318903319 on epoch=833
06/05/2022 05:55:49 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 05:55:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 05:55:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/05/2022 05:55:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/05/2022 05:56:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/05/2022 05:56:02 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.2658392658392658 on epoch=849
06/05/2022 05:56:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 05:56:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 05:56:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 05:56:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/05/2022 05:56:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=866
06/05/2022 05:56:17 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.23352832512315272 on epoch=866
06/05/2022 05:56:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 05:56:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/05/2022 05:56:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=876
06/05/2022 05:56:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 05:56:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 05:56:32 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.21623931623931625 on epoch=883
06/05/2022 05:56:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 05:56:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 05:56:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=893
06/05/2022 05:56:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=896
06/05/2022 05:56:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 05:56:47 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.18811576354679801 on epoch=899
06/05/2022 05:56:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 05:56:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 05:56:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 05:56:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 05:57:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/05/2022 05:57:02 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.1964605734767025 on epoch=916
06/05/2022 05:57:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/05/2022 05:57:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 05:57:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 05:57:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 05:57:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 05:57:17 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.1378205128205128 on epoch=933
06/05/2022 05:57:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 05:57:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 05:57:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 05:57:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/05/2022 05:57:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
06/05/2022 05:57:33 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.1990783410138249 on epoch=949
06/05/2022 05:57:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 05:57:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 05:57:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 05:57:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/05/2022 05:57:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
06/05/2022 05:57:47 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.1565734989648033 on epoch=966
06/05/2022 05:57:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 05:57:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
06/05/2022 05:57:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=976
06/05/2022 05:57:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 05:58:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 05:58:03 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.15008278867102395 on epoch=983
06/05/2022 05:58:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 05:58:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 05:58:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=993
06/05/2022 05:58:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 05:58:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 05:58:18 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.1426497277676951 on epoch=999
06/05/2022 05:58:18 - INFO - __main__ - save last model!
06/05/2022 05:58:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 05:58:18 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 05:58:18 - INFO - __main__ - Printing 3 examples
06/05/2022 05:58:18 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 05:58:18 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:18 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 05:58:18 - INFO - __main__ - ['entailment']
06/05/2022 05:58:18 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 05:58:18 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:18 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:58:18 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:58:18 - INFO - __main__ - Printing 3 examples
06/05/2022 05:58:18 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/05/2022 05:58:18 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:18 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/05/2022 05:58:18 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:18 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/05/2022 05:58:18 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:18 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:58:18 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:58:18 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 05:58:18 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:58:18 - INFO - __main__ - Printing 3 examples
06/05/2022 05:58:18 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/05/2022 05:58:18 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:18 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/05/2022 05:58:18 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:18 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/05/2022 05:58:18 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:18 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:58:18 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:58:18 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 05:58:19 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:58:20 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 05:58:37 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 05:58:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 05:58:38 - INFO - __main__ - Starting training!
06/05/2022 05:58:49 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_13_0.4_8_predictions.txt
06/05/2022 05:58:49 - INFO - __main__ - Classification-F1 on test data: 0.1469
06/05/2022 05:58:49 - INFO - __main__ - prefix=anli_16_13, lr=0.4, bsz=8, dev_performance=0.39994425863991084, test_performance=0.14685633442411974
06/05/2022 05:58:49 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.3, bsz=8 ...
06/05/2022 05:58:50 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:58:50 - INFO - __main__ - Printing 3 examples
06/05/2022 05:58:50 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/05/2022 05:58:50 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:50 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/05/2022 05:58:50 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:50 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/05/2022 05:58:50 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:50 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:58:50 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:58:50 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 05:58:50 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 05:58:50 - INFO - __main__ - Printing 3 examples
06/05/2022 05:58:50 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/05/2022 05:58:50 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:50 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/05/2022 05:58:50 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:50 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/05/2022 05:58:50 - INFO - __main__ - ['contradiction']
06/05/2022 05:58:50 - INFO - __main__ - Tokenizing Input ...
06/05/2022 05:58:50 - INFO - __main__ - Tokenizing Output ...
06/05/2022 05:58:50 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 05:59:06 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 05:59:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 05:59:06 - INFO - __main__ - Starting training!
06/05/2022 05:59:10 - INFO - __main__ - Step 10 Global step 10 Train loss 0.99 on epoch=3
06/05/2022 05:59:13 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=6
06/05/2022 05:59:15 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=9
06/05/2022 05:59:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=13
06/05/2022 05:59:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=16
06/05/2022 05:59:22 - INFO - __main__ - Global step 50 Train loss 0.69 Classification-F1 0.16666666666666666 on epoch=16
06/05/2022 05:59:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/05/2022 05:59:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=19
06/05/2022 05:59:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=23
06/05/2022 05:59:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
06/05/2022 05:59:33 - INFO - __main__ - Step 90 Global step 90 Train loss 0.57 on epoch=29
06/05/2022 05:59:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=33
06/05/2022 05:59:37 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.2647296206618241 on epoch=33
06/05/2022 05:59:37 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2647296206618241 on epoch=33, global_step=100
06/05/2022 05:59:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=36
06/05/2022 05:59:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=39
06/05/2022 05:59:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=43
06/05/2022 05:59:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=46
06/05/2022 05:59:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=49
06/05/2022 05:59:51 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.1693121693121693 on epoch=49
06/05/2022 05:59:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=53
06/05/2022 05:59:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=56
06/05/2022 05:59:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=59
06/05/2022 06:00:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=63
06/05/2022 06:00:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=66
06/05/2022 06:00:06 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=66
06/05/2022 06:00:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
06/05/2022 06:00:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=73
06/05/2022 06:00:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=76
06/05/2022 06:00:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
06/05/2022 06:00:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
06/05/2022 06:00:21 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.32424877707896577 on epoch=83
06/05/2022 06:00:21 - INFO - __main__ - Saving model with best Classification-F1: 0.2647296206618241 -> 0.32424877707896577 on epoch=83, global_step=250
06/05/2022 06:00:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=86
06/05/2022 06:00:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=89
06/05/2022 06:00:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
06/05/2022 06:00:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=96
06/05/2022 06:00:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
06/05/2022 06:00:35 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=99
06/05/2022 06:00:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=103
06/05/2022 06:00:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=106
06/05/2022 06:00:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
06/05/2022 06:00:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=113
06/05/2022 06:00:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=116
06/05/2022 06:00:50 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=116
06/05/2022 06:00:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=119
06/05/2022 06:00:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=123
06/05/2022 06:00:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=126
06/05/2022 06:01:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=129
06/05/2022 06:01:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=133
06/05/2022 06:01:05 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.27602905569007263 on epoch=133
06/05/2022 06:01:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=136
06/05/2022 06:01:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=139
06/05/2022 06:01:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=143
06/05/2022 06:01:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=146
06/05/2022 06:01:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=149
06/05/2022 06:01:20 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.1693121693121693 on epoch=149
06/05/2022 06:01:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=153
06/05/2022 06:01:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=156
06/05/2022 06:01:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=159
06/05/2022 06:01:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=163
06/05/2022 06:01:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=166
06/05/2022 06:01:34 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=166
06/05/2022 06:01:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=169
06/05/2022 06:01:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=173
06/05/2022 06:01:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=176
06/05/2022 06:01:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=179
06/05/2022 06:01:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=183
06/05/2022 06:01:49 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.2833333333333334 on epoch=183
06/05/2022 06:01:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=186
06/05/2022 06:01:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=189
06/05/2022 06:01:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=193
06/05/2022 06:02:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=196
06/05/2022 06:02:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=199
06/05/2022 06:02:04 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.2099511072763877 on epoch=199
06/05/2022 06:02:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=203
06/05/2022 06:02:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=206
06/05/2022 06:02:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=209
06/05/2022 06:02:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=213
06/05/2022 06:02:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=216
06/05/2022 06:02:19 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.3331138907175774 on epoch=216
06/05/2022 06:02:19 - INFO - __main__ - Saving model with best Classification-F1: 0.32424877707896577 -> 0.3331138907175774 on epoch=216, global_step=650
06/05/2022 06:02:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=219
06/05/2022 06:02:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=223
06/05/2022 06:02:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=226
06/05/2022 06:02:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=229
06/05/2022 06:02:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=233
06/05/2022 06:02:34 - INFO - __main__ - Global step 700 Train loss 0.32 Classification-F1 0.3 on epoch=233
06/05/2022 06:02:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.31 on epoch=236
06/05/2022 06:02:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=239
06/05/2022 06:02:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=243
06/05/2022 06:02:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=246
06/05/2022 06:02:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=249
06/05/2022 06:02:49 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.24444444444444446 on epoch=249
06/05/2022 06:02:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=253
06/05/2022 06:02:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=256
06/05/2022 06:02:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=259
06/05/2022 06:02:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=263
06/05/2022 06:03:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=266
06/05/2022 06:03:04 - INFO - __main__ - Global step 800 Train loss 0.31 Classification-F1 0.32566069906223366 on epoch=266
06/05/2022 06:03:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=269
06/05/2022 06:03:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=273
06/05/2022 06:03:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=276
06/05/2022 06:03:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=279
06/05/2022 06:03:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=283
06/05/2022 06:03:19 - INFO - __main__ - Global step 850 Train loss 0.28 Classification-F1 0.34074074074074073 on epoch=283
06/05/2022 06:03:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3331138907175774 -> 0.34074074074074073 on epoch=283, global_step=850
06/05/2022 06:03:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=286
06/05/2022 06:03:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=289
06/05/2022 06:03:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=293
06/05/2022 06:03:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=296
06/05/2022 06:03:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=299
06/05/2022 06:03:34 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.3308270676691729 on epoch=299
06/05/2022 06:03:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=303
06/05/2022 06:03:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=306
06/05/2022 06:03:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=309
06/05/2022 06:03:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.28 on epoch=313
06/05/2022 06:03:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=316
06/05/2022 06:03:49 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.2874446773817843 on epoch=316
06/05/2022 06:03:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=319
06/05/2022 06:03:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=323
06/05/2022 06:03:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=326
06/05/2022 06:04:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=329
06/05/2022 06:04:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=333
06/05/2022 06:04:04 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3542087542087542 on epoch=333
06/05/2022 06:04:04 - INFO - __main__ - Saving model with best Classification-F1: 0.34074074074074073 -> 0.3542087542087542 on epoch=333, global_step=1000
06/05/2022 06:04:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=336
06/05/2022 06:04:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=339
06/05/2022 06:04:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=343
06/05/2022 06:04:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=346
06/05/2022 06:04:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=349
06/05/2022 06:04:19 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.30072930072930076 on epoch=349
06/05/2022 06:04:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=353
06/05/2022 06:04:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=356
06/05/2022 06:04:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=359
06/05/2022 06:04:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=363
06/05/2022 06:04:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=366
06/05/2022 06:04:34 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.1622512010981469 on epoch=366
06/05/2022 06:04:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=369
06/05/2022 06:04:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=373
06/05/2022 06:04:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=376
06/05/2022 06:04:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=379
06/05/2022 06:04:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=383
06/05/2022 06:04:48 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.19460633484162898 on epoch=383
06/05/2022 06:04:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=386
06/05/2022 06:04:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=389
06/05/2022 06:04:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=393
06/05/2022 06:04:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=396
06/05/2022 06:05:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=399
06/05/2022 06:05:03 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.3111111111111111 on epoch=399
06/05/2022 06:05:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=403
06/05/2022 06:05:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=406
06/05/2022 06:05:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=409
06/05/2022 06:05:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=413
06/05/2022 06:05:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=416
06/05/2022 06:05:18 - INFO - __main__ - Global step 1250 Train loss 0.12 Classification-F1 0.15614973262032086 on epoch=416
06/05/2022 06:05:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=419
06/05/2022 06:05:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=423
06/05/2022 06:05:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=426
06/05/2022 06:05:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=429
06/05/2022 06:05:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=433
06/05/2022 06:05:33 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.14523809523809525 on epoch=433
06/05/2022 06:05:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=436
06/05/2022 06:05:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=439
06/05/2022 06:05:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=443
06/05/2022 06:05:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=446
06/05/2022 06:05:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=449
06/05/2022 06:05:48 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.15211864406779663 on epoch=449
06/05/2022 06:05:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=453
06/05/2022 06:05:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=456
06/05/2022 06:05:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
06/05/2022 06:05:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=463
06/05/2022 06:06:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
06/05/2022 06:06:03 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.1458937198067633 on epoch=466
06/05/2022 06:06:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=469
06/05/2022 06:06:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
06/05/2022 06:06:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=476
06/05/2022 06:06:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
06/05/2022 06:06:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
06/05/2022 06:06:18 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.18969144460028053 on epoch=483
06/05/2022 06:06:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
06/05/2022 06:06:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=489
06/05/2022 06:06:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
06/05/2022 06:06:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/05/2022 06:06:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=499
06/05/2022 06:06:33 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.15117294053464267 on epoch=499
06/05/2022 06:06:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
06/05/2022 06:06:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=506
06/05/2022 06:06:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
06/05/2022 06:06:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
06/05/2022 06:06:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=516
06/05/2022 06:06:49 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.19243598778482499 on epoch=516
06/05/2022 06:06:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
06/05/2022 06:06:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=523
06/05/2022 06:06:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/05/2022 06:07:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
06/05/2022 06:07:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
06/05/2022 06:07:04 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.2303668022268725 on epoch=533
06/05/2022 06:07:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=536
06/05/2022 06:07:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=539
06/05/2022 06:07:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/05/2022 06:07:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
06/05/2022 06:07:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=549
06/05/2022 06:07:19 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.32002934702861335 on epoch=549
06/05/2022 06:07:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=553
06/05/2022 06:07:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
06/05/2022 06:07:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=559
06/05/2022 06:07:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/05/2022 06:07:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
06/05/2022 06:07:34 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.2224537037037037 on epoch=566
06/05/2022 06:07:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=569
06/05/2022 06:07:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=573
06/05/2022 06:07:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=576
06/05/2022 06:07:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=579
06/05/2022 06:07:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
06/05/2022 06:07:50 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.19744360902255637 on epoch=583
06/05/2022 06:07:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/05/2022 06:07:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=589
06/05/2022 06:07:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/05/2022 06:08:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/05/2022 06:08:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=599
06/05/2022 06:08:05 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.15507731547903716 on epoch=599
06/05/2022 06:08:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
06/05/2022 06:08:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/05/2022 06:08:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/05/2022 06:08:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/05/2022 06:08:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=616
06/05/2022 06:08:20 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.14603174603174604 on epoch=616
06/05/2022 06:08:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=619
06/05/2022 06:08:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/05/2022 06:08:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/05/2022 06:08:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=629
06/05/2022 06:08:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/05/2022 06:08:35 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.16111673414304992 on epoch=633
06/05/2022 06:08:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/05/2022 06:08:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
06/05/2022 06:08:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
06/05/2022 06:08:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/05/2022 06:08:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/05/2022 06:08:51 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.19743707093821508 on epoch=649
06/05/2022 06:08:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/05/2022 06:08:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
06/05/2022 06:08:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/05/2022 06:09:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 06:09:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/05/2022 06:09:06 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.27211538461538465 on epoch=666
06/05/2022 06:09:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/05/2022 06:09:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=673
06/05/2022 06:09:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
06/05/2022 06:09:17 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
06/05/2022 06:09:20 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
06/05/2022 06:09:21 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.19047619047619047 on epoch=683
06/05/2022 06:09:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
06/05/2022 06:09:27 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=689
06/05/2022 06:09:30 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=693
06/05/2022 06:09:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/05/2022 06:09:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
06/05/2022 06:09:37 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.17835410026079396 on epoch=699
06/05/2022 06:09:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/05/2022 06:09:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/05/2022 06:09:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/05/2022 06:09:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
06/05/2022 06:09:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/05/2022 06:09:52 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.19260606060606061 on epoch=716
06/05/2022 06:09:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=719
06/05/2022 06:09:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/05/2022 06:10:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
06/05/2022 06:10:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/05/2022 06:10:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/05/2022 06:10:08 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.2582456140350877 on epoch=733
06/05/2022 06:10:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/05/2022 06:10:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=739
06/05/2022 06:10:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 06:10:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 06:10:22 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
06/05/2022 06:10:23 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.2735759953501889 on epoch=749
06/05/2022 06:10:26 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 06:10:29 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/05/2022 06:10:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/05/2022 06:10:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=763
06/05/2022 06:10:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/05/2022 06:10:39 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.20457221500114686 on epoch=766
06/05/2022 06:10:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 06:10:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/05/2022 06:10:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
06/05/2022 06:10:51 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 06:10:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/05/2022 06:10:55 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.14408627087198517 on epoch=783
06/05/2022 06:10:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 06:11:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/05/2022 06:11:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/05/2022 06:11:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
06/05/2022 06:11:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/05/2022 06:11:10 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.225982905982906 on epoch=799
06/05/2022 06:11:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
06/05/2022 06:11:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 06:11:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 06:11:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 06:11:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=816
06/05/2022 06:11:26 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.17946543778801843 on epoch=816
06/05/2022 06:11:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/05/2022 06:11:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=823
06/05/2022 06:11:34 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 06:11:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 06:11:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 06:11:41 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.15883962780514507 on epoch=833
06/05/2022 06:11:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 06:11:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 06:11:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/05/2022 06:11:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/05/2022 06:11:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/05/2022 06:11:57 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.2387218045112782 on epoch=849
06/05/2022 06:11:59 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 06:12:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 06:12:05 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
06/05/2022 06:12:08 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/05/2022 06:12:10 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 06:12:12 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.15400170285227757 on epoch=866
06/05/2022 06:12:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 06:12:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/05/2022 06:12:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 06:12:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 06:12:26 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/05/2022 06:12:27 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.21312820512820516 on epoch=883
06/05/2022 06:12:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 06:12:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 06:12:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 06:12:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=896
06/05/2022 06:12:41 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 06:12:42 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.2447293447293447 on epoch=899
06/05/2022 06:12:45 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
06/05/2022 06:12:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/05/2022 06:12:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 06:12:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=913
06/05/2022 06:12:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 06:12:58 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.17998843262001157 on epoch=916
06/05/2022 06:13:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 06:13:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 06:13:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 06:13:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 06:13:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=933
06/05/2022 06:13:13 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.21120689655172414 on epoch=933
06/05/2022 06:13:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
06/05/2022 06:13:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/05/2022 06:13:22 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=943
06/05/2022 06:13:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 06:13:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 06:13:29 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.166797385620915 on epoch=949
06/05/2022 06:13:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 06:13:34 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 06:13:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=959
06/05/2022 06:13:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 06:13:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/05/2022 06:13:44 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.17536017991425962 on epoch=966
06/05/2022 06:13:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 06:13:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
06/05/2022 06:13:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 06:13:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 06:13:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 06:13:59 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.24241482862172514 on epoch=983
06/05/2022 06:14:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 06:14:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 06:14:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 06:14:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 06:14:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 06:14:15 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.2143969489909423 on epoch=999
06/05/2022 06:14:15 - INFO - __main__ - save last model!
06/05/2022 06:14:15 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:14:15 - INFO - __main__ - Printing 3 examples
06/05/2022 06:14:15 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/05/2022 06:14:15 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:15 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/05/2022 06:14:15 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:15 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/05/2022 06:14:15 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:15 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:14:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 06:14:15 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 06:14:15 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:14:15 - INFO - __main__ - Printing 3 examples
06/05/2022 06:14:15 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 06:14:15 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:15 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 06:14:15 - INFO - __main__ - ['entailment']
06/05/2022 06:14:15 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 06:14:15 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:15 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:14:15 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 06:14:15 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:14:15 - INFO - __main__ - Printing 3 examples
06/05/2022 06:14:15 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/05/2022 06:14:15 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:15 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/05/2022 06:14:15 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:15 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/05/2022 06:14:15 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:15 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:14:15 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:14:15 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 06:14:16 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:14:17 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 06:14:35 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 06:14:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 06:14:36 - INFO - __main__ - Starting training!
06/05/2022 06:14:48 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_13_0.3_8_predictions.txt
06/05/2022 06:14:48 - INFO - __main__ - Classification-F1 on test data: 0.1628
06/05/2022 06:14:49 - INFO - __main__ - prefix=anli_16_13, lr=0.3, bsz=8, dev_performance=0.3542087542087542, test_performance=0.1628192535166647
06/05/2022 06:14:49 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.2, bsz=8 ...
06/05/2022 06:14:50 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:14:50 - INFO - __main__ - Printing 3 examples
06/05/2022 06:14:50 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/05/2022 06:14:50 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:50 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/05/2022 06:14:50 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:50 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/05/2022 06:14:50 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:50 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:14:50 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:14:50 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 06:14:50 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:14:50 - INFO - __main__ - Printing 3 examples
06/05/2022 06:14:50 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
06/05/2022 06:14:50 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:50 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
06/05/2022 06:14:50 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:50 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
06/05/2022 06:14:50 - INFO - __main__ - ['contradiction']
06/05/2022 06:14:50 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:14:50 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:14:50 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 06:15:06 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 06:15:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 06:15:06 - INFO - __main__ - Starting training!
06/05/2022 06:15:10 - INFO - __main__ - Step 10 Global step 10 Train loss 1.08 on epoch=3
06/05/2022 06:15:13 - INFO - __main__ - Step 20 Global step 20 Train loss 0.79 on epoch=6
06/05/2022 06:15:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.67 on epoch=9
06/05/2022 06:15:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.63 on epoch=13
06/05/2022 06:15:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=16
06/05/2022 06:15:22 - INFO - __main__ - Global step 50 Train loss 0.75 Classification-F1 0.16666666666666666 on epoch=16
06/05/2022 06:15:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/05/2022 06:15:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=19
06/05/2022 06:15:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=23
06/05/2022 06:15:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=26
06/05/2022 06:15:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=29
06/05/2022 06:15:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=33
06/05/2022 06:15:38 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.28392745465916197 on epoch=33
06/05/2022 06:15:38 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.28392745465916197 on epoch=33, global_step=100
06/05/2022 06:15:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=36
06/05/2022 06:15:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=39
06/05/2022 06:15:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=43
06/05/2022 06:15:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=46
06/05/2022 06:15:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=49
06/05/2022 06:15:53 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=49
06/05/2022 06:15:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
06/05/2022 06:15:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=56
06/05/2022 06:16:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=59
06/05/2022 06:16:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=63
06/05/2022 06:16:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=66
06/05/2022 06:16:08 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=66
06/05/2022 06:16:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=69
06/05/2022 06:16:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=73
06/05/2022 06:16:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=76
06/05/2022 06:16:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
06/05/2022 06:16:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=83
06/05/2022 06:16:22 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.2913806254767353 on epoch=83
06/05/2022 06:16:22 - INFO - __main__ - Saving model with best Classification-F1: 0.28392745465916197 -> 0.2913806254767353 on epoch=83, global_step=250
06/05/2022 06:16:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=86
06/05/2022 06:16:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=89
06/05/2022 06:16:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
06/05/2022 06:16:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=96
06/05/2022 06:16:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=99
06/05/2022 06:16:37 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.2708333333333333 on epoch=99
06/05/2022 06:16:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=103
06/05/2022 06:16:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=106
06/05/2022 06:16:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
06/05/2022 06:16:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
06/05/2022 06:16:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=116
06/05/2022 06:16:52 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=116
06/05/2022 06:16:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=119
06/05/2022 06:16:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=123
06/05/2022 06:17:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=126
06/05/2022 06:17:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=129
06/05/2022 06:17:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=133
06/05/2022 06:17:06 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=133
06/05/2022 06:17:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=136
06/05/2022 06:17:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=139
06/05/2022 06:17:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=143
06/05/2022 06:17:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=146
06/05/2022 06:17:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=149
06/05/2022 06:17:21 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=149
06/05/2022 06:17:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=153
06/05/2022 06:17:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=156
06/05/2022 06:17:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=159
06/05/2022 06:17:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=163
06/05/2022 06:17:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=166
06/05/2022 06:17:36 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.32424877707896577 on epoch=166
06/05/2022 06:17:36 - INFO - __main__ - Saving model with best Classification-F1: 0.2913806254767353 -> 0.32424877707896577 on epoch=166, global_step=500
06/05/2022 06:17:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=169
06/05/2022 06:17:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=173
06/05/2022 06:17:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=176
06/05/2022 06:17:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=179
06/05/2022 06:17:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=183
06/05/2022 06:17:50 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.28114478114478114 on epoch=183
06/05/2022 06:17:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=186
06/05/2022 06:17:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=189
06/05/2022 06:17:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=193
06/05/2022 06:18:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=196
06/05/2022 06:18:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=199
06/05/2022 06:18:04 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.303030303030303 on epoch=199
06/05/2022 06:18:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=203
06/05/2022 06:18:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=206
06/05/2022 06:18:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=209
06/05/2022 06:18:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=213
06/05/2022 06:18:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=216
06/05/2022 06:18:19 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.2623951182303585 on epoch=216
06/05/2022 06:18:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=219
06/05/2022 06:18:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=223
06/05/2022 06:18:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=226
06/05/2022 06:18:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=229
06/05/2022 06:18:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=233
06/05/2022 06:18:33 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.2623951182303585 on epoch=233
06/05/2022 06:18:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=236
06/05/2022 06:18:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=239
06/05/2022 06:18:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=243
06/05/2022 06:18:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=246
06/05/2022 06:18:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=249
06/05/2022 06:18:48 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.2380952380952381 on epoch=249
06/05/2022 06:18:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=253
06/05/2022 06:18:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=256
06/05/2022 06:18:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=259
06/05/2022 06:18:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=263
06/05/2022 06:19:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=266
06/05/2022 06:19:02 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.21012849584278157 on epoch=266
06/05/2022 06:19:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.30 on epoch=269
06/05/2022 06:19:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=273
06/05/2022 06:19:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=276
06/05/2022 06:19:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=279
06/05/2022 06:19:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=283
06/05/2022 06:19:17 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.3146867798030588 on epoch=283
06/05/2022 06:19:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=286
06/05/2022 06:19:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=289
06/05/2022 06:19:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=293
06/05/2022 06:19:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=296
06/05/2022 06:19:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=299
06/05/2022 06:19:32 - INFO - __main__ - Global step 900 Train loss 0.29 Classification-F1 0.29777777777777775 on epoch=299
06/05/2022 06:19:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=303
06/05/2022 06:19:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=306
06/05/2022 06:19:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.31 on epoch=309
06/05/2022 06:19:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=313
06/05/2022 06:19:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=316
06/05/2022 06:19:48 - INFO - __main__ - Global step 950 Train loss 0.31 Classification-F1 0.24979114452798665 on epoch=316
06/05/2022 06:19:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=319
06/05/2022 06:19:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=323
06/05/2022 06:19:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=326
06/05/2022 06:19:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=329
06/05/2022 06:20:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=333
06/05/2022 06:20:03 - INFO - __main__ - Global step 1000 Train loss 0.26 Classification-F1 0.2355555555555556 on epoch=333
06/05/2022 06:20:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=336
06/05/2022 06:20:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.28 on epoch=339
06/05/2022 06:20:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.28 on epoch=343
06/05/2022 06:20:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=346
06/05/2022 06:20:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=349
06/05/2022 06:20:18 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.27112827112827115 on epoch=349
06/05/2022 06:20:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=353
06/05/2022 06:20:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=356
06/05/2022 06:20:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=359
06/05/2022 06:20:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=363
06/05/2022 06:20:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=366
06/05/2022 06:20:33 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.25210084033613445 on epoch=366
06/05/2022 06:20:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=369
06/05/2022 06:20:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=373
06/05/2022 06:20:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=376
06/05/2022 06:20:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=379
06/05/2022 06:20:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=383
06/05/2022 06:20:48 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.25718674566600835 on epoch=383
06/05/2022 06:20:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=386
06/05/2022 06:20:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=389
06/05/2022 06:20:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=393
06/05/2022 06:21:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=396
06/05/2022 06:21:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=399
06/05/2022 06:21:04 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.29998666133119917 on epoch=399
06/05/2022 06:21:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=403
06/05/2022 06:21:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=406
06/05/2022 06:21:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=409
06/05/2022 06:21:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.16 on epoch=413
06/05/2022 06:21:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=416
06/05/2022 06:21:19 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.2923809523809524 on epoch=416
06/05/2022 06:21:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=419
06/05/2022 06:21:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=423
06/05/2022 06:21:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=426
06/05/2022 06:21:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=429
06/05/2022 06:21:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=433
06/05/2022 06:21:35 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.2490842490842491 on epoch=433
06/05/2022 06:21:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=436
06/05/2022 06:21:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=439
06/05/2022 06:21:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=443
06/05/2022 06:21:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=446
06/05/2022 06:21:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=449
06/05/2022 06:21:50 - INFO - __main__ - Global step 1350 Train loss 0.17 Classification-F1 0.22528735632183908 on epoch=449
06/05/2022 06:21:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=453
06/05/2022 06:21:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=456
06/05/2022 06:21:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=459
06/05/2022 06:22:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=463
06/05/2022 06:22:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=466
06/05/2022 06:22:05 - INFO - __main__ - Global step 1400 Train loss 0.11 Classification-F1 0.2546437014522121 on epoch=466
06/05/2022 06:22:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=469
06/05/2022 06:22:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=473
06/05/2022 06:22:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=476
06/05/2022 06:22:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=479
06/05/2022 06:22:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=483
06/05/2022 06:22:20 - INFO - __main__ - Global step 1450 Train loss 0.11 Classification-F1 0.15842293906810034 on epoch=483
06/05/2022 06:22:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=486
06/05/2022 06:22:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=489
06/05/2022 06:22:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=493
06/05/2022 06:22:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=496
06/05/2022 06:22:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=499
06/05/2022 06:22:36 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.10554177005789908 on epoch=499
06/05/2022 06:22:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=503
06/05/2022 06:22:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=506
06/05/2022 06:22:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=509
06/05/2022 06:22:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=513
06/05/2022 06:22:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=516
06/05/2022 06:22:51 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.22573529411764703 on epoch=516
06/05/2022 06:22:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=519
06/05/2022 06:22:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=523
06/05/2022 06:22:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=526
06/05/2022 06:23:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=529
06/05/2022 06:23:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=533
06/05/2022 06:23:06 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.20348532494758909 on epoch=533
06/05/2022 06:23:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=536
06/05/2022 06:23:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=539
06/05/2022 06:23:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=543
06/05/2022 06:23:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=546
06/05/2022 06:23:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=549
06/05/2022 06:23:21 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.1085423197492163 on epoch=549
06/05/2022 06:23:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
06/05/2022 06:23:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=556
06/05/2022 06:23:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=559
06/05/2022 06:23:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=563
06/05/2022 06:23:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=566
06/05/2022 06:23:37 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.1622727272727273 on epoch=566
06/05/2022 06:23:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
06/05/2022 06:23:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=573
06/05/2022 06:23:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
06/05/2022 06:23:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/05/2022 06:23:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=583
06/05/2022 06:23:52 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.12061403508771928 on epoch=583
06/05/2022 06:23:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=586
06/05/2022 06:23:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=589
06/05/2022 06:24:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=593
06/05/2022 06:24:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=596
06/05/2022 06:24:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
06/05/2022 06:24:07 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.16666666666666669 on epoch=599
06/05/2022 06:24:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
06/05/2022 06:24:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=606
06/05/2022 06:24:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=609
06/05/2022 06:24:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
06/05/2022 06:24:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
06/05/2022 06:24:23 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.1543560606060606 on epoch=616
06/05/2022 06:24:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
06/05/2022 06:24:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
06/05/2022 06:24:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/05/2022 06:24:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/05/2022 06:24:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
06/05/2022 06:24:38 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.15269230769230768 on epoch=633
06/05/2022 06:24:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=636
06/05/2022 06:24:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
06/05/2022 06:24:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=643
06/05/2022 06:24:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=646
06/05/2022 06:24:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=649
06/05/2022 06:24:53 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.12727272727272726 on epoch=649
06/05/2022 06:24:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/05/2022 06:24:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/05/2022 06:25:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=659
06/05/2022 06:25:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=663
06/05/2022 06:25:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=666
06/05/2022 06:25:09 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.12359098228663444 on epoch=666
06/05/2022 06:25:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=669
06/05/2022 06:25:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
06/05/2022 06:25:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=676
06/05/2022 06:25:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=679
06/05/2022 06:25:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/05/2022 06:25:24 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.12957157784743992 on epoch=683
06/05/2022 06:25:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
06/05/2022 06:25:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
06/05/2022 06:25:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/05/2022 06:25:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
06/05/2022 06:25:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=699
06/05/2022 06:25:39 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.15207373271889402 on epoch=699
06/05/2022 06:25:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/05/2022 06:25:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/05/2022 06:25:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=709
06/05/2022 06:25:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
06/05/2022 06:25:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=716
06/05/2022 06:25:54 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.1346153846153846 on epoch=716
06/05/2022 06:25:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=719
06/05/2022 06:26:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
06/05/2022 06:26:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/05/2022 06:26:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/05/2022 06:26:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=733
06/05/2022 06:26:09 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.15037593984962405 on epoch=733
06/05/2022 06:26:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=736
06/05/2022 06:26:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=739
06/05/2022 06:26:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
06/05/2022 06:26:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/05/2022 06:26:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=749
06/05/2022 06:26:25 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.15294117647058825 on epoch=749
06/05/2022 06:26:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
06/05/2022 06:26:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/05/2022 06:26:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
06/05/2022 06:26:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=763
06/05/2022 06:26:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=766
06/05/2022 06:26:40 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.1821631878557875 on epoch=766
06/05/2022 06:26:43 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 06:26:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/05/2022 06:26:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
06/05/2022 06:26:51 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=779
06/05/2022 06:26:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=783
06/05/2022 06:26:55 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.16428571428571428 on epoch=783
06/05/2022 06:26:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 06:27:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=789
06/05/2022 06:27:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/05/2022 06:27:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 06:27:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 06:27:11 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.1 on epoch=799
06/05/2022 06:27:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/05/2022 06:27:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 06:27:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/05/2022 06:27:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/05/2022 06:27:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/05/2022 06:27:26 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.12183908045977013 on epoch=816
06/05/2022 06:27:29 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
06/05/2022 06:27:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
06/05/2022 06:27:34 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 06:27:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 06:27:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=833
06/05/2022 06:27:41 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.1544477028347996 on epoch=833
06/05/2022 06:27:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 06:27:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=839
06/05/2022 06:27:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
06/05/2022 06:27:53 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=846
06/05/2022 06:27:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/05/2022 06:27:57 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.14813218390804597 on epoch=849
06/05/2022 06:28:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 06:28:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 06:28:05 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=859
06/05/2022 06:28:08 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=863
06/05/2022 06:28:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/05/2022 06:28:12 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.12993979200875752 on epoch=866
06/05/2022 06:28:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
06/05/2022 06:28:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/05/2022 06:28:21 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 06:28:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=879
06/05/2022 06:28:26 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 06:28:28 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.1470899470899471 on epoch=883
06/05/2022 06:28:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 06:28:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=889
06/05/2022 06:28:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/05/2022 06:28:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=896
06/05/2022 06:28:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 06:28:43 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.09961685823754789 on epoch=899
06/05/2022 06:28:45 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
06/05/2022 06:28:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 06:28:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
06/05/2022 06:28:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=913
06/05/2022 06:28:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 06:28:58 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.15391705069124426 on epoch=916
06/05/2022 06:29:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/05/2022 06:29:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
06/05/2022 06:29:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=926
06/05/2022 06:29:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 06:29:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
06/05/2022 06:29:13 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.14416006787330315 on epoch=933
06/05/2022 06:29:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=936
06/05/2022 06:29:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 06:29:22 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 06:29:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 06:29:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 06:29:29 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.1427750410509031 on epoch=949
06/05/2022 06:29:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/05/2022 06:29:34 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 06:29:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
06/05/2022 06:29:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/05/2022 06:29:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/05/2022 06:29:44 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.18963831867057676 on epoch=966
06/05/2022 06:29:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/05/2022 06:29:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
06/05/2022 06:29:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=976
06/05/2022 06:29:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 06:29:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
06/05/2022 06:30:00 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.14656084656084659 on epoch=983
06/05/2022 06:30:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
06/05/2022 06:30:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/05/2022 06:30:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=993
06/05/2022 06:30:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=996
06/05/2022 06:30:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 06:30:15 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.14481065918653577 on epoch=999
06/05/2022 06:30:15 - INFO - __main__ - save last model!
06/05/2022 06:30:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 06:30:15 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 06:30:15 - INFO - __main__ - Printing 3 examples
06/05/2022 06:30:15 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 06:30:15 - INFO - __main__ - ['contradiction']
06/05/2022 06:30:15 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 06:30:15 - INFO - __main__ - ['entailment']
06/05/2022 06:30:15 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 06:30:15 - INFO - __main__ - ['contradiction']
06/05/2022 06:30:15 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:30:15 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:30:15 - INFO - __main__ - Printing 3 examples
06/05/2022 06:30:15 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/05/2022 06:30:15 - INFO - __main__ - ['entailment']
06/05/2022 06:30:15 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/05/2022 06:30:15 - INFO - __main__ - ['entailment']
06/05/2022 06:30:15 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/05/2022 06:30:15 - INFO - __main__ - ['entailment']
06/05/2022 06:30:15 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:30:15 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:30:15 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 06:30:15 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:30:15 - INFO - __main__ - Printing 3 examples
06/05/2022 06:30:15 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/05/2022 06:30:15 - INFO - __main__ - ['entailment']
06/05/2022 06:30:15 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/05/2022 06:30:15 - INFO - __main__ - ['entailment']
06/05/2022 06:30:15 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/05/2022 06:30:15 - INFO - __main__ - ['entailment']
06/05/2022 06:30:15 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:30:15 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:30:15 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 06:30:16 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:30:17 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 06:30:35 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 06:30:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 06:30:36 - INFO - __main__ - Starting training!
06/05/2022 06:30:46 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_13_0.2_8_predictions.txt
06/05/2022 06:30:46 - INFO - __main__ - Classification-F1 on test data: 0.1493
06/05/2022 06:30:46 - INFO - __main__ - prefix=anli_16_13, lr=0.2, bsz=8, dev_performance=0.32424877707896577, test_performance=0.14934961745042732
06/05/2022 06:30:46 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.5, bsz=8 ...
06/05/2022 06:30:47 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:30:47 - INFO - __main__ - Printing 3 examples
06/05/2022 06:30:47 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/05/2022 06:30:47 - INFO - __main__ - ['entailment']
06/05/2022 06:30:47 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/05/2022 06:30:47 - INFO - __main__ - ['entailment']
06/05/2022 06:30:47 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/05/2022 06:30:47 - INFO - __main__ - ['entailment']
06/05/2022 06:30:47 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:30:47 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:30:47 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 06:30:47 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:30:47 - INFO - __main__ - Printing 3 examples
06/05/2022 06:30:47 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/05/2022 06:30:47 - INFO - __main__ - ['entailment']
06/05/2022 06:30:47 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/05/2022 06:30:47 - INFO - __main__ - ['entailment']
06/05/2022 06:30:47 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/05/2022 06:30:47 - INFO - __main__ - ['entailment']
06/05/2022 06:30:47 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:30:47 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:30:47 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 06:31:07 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 06:31:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 06:31:08 - INFO - __main__ - Starting training!
06/05/2022 06:31:12 - INFO - __main__ - Step 10 Global step 10 Train loss 0.94 on epoch=3
06/05/2022 06:31:15 - INFO - __main__ - Step 20 Global step 20 Train loss 0.71 on epoch=6
06/05/2022 06:31:17 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=9
06/05/2022 06:31:20 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=13
06/05/2022 06:31:23 - INFO - __main__ - Step 50 Global step 50 Train loss 0.61 on epoch=16
06/05/2022 06:31:24 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.15873015873015875 on epoch=16
06/05/2022 06:31:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15873015873015875 on epoch=16, global_step=50
06/05/2022 06:31:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=19
06/05/2022 06:31:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=23
06/05/2022 06:31:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=26
06/05/2022 06:31:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=29
06/05/2022 06:31:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=33
06/05/2022 06:31:39 - INFO - __main__ - Global step 100 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=33
06/05/2022 06:31:39 - INFO - __main__ - Saving model with best Classification-F1: 0.15873015873015875 -> 0.16666666666666666 on epoch=33, global_step=100
06/05/2022 06:31:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=36
06/05/2022 06:31:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=39
06/05/2022 06:31:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=43
06/05/2022 06:31:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=46
06/05/2022 06:31:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=49
06/05/2022 06:31:54 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=49
06/05/2022 06:31:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=53
06/05/2022 06:31:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=56
06/05/2022 06:32:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=59
06/05/2022 06:32:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=63
06/05/2022 06:32:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=66
06/05/2022 06:32:08 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
06/05/2022 06:32:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=69
06/05/2022 06:32:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
06/05/2022 06:32:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=76
06/05/2022 06:32:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=79
06/05/2022 06:32:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=83
06/05/2022 06:32:23 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.1693121693121693 on epoch=83
06/05/2022 06:32:23 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1693121693121693 on epoch=83, global_step=250
06/05/2022 06:32:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=86
06/05/2022 06:32:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=89
06/05/2022 06:32:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=93
06/05/2022 06:32:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=96
06/05/2022 06:32:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=99
06/05/2022 06:32:38 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.2117758784425451 on epoch=99
06/05/2022 06:32:38 - INFO - __main__ - Saving model with best Classification-F1: 0.1693121693121693 -> 0.2117758784425451 on epoch=99, global_step=300
06/05/2022 06:32:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=103
06/05/2022 06:32:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=106
06/05/2022 06:32:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
06/05/2022 06:32:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=113
06/05/2022 06:32:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=116
06/05/2022 06:32:53 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.22434875066454016 on epoch=116
06/05/2022 06:32:53 - INFO - __main__ - Saving model with best Classification-F1: 0.2117758784425451 -> 0.22434875066454016 on epoch=116, global_step=350
06/05/2022 06:32:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=119
06/05/2022 06:32:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=123
06/05/2022 06:33:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=126
06/05/2022 06:33:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=129
06/05/2022 06:33:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=133
06/05/2022 06:33:08 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.18707482993197277 on epoch=133
06/05/2022 06:33:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=136
06/05/2022 06:33:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=139
06/05/2022 06:33:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=143
06/05/2022 06:33:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=146
06/05/2022 06:33:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=149
06/05/2022 06:33:24 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.1837474120082816 on epoch=149
06/05/2022 06:33:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=153
06/05/2022 06:33:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=156
06/05/2022 06:33:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=159
06/05/2022 06:33:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=163
06/05/2022 06:33:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=166
06/05/2022 06:33:39 - INFO - __main__ - Global step 500 Train loss 0.36 Classification-F1 0.26575963718820855 on epoch=166
06/05/2022 06:33:39 - INFO - __main__ - Saving model with best Classification-F1: 0.22434875066454016 -> 0.26575963718820855 on epoch=166, global_step=500
06/05/2022 06:33:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=169
06/05/2022 06:33:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.31 on epoch=173
06/05/2022 06:33:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=176
06/05/2022 06:33:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=179
06/05/2022 06:33:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=183
06/05/2022 06:33:54 - INFO - __main__ - Global step 550 Train loss 0.32 Classification-F1 0.2623951182303585 on epoch=183
06/05/2022 06:33:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=186
06/05/2022 06:33:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=189
06/05/2022 06:34:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=193
06/05/2022 06:34:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=196
06/05/2022 06:34:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=199
06/05/2022 06:34:09 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.29848484848484846 on epoch=199
06/05/2022 06:34:09 - INFO - __main__ - Saving model with best Classification-F1: 0.26575963718820855 -> 0.29848484848484846 on epoch=199, global_step=600
06/05/2022 06:34:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=203
06/05/2022 06:34:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=206
06/05/2022 06:34:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=209
06/05/2022 06:34:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=213
06/05/2022 06:34:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=216
06/05/2022 06:34:24 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.27619047619047615 on epoch=216
06/05/2022 06:34:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=219
06/05/2022 06:34:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=223
06/05/2022 06:34:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=226
06/05/2022 06:34:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=229
06/05/2022 06:34:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=233
06/05/2022 06:34:39 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.30666666666666664 on epoch=233
06/05/2022 06:34:39 - INFO - __main__ - Saving model with best Classification-F1: 0.29848484848484846 -> 0.30666666666666664 on epoch=233, global_step=700
06/05/2022 06:34:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.15 on epoch=236
06/05/2022 06:34:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=239
06/05/2022 06:34:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=243
06/05/2022 06:34:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=246
06/05/2022 06:34:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=249
06/05/2022 06:34:54 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.2486449864498645 on epoch=249
06/05/2022 06:34:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=253
06/05/2022 06:35:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=256
06/05/2022 06:35:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=259
06/05/2022 06:35:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=263
06/05/2022 06:35:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=266
06/05/2022 06:35:09 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.22765957446808505 on epoch=266
06/05/2022 06:35:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=269
06/05/2022 06:35:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=273
06/05/2022 06:35:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=276
06/05/2022 06:35:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=279
06/05/2022 06:35:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=283
06/05/2022 06:35:24 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.24101307189542487 on epoch=283
06/05/2022 06:35:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=286
06/05/2022 06:35:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=289
06/05/2022 06:35:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=293
06/05/2022 06:35:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=296
06/05/2022 06:35:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
06/05/2022 06:35:39 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.1320385041315274 on epoch=299
06/05/2022 06:35:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=303
06/05/2022 06:35:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=306
06/05/2022 06:35:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=309
06/05/2022 06:35:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=313
06/05/2022 06:35:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
06/05/2022 06:35:54 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.21710526315789475 on epoch=316
06/05/2022 06:35:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=319
06/05/2022 06:35:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=323
06/05/2022 06:36:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
06/05/2022 06:36:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
06/05/2022 06:36:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=333
06/05/2022 06:36:09 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.1877034214495515 on epoch=333
06/05/2022 06:36:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
06/05/2022 06:36:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
06/05/2022 06:36:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=343
06/05/2022 06:36:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
06/05/2022 06:36:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=349
06/05/2022 06:36:24 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.2213020473890039 on epoch=349
06/05/2022 06:36:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=353
06/05/2022 06:36:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=356
06/05/2022 06:36:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=359
06/05/2022 06:36:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=363
06/05/2022 06:36:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
06/05/2022 06:36:40 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.19209130078695297 on epoch=366
06/05/2022 06:36:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=369
06/05/2022 06:36:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
06/05/2022 06:36:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
06/05/2022 06:36:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
06/05/2022 06:36:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=383
06/05/2022 06:36:55 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.21840411840411839 on epoch=383
06/05/2022 06:36:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
06/05/2022 06:37:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=389
06/05/2022 06:37:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
06/05/2022 06:37:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=396
06/05/2022 06:37:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=399
06/05/2022 06:37:11 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.20601332398316974 on epoch=399
06/05/2022 06:37:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
06/05/2022 06:37:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
06/05/2022 06:37:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
06/05/2022 06:37:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=413
06/05/2022 06:37:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
06/05/2022 06:37:27 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.1980952380952381 on epoch=416
06/05/2022 06:37:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
06/05/2022 06:37:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=423
06/05/2022 06:37:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=426
06/05/2022 06:37:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
06/05/2022 06:37:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
06/05/2022 06:37:42 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.30508157927512763 on epoch=433
06/05/2022 06:37:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
06/05/2022 06:37:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
06/05/2022 06:37:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=443
06/05/2022 06:37:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
06/05/2022 06:37:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
06/05/2022 06:37:57 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.2213628762541806 on epoch=449
06/05/2022 06:38:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
06/05/2022 06:38:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=456
06/05/2022 06:38:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
06/05/2022 06:38:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/05/2022 06:38:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
06/05/2022 06:38:12 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.20055555555555554 on epoch=466
06/05/2022 06:38:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
06/05/2022 06:38:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
06/05/2022 06:38:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
06/05/2022 06:38:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/05/2022 06:38:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
06/05/2022 06:38:28 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.1645161290322581 on epoch=483
06/05/2022 06:38:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
06/05/2022 06:38:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
06/05/2022 06:38:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/05/2022 06:38:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
06/05/2022 06:38:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
06/05/2022 06:38:43 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.23596829912619383 on epoch=499
06/05/2022 06:38:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
06/05/2022 06:38:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
06/05/2022 06:38:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
06/05/2022 06:38:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=513
06/05/2022 06:38:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
06/05/2022 06:38:58 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.16994847258005152 on epoch=516
06/05/2022 06:39:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/05/2022 06:39:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
06/05/2022 06:39:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
06/05/2022 06:39:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
06/05/2022 06:39:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/05/2022 06:39:14 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.22346153846153846 on epoch=533
06/05/2022 06:39:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=536
06/05/2022 06:39:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
06/05/2022 06:39:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/05/2022 06:39:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
06/05/2022 06:39:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
06/05/2022 06:39:29 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.17220438363814045 on epoch=549
06/05/2022 06:39:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/05/2022 06:39:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=556
06/05/2022 06:39:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/05/2022 06:39:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/05/2022 06:39:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
06/05/2022 06:39:44 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.17223346828609987 on epoch=566
06/05/2022 06:39:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/05/2022 06:39:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/05/2022 06:39:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/05/2022 06:39:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/05/2022 06:39:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/05/2022 06:40:00 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.17754199211999722 on epoch=583
06/05/2022 06:40:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
06/05/2022 06:40:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/05/2022 06:40:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/05/2022 06:40:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/05/2022 06:40:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
06/05/2022 06:40:15 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.2667413957736538 on epoch=599
06/05/2022 06:40:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/05/2022 06:40:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
06/05/2022 06:40:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/05/2022 06:40:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/05/2022 06:40:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 06:40:30 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.2025 on epoch=616
06/05/2022 06:40:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
06/05/2022 06:40:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/05/2022 06:40:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/05/2022 06:40:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/05/2022 06:40:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/05/2022 06:40:45 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.18681318681318684 on epoch=633
06/05/2022 06:40:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/05/2022 06:40:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/05/2022 06:40:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
06/05/2022 06:40:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/05/2022 06:40:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=649
06/05/2022 06:41:00 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.22386568386568384 on epoch=649
06/05/2022 06:41:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/05/2022 06:41:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
06/05/2022 06:41:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/05/2022 06:41:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 06:41:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/05/2022 06:41:16 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.22227083052900798 on epoch=666
06/05/2022 06:41:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/05/2022 06:41:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
06/05/2022 06:41:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/05/2022 06:41:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/05/2022 06:41:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/05/2022 06:41:31 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.24144144144144145 on epoch=683
06/05/2022 06:41:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 06:41:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/05/2022 06:41:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=693
06/05/2022 06:41:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/05/2022 06:41:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/05/2022 06:41:45 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.23436857260386673 on epoch=699
06/05/2022 06:41:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/05/2022 06:41:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/05/2022 06:41:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/05/2022 06:41:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/05/2022 06:42:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/05/2022 06:42:01 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.2745123678762123 on epoch=716
06/05/2022 06:42:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/05/2022 06:42:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/05/2022 06:42:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/05/2022 06:42:12 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=729
06/05/2022 06:42:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/05/2022 06:42:16 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.2371794871794872 on epoch=733
06/05/2022 06:42:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/05/2022 06:42:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/05/2022 06:42:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 06:42:27 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 06:42:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
06/05/2022 06:42:31 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.1875 on epoch=749
06/05/2022 06:42:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 06:42:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
06/05/2022 06:42:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/05/2022 06:42:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/05/2022 06:42:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/05/2022 06:42:46 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.2708333333333333 on epoch=766
06/05/2022 06:42:49 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 06:42:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/05/2022 06:42:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
06/05/2022 06:42:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 06:43:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/05/2022 06:43:02 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.25003169371196754 on epoch=783
06/05/2022 06:43:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 06:43:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 06:43:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
06/05/2022 06:43:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 06:43:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 06:43:17 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.28347701149425286 on epoch=799
06/05/2022 06:43:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 06:43:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 06:43:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 06:43:28 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 06:43:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/05/2022 06:43:32 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.2814153439153439 on epoch=816
06/05/2022 06:43:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
06/05/2022 06:43:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 06:43:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 06:43:43 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 06:43:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
06/05/2022 06:43:47 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.24383797406109778 on epoch=833
06/05/2022 06:43:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
06/05/2022 06:43:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 06:43:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
06/05/2022 06:43:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/05/2022 06:44:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/05/2022 06:44:02 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.1982679167328778 on epoch=849
06/05/2022 06:44:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 06:44:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
06/05/2022 06:44:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 06:44:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/05/2022 06:44:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 06:44:17 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.2829131652661065 on epoch=866
06/05/2022 06:44:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 06:44:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
06/05/2022 06:44:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 06:44:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 06:44:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 06:44:32 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.2639365120441655 on epoch=883
06/05/2022 06:44:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 06:44:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 06:44:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 06:44:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/05/2022 06:44:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 06:44:47 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.28725490196078435 on epoch=899
06/05/2022 06:44:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 06:44:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=906
06/05/2022 06:44:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 06:44:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 06:45:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/05/2022 06:45:03 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.33147872592784283 on epoch=916
06/05/2022 06:45:03 - INFO - __main__ - Saving model with best Classification-F1: 0.30666666666666664 -> 0.33147872592784283 on epoch=916, global_step=2750
06/05/2022 06:45:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 06:45:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 06:45:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 06:45:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 06:45:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 06:45:18 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.2305442260948379 on epoch=933
06/05/2022 06:45:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
06/05/2022 06:45:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 06:45:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/05/2022 06:45:29 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 06:45:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 06:45:33 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.301010101010101 on epoch=949
06/05/2022 06:45:36 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 06:45:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 06:45:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 06:45:44 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/05/2022 06:45:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/05/2022 06:45:48 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.2986651835372636 on epoch=966
06/05/2022 06:45:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 06:45:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 06:45:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 06:45:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=979
06/05/2022 06:46:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 06:46:03 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.20114942528735635 on epoch=983
06/05/2022 06:46:05 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 06:46:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 06:46:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 06:46:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 06:46:16 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 06:46:17 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.2996480643539467 on epoch=999
06/05/2022 06:46:17 - INFO - __main__ - save last model!
06/05/2022 06:46:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 06:46:18 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 06:46:18 - INFO - __main__ - Printing 3 examples
06/05/2022 06:46:18 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 06:46:18 - INFO - __main__ - ['contradiction']
06/05/2022 06:46:18 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 06:46:18 - INFO - __main__ - ['entailment']
06/05/2022 06:46:18 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 06:46:18 - INFO - __main__ - ['contradiction']
06/05/2022 06:46:18 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:46:18 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:46:18 - INFO - __main__ - Printing 3 examples
06/05/2022 06:46:18 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/05/2022 06:46:18 - INFO - __main__ - ['entailment']
06/05/2022 06:46:18 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/05/2022 06:46:18 - INFO - __main__ - ['entailment']
06/05/2022 06:46:18 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/05/2022 06:46:18 - INFO - __main__ - ['entailment']
06/05/2022 06:46:18 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:46:18 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:46:18 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 06:46:18 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:46:18 - INFO - __main__ - Printing 3 examples
06/05/2022 06:46:18 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/05/2022 06:46:18 - INFO - __main__ - ['entailment']
06/05/2022 06:46:18 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/05/2022 06:46:18 - INFO - __main__ - ['entailment']
06/05/2022 06:46:18 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/05/2022 06:46:18 - INFO - __main__ - ['entailment']
06/05/2022 06:46:18 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:46:18 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:46:18 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 06:46:18 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:46:19 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 06:46:37 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 06:46:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 06:46:38 - INFO - __main__ - Starting training!
06/05/2022 06:46:49 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_21_0.5_8_predictions.txt
06/05/2022 06:46:49 - INFO - __main__ - Classification-F1 on test data: 0.2359
06/05/2022 06:46:49 - INFO - __main__ - prefix=anli_16_21, lr=0.5, bsz=8, dev_performance=0.33147872592784283, test_performance=0.23585728789830201
06/05/2022 06:46:49 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.4, bsz=8 ...
06/05/2022 06:46:50 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:46:50 - INFO - __main__ - Printing 3 examples
06/05/2022 06:46:50 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/05/2022 06:46:50 - INFO - __main__ - ['entailment']
06/05/2022 06:46:50 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/05/2022 06:46:50 - INFO - __main__ - ['entailment']
06/05/2022 06:46:50 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/05/2022 06:46:50 - INFO - __main__ - ['entailment']
06/05/2022 06:46:50 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:46:50 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:46:50 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 06:46:50 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 06:46:50 - INFO - __main__ - Printing 3 examples
06/05/2022 06:46:50 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/05/2022 06:46:50 - INFO - __main__ - ['entailment']
06/05/2022 06:46:50 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/05/2022 06:46:50 - INFO - __main__ - ['entailment']
06/05/2022 06:46:50 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/05/2022 06:46:50 - INFO - __main__ - ['entailment']
06/05/2022 06:46:50 - INFO - __main__ - Tokenizing Input ...
06/05/2022 06:46:50 - INFO - __main__ - Tokenizing Output ...
06/05/2022 06:46:50 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 06:47:09 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 06:47:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 06:47:10 - INFO - __main__ - Starting training!
06/05/2022 06:47:14 - INFO - __main__ - Step 10 Global step 10 Train loss 0.99 on epoch=3
06/05/2022 06:47:16 - INFO - __main__ - Step 20 Global step 20 Train loss 0.66 on epoch=6
06/05/2022 06:47:19 - INFO - __main__ - Step 30 Global step 30 Train loss 0.69 on epoch=9
06/05/2022 06:47:22 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=13
06/05/2022 06:47:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.63 on epoch=16
06/05/2022 06:47:26 - INFO - __main__ - Global step 50 Train loss 0.71 Classification-F1 0.16666666666666666 on epoch=16
06/05/2022 06:47:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/05/2022 06:47:29 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=19
06/05/2022 06:47:31 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=23
06/05/2022 06:47:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=26
06/05/2022 06:47:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=29
06/05/2022 06:47:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=33
06/05/2022 06:47:40 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=33
06/05/2022 06:47:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=36
06/05/2022 06:47:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=39
06/05/2022 06:47:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.62 on epoch=43
06/05/2022 06:47:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=46
06/05/2022 06:47:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=49
06/05/2022 06:47:55 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.24444444444444446 on epoch=49
06/05/2022 06:47:55 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.24444444444444446 on epoch=49, global_step=150
06/05/2022 06:47:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
06/05/2022 06:48:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
06/05/2022 06:48:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=59
06/05/2022 06:48:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=63
06/05/2022 06:48:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=66
06/05/2022 06:48:10 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.21412109784202804 on epoch=66
06/05/2022 06:48:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=69
06/05/2022 06:48:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=73
06/05/2022 06:48:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=76
06/05/2022 06:48:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=79
06/05/2022 06:48:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=83
06/05/2022 06:48:25 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.1766959657426189 on epoch=83
06/05/2022 06:48:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=86
06/05/2022 06:48:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=89
06/05/2022 06:48:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=93
06/05/2022 06:48:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=96
06/05/2022 06:48:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=99
06/05/2022 06:48:39 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.21398124467178178 on epoch=99
06/05/2022 06:48:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=103
06/05/2022 06:48:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=106
06/05/2022 06:48:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=109
06/05/2022 06:48:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.32 on epoch=113
06/05/2022 06:48:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=116
06/05/2022 06:48:54 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.24973942047112777 on epoch=116
06/05/2022 06:48:54 - INFO - __main__ - Saving model with best Classification-F1: 0.24444444444444446 -> 0.24973942047112777 on epoch=116, global_step=350
06/05/2022 06:48:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=119
06/05/2022 06:48:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=123
06/05/2022 06:49:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=126
06/05/2022 06:49:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=129
06/05/2022 06:49:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=133
06/05/2022 06:49:09 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.2216841538875437 on epoch=133
06/05/2022 06:49:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=136
06/05/2022 06:49:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=139
06/05/2022 06:49:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.18 on epoch=143
06/05/2022 06:49:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.14 on epoch=146
06/05/2022 06:49:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.11 on epoch=149
06/05/2022 06:49:24 - INFO - __main__ - Global step 450 Train loss 0.17 Classification-F1 0.14784313725490197 on epoch=149
06/05/2022 06:49:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=153
06/05/2022 06:49:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=156
06/05/2022 06:49:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.15 on epoch=159
06/05/2022 06:49:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=163
06/05/2022 06:49:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=166
06/05/2022 06:49:39 - INFO - __main__ - Global step 500 Train loss 0.14 Classification-F1 0.15030946065428824 on epoch=166
06/05/2022 06:49:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=169
06/05/2022 06:49:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.09 on epoch=173
06/05/2022 06:49:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=176
06/05/2022 06:49:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=179
06/05/2022 06:49:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.10 on epoch=183
06/05/2022 06:49:54 - INFO - __main__ - Global step 550 Train loss 0.10 Classification-F1 0.18157423971377462 on epoch=183
06/05/2022 06:49:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=186
06/05/2022 06:49:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=189
06/05/2022 06:50:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.08 on epoch=193
06/05/2022 06:50:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=196
06/05/2022 06:50:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=199
06/05/2022 06:50:08 - INFO - __main__ - Global step 600 Train loss 0.05 Classification-F1 0.2372294372294372 on epoch=199
06/05/2022 06:50:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=203
06/05/2022 06:50:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=206
06/05/2022 06:50:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=209
06/05/2022 06:50:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=213
06/05/2022 06:50:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=216
06/05/2022 06:50:23 - INFO - __main__ - Global step 650 Train loss 0.10 Classification-F1 0.1716589861751152 on epoch=216
06/05/2022 06:50:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=219
06/05/2022 06:50:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=223
06/05/2022 06:50:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=226
06/05/2022 06:50:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=229
06/05/2022 06:50:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=233
06/05/2022 06:50:38 - INFO - __main__ - Global step 700 Train loss 0.03 Classification-F1 0.13449920508744037 on epoch=233
06/05/2022 06:50:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=236
06/05/2022 06:50:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=239
06/05/2022 06:50:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=243
06/05/2022 06:50:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=246
06/05/2022 06:50:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=249
06/05/2022 06:50:53 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.10859728506787329 on epoch=249
06/05/2022 06:50:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=253
06/05/2022 06:50:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=256
06/05/2022 06:51:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=259
06/05/2022 06:51:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=263
06/05/2022 06:51:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=266
06/05/2022 06:51:08 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.16185383244206775 on epoch=266
06/05/2022 06:51:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=269
06/05/2022 06:51:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=273
06/05/2022 06:51:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=276
06/05/2022 06:51:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=279
06/05/2022 06:51:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=283
06/05/2022 06:51:23 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.10387153266410232 on epoch=283
06/05/2022 06:51:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=286
06/05/2022 06:51:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
06/05/2022 06:51:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=293
06/05/2022 06:51:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=296
06/05/2022 06:51:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=299
06/05/2022 06:51:38 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.08530405405405406 on epoch=299
06/05/2022 06:51:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=303
06/05/2022 06:51:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=306
06/05/2022 06:51:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=309
06/05/2022 06:51:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=313
06/05/2022 06:51:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=316
06/05/2022 06:51:53 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.14422994422994423 on epoch=316
06/05/2022 06:51:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=319
06/05/2022 06:51:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=323
06/05/2022 06:52:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=326
06/05/2022 06:52:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
06/05/2022 06:52:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=333
06/05/2022 06:52:08 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.11878787878787878 on epoch=333
06/05/2022 06:52:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=336
06/05/2022 06:52:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=339
06/05/2022 06:52:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=343
06/05/2022 06:52:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
06/05/2022 06:52:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=349
06/05/2022 06:52:24 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.15512820512820513 on epoch=349
06/05/2022 06:52:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=353
06/05/2022 06:52:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=356
06/05/2022 06:52:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=359
06/05/2022 06:52:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=363
06/05/2022 06:52:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=366
06/05/2022 06:52:39 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.12424242424242424 on epoch=366
06/05/2022 06:52:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=369
06/05/2022 06:52:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/05/2022 06:52:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=376
06/05/2022 06:52:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=379
06/05/2022 06:52:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=383
06/05/2022 06:52:54 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.15720121334681497 on epoch=383
06/05/2022 06:52:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=386
06/05/2022 06:52:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
06/05/2022 06:53:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
06/05/2022 06:53:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=396
06/05/2022 06:53:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
06/05/2022 06:53:09 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.11598248494800219 on epoch=399
06/05/2022 06:53:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=403
06/05/2022 06:53:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
06/05/2022 06:53:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=409
06/05/2022 06:53:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
06/05/2022 06:53:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=416
06/05/2022 06:53:24 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.13753320683111955 on epoch=416
06/05/2022 06:53:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=419
06/05/2022 06:53:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=423
06/05/2022 06:53:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=426
06/05/2022 06:53:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
06/05/2022 06:53:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
06/05/2022 06:53:39 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.10404040404040404 on epoch=433
06/05/2022 06:53:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
06/05/2022 06:53:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=439
06/05/2022 06:53:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
06/05/2022 06:53:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
06/05/2022 06:53:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
06/05/2022 06:53:54 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.1576838308956121 on epoch=449
06/05/2022 06:53:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
06/05/2022 06:54:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=456
06/05/2022 06:54:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
06/05/2022 06:54:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
06/05/2022 06:54:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
06/05/2022 06:54:10 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.13418976786116746 on epoch=466
06/05/2022 06:54:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=469
06/05/2022 06:54:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
06/05/2022 06:54:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
06/05/2022 06:54:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
06/05/2022 06:54:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
06/05/2022 06:54:25 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.09 on epoch=483
06/05/2022 06:54:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
06/05/2022 06:54:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
06/05/2022 06:54:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/05/2022 06:54:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
06/05/2022 06:54:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/05/2022 06:54:40 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.10009775171065494 on epoch=499
06/05/2022 06:54:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
06/05/2022 06:54:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
06/05/2022 06:54:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
06/05/2022 06:54:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=513
06/05/2022 06:54:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
06/05/2022 06:54:54 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.12701612903225806 on epoch=516
06/05/2022 06:54:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/05/2022 06:55:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
06/05/2022 06:55:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
06/05/2022 06:55:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/05/2022 06:55:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/05/2022 06:55:10 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.10771604938271606 on epoch=533
06/05/2022 06:55:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
06/05/2022 06:55:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/05/2022 06:55:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/05/2022 06:55:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
06/05/2022 06:55:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/05/2022 06:55:25 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.12220113851992412 on epoch=549
06/05/2022 06:55:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
06/05/2022 06:55:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
06/05/2022 06:55:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/05/2022 06:55:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/05/2022 06:55:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
06/05/2022 06:55:40 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.10955882352941178 on epoch=566
06/05/2022 06:55:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/05/2022 06:55:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/05/2022 06:55:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/05/2022 06:55:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/05/2022 06:55:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/05/2022 06:55:55 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.12425324675324675 on epoch=583
06/05/2022 06:55:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=586
06/05/2022 06:56:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/05/2022 06:56:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/05/2022 06:56:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/05/2022 06:56:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/05/2022 06:56:10 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.15718157181571815 on epoch=599
06/05/2022 06:56:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/05/2022 06:56:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/05/2022 06:56:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/05/2022 06:56:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/05/2022 06:56:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 06:56:25 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.12938099389712293 on epoch=616
06/05/2022 06:56:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/05/2022 06:56:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/05/2022 06:56:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
06/05/2022 06:56:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/05/2022 06:56:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/05/2022 06:56:40 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.16518978605935125 on epoch=633
06/05/2022 06:56:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/05/2022 06:56:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/05/2022 06:56:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/05/2022 06:56:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/05/2022 06:56:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/05/2022 06:56:55 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.07821969696969695 on epoch=649
06/05/2022 06:56:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/05/2022 06:57:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/05/2022 06:57:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/05/2022 06:57:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 06:57:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
06/05/2022 06:57:11 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.12225877192982455 on epoch=666
06/05/2022 06:57:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/05/2022 06:57:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/05/2022 06:57:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/05/2022 06:57:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/05/2022 06:57:24 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/05/2022 06:57:26 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.08869047619047618 on epoch=683
06/05/2022 06:57:29 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 06:57:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/05/2022 06:57:34 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/05/2022 06:57:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/05/2022 06:57:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/05/2022 06:57:41 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.11183261183261183 on epoch=699
06/05/2022 06:57:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/05/2022 06:57:46 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/05/2022 06:57:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/05/2022 06:57:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/05/2022 06:57:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/05/2022 06:57:56 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.10763888888888888 on epoch=716
06/05/2022 06:57:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/05/2022 06:58:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/05/2022 06:58:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/05/2022 06:58:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=729
06/05/2022 06:58:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/05/2022 06:58:12 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.1524767801857585 on epoch=733
06/05/2022 06:58:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/05/2022 06:58:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/05/2022 06:58:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 06:58:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 06:58:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/05/2022 06:58:27 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.09776731205302634 on epoch=749
06/05/2022 06:58:30 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 06:58:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/05/2022 06:58:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
06/05/2022 06:58:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/05/2022 06:58:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/05/2022 06:58:42 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.08584595128373931 on epoch=766
06/05/2022 06:58:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 06:58:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/05/2022 06:58:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
06/05/2022 06:58:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 06:58:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
06/05/2022 06:58:57 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.0920404459424423 on epoch=783
06/05/2022 06:59:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 06:59:03 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 06:59:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/05/2022 06:59:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 06:59:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 06:59:13 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.08850891708034565 on epoch=799
06/05/2022 06:59:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 06:59:18 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 06:59:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
06/05/2022 06:59:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 06:59:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/05/2022 06:59:28 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.10989010989010989 on epoch=816
06/05/2022 06:59:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/05/2022 06:59:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 06:59:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 06:59:40 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 06:59:42 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 06:59:44 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.07775477340694732 on epoch=833
06/05/2022 06:59:46 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 06:59:49 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 06:59:52 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/05/2022 06:59:55 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/05/2022 06:59:57 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/05/2022 06:59:59 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.1292270531400966 on epoch=849
06/05/2022 07:00:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 07:00:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 07:00:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 07:00:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/05/2022 07:00:13 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 07:00:14 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.1470175438596491 on epoch=866
06/05/2022 07:00:17 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 07:00:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/05/2022 07:00:22 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 07:00:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/05/2022 07:00:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 07:00:29 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.10288032454361055 on epoch=883
06/05/2022 07:00:32 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=886
06/05/2022 07:00:35 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/05/2022 07:00:38 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 07:00:40 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/05/2022 07:00:43 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 07:00:45 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.15183888575192922 on epoch=899
06/05/2022 07:00:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=903
06/05/2022 07:00:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 07:00:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
06/05/2022 07:00:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 07:00:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 07:01:00 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.12285714285714286 on epoch=916
06/05/2022 07:01:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
06/05/2022 07:01:05 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 07:01:08 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 07:01:11 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 07:01:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 07:01:15 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.15292076344707922 on epoch=933
06/05/2022 07:01:17 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 07:01:20 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 07:01:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/05/2022 07:01:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 07:01:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 07:01:30 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.07987711213517666 on epoch=949
06/05/2022 07:01:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 07:01:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 07:01:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 07:01:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 07:01:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/05/2022 07:01:45 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.13465958627248947 on epoch=966
06/05/2022 07:01:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 07:01:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 07:01:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 07:01:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 07:01:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 07:02:00 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.24835526315789475 on epoch=983
06/05/2022 07:02:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 07:02:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 07:02:09 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 07:02:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 07:02:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 07:02:16 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.22898954703832752 on epoch=999
06/05/2022 07:02:16 - INFO - __main__ - save last model!
06/05/2022 07:02:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 07:02:16 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:02:16 - INFO - __main__ - Printing 3 examples
06/05/2022 07:02:16 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/05/2022 07:02:16 - INFO - __main__ - ['entailment']
06/05/2022 07:02:16 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/05/2022 07:02:16 - INFO - __main__ - ['entailment']
06/05/2022 07:02:16 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/05/2022 07:02:16 - INFO - __main__ - ['entailment']
06/05/2022 07:02:16 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:02:16 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 07:02:16 - INFO - __main__ - Printing 3 examples
06/05/2022 07:02:16 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 07:02:16 - INFO - __main__ - ['contradiction']
06/05/2022 07:02:16 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 07:02:16 - INFO - __main__ - ['entailment']
06/05/2022 07:02:16 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 07:02:16 - INFO - __main__ - ['contradiction']
06/05/2022 07:02:16 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:02:16 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:02:16 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 07:02:16 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:02:16 - INFO - __main__ - Printing 3 examples
06/05/2022 07:02:16 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/05/2022 07:02:16 - INFO - __main__ - ['entailment']
06/05/2022 07:02:16 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/05/2022 07:02:16 - INFO - __main__ - ['entailment']
06/05/2022 07:02:16 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/05/2022 07:02:16 - INFO - __main__ - ['entailment']
06/05/2022 07:02:16 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:02:16 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:02:16 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 07:02:16 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:02:17 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 07:02:32 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 07:02:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 07:02:33 - INFO - __main__ - Starting training!
06/05/2022 07:02:48 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_21_0.4_8_predictions.txt
06/05/2022 07:02:48 - INFO - __main__ - Classification-F1 on test data: 0.1060
06/05/2022 07:02:48 - INFO - __main__ - prefix=anli_16_21, lr=0.4, bsz=8, dev_performance=0.24973942047112777, test_performance=0.10599030814719942
06/05/2022 07:02:48 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.3, bsz=8 ...
06/05/2022 07:02:49 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:02:49 - INFO - __main__ - Printing 3 examples
06/05/2022 07:02:49 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/05/2022 07:02:49 - INFO - __main__ - ['entailment']
06/05/2022 07:02:49 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/05/2022 07:02:49 - INFO - __main__ - ['entailment']
06/05/2022 07:02:49 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/05/2022 07:02:49 - INFO - __main__ - ['entailment']
06/05/2022 07:02:49 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:02:49 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:02:49 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 07:02:49 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:02:49 - INFO - __main__ - Printing 3 examples
06/05/2022 07:02:49 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/05/2022 07:02:49 - INFO - __main__ - ['entailment']
06/05/2022 07:02:49 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/05/2022 07:02:49 - INFO - __main__ - ['entailment']
06/05/2022 07:02:49 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/05/2022 07:02:49 - INFO - __main__ - ['entailment']
06/05/2022 07:02:49 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:02:49 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:02:49 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 07:03:09 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 07:03:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 07:03:10 - INFO - __main__ - Starting training!
06/05/2022 07:03:13 - INFO - __main__ - Step 10 Global step 10 Train loss 1.11 on epoch=3
06/05/2022 07:03:16 - INFO - __main__ - Step 20 Global step 20 Train loss 0.76 on epoch=6
06/05/2022 07:03:19 - INFO - __main__ - Step 30 Global step 30 Train loss 0.63 on epoch=9
06/05/2022 07:03:22 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=13
06/05/2022 07:03:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.68 on epoch=16
06/05/2022 07:03:26 - INFO - __main__ - Global step 50 Train loss 0.75 Classification-F1 0.16666666666666666 on epoch=16
06/05/2022 07:03:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/05/2022 07:03:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=19
06/05/2022 07:03:31 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=23
06/05/2022 07:03:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.62 on epoch=26
06/05/2022 07:03:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=29
06/05/2022 07:03:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=33
06/05/2022 07:03:41 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.3288888888888889 on epoch=33
06/05/2022 07:03:41 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.3288888888888889 on epoch=33, global_step=100
06/05/2022 07:03:44 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=36
06/05/2022 07:03:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=39
06/05/2022 07:03:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=43
06/05/2022 07:03:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=46
06/05/2022 07:03:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=49
06/05/2022 07:03:56 - INFO - __main__ - Global step 150 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=49
06/05/2022 07:03:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=53
06/05/2022 07:04:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=56
06/05/2022 07:04:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=59
06/05/2022 07:04:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=63
06/05/2022 07:04:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=66
06/05/2022 07:04:11 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.2770673486786019 on epoch=66
06/05/2022 07:04:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=69
06/05/2022 07:04:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=73
06/05/2022 07:04:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=76
06/05/2022 07:04:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=79
06/05/2022 07:04:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=83
06/05/2022 07:04:25 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.3017902813299233 on epoch=83
06/05/2022 07:04:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=86
06/05/2022 07:04:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=89
06/05/2022 07:04:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
06/05/2022 07:04:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=96
06/05/2022 07:04:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=99
06/05/2022 07:04:40 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.22434875066454016 on epoch=99
06/05/2022 07:04:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=103
06/05/2022 07:04:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=106
06/05/2022 07:04:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=109
06/05/2022 07:04:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=113
06/05/2022 07:04:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=116
06/05/2022 07:04:55 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.25703846980442724 on epoch=116
06/05/2022 07:04:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=119
06/05/2022 07:05:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=123
06/05/2022 07:05:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=126
06/05/2022 07:05:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=129
06/05/2022 07:05:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=133
06/05/2022 07:05:10 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3138029405310511 on epoch=133
06/05/2022 07:05:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=136
06/05/2022 07:05:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=139
06/05/2022 07:05:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=143
06/05/2022 07:05:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=146
06/05/2022 07:05:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=149
06/05/2022 07:05:25 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.2355555555555556 on epoch=149
06/05/2022 07:05:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=153
06/05/2022 07:05:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=156
06/05/2022 07:05:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=159
06/05/2022 07:05:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=163
06/05/2022 07:05:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=166
06/05/2022 07:05:40 - INFO - __main__ - Global step 500 Train loss 0.28 Classification-F1 0.3582608695652174 on epoch=166
06/05/2022 07:05:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3288888888888889 -> 0.3582608695652174 on epoch=166, global_step=500
06/05/2022 07:05:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=169
06/05/2022 07:05:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=173
06/05/2022 07:05:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=176
06/05/2022 07:05:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=179
06/05/2022 07:05:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=183
06/05/2022 07:05:55 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3628441275500099 on epoch=183
06/05/2022 07:05:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3582608695652174 -> 0.3628441275500099 on epoch=183, global_step=550
06/05/2022 07:05:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=186
06/05/2022 07:06:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=189
06/05/2022 07:06:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=193
06/05/2022 07:06:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=196
06/05/2022 07:06:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=199
06/05/2022 07:06:10 - INFO - __main__ - Global step 600 Train loss 0.15 Classification-F1 0.22771575403154348 on epoch=199
06/05/2022 07:06:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.16 on epoch=203
06/05/2022 07:06:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=206
06/05/2022 07:06:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=209
06/05/2022 07:06:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=213
06/05/2022 07:06:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=216
06/05/2022 07:06:25 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.2812916908972736 on epoch=216
06/05/2022 07:06:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.10 on epoch=219
06/05/2022 07:06:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=223
06/05/2022 07:06:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.11 on epoch=226
06/05/2022 07:06:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=229
06/05/2022 07:06:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=233
06/05/2022 07:06:40 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.3679095762429096 on epoch=233
06/05/2022 07:06:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3628441275500099 -> 0.3679095762429096 on epoch=233, global_step=700
06/05/2022 07:06:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=236
06/05/2022 07:06:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=239
06/05/2022 07:06:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=243
06/05/2022 07:06:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=246
06/05/2022 07:06:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=249
06/05/2022 07:06:56 - INFO - __main__ - Global step 750 Train loss 0.09 Classification-F1 0.3309292770347058 on epoch=249
06/05/2022 07:06:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=253
06/05/2022 07:07:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=256
06/05/2022 07:07:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=259
06/05/2022 07:07:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=263
06/05/2022 07:07:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=266
06/05/2022 07:07:11 - INFO - __main__ - Global step 800 Train loss 0.08 Classification-F1 0.2489655172413793 on epoch=266
06/05/2022 07:07:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=269
06/05/2022 07:07:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=273
06/05/2022 07:07:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=276
06/05/2022 07:07:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=279
06/05/2022 07:07:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=283
06/05/2022 07:07:26 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.3238095238095238 on epoch=283
06/05/2022 07:07:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=286
06/05/2022 07:07:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=289
06/05/2022 07:07:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=293
06/05/2022 07:07:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=296
06/05/2022 07:07:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=299
06/05/2022 07:07:42 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.2326007326007326 on epoch=299
06/05/2022 07:07:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=303
06/05/2022 07:07:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=306
06/05/2022 07:07:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=309
06/05/2022 07:07:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=313
06/05/2022 07:07:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=316
06/05/2022 07:07:57 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.21105072463768113 on epoch=316
06/05/2022 07:08:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=319
06/05/2022 07:08:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=323
06/05/2022 07:08:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
06/05/2022 07:08:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
06/05/2022 07:08:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=333
06/05/2022 07:08:12 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.18936992900608518 on epoch=333
06/05/2022 07:08:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
06/05/2022 07:08:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=339
06/05/2022 07:08:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=343
06/05/2022 07:08:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=346
06/05/2022 07:08:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=349
06/05/2022 07:08:28 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.307671255947118 on epoch=349
06/05/2022 07:08:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=353
06/05/2022 07:08:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=356
06/05/2022 07:08:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=359
06/05/2022 07:08:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=363
06/05/2022 07:08:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
06/05/2022 07:08:43 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.20957019171304883 on epoch=366
06/05/2022 07:08:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
06/05/2022 07:08:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/05/2022 07:08:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
06/05/2022 07:08:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=379
06/05/2022 07:08:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
06/05/2022 07:08:59 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.23202898550724638 on epoch=383
06/05/2022 07:09:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
06/05/2022 07:09:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
06/05/2022 07:09:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
06/05/2022 07:09:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
06/05/2022 07:09:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=399
06/05/2022 07:09:14 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.2721661054994388 on epoch=399
06/05/2022 07:09:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
06/05/2022 07:09:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=406
06/05/2022 07:09:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
06/05/2022 07:09:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=413
06/05/2022 07:09:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
06/05/2022 07:09:29 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.31521856332621684 on epoch=416
06/05/2022 07:09:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
06/05/2022 07:09:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
06/05/2022 07:09:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=426
06/05/2022 07:09:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
06/05/2022 07:09:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=433
06/05/2022 07:09:44 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.31209150326797386 on epoch=433
06/05/2022 07:09:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
06/05/2022 07:09:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/05/2022 07:09:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
06/05/2022 07:09:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
06/05/2022 07:09:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=449
06/05/2022 07:09:59 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.2926950354609929 on epoch=449
06/05/2022 07:10:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
06/05/2022 07:10:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=456
06/05/2022 07:10:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
06/05/2022 07:10:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/05/2022 07:10:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
06/05/2022 07:10:15 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.2913165266106443 on epoch=466
06/05/2022 07:10:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
06/05/2022 07:10:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=473
06/05/2022 07:10:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
06/05/2022 07:10:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/05/2022 07:10:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
06/05/2022 07:10:30 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.27846705266060107 on epoch=483
06/05/2022 07:10:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/05/2022 07:10:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
06/05/2022 07:10:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/05/2022 07:10:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
06/05/2022 07:10:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
06/05/2022 07:10:45 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.2719797596457938 on epoch=499
06/05/2022 07:10:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
06/05/2022 07:10:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/05/2022 07:10:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
06/05/2022 07:10:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
06/05/2022 07:10:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
06/05/2022 07:11:01 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.13596176821983272 on epoch=516
06/05/2022 07:11:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/05/2022 07:11:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
06/05/2022 07:11:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
06/05/2022 07:11:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
06/05/2022 07:11:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/05/2022 07:11:16 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.241231884057971 on epoch=533
06/05/2022 07:11:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
06/05/2022 07:11:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
06/05/2022 07:11:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
06/05/2022 07:11:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/05/2022 07:11:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/05/2022 07:11:32 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.18115942028985504 on epoch=549
06/05/2022 07:11:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
06/05/2022 07:11:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
06/05/2022 07:11:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/05/2022 07:11:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/05/2022 07:11:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
06/05/2022 07:11:48 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.31825059101654846 on epoch=566
06/05/2022 07:11:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
06/05/2022 07:11:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
06/05/2022 07:11:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/05/2022 07:11:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/05/2022 07:12:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/05/2022 07:12:03 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.1814958091553836 on epoch=583
06/05/2022 07:12:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/05/2022 07:12:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
06/05/2022 07:12:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/05/2022 07:12:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
06/05/2022 07:12:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/05/2022 07:12:18 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.21955375253549697 on epoch=599
06/05/2022 07:12:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/05/2022 07:12:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/05/2022 07:12:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/05/2022 07:12:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/05/2022 07:12:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 07:12:34 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.145297157622739 on epoch=616
06/05/2022 07:12:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/05/2022 07:12:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
06/05/2022 07:12:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/05/2022 07:12:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/05/2022 07:12:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/05/2022 07:12:49 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.16137320044296785 on epoch=633
06/05/2022 07:12:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/05/2022 07:12:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=639
06/05/2022 07:12:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/05/2022 07:13:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/05/2022 07:13:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/05/2022 07:13:05 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.29197860962566846 on epoch=649
06/05/2022 07:13:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/05/2022 07:13:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/05/2022 07:13:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/05/2022 07:13:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 07:13:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/05/2022 07:13:20 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.19234006734006734 on epoch=666
06/05/2022 07:13:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
06/05/2022 07:13:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=673
06/05/2022 07:13:29 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/05/2022 07:13:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/05/2022 07:13:34 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/05/2022 07:13:36 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.2676453980801807 on epoch=683
06/05/2022 07:13:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 07:13:41 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/05/2022 07:13:44 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=693
06/05/2022 07:13:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/05/2022 07:13:50 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/05/2022 07:13:51 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.32142857142857145 on epoch=699
06/05/2022 07:13:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/05/2022 07:13:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/05/2022 07:14:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/05/2022 07:14:03 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/05/2022 07:14:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/05/2022 07:14:07 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.17578347578347578 on epoch=716
06/05/2022 07:14:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/05/2022 07:14:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=723
06/05/2022 07:14:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/05/2022 07:14:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/05/2022 07:14:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/05/2022 07:14:22 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.21151716500553708 on epoch=733
06/05/2022 07:14:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/05/2022 07:14:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=739
06/05/2022 07:14:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 07:14:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 07:14:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/05/2022 07:14:38 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.21172897034966 on epoch=749
06/05/2022 07:14:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 07:14:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/05/2022 07:14:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/05/2022 07:14:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
06/05/2022 07:14:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
06/05/2022 07:14:53 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.23974358974358972 on epoch=766
06/05/2022 07:14:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 07:14:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/05/2022 07:15:02 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=776
06/05/2022 07:15:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 07:15:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/05/2022 07:15:09 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.2227398205659075 on epoch=783
06/05/2022 07:15:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/05/2022 07:15:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
06/05/2022 07:15:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
06/05/2022 07:15:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 07:15:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 07:15:25 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.17249780425401914 on epoch=799
06/05/2022 07:15:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 07:15:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 07:15:33 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 07:15:36 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 07:15:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/05/2022 07:15:40 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.17076620252241742 on epoch=816
06/05/2022 07:15:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/05/2022 07:15:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 07:15:49 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 07:15:51 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 07:15:54 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 07:15:56 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.21151716500553708 on epoch=833
06/05/2022 07:15:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=836
06/05/2022 07:16:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 07:16:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/05/2022 07:16:07 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/05/2022 07:16:10 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/05/2022 07:16:11 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.22222222222222224 on epoch=849
06/05/2022 07:16:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 07:16:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 07:16:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 07:16:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
06/05/2022 07:16:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 07:16:27 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.22879776328052187 on epoch=866
06/05/2022 07:16:30 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 07:16:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/05/2022 07:16:35 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 07:16:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 07:16:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/05/2022 07:16:42 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.23242518351214 on epoch=883
06/05/2022 07:16:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 07:16:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 07:16:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 07:16:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/05/2022 07:16:56 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 07:16:57 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.2808978675645342 on epoch=899
06/05/2022 07:17:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 07:17:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 07:17:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 07:17:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 07:17:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 07:17:13 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.24102564102564103 on epoch=916
06/05/2022 07:17:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 07:17:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=923
06/05/2022 07:17:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 07:17:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 07:17:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 07:17:28 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.18224309772024286 on epoch=933
06/05/2022 07:17:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 07:17:34 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/05/2022 07:17:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 07:17:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/05/2022 07:17:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
06/05/2022 07:17:44 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.22390533456675912 on epoch=949
06/05/2022 07:17:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 07:17:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 07:17:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 07:17:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 07:17:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/05/2022 07:17:59 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.1547272727272727 on epoch=966
06/05/2022 07:18:02 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 07:18:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 07:18:07 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 07:18:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
06/05/2022 07:18:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 07:18:14 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.17339194139194142 on epoch=983
06/05/2022 07:18:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 07:18:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 07:18:23 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 07:18:26 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 07:18:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 07:18:30 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:18:30 - INFO - __main__ - Printing 3 examples
06/05/2022 07:18:30 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/05/2022 07:18:30 - INFO - __main__ - ['entailment']
06/05/2022 07:18:30 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/05/2022 07:18:30 - INFO - __main__ - ['entailment']
06/05/2022 07:18:30 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/05/2022 07:18:30 - INFO - __main__ - ['entailment']
06/05/2022 07:18:30 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:18:30 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.25838859416445625 on epoch=999
06/05/2022 07:18:30 - INFO - __main__ - save last model!
06/05/2022 07:18:30 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:18:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 07:18:30 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 07:18:30 - INFO - __main__ - Printing 3 examples
06/05/2022 07:18:30 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 07:18:30 - INFO - __main__ - ['contradiction']
06/05/2022 07:18:30 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 07:18:30 - INFO - __main__ - ['entailment']
06/05/2022 07:18:30 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 07:18:30 - INFO - __main__ - ['contradiction']
06/05/2022 07:18:30 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:18:30 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 07:18:30 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:18:30 - INFO - __main__ - Printing 3 examples
06/05/2022 07:18:30 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/05/2022 07:18:30 - INFO - __main__ - ['entailment']
06/05/2022 07:18:30 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/05/2022 07:18:30 - INFO - __main__ - ['entailment']
06/05/2022 07:18:30 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/05/2022 07:18:30 - INFO - __main__ - ['entailment']
06/05/2022 07:18:30 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:18:30 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:18:30 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 07:18:31 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:18:32 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 07:18:48 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 07:18:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 07:18:49 - INFO - __main__ - Starting training!
06/05/2022 07:19:01 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_21_0.3_8_predictions.txt
06/05/2022 07:19:01 - INFO - __main__ - Classification-F1 on test data: 0.1493
06/05/2022 07:19:02 - INFO - __main__ - prefix=anli_16_21, lr=0.3, bsz=8, dev_performance=0.3679095762429096, test_performance=0.1492733370735851
06/05/2022 07:19:02 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.2, bsz=8 ...
06/05/2022 07:19:02 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:19:02 - INFO - __main__ - Printing 3 examples
06/05/2022 07:19:02 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/05/2022 07:19:02 - INFO - __main__ - ['entailment']
06/05/2022 07:19:02 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/05/2022 07:19:02 - INFO - __main__ - ['entailment']
06/05/2022 07:19:02 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/05/2022 07:19:02 - INFO - __main__ - ['entailment']
06/05/2022 07:19:02 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:19:03 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:19:03 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 07:19:03 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:19:03 - INFO - __main__ - Printing 3 examples
06/05/2022 07:19:03 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
06/05/2022 07:19:03 - INFO - __main__ - ['entailment']
06/05/2022 07:19:03 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
06/05/2022 07:19:03 - INFO - __main__ - ['entailment']
06/05/2022 07:19:03 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
06/05/2022 07:19:03 - INFO - __main__ - ['entailment']
06/05/2022 07:19:03 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:19:03 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:19:03 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 07:19:18 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 07:19:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 07:19:19 - INFO - __main__ - Starting training!
06/05/2022 07:19:22 - INFO - __main__ - Step 10 Global step 10 Train loss 1.24 on epoch=3
06/05/2022 07:19:25 - INFO - __main__ - Step 20 Global step 20 Train loss 0.82 on epoch=6
06/05/2022 07:19:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=9
06/05/2022 07:19:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=13
06/05/2022 07:19:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.60 on epoch=16
06/05/2022 07:19:34 - INFO - __main__ - Global step 50 Train loss 0.77 Classification-F1 0.16666666666666666 on epoch=16
06/05/2022 07:19:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/05/2022 07:19:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.60 on epoch=19
06/05/2022 07:19:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=23
06/05/2022 07:19:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=26
06/05/2022 07:19:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.64 on epoch=29
06/05/2022 07:19:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=33
06/05/2022 07:19:48 - INFO - __main__ - Global step 100 Train loss 0.56 Classification-F1 0.2433862433862434 on epoch=33
06/05/2022 07:19:48 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2433862433862434 on epoch=33, global_step=100
06/05/2022 07:19:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=36
06/05/2022 07:19:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=39
06/05/2022 07:19:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=43
06/05/2022 07:19:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.62 on epoch=46
06/05/2022 07:20:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=49
06/05/2022 07:20:03 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.2350922350922351 on epoch=49
06/05/2022 07:20:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=53
06/05/2022 07:20:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=56
06/05/2022 07:20:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=59
06/05/2022 07:20:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=63
06/05/2022 07:20:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.53 on epoch=66
06/05/2022 07:20:17 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.20634920634920637 on epoch=66
06/05/2022 07:20:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=69
06/05/2022 07:20:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=73
06/05/2022 07:20:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=76
06/05/2022 07:20:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=79
06/05/2022 07:20:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=83
06/05/2022 07:20:32 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.24761904761904763 on epoch=83
06/05/2022 07:20:32 - INFO - __main__ - Saving model with best Classification-F1: 0.2433862433862434 -> 0.24761904761904763 on epoch=83, global_step=250
06/05/2022 07:20:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=86
06/05/2022 07:20:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=89
06/05/2022 07:20:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=93
06/05/2022 07:20:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=96
06/05/2022 07:20:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=99
06/05/2022 07:20:46 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.2216841538875437 on epoch=99
06/05/2022 07:20:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.54 on epoch=103
06/05/2022 07:20:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
06/05/2022 07:20:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=109
06/05/2022 07:20:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=113
06/05/2022 07:20:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=116
06/05/2022 07:21:00 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.2254545454545455 on epoch=116
06/05/2022 07:21:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=119
06/05/2022 07:21:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=123
06/05/2022 07:21:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=126
06/05/2022 07:21:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=129
06/05/2022 07:21:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=133
06/05/2022 07:21:15 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.18666666666666668 on epoch=133
06/05/2022 07:21:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=136
06/05/2022 07:21:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=139
06/05/2022 07:21:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=143
06/05/2022 07:21:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=146
06/05/2022 07:21:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=149
06/05/2022 07:21:30 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.3019528619528619 on epoch=149
06/05/2022 07:21:30 - INFO - __main__ - Saving model with best Classification-F1: 0.24761904761904763 -> 0.3019528619528619 on epoch=149, global_step=450
06/05/2022 07:21:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=153
06/05/2022 07:21:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=156
06/05/2022 07:21:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=159
06/05/2022 07:21:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=163
06/05/2022 07:21:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=166
06/05/2022 07:21:44 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.2662248213125406 on epoch=166
06/05/2022 07:21:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=169
06/05/2022 07:21:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=173
06/05/2022 07:21:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=176
06/05/2022 07:21:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=179
06/05/2022 07:21:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=183
06/05/2022 07:21:59 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.34807987711213517 on epoch=183
06/05/2022 07:21:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3019528619528619 -> 0.34807987711213517 on epoch=183, global_step=550
06/05/2022 07:22:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=186
06/05/2022 07:22:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=189
06/05/2022 07:22:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=193
06/05/2022 07:22:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=196
06/05/2022 07:22:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=199
06/05/2022 07:22:14 - INFO - __main__ - Global step 600 Train loss 0.30 Classification-F1 0.25295508274231676 on epoch=199
06/05/2022 07:22:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=203
06/05/2022 07:22:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.29 on epoch=206
06/05/2022 07:22:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=209
06/05/2022 07:22:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=213
06/05/2022 07:22:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=216
06/05/2022 07:22:29 - INFO - __main__ - Global step 650 Train loss 0.29 Classification-F1 0.16466466466466467 on epoch=216
06/05/2022 07:22:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=219
06/05/2022 07:22:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=223
06/05/2022 07:22:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=226
06/05/2022 07:22:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=229
06/05/2022 07:22:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=233
06/05/2022 07:22:44 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.2971355971355971 on epoch=233
06/05/2022 07:22:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=236
06/05/2022 07:22:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=239
06/05/2022 07:22:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=243
06/05/2022 07:22:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=246
06/05/2022 07:22:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=249
06/05/2022 07:22:58 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.34696784696784694 on epoch=249
06/05/2022 07:23:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=253
06/05/2022 07:23:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=256
06/05/2022 07:23:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=259
06/05/2022 07:23:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=263
06/05/2022 07:23:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=266
06/05/2022 07:23:13 - INFO - __main__ - Global step 800 Train loss 0.17 Classification-F1 0.33555008210180626 on epoch=266
06/05/2022 07:23:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=269
06/05/2022 07:23:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=273
06/05/2022 07:23:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=276
06/05/2022 07:23:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=279
06/05/2022 07:23:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=283
06/05/2022 07:23:28 - INFO - __main__ - Global step 850 Train loss 0.13 Classification-F1 0.29074074074074074 on epoch=283
06/05/2022 07:23:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=286
06/05/2022 07:23:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=289
06/05/2022 07:23:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=293
06/05/2022 07:23:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=296
06/05/2022 07:23:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=299
06/05/2022 07:23:42 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.3006683375104428 on epoch=299
06/05/2022 07:23:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=303
06/05/2022 07:23:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=306
06/05/2022 07:23:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=309
06/05/2022 07:23:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=313
06/05/2022 07:23:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=316
06/05/2022 07:23:57 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.33362318840579713 on epoch=316
06/05/2022 07:24:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=319
06/05/2022 07:24:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=323
06/05/2022 07:24:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
06/05/2022 07:24:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=329
06/05/2022 07:24:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=333
06/05/2022 07:24:12 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.3301379811183733 on epoch=333
06/05/2022 07:24:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=336
06/05/2022 07:24:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
06/05/2022 07:24:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=343
06/05/2022 07:24:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=346
06/05/2022 07:24:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=349
06/05/2022 07:24:27 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.32119241192411924 on epoch=349
06/05/2022 07:24:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=353
06/05/2022 07:24:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=356
06/05/2022 07:24:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=359
06/05/2022 07:24:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=363
06/05/2022 07:24:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
06/05/2022 07:24:42 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.18382342954159597 on epoch=366
06/05/2022 07:24:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=369
06/05/2022 07:24:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=373
06/05/2022 07:24:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=376
06/05/2022 07:24:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=379
06/05/2022 07:24:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=383
06/05/2022 07:24:57 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.2546296296296296 on epoch=383
06/05/2022 07:24:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=386
06/05/2022 07:25:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
06/05/2022 07:25:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=393
06/05/2022 07:25:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=396
06/05/2022 07:25:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=399
06/05/2022 07:25:12 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.2311886985800029 on epoch=399
06/05/2022 07:25:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=403
06/05/2022 07:25:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=406
06/05/2022 07:25:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=409
06/05/2022 07:25:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
06/05/2022 07:25:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=416
06/05/2022 07:25:27 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.23540706605222733 on epoch=416
06/05/2022 07:25:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
06/05/2022 07:25:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=423
06/05/2022 07:25:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=426
06/05/2022 07:25:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
06/05/2022 07:25:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
06/05/2022 07:25:43 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.19632107023411371 on epoch=433
06/05/2022 07:25:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
06/05/2022 07:25:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=439
06/05/2022 07:25:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=443
06/05/2022 07:25:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=446
06/05/2022 07:25:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
06/05/2022 07:25:58 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.15089377494760167 on epoch=449
06/05/2022 07:26:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
06/05/2022 07:26:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=456
06/05/2022 07:26:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=459
06/05/2022 07:26:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
06/05/2022 07:26:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
06/05/2022 07:26:13 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.18105413105413107 on epoch=466
06/05/2022 07:26:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
06/05/2022 07:26:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
06/05/2022 07:26:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=476
06/05/2022 07:26:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
06/05/2022 07:26:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
06/05/2022 07:26:29 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.21145833333333333 on epoch=483
06/05/2022 07:26:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=486
06/05/2022 07:26:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
06/05/2022 07:26:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
06/05/2022 07:26:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
06/05/2022 07:26:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
06/05/2022 07:26:44 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.22396584896584898 on epoch=499
06/05/2022 07:26:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
06/05/2022 07:26:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=506
06/05/2022 07:26:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
06/05/2022 07:26:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
06/05/2022 07:26:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
06/05/2022 07:27:00 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.19743330481449792 on epoch=516
06/05/2022 07:27:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
06/05/2022 07:27:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=523
06/05/2022 07:27:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
06/05/2022 07:27:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/05/2022 07:27:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/05/2022 07:27:15 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.21131584983561433 on epoch=533
06/05/2022 07:27:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
06/05/2022 07:27:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/05/2022 07:27:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/05/2022 07:27:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
06/05/2022 07:27:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
06/05/2022 07:27:31 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.2271042471042471 on epoch=549
06/05/2022 07:27:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
06/05/2022 07:27:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=556
06/05/2022 07:27:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/05/2022 07:27:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/05/2022 07:27:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
06/05/2022 07:27:46 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.11111111111111112 on epoch=566
06/05/2022 07:27:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
06/05/2022 07:27:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
06/05/2022 07:27:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
06/05/2022 07:27:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/05/2022 07:28:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
06/05/2022 07:28:01 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.13151640241423523 on epoch=583
06/05/2022 07:28:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/05/2022 07:28:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
06/05/2022 07:28:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/05/2022 07:28:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/05/2022 07:28:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=599
06/05/2022 07:28:16 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.17063492063492064 on epoch=599
06/05/2022 07:28:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=603
06/05/2022 07:28:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/05/2022 07:28:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/05/2022 07:28:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/05/2022 07:28:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 07:28:31 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.17714285714285713 on epoch=616
06/05/2022 07:28:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
06/05/2022 07:28:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/05/2022 07:28:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/05/2022 07:28:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/05/2022 07:28:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/05/2022 07:28:46 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.13935185185185187 on epoch=633
06/05/2022 07:28:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/05/2022 07:28:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/05/2022 07:28:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/05/2022 07:28:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=646
06/05/2022 07:28:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/05/2022 07:29:01 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.1123931623931624 on epoch=649
06/05/2022 07:29:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/05/2022 07:29:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/05/2022 07:29:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
06/05/2022 07:29:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/05/2022 07:29:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/05/2022 07:29:15 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.10188014101057578 on epoch=666
06/05/2022 07:29:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/05/2022 07:29:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
06/05/2022 07:29:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
06/05/2022 07:29:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
06/05/2022 07:29:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/05/2022 07:29:30 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.17411764705882354 on epoch=683
06/05/2022 07:29:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 07:29:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
06/05/2022 07:29:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/05/2022 07:29:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/05/2022 07:29:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
06/05/2022 07:29:46 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.18094315245478038 on epoch=699
06/05/2022 07:29:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/05/2022 07:29:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/05/2022 07:29:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/05/2022 07:29:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/05/2022 07:30:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/05/2022 07:30:01 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.1975733984844093 on epoch=716
06/05/2022 07:30:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/05/2022 07:30:07 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
06/05/2022 07:30:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=726
06/05/2022 07:30:12 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/05/2022 07:30:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/05/2022 07:30:16 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.08583797155225727 on epoch=733
06/05/2022 07:30:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/05/2022 07:30:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
06/05/2022 07:30:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 07:30:27 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=746
06/05/2022 07:30:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/05/2022 07:30:31 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.1596911196911197 on epoch=749
06/05/2022 07:30:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=753
06/05/2022 07:30:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/05/2022 07:30:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
06/05/2022 07:30:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/05/2022 07:30:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=766
06/05/2022 07:30:46 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.1461904761904762 on epoch=766
06/05/2022 07:30:49 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 07:30:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/05/2022 07:30:54 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/05/2022 07:30:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 07:31:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/05/2022 07:31:02 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.13492063492063494 on epoch=783
06/05/2022 07:31:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 07:31:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 07:31:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/05/2022 07:31:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 07:31:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 07:31:17 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.1651127819548872 on epoch=799
06/05/2022 07:31:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 07:31:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 07:31:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 07:31:28 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 07:31:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
06/05/2022 07:31:32 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.1816553544494721 on epoch=816
06/05/2022 07:31:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/05/2022 07:31:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 07:31:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/05/2022 07:31:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=829
06/05/2022 07:31:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 07:31:46 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.1030736618971913 on epoch=833
06/05/2022 07:31:49 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 07:31:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/05/2022 07:31:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/05/2022 07:31:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/05/2022 07:32:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/05/2022 07:32:01 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.21820512820512822 on epoch=849
06/05/2022 07:32:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 07:32:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 07:32:09 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 07:32:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/05/2022 07:32:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 07:32:16 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.15161290322580645 on epoch=866
06/05/2022 07:32:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 07:32:22 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/05/2022 07:32:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
06/05/2022 07:32:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 07:32:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/05/2022 07:32:31 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.20670807453416148 on epoch=883
06/05/2022 07:32:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 07:32:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=889
06/05/2022 07:32:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=893
06/05/2022 07:32:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/05/2022 07:32:45 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 07:32:46 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.17146800069001208 on epoch=899
06/05/2022 07:32:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
06/05/2022 07:32:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 07:32:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/05/2022 07:32:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 07:33:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 07:33:02 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.14256490952006293 on epoch=916
06/05/2022 07:33:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 07:33:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 07:33:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 07:33:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 07:33:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 07:33:17 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.1505764411027569 on epoch=933
06/05/2022 07:33:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 07:33:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 07:33:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=943
06/05/2022 07:33:29 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 07:33:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
06/05/2022 07:33:33 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.22533875338753384 on epoch=949
06/05/2022 07:33:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 07:33:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 07:33:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
06/05/2022 07:33:44 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
06/05/2022 07:33:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=966
06/05/2022 07:33:48 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.16646932646932647 on epoch=966
06/05/2022 07:33:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/05/2022 07:33:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 07:33:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 07:33:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 07:34:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
06/05/2022 07:34:03 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.14424710424710424 on epoch=983
06/05/2022 07:34:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 07:34:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 07:34:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 07:34:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 07:34:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 07:34:18 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.15479876160990713 on epoch=999
06/05/2022 07:34:18 - INFO - __main__ - save last model!
06/05/2022 07:34:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 07:34:18 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 07:34:18 - INFO - __main__ - Printing 3 examples
06/05/2022 07:34:18 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 07:34:18 - INFO - __main__ - ['contradiction']
06/05/2022 07:34:18 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 07:34:18 - INFO - __main__ - ['entailment']
06/05/2022 07:34:18 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 07:34:18 - INFO - __main__ - ['contradiction']
06/05/2022 07:34:18 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:34:18 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:34:18 - INFO - __main__ - Printing 3 examples
06/05/2022 07:34:18 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/05/2022 07:34:18 - INFO - __main__ - ['neutral']
06/05/2022 07:34:18 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/05/2022 07:34:18 - INFO - __main__ - ['neutral']
06/05/2022 07:34:18 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/05/2022 07:34:18 - INFO - __main__ - ['neutral']
06/05/2022 07:34:18 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:34:18 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:34:18 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 07:34:18 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:34:18 - INFO - __main__ - Printing 3 examples
06/05/2022 07:34:18 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/05/2022 07:34:18 - INFO - __main__ - ['neutral']
06/05/2022 07:34:18 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/05/2022 07:34:18 - INFO - __main__ - ['neutral']
06/05/2022 07:34:18 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/05/2022 07:34:18 - INFO - __main__ - ['neutral']
06/05/2022 07:34:18 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:34:18 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:34:18 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 07:34:18 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:34:19 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 07:34:37 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 07:34:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 07:34:38 - INFO - __main__ - Starting training!
06/05/2022 07:34:48 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_21_0.2_8_predictions.txt
06/05/2022 07:34:48 - INFO - __main__ - Classification-F1 on test data: 0.1006
06/05/2022 07:34:48 - INFO - __main__ - prefix=anli_16_21, lr=0.2, bsz=8, dev_performance=0.34807987711213517, test_performance=0.10057203998608653
06/05/2022 07:34:48 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.5, bsz=8 ...
06/05/2022 07:34:49 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:34:49 - INFO - __main__ - Printing 3 examples
06/05/2022 07:34:49 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/05/2022 07:34:49 - INFO - __main__ - ['neutral']
06/05/2022 07:34:49 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/05/2022 07:34:49 - INFO - __main__ - ['neutral']
06/05/2022 07:34:49 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/05/2022 07:34:49 - INFO - __main__ - ['neutral']
06/05/2022 07:34:49 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:34:49 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:34:49 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 07:34:49 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:34:49 - INFO - __main__ - Printing 3 examples
06/05/2022 07:34:49 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/05/2022 07:34:49 - INFO - __main__ - ['neutral']
06/05/2022 07:34:49 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/05/2022 07:34:49 - INFO - __main__ - ['neutral']
06/05/2022 07:34:49 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/05/2022 07:34:49 - INFO - __main__ - ['neutral']
06/05/2022 07:34:49 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:34:49 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:34:50 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 07:35:05 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 07:35:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 07:35:06 - INFO - __main__ - Starting training!
06/05/2022 07:35:09 - INFO - __main__ - Step 10 Global step 10 Train loss 0.93 on epoch=3
06/05/2022 07:35:12 - INFO - __main__ - Step 20 Global step 20 Train loss 0.66 on epoch=6
06/05/2022 07:35:15 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=9
06/05/2022 07:35:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=13
06/05/2022 07:35:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=16
06/05/2022 07:35:21 - INFO - __main__ - Global step 50 Train loss 0.64 Classification-F1 0.15873015873015875 on epoch=16
06/05/2022 07:35:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15873015873015875 on epoch=16, global_step=50
06/05/2022 07:35:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=19
06/05/2022 07:35:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=23
06/05/2022 07:35:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
06/05/2022 07:35:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
06/05/2022 07:35:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=33
06/05/2022 07:35:35 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=33
06/05/2022 07:35:35 - INFO - __main__ - Saving model with best Classification-F1: 0.15873015873015875 -> 0.16666666666666666 on epoch=33, global_step=100
06/05/2022 07:35:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
06/05/2022 07:35:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=39
06/05/2022 07:35:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=43
06/05/2022 07:35:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
06/05/2022 07:35:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
06/05/2022 07:35:50 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.19272819730485635 on epoch=49
06/05/2022 07:35:50 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.19272819730485635 on epoch=49, global_step=150
06/05/2022 07:35:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=53
06/05/2022 07:35:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=56
06/05/2022 07:35:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
06/05/2022 07:36:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=63
06/05/2022 07:36:03 - INFO - __main__ - Step 200 Global step 200 Train loss 1.63 on epoch=66
06/05/2022 07:36:04 - INFO - __main__ - Global step 200 Train loss 0.74 Classification-F1 0.24999999999999997 on epoch=66
06/05/2022 07:36:04 - INFO - __main__ - Saving model with best Classification-F1: 0.19272819730485635 -> 0.24999999999999997 on epoch=66, global_step=200
06/05/2022 07:36:07 - INFO - __main__ - Step 210 Global step 210 Train loss 2.86 on epoch=69
06/05/2022 07:36:10 - INFO - __main__ - Step 220 Global step 220 Train loss 3.59 on epoch=73
06/05/2022 07:36:12 - INFO - __main__ - Step 230 Global step 230 Train loss 2.96 on epoch=76
06/05/2022 07:36:15 - INFO - __main__ - Step 240 Global step 240 Train loss 2.24 on epoch=79
06/05/2022 07:36:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.70 on epoch=83
06/05/2022 07:36:19 - INFO - __main__ - Global step 250 Train loss 2.47 Classification-F1 0.15555555555555556 on epoch=83
06/05/2022 07:36:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=86
06/05/2022 07:36:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=89
06/05/2022 07:36:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.55 on epoch=93
06/05/2022 07:36:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=96
06/05/2022 07:36:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=99
06/05/2022 07:36:34 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.15300546448087435 on epoch=99
06/05/2022 07:36:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=103
06/05/2022 07:36:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=106
06/05/2022 07:36:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
06/05/2022 07:36:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=113
06/05/2022 07:36:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=116
06/05/2022 07:36:48 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.19222115075538415 on epoch=116
06/05/2022 07:36:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=119
06/05/2022 07:36:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=123
06/05/2022 07:36:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.57 on epoch=126
06/05/2022 07:36:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=129
06/05/2022 07:37:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=133
06/05/2022 07:37:03 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.18993812214151196 on epoch=133
06/05/2022 07:37:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.61 on epoch=136
06/05/2022 07:37:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=139
06/05/2022 07:37:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=143
06/05/2022 07:37:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=146
06/05/2022 07:37:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=149
06/05/2022 07:37:18 - INFO - __main__ - Global step 450 Train loss 0.53 Classification-F1 0.275974025974026 on epoch=149
06/05/2022 07:37:18 - INFO - __main__ - Saving model with best Classification-F1: 0.24999999999999997 -> 0.275974025974026 on epoch=149, global_step=450
06/05/2022 07:37:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=153
06/05/2022 07:37:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=156
06/05/2022 07:37:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=159
06/05/2022 07:37:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=163
06/05/2022 07:37:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=166
06/05/2022 07:37:32 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.19633579436696746 on epoch=166
06/05/2022 07:37:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=169
06/05/2022 07:37:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=173
06/05/2022 07:37:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=176
06/05/2022 07:37:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.53 on epoch=179
06/05/2022 07:37:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.75 on epoch=183
06/05/2022 07:37:47 - INFO - __main__ - Global step 550 Train loss 0.54 Classification-F1 0.19130434782608696 on epoch=183
06/05/2022 07:37:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.56 on epoch=186
06/05/2022 07:37:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=189
06/05/2022 07:37:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=193
06/05/2022 07:37:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.53 on epoch=196
06/05/2022 07:38:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=199
06/05/2022 07:38:02 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=199
06/05/2022 07:38:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=203
06/05/2022 07:38:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=206
06/05/2022 07:38:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.49 on epoch=209
06/05/2022 07:38:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=213
06/05/2022 07:38:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=216
06/05/2022 07:38:16 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=216
06/05/2022 07:38:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=219
06/05/2022 07:38:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=223
06/05/2022 07:38:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=226
06/05/2022 07:38:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=229
06/05/2022 07:38:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=233
06/05/2022 07:38:31 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.1693121693121693 on epoch=233
06/05/2022 07:38:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=236
06/05/2022 07:38:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=239
06/05/2022 07:38:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=243
06/05/2022 07:38:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=246
06/05/2022 07:38:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=249
06/05/2022 07:38:46 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.22152560083594564 on epoch=249
06/05/2022 07:38:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=253
06/05/2022 07:38:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=256
06/05/2022 07:38:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=259
06/05/2022 07:38:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=263
06/05/2022 07:38:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=266
06/05/2022 07:39:01 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.18022598870056494 on epoch=266
06/05/2022 07:39:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.53 on epoch=269
06/05/2022 07:39:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=273
06/05/2022 07:39:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=276
06/05/2022 07:39:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=279
06/05/2022 07:39:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=283
06/05/2022 07:39:16 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.1795321637426901 on epoch=283
06/05/2022 07:39:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=286
06/05/2022 07:39:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=289
06/05/2022 07:39:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=293
06/05/2022 07:39:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=296
06/05/2022 07:39:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=299
06/05/2022 07:39:31 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.26902935087078306 on epoch=299
06/05/2022 07:39:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.49 on epoch=303
06/05/2022 07:39:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=306
06/05/2022 07:39:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=309
06/05/2022 07:39:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=313
06/05/2022 07:39:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.48 on epoch=316
06/05/2022 07:39:46 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.15873015873015875 on epoch=316
06/05/2022 07:39:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=319
06/05/2022 07:39:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=323
06/05/2022 07:39:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=326
06/05/2022 07:39:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=329
06/05/2022 07:39:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.49 on epoch=333
06/05/2022 07:40:00 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.2088927525033002 on epoch=333
06/05/2022 07:40:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=336
06/05/2022 07:40:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=339
06/05/2022 07:40:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=343
06/05/2022 07:40:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=346
06/05/2022 07:40:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=349
06/05/2022 07:40:15 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.2640744433366567 on epoch=349
06/05/2022 07:40:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=353
06/05/2022 07:40:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.51 on epoch=356
06/05/2022 07:40:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.53 on epoch=359
06/05/2022 07:40:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=363
06/05/2022 07:40:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=366
06/05/2022 07:40:30 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.3197450697450698 on epoch=366
06/05/2022 07:40:30 - INFO - __main__ - Saving model with best Classification-F1: 0.275974025974026 -> 0.3197450697450698 on epoch=366, global_step=1100
06/05/2022 07:40:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=369
06/05/2022 07:40:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=373
06/05/2022 07:40:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=376
06/05/2022 07:40:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=379
06/05/2022 07:40:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=383
06/05/2022 07:40:45 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.15873015873015875 on epoch=383
06/05/2022 07:40:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.47 on epoch=386
06/05/2022 07:40:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=389
06/05/2022 07:40:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=393
06/05/2022 07:40:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=396
06/05/2022 07:40:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=399
06/05/2022 07:41:00 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.2640744433366567 on epoch=399
06/05/2022 07:41:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=403
06/05/2022 07:41:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=406
06/05/2022 07:41:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=409
06/05/2022 07:41:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=413
06/05/2022 07:41:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=416
06/05/2022 07:41:15 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.2760577915376677 on epoch=416
06/05/2022 07:41:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=419
06/05/2022 07:41:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=423
06/05/2022 07:41:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=426
06/05/2022 07:41:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=429
06/05/2022 07:41:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=433
06/05/2022 07:41:29 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.2511161112896889 on epoch=433
06/05/2022 07:41:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=436
06/05/2022 07:41:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=439
06/05/2022 07:41:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=443
06/05/2022 07:41:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=446
06/05/2022 07:41:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=449
06/05/2022 07:41:44 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.21836477987421388 on epoch=449
06/05/2022 07:41:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=453
06/05/2022 07:41:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=456
06/05/2022 07:41:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=459
06/05/2022 07:41:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=463
06/05/2022 07:41:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=466
06/05/2022 07:41:59 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.19130434782608696 on epoch=466
06/05/2022 07:42:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=469
06/05/2022 07:42:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=473
06/05/2022 07:42:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=476
06/05/2022 07:42:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=479
06/05/2022 07:42:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=483
06/05/2022 07:42:14 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.2908730158730159 on epoch=483
06/05/2022 07:42:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=486
06/05/2022 07:42:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=489
06/05/2022 07:42:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=493
06/05/2022 07:42:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=496
06/05/2022 07:42:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=499
06/05/2022 07:42:29 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.30052910052910053 on epoch=499
06/05/2022 07:42:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=503
06/05/2022 07:42:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=506
06/05/2022 07:42:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=509
06/05/2022 07:42:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=513
06/05/2022 07:42:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=516
06/05/2022 07:42:43 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.26248871901045817 on epoch=516
06/05/2022 07:42:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=519
06/05/2022 07:42:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=523
06/05/2022 07:42:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=526
06/05/2022 07:42:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=529
06/05/2022 07:42:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=533
06/05/2022 07:42:58 - INFO - __main__ - Global step 1600 Train loss 0.38 Classification-F1 0.2460106382978723 on epoch=533
06/05/2022 07:43:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.30 on epoch=536
06/05/2022 07:43:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=539
06/05/2022 07:43:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=543
06/05/2022 07:43:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=546
06/05/2022 07:43:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=549
06/05/2022 07:43:13 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.33151849463906197 on epoch=549
06/05/2022 07:43:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3197450697450698 -> 0.33151849463906197 on epoch=549, global_step=1650
06/05/2022 07:43:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=553
06/05/2022 07:43:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=556
06/05/2022 07:43:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=559
06/05/2022 07:43:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=563
06/05/2022 07:43:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=566
06/05/2022 07:43:28 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.2711111111111111 on epoch=566
06/05/2022 07:43:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=569
06/05/2022 07:43:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=573
06/05/2022 07:43:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.30 on epoch=576
06/05/2022 07:43:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.27 on epoch=579
06/05/2022 07:43:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=583
06/05/2022 07:43:43 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.3038461538461539 on epoch=583
06/05/2022 07:43:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.27 on epoch=586
06/05/2022 07:43:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=589
06/05/2022 07:43:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=593
06/05/2022 07:43:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.29 on epoch=596
06/05/2022 07:43:56 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=599
06/05/2022 07:43:57 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.20013522650439486 on epoch=599
06/05/2022 07:44:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=603
06/05/2022 07:44:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=606
06/05/2022 07:44:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=609
06/05/2022 07:44:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=613
06/05/2022 07:44:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=616
06/05/2022 07:44:11 - INFO - __main__ - Global step 1850 Train loss 0.29 Classification-F1 0.21794871794871795 on epoch=616
06/05/2022 07:44:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.27 on epoch=619
06/05/2022 07:44:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.28 on epoch=623
06/05/2022 07:44:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=626
06/05/2022 07:44:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=629
06/05/2022 07:44:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=633
06/05/2022 07:44:26 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.27543982019685337 on epoch=633
06/05/2022 07:44:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=636
06/05/2022 07:44:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=639
06/05/2022 07:44:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=643
06/05/2022 07:44:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=646
06/05/2022 07:44:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=649
06/05/2022 07:44:40 - INFO - __main__ - Global step 1950 Train loss 0.27 Classification-F1 0.26750700280112044 on epoch=649
06/05/2022 07:44:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=653
06/05/2022 07:44:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.27 on epoch=656
06/05/2022 07:44:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=659
06/05/2022 07:44:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=663
06/05/2022 07:44:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=666
06/05/2022 07:44:55 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.3010826210826211 on epoch=666
06/05/2022 07:44:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=669
06/05/2022 07:45:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.29 on epoch=673
06/05/2022 07:45:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.24 on epoch=676
06/05/2022 07:45:05 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=679
06/05/2022 07:45:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.25 on epoch=683
06/05/2022 07:45:09 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.2764695621838479 on epoch=683
06/05/2022 07:45:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=686
06/05/2022 07:45:14 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.13 on epoch=689
06/05/2022 07:45:17 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=693
06/05/2022 07:45:19 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.18 on epoch=696
06/05/2022 07:45:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=699
06/05/2022 07:45:23 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.32342759426092754 on epoch=699
06/05/2022 07:45:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=703
06/05/2022 07:45:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=706
06/05/2022 07:45:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=709
06/05/2022 07:45:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.14 on epoch=713
06/05/2022 07:45:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=716
06/05/2022 07:45:38 - INFO - __main__ - Global step 2150 Train loss 0.16 Classification-F1 0.22629447181171317 on epoch=716
06/05/2022 07:45:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=719
06/05/2022 07:45:43 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=723
06/05/2022 07:45:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=726
06/05/2022 07:45:48 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=729
06/05/2022 07:45:51 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=733
06/05/2022 07:45:52 - INFO - __main__ - Global step 2200 Train loss 0.15 Classification-F1 0.2769230769230769 on epoch=733
06/05/2022 07:45:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=736
06/05/2022 07:45:57 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=739
06/05/2022 07:46:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=743
06/05/2022 07:46:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=746
06/05/2022 07:46:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=749
06/05/2022 07:46:07 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.31199392712550605 on epoch=749
06/05/2022 07:46:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=753
06/05/2022 07:46:12 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=756
06/05/2022 07:46:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=759
06/05/2022 07:46:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=763
06/05/2022 07:46:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.09 on epoch=766
06/05/2022 07:46:21 - INFO - __main__ - Global step 2300 Train loss 0.12 Classification-F1 0.2545257452574526 on epoch=766
06/05/2022 07:46:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=769
06/05/2022 07:46:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=773
06/05/2022 07:46:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=776
06/05/2022 07:46:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=779
06/05/2022 07:46:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.14 on epoch=783
06/05/2022 07:46:36 - INFO - __main__ - Global step 2350 Train loss 0.10 Classification-F1 0.2679116090880797 on epoch=783
06/05/2022 07:46:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=786
06/05/2022 07:46:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=789
06/05/2022 07:46:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=793
06/05/2022 07:46:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=796
06/05/2022 07:46:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=799
06/05/2022 07:46:50 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.1895424836601307 on epoch=799
06/05/2022 07:46:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=803
06/05/2022 07:46:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=806
06/05/2022 07:46:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=809
06/05/2022 07:47:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=813
06/05/2022 07:47:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=816
06/05/2022 07:47:05 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.2678623323784614 on epoch=816
06/05/2022 07:47:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=819
06/05/2022 07:47:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=823
06/05/2022 07:47:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=826
06/05/2022 07:47:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=829
06/05/2022 07:47:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=833
06/05/2022 07:47:20 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.21736215853862917 on epoch=833
06/05/2022 07:47:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=836
06/05/2022 07:47:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=839
06/05/2022 07:47:28 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=843
06/05/2022 07:47:31 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=846
06/05/2022 07:47:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=849
06/05/2022 07:47:35 - INFO - __main__ - Global step 2550 Train loss 0.08 Classification-F1 0.20547706261991974 on epoch=849
06/05/2022 07:47:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
06/05/2022 07:47:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=856
06/05/2022 07:47:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=859
06/05/2022 07:47:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=863
06/05/2022 07:47:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=866
06/05/2022 07:47:50 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.29445006747638325 on epoch=866
06/05/2022 07:47:53 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=869
06/05/2022 07:47:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=873
06/05/2022 07:47:58 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=876
06/05/2022 07:48:01 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=879
06/05/2022 07:48:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=883
06/05/2022 07:48:05 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.2529149839494667 on epoch=883
06/05/2022 07:48:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=886
06/05/2022 07:48:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=889
06/05/2022 07:48:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/05/2022 07:48:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=896
06/05/2022 07:48:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=899
06/05/2022 07:48:21 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.23716153127917836 on epoch=899
06/05/2022 07:48:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
06/05/2022 07:48:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/05/2022 07:48:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/05/2022 07:48:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=913
06/05/2022 07:48:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=916
06/05/2022 07:48:37 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.2057126250674638 on epoch=916
06/05/2022 07:48:39 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
06/05/2022 07:48:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=923
06/05/2022 07:48:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=926
06/05/2022 07:48:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=929
06/05/2022 07:48:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=933
06/05/2022 07:48:52 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.1724941724941725 on epoch=933
06/05/2022 07:48:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
06/05/2022 07:48:58 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
06/05/2022 07:49:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=943
06/05/2022 07:49:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/05/2022 07:49:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=949
06/05/2022 07:49:08 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.27040566959921797 on epoch=949
06/05/2022 07:49:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
06/05/2022 07:49:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=956
06/05/2022 07:49:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=959
06/05/2022 07:49:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/05/2022 07:49:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
06/05/2022 07:49:24 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.22570404505888378 on epoch=966
06/05/2022 07:49:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=969
06/05/2022 07:49:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
06/05/2022 07:49:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
06/05/2022 07:49:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
06/05/2022 07:49:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=983
06/05/2022 07:49:40 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.22433862433862437 on epoch=983
06/05/2022 07:49:43 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=986
06/05/2022 07:49:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=989
06/05/2022 07:49:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=993
06/05/2022 07:49:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=996
06/05/2022 07:49:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=999
06/05/2022 07:49:56 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:49:56 - INFO - __main__ - Printing 3 examples
06/05/2022 07:49:56 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/05/2022 07:49:56 - INFO - __main__ - ['neutral']
06/05/2022 07:49:56 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/05/2022 07:49:56 - INFO - __main__ - ['neutral']
06/05/2022 07:49:56 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/05/2022 07:49:56 - INFO - __main__ - ['neutral']
06/05/2022 07:49:56 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:49:56 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:49:56 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.24886399004046064 on epoch=999
06/05/2022 07:49:56 - INFO - __main__ - save last model!
06/05/2022 07:49:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 07:49:56 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 07:49:56 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:49:56 - INFO - __main__ - Printing 3 examples
06/05/2022 07:49:56 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/05/2022 07:49:56 - INFO - __main__ - ['neutral']
06/05/2022 07:49:56 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/05/2022 07:49:56 - INFO - __main__ - ['neutral']
06/05/2022 07:49:56 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/05/2022 07:49:56 - INFO - __main__ - ['neutral']
06/05/2022 07:49:56 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:49:56 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 07:49:56 - INFO - __main__ - Printing 3 examples
06/05/2022 07:49:56 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 07:49:56 - INFO - __main__ - ['contradiction']
06/05/2022 07:49:56 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 07:49:56 - INFO - __main__ - ['entailment']
06/05/2022 07:49:56 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 07:49:56 - INFO - __main__ - ['contradiction']
06/05/2022 07:49:56 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:49:56 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:49:56 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 07:49:57 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:49:58 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 07:50:16 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 07:50:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 07:50:17 - INFO - __main__ - Starting training!
06/05/2022 07:50:29 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_42_0.5_8_predictions.txt
06/05/2022 07:50:29 - INFO - __main__ - Classification-F1 on test data: 0.0966
06/05/2022 07:50:29 - INFO - __main__ - prefix=anli_16_42, lr=0.5, bsz=8, dev_performance=0.33151849463906197, test_performance=0.09664864802761566
06/05/2022 07:50:29 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.4, bsz=8 ...
06/05/2022 07:50:30 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:50:30 - INFO - __main__ - Printing 3 examples
06/05/2022 07:50:30 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/05/2022 07:50:30 - INFO - __main__ - ['neutral']
06/05/2022 07:50:30 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/05/2022 07:50:30 - INFO - __main__ - ['neutral']
06/05/2022 07:50:30 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/05/2022 07:50:30 - INFO - __main__ - ['neutral']
06/05/2022 07:50:30 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:50:30 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:50:30 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 07:50:30 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 07:50:30 - INFO - __main__ - Printing 3 examples
06/05/2022 07:50:30 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/05/2022 07:50:30 - INFO - __main__ - ['neutral']
06/05/2022 07:50:30 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/05/2022 07:50:30 - INFO - __main__ - ['neutral']
06/05/2022 07:50:30 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/05/2022 07:50:30 - INFO - __main__ - ['neutral']
06/05/2022 07:50:30 - INFO - __main__ - Tokenizing Input ...
06/05/2022 07:50:31 - INFO - __main__ - Tokenizing Output ...
06/05/2022 07:50:31 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 07:50:46 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 07:50:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 07:50:46 - INFO - __main__ - Starting training!
06/05/2022 07:50:50 - INFO - __main__ - Step 10 Global step 10 Train loss 0.99 on epoch=3
06/05/2022 07:50:52 - INFO - __main__ - Step 20 Global step 20 Train loss 0.65 on epoch=6
06/05/2022 07:50:55 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=9
06/05/2022 07:50:58 - INFO - __main__ - Step 40 Global step 40 Train loss 0.59 on epoch=13
06/05/2022 07:51:00 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=16
06/05/2022 07:51:01 - INFO - __main__ - Global step 50 Train loss 0.66 Classification-F1 0.16666666666666666 on epoch=16
06/05/2022 07:51:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/05/2022 07:51:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=19
06/05/2022 07:51:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=23
06/05/2022 07:51:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
06/05/2022 07:51:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=29
06/05/2022 07:51:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=33
06/05/2022 07:51:16 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=33
06/05/2022 07:51:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
06/05/2022 07:51:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=39
06/05/2022 07:51:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=43
06/05/2022 07:51:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=46
06/05/2022 07:51:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=49
06/05/2022 07:51:30 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.17204301075268816 on epoch=49
06/05/2022 07:51:30 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17204301075268816 on epoch=49, global_step=150
06/05/2022 07:51:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
06/05/2022 07:51:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
06/05/2022 07:51:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
06/05/2022 07:51:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
06/05/2022 07:51:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=66
06/05/2022 07:51:45 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
06/05/2022 07:51:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=69
06/05/2022 07:51:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=73
06/05/2022 07:51:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=76
06/05/2022 07:51:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=79
06/05/2022 07:51:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=83
06/05/2022 07:52:00 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=83
06/05/2022 07:52:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
06/05/2022 07:52:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=89
06/05/2022 07:52:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=93
06/05/2022 07:52:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=96
06/05/2022 07:52:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=99
06/05/2022 07:52:15 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.2014652014652015 on epoch=99
06/05/2022 07:52:15 - INFO - __main__ - Saving model with best Classification-F1: 0.17204301075268816 -> 0.2014652014652015 on epoch=99, global_step=300
06/05/2022 07:52:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=103
06/05/2022 07:52:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=106
06/05/2022 07:52:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=109
06/05/2022 07:52:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=113
06/05/2022 07:52:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=116
06/05/2022 07:52:29 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.22778143515470703 on epoch=116
06/05/2022 07:52:29 - INFO - __main__ - Saving model with best Classification-F1: 0.2014652014652015 -> 0.22778143515470703 on epoch=116, global_step=350
06/05/2022 07:52:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=119
06/05/2022 07:52:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=123
06/05/2022 07:52:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=126
06/05/2022 07:52:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=129
06/05/2022 07:52:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=133
06/05/2022 07:52:44 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=133
06/05/2022 07:52:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=136
06/05/2022 07:52:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=139
06/05/2022 07:52:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=143
06/05/2022 07:52:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=146
06/05/2022 07:52:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=149
06/05/2022 07:52:59 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.3548717948717948 on epoch=149
06/05/2022 07:52:59 - INFO - __main__ - Saving model with best Classification-F1: 0.22778143515470703 -> 0.3548717948717948 on epoch=149, global_step=450
06/05/2022 07:53:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=153
06/05/2022 07:53:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=156
06/05/2022 07:53:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=159
06/05/2022 07:53:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=163
06/05/2022 07:53:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=166
06/05/2022 07:53:14 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.20050600885515493 on epoch=166
06/05/2022 07:53:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=169
06/05/2022 07:53:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=173
06/05/2022 07:53:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=176
06/05/2022 07:53:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=179
06/05/2022 07:53:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=183
06/05/2022 07:53:28 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.1693121693121693 on epoch=183
06/05/2022 07:53:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=186
06/05/2022 07:53:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=189
06/05/2022 07:53:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=193
06/05/2022 07:53:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=196
06/05/2022 07:53:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=199
06/05/2022 07:53:43 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.2450388265746333 on epoch=199
06/05/2022 07:53:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=203
06/05/2022 07:53:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=206
06/05/2022 07:53:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=209
06/05/2022 07:53:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=213
06/05/2022 07:53:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=216
06/05/2022 07:53:57 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.3918003565062389 on epoch=216
06/05/2022 07:53:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3548717948717948 -> 0.3918003565062389 on epoch=216, global_step=650
06/05/2022 07:54:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.32 on epoch=219
06/05/2022 07:54:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=223
06/05/2022 07:54:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=226
06/05/2022 07:54:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=229
06/05/2022 07:54:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=233
06/05/2022 07:54:12 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.16666666666666666 on epoch=233
06/05/2022 07:54:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=236
06/05/2022 07:54:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=239
06/05/2022 07:54:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=243
06/05/2022 07:54:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=246
06/05/2022 07:54:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=249
06/05/2022 07:54:27 - INFO - __main__ - Global step 750 Train loss 0.29 Classification-F1 0.26019503546099293 on epoch=249
06/05/2022 07:54:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.30 on epoch=253
06/05/2022 07:54:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=256
06/05/2022 07:54:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.32 on epoch=259
06/05/2022 07:54:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=263
06/05/2022 07:54:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=266
06/05/2022 07:54:42 - INFO - __main__ - Global step 800 Train loss 0.29 Classification-F1 0.2934472934472934 on epoch=266
06/05/2022 07:54:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=269
06/05/2022 07:54:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=273
06/05/2022 07:54:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.28 on epoch=276
06/05/2022 07:54:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=279
06/05/2022 07:54:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=283
06/05/2022 07:54:57 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.25295508274231676 on epoch=283
06/05/2022 07:55:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=286
06/05/2022 07:55:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=289
06/05/2022 07:55:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=293
06/05/2022 07:55:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=296
06/05/2022 07:55:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=299
06/05/2022 07:55:13 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.30048309178743965 on epoch=299
06/05/2022 07:55:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=303
06/05/2022 07:55:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=306
06/05/2022 07:55:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=309
06/05/2022 07:55:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=313
06/05/2022 07:55:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=316
06/05/2022 07:55:28 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.3015151515151515 on epoch=316
06/05/2022 07:55:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=319
06/05/2022 07:55:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=323
06/05/2022 07:55:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=326
06/05/2022 07:55:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=329
06/05/2022 07:55:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=333
06/05/2022 07:55:43 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.2708174178762414 on epoch=333
06/05/2022 07:55:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=336
06/05/2022 07:55:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=339
06/05/2022 07:55:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=343
06/05/2022 07:55:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=346
06/05/2022 07:55:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=349
06/05/2022 07:55:58 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.17914438502673796 on epoch=349
06/05/2022 07:56:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=353
06/05/2022 07:56:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=356
06/05/2022 07:56:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=359
06/05/2022 07:56:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=363
06/05/2022 07:56:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=366
06/05/2022 07:56:13 - INFO - __main__ - Global step 1100 Train loss 0.17 Classification-F1 0.21073869560180714 on epoch=366
06/05/2022 07:56:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=369
06/05/2022 07:56:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=373
06/05/2022 07:56:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=376
06/05/2022 07:56:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=379
06/05/2022 07:56:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.15 on epoch=383
06/05/2022 07:56:29 - INFO - __main__ - Global step 1150 Train loss 0.13 Classification-F1 0.10105263157894737 on epoch=383
06/05/2022 07:56:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=386
06/05/2022 07:56:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=389
06/05/2022 07:56:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=393
06/05/2022 07:56:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=396
06/05/2022 07:56:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=399
06/05/2022 07:56:44 - INFO - __main__ - Global step 1200 Train loss 0.10 Classification-F1 0.25064599483204136 on epoch=399
06/05/2022 07:56:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
06/05/2022 07:56:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=406
06/05/2022 07:56:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=409
06/05/2022 07:56:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=413
06/05/2022 07:56:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=416
06/05/2022 07:56:58 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.25490196078431376 on epoch=416
06/05/2022 07:57:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
06/05/2022 07:57:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=423
06/05/2022 07:57:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=426
06/05/2022 07:57:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=429
06/05/2022 07:57:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=433
06/05/2022 07:57:13 - INFO - __main__ - Global step 1300 Train loss 0.11 Classification-F1 0.1902834008097166 on epoch=433
06/05/2022 07:57:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=436
06/05/2022 07:57:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=439
06/05/2022 07:57:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=443
06/05/2022 07:57:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=446
06/05/2022 07:57:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=449
06/05/2022 07:57:28 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.30048309178743965 on epoch=449
06/05/2022 07:57:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
06/05/2022 07:57:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=456
06/05/2022 07:57:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=459
06/05/2022 07:57:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=463
06/05/2022 07:57:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=466
06/05/2022 07:57:43 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.3038461538461539 on epoch=466
06/05/2022 07:57:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
06/05/2022 07:57:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
06/05/2022 07:57:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=476
06/05/2022 07:57:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
06/05/2022 07:57:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
06/05/2022 07:57:58 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.1880952380952381 on epoch=483
06/05/2022 07:58:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
06/05/2022 07:58:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
06/05/2022 07:58:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
06/05/2022 07:58:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
06/05/2022 07:58:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=499
06/05/2022 07:58:12 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.28663172658153097 on epoch=499
06/05/2022 07:58:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=503
06/05/2022 07:58:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
06/05/2022 07:58:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=509
06/05/2022 07:58:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=513
06/05/2022 07:58:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=516
06/05/2022 07:58:27 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.19585913312693498 on epoch=516
06/05/2022 07:58:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
06/05/2022 07:58:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=523
06/05/2022 07:58:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/05/2022 07:58:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=529
06/05/2022 07:58:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/05/2022 07:58:42 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.2560096153846154 on epoch=533
06/05/2022 07:58:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=536
06/05/2022 07:58:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=539
06/05/2022 07:58:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=543
06/05/2022 07:58:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=546
06/05/2022 07:58:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
06/05/2022 07:58:57 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.2604878048780488 on epoch=549
06/05/2022 07:58:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=553
06/05/2022 07:59:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
06/05/2022 07:59:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=559
06/05/2022 07:59:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=563
06/05/2022 07:59:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
06/05/2022 07:59:11 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.28587720523204396 on epoch=566
06/05/2022 07:59:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=569
06/05/2022 07:59:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
06/05/2022 07:59:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/05/2022 07:59:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=579
06/05/2022 07:59:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=583
06/05/2022 07:59:26 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.32600732600732596 on epoch=583
06/05/2022 07:59:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/05/2022 07:59:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=589
06/05/2022 07:59:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=593
06/05/2022 07:59:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=596
06/05/2022 07:59:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
06/05/2022 07:59:41 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.27628013122236483 on epoch=599
06/05/2022 07:59:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
06/05/2022 07:59:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=606
06/05/2022 07:59:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
06/05/2022 07:59:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=613
06/05/2022 07:59:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
06/05/2022 07:59:55 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.3663260535630802 on epoch=616
06/05/2022 07:59:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/05/2022 08:00:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/05/2022 08:00:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=626
06/05/2022 08:00:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=629
06/05/2022 08:00:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/05/2022 08:00:10 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.3165303908337964 on epoch=633
06/05/2022 08:00:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
06/05/2022 08:00:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
06/05/2022 08:00:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
06/05/2022 08:00:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
06/05/2022 08:00:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/05/2022 08:00:25 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.2718100159960625 on epoch=649
06/05/2022 08:00:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/05/2022 08:00:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=656
06/05/2022 08:00:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/05/2022 08:00:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 08:00:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/05/2022 08:00:40 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.23389526542324246 on epoch=666
06/05/2022 08:00:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
06/05/2022 08:00:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
06/05/2022 08:00:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=676
06/05/2022 08:00:52 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
06/05/2022 08:00:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/05/2022 08:00:56 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.1967673248161053 on epoch=683
06/05/2022 08:00:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 08:01:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/05/2022 08:01:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/05/2022 08:01:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=696
06/05/2022 08:01:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
06/05/2022 08:01:11 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.26860119047619047 on epoch=699
06/05/2022 08:01:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/05/2022 08:01:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
06/05/2022 08:01:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/05/2022 08:01:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
06/05/2022 08:01:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/05/2022 08:01:26 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.225177304964539 on epoch=716
06/05/2022 08:01:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
06/05/2022 08:01:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
06/05/2022 08:01:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
06/05/2022 08:01:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/05/2022 08:01:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/05/2022 08:01:41 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.3220004927322 on epoch=733
06/05/2022 08:01:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/05/2022 08:01:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=739
06/05/2022 08:01:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/05/2022 08:01:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/05/2022 08:01:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
06/05/2022 08:01:56 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.20082742316784868 on epoch=749
06/05/2022 08:01:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 08:02:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=756
06/05/2022 08:02:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=759
06/05/2022 08:02:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/05/2022 08:02:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
06/05/2022 08:02:11 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.29452418926103135 on epoch=766
06/05/2022 08:02:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
06/05/2022 08:02:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/05/2022 08:02:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/05/2022 08:02:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 08:02:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=783
06/05/2022 08:02:26 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.20521390374331555 on epoch=783
06/05/2022 08:02:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/05/2022 08:02:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=789
06/05/2022 08:02:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=793
06/05/2022 08:02:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=796
06/05/2022 08:02:40 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/05/2022 08:02:41 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.23212747631352282 on epoch=799
06/05/2022 08:02:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 08:02:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 08:02:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/05/2022 08:02:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/05/2022 08:02:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/05/2022 08:02:57 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.27781458108155227 on epoch=816
06/05/2022 08:02:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/05/2022 08:03:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=823
06/05/2022 08:03:05 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 08:03:08 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/05/2022 08:03:10 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 08:03:12 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.32270069112174377 on epoch=833
06/05/2022 08:03:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 08:03:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/05/2022 08:03:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/05/2022 08:03:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/05/2022 08:03:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=849
06/05/2022 08:03:27 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.184640522875817 on epoch=849
06/05/2022 08:03:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 08:03:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 08:03:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
06/05/2022 08:03:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/05/2022 08:03:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 08:03:42 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.32249025341130605 on epoch=866
06/05/2022 08:03:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 08:03:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
06/05/2022 08:03:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
06/05/2022 08:03:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/05/2022 08:03:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/05/2022 08:03:56 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.29170777542870563 on epoch=883
06/05/2022 08:03:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 08:04:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/05/2022 08:04:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 08:04:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
06/05/2022 08:04:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=899
06/05/2022 08:04:11 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.271439889086948 on epoch=899
06/05/2022 08:04:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 08:04:17 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/05/2022 08:04:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 08:04:22 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 08:04:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 08:04:27 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.27163848631239934 on epoch=916
06/05/2022 08:04:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/05/2022 08:04:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 08:04:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 08:04:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 08:04:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
06/05/2022 08:04:42 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.2611111111111111 on epoch=933
06/05/2022 08:04:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 08:04:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
06/05/2022 08:04:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 08:04:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 08:04:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
06/05/2022 08:04:57 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.2506493506493507 on epoch=949
06/05/2022 08:05:00 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/05/2022 08:05:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 08:05:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 08:05:08 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 08:05:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
06/05/2022 08:05:12 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.30492753623188407 on epoch=966
06/05/2022 08:05:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 08:05:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 08:05:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 08:05:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
06/05/2022 08:05:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 08:05:27 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.29204545454545455 on epoch=983
06/05/2022 08:05:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 08:05:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/05/2022 08:05:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 08:05:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 08:05:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 08:05:43 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.30774953800369603 on epoch=999
06/05/2022 08:05:43 - INFO - __main__ - save last model!
06/05/2022 08:05:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 08:05:43 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 08:05:43 - INFO - __main__ - Printing 3 examples
06/05/2022 08:05:43 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 08:05:43 - INFO - __main__ - ['contradiction']
06/05/2022 08:05:43 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 08:05:43 - INFO - __main__ - ['entailment']
06/05/2022 08:05:43 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 08:05:43 - INFO - __main__ - ['contradiction']
06/05/2022 08:05:43 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:05:43 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:05:43 - INFO - __main__ - Printing 3 examples
06/05/2022 08:05:43 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/05/2022 08:05:43 - INFO - __main__ - ['neutral']
06/05/2022 08:05:43 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/05/2022 08:05:43 - INFO - __main__ - ['neutral']
06/05/2022 08:05:43 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/05/2022 08:05:43 - INFO - __main__ - ['neutral']
06/05/2022 08:05:43 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:05:43 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:05:43 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 08:05:43 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:05:43 - INFO - __main__ - Printing 3 examples
06/05/2022 08:05:43 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/05/2022 08:05:43 - INFO - __main__ - ['neutral']
06/05/2022 08:05:43 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/05/2022 08:05:43 - INFO - __main__ - ['neutral']
06/05/2022 08:05:43 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/05/2022 08:05:43 - INFO - __main__ - ['neutral']
06/05/2022 08:05:43 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:05:43 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:05:43 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 08:05:43 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:05:44 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 08:06:03 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 08:06:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 08:06:03 - INFO - __main__ - Starting training!
06/05/2022 08:06:13 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_42_0.4_8_predictions.txt
06/05/2022 08:06:13 - INFO - __main__ - Classification-F1 on test data: 0.1151
06/05/2022 08:06:14 - INFO - __main__ - prefix=anli_16_42, lr=0.4, bsz=8, dev_performance=0.3918003565062389, test_performance=0.11513917369180528
06/05/2022 08:06:14 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.3, bsz=8 ...
06/05/2022 08:06:15 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:06:15 - INFO - __main__ - Printing 3 examples
06/05/2022 08:06:15 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/05/2022 08:06:15 - INFO - __main__ - ['neutral']
06/05/2022 08:06:15 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/05/2022 08:06:15 - INFO - __main__ - ['neutral']
06/05/2022 08:06:15 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/05/2022 08:06:15 - INFO - __main__ - ['neutral']
06/05/2022 08:06:15 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:06:15 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:06:15 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 08:06:15 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:06:15 - INFO - __main__ - Printing 3 examples
06/05/2022 08:06:15 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/05/2022 08:06:15 - INFO - __main__ - ['neutral']
06/05/2022 08:06:15 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/05/2022 08:06:15 - INFO - __main__ - ['neutral']
06/05/2022 08:06:15 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/05/2022 08:06:15 - INFO - __main__ - ['neutral']
06/05/2022 08:06:15 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:06:15 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:06:15 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 08:06:30 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 08:06:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 08:06:31 - INFO - __main__ - Starting training!
06/05/2022 08:06:34 - INFO - __main__ - Step 10 Global step 10 Train loss 1.15 on epoch=3
06/05/2022 08:06:37 - INFO - __main__ - Step 20 Global step 20 Train loss 0.68 on epoch=6
06/05/2022 08:06:40 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=9
06/05/2022 08:06:42 - INFO - __main__ - Step 40 Global step 40 Train loss 0.59 on epoch=13
06/05/2022 08:06:45 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=16
06/05/2022 08:06:46 - INFO - __main__ - Global step 50 Train loss 0.71 Classification-F1 0.16374269005847955 on epoch=16
06/05/2022 08:06:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16374269005847955 on epoch=16, global_step=50
06/05/2022 08:06:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=19
06/05/2022 08:06:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=23
06/05/2022 08:06:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=26
06/05/2022 08:06:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
06/05/2022 08:07:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=33
06/05/2022 08:07:01 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=33
06/05/2022 08:07:01 - INFO - __main__ - Saving model with best Classification-F1: 0.16374269005847955 -> 0.16666666666666666 on epoch=33, global_step=100
06/05/2022 08:07:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=36
06/05/2022 08:07:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=39
06/05/2022 08:07:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=43
06/05/2022 08:07:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=46
06/05/2022 08:07:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
06/05/2022 08:07:17 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.20366013071895425 on epoch=49
06/05/2022 08:07:17 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.20366013071895425 on epoch=49, global_step=150
06/05/2022 08:07:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=53
06/05/2022 08:07:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=56
06/05/2022 08:07:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=59
06/05/2022 08:07:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=63
06/05/2022 08:07:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=66
06/05/2022 08:07:32 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.25213303712289514 on epoch=66
06/05/2022 08:07:32 - INFO - __main__ - Saving model with best Classification-F1: 0.20366013071895425 -> 0.25213303712289514 on epoch=66, global_step=200
06/05/2022 08:07:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=69
06/05/2022 08:07:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=73
06/05/2022 08:07:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=76
06/05/2022 08:07:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
06/05/2022 08:07:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=83
06/05/2022 08:07:47 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.22152560083594564 on epoch=83
06/05/2022 08:07:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=86
06/05/2022 08:07:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=89
06/05/2022 08:07:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=93
06/05/2022 08:07:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=96
06/05/2022 08:08:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=99
06/05/2022 08:08:02 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.16259026321919404 on epoch=99
06/05/2022 08:08:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=103
06/05/2022 08:08:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=106
06/05/2022 08:08:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=109
06/05/2022 08:08:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
06/05/2022 08:08:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
06/05/2022 08:08:17 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.25071225071225073 on epoch=116
06/05/2022 08:08:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=119
06/05/2022 08:08:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=123
06/05/2022 08:08:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=126
06/05/2022 08:08:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=129
06/05/2022 08:08:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=133
06/05/2022 08:08:32 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=133
06/05/2022 08:08:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=136
06/05/2022 08:08:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=139
06/05/2022 08:08:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=143
06/05/2022 08:08:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=146
06/05/2022 08:08:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=149
06/05/2022 08:08:47 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.2626262626262626 on epoch=149
06/05/2022 08:08:47 - INFO - __main__ - Saving model with best Classification-F1: 0.25213303712289514 -> 0.2626262626262626 on epoch=149, global_step=450
06/05/2022 08:08:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=153
06/05/2022 08:08:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=156
06/05/2022 08:08:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=159
06/05/2022 08:08:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=163
06/05/2022 08:09:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=166
06/05/2022 08:09:01 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.20138888888888892 on epoch=166
06/05/2022 08:09:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=169
06/05/2022 08:09:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=173
06/05/2022 08:09:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=176
06/05/2022 08:09:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=179
06/05/2022 08:09:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=183
06/05/2022 08:09:17 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.18993812214151196 on epoch=183
06/05/2022 08:09:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=186
06/05/2022 08:09:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=189
06/05/2022 08:09:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=193
06/05/2022 08:09:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=196
06/05/2022 08:09:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=199
06/05/2022 08:09:32 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.26077097505668934 on epoch=199
06/05/2022 08:09:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=203
06/05/2022 08:09:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=206
06/05/2022 08:09:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=209
06/05/2022 08:09:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=213
06/05/2022 08:09:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=216
06/05/2022 08:09:47 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.26517273576097106 on epoch=216
06/05/2022 08:09:47 - INFO - __main__ - Saving model with best Classification-F1: 0.2626262626262626 -> 0.26517273576097106 on epoch=216, global_step=650
06/05/2022 08:09:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=219
06/05/2022 08:09:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=223
06/05/2022 08:09:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=226
06/05/2022 08:09:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=229
06/05/2022 08:10:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=233
06/05/2022 08:10:02 - INFO - __main__ - Global step 700 Train loss 0.28 Classification-F1 0.25718674566600835 on epoch=233
06/05/2022 08:10:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=236
06/05/2022 08:10:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=239
06/05/2022 08:10:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=243
06/05/2022 08:10:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=246
06/05/2022 08:10:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.30 on epoch=249
06/05/2022 08:10:17 - INFO - __main__ - Global step 750 Train loss 0.29 Classification-F1 0.28485628485628484 on epoch=249
06/05/2022 08:10:17 - INFO - __main__ - Saving model with best Classification-F1: 0.26517273576097106 -> 0.28485628485628484 on epoch=249, global_step=750
06/05/2022 08:10:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=253
06/05/2022 08:10:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=256
06/05/2022 08:10:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.25 on epoch=259
06/05/2022 08:10:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=263
06/05/2022 08:10:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=266
06/05/2022 08:10:32 - INFO - __main__ - Global step 800 Train loss 0.27 Classification-F1 0.26077097505668934 on epoch=266
06/05/2022 08:10:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=269
06/05/2022 08:10:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=273
06/05/2022 08:10:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=276
06/05/2022 08:10:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=279
06/05/2022 08:10:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=283
06/05/2022 08:10:47 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.21836477987421388 on epoch=283
06/05/2022 08:10:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=286
06/05/2022 08:10:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=289
06/05/2022 08:10:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=293
06/05/2022 08:10:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=296
06/05/2022 08:11:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=299
06/05/2022 08:11:02 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.2318262411347518 on epoch=299
06/05/2022 08:11:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=303
06/05/2022 08:11:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=306
06/05/2022 08:11:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=309
06/05/2022 08:11:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=313
06/05/2022 08:11:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=316
06/05/2022 08:11:18 - INFO - __main__ - Global step 950 Train loss 0.18 Classification-F1 0.257145732689211 on epoch=316
06/05/2022 08:11:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.14 on epoch=319
06/05/2022 08:11:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=323
06/05/2022 08:11:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=326
06/05/2022 08:11:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=329
06/05/2022 08:11:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=333
06/05/2022 08:11:33 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.19927536231884058 on epoch=333
06/05/2022 08:11:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=336
06/05/2022 08:11:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.14 on epoch=339
06/05/2022 08:11:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=343
06/05/2022 08:11:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=346
06/05/2022 08:11:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=349
06/05/2022 08:11:48 - INFO - __main__ - Global step 1050 Train loss 0.13 Classification-F1 0.22034632034632037 on epoch=349
06/05/2022 08:11:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.15 on epoch=353
06/05/2022 08:11:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=356
06/05/2022 08:11:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=359
06/05/2022 08:11:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=363
06/05/2022 08:12:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=366
06/05/2022 08:12:03 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.2219292463194902 on epoch=366
06/05/2022 08:12:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=369
06/05/2022 08:12:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=373
06/05/2022 08:12:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=376
06/05/2022 08:12:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=379
06/05/2022 08:12:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=383
06/05/2022 08:12:19 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.22100205902539463 on epoch=383
06/05/2022 08:12:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=386
06/05/2022 08:12:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=389
06/05/2022 08:12:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=393
06/05/2022 08:12:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=396
06/05/2022 08:12:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=399
06/05/2022 08:12:34 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.2354978354978355 on epoch=399
06/05/2022 08:12:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
06/05/2022 08:12:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=406
06/05/2022 08:12:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=409
06/05/2022 08:12:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
06/05/2022 08:12:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=416
06/05/2022 08:12:49 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.21655773420479305 on epoch=416
06/05/2022 08:12:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
06/05/2022 08:12:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=423
06/05/2022 08:12:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=426
06/05/2022 08:13:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=429
06/05/2022 08:13:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
06/05/2022 08:13:04 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.23514211886304906 on epoch=433
06/05/2022 08:13:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=436
06/05/2022 08:13:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=439
06/05/2022 08:13:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=443
06/05/2022 08:13:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=446
06/05/2022 08:13:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
06/05/2022 08:13:19 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.2573643410852713 on epoch=449
06/05/2022 08:13:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=453
06/05/2022 08:13:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=456
06/05/2022 08:13:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
06/05/2022 08:13:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/05/2022 08:13:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
06/05/2022 08:13:34 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.25614035087719295 on epoch=466
06/05/2022 08:13:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
06/05/2022 08:13:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=473
06/05/2022 08:13:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
06/05/2022 08:13:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=479
06/05/2022 08:13:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=483
06/05/2022 08:13:49 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.2519540229885057 on epoch=483
06/05/2022 08:13:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
06/05/2022 08:13:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=489
06/05/2022 08:13:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
06/05/2022 08:14:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
06/05/2022 08:14:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
06/05/2022 08:14:04 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.2925925925925926 on epoch=499
06/05/2022 08:14:04 - INFO - __main__ - Saving model with best Classification-F1: 0.28485628485628484 -> 0.2925925925925926 on epoch=499, global_step=1500
06/05/2022 08:14:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=503
06/05/2022 08:14:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=506
06/05/2022 08:14:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
06/05/2022 08:14:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=513
06/05/2022 08:14:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=516
06/05/2022 08:14:19 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.23611111111111113 on epoch=516
06/05/2022 08:14:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
06/05/2022 08:14:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
06/05/2022 08:14:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=526
06/05/2022 08:14:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=529
06/05/2022 08:14:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=533
06/05/2022 08:14:34 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.34772829509671616 on epoch=533
06/05/2022 08:14:34 - INFO - __main__ - Saving model with best Classification-F1: 0.2925925925925926 -> 0.34772829509671616 on epoch=533, global_step=1600
06/05/2022 08:14:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
06/05/2022 08:14:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=539
06/05/2022 08:14:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
06/05/2022 08:14:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
06/05/2022 08:14:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
06/05/2022 08:14:49 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.252991452991453 on epoch=549
06/05/2022 08:14:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
06/05/2022 08:14:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
06/05/2022 08:14:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
06/05/2022 08:15:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=563
06/05/2022 08:15:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
06/05/2022 08:15:04 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.2772404900064474 on epoch=566
06/05/2022 08:15:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
06/05/2022 08:15:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=573
06/05/2022 08:15:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
06/05/2022 08:15:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/05/2022 08:15:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=583
06/05/2022 08:15:19 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.2731829573934837 on epoch=583
06/05/2022 08:15:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/05/2022 08:15:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=589
06/05/2022 08:15:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/05/2022 08:15:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
06/05/2022 08:15:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=599
06/05/2022 08:15:34 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.2922563661479917 on epoch=599
06/05/2022 08:15:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
06/05/2022 08:15:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/05/2022 08:15:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/05/2022 08:15:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
06/05/2022 08:15:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 08:15:49 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.28424076250163205 on epoch=616
06/05/2022 08:15:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/05/2022 08:15:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/05/2022 08:15:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/05/2022 08:16:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
06/05/2022 08:16:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
06/05/2022 08:16:04 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.27079218784287906 on epoch=633
06/05/2022 08:16:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/05/2022 08:16:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
06/05/2022 08:16:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
06/05/2022 08:16:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
06/05/2022 08:16:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
06/05/2022 08:16:19 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.15789473684210525 on epoch=649
06/05/2022 08:16:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
06/05/2022 08:16:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=656
06/05/2022 08:16:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=659
06/05/2022 08:16:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 08:16:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/05/2022 08:16:34 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.34724988146040775 on epoch=666
06/05/2022 08:16:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=669
06/05/2022 08:16:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
06/05/2022 08:16:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/05/2022 08:16:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
06/05/2022 08:16:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/05/2022 08:16:49 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.2608543417366947 on epoch=683
06/05/2022 08:16:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 08:16:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
06/05/2022 08:16:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/05/2022 08:17:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
06/05/2022 08:17:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=699
06/05/2022 08:17:04 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.2727272727272727 on epoch=699
06/05/2022 08:17:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/05/2022 08:17:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/05/2022 08:17:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/05/2022 08:17:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/05/2022 08:17:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/05/2022 08:17:19 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.2395124716553288 on epoch=716
06/05/2022 08:17:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/05/2022 08:17:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/05/2022 08:17:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/05/2022 08:17:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/05/2022 08:17:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/05/2022 08:17:34 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.3803418803418803 on epoch=733
06/05/2022 08:17:34 - INFO - __main__ - Saving model with best Classification-F1: 0.34772829509671616 -> 0.3803418803418803 on epoch=733, global_step=2200
06/05/2022 08:17:37 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/05/2022 08:17:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/05/2022 08:17:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 08:17:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
06/05/2022 08:17:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
06/05/2022 08:17:50 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.36824670371581125 on epoch=749
06/05/2022 08:17:53 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 08:17:55 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/05/2022 08:17:58 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
06/05/2022 08:18:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/05/2022 08:18:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/05/2022 08:18:05 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.3006683375104428 on epoch=766
06/05/2022 08:18:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
06/05/2022 08:18:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/05/2022 08:18:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
06/05/2022 08:18:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/05/2022 08:18:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
06/05/2022 08:18:20 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.33720167846839755 on epoch=783
06/05/2022 08:18:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 08:18:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 08:18:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
06/05/2022 08:18:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 08:18:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 08:18:35 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.33359968856926264 on epoch=799
06/05/2022 08:18:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
06/05/2022 08:18:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 08:18:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 08:18:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
06/05/2022 08:18:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
06/05/2022 08:18:51 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.37072489155822486 on epoch=816
06/05/2022 08:18:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
06/05/2022 08:18:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 08:18:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 08:19:02 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 08:19:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 08:19:06 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.3381895881895882 on epoch=833
06/05/2022 08:19:08 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 08:19:11 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
06/05/2022 08:19:14 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/05/2022 08:19:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/05/2022 08:19:19 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/05/2022 08:19:21 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.3392672858617131 on epoch=849
06/05/2022 08:19:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
06/05/2022 08:19:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 08:19:29 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=859
06/05/2022 08:19:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/05/2022 08:19:34 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 08:19:36 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.3148111658456486 on epoch=866
06/05/2022 08:19:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 08:19:41 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/05/2022 08:19:44 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 08:19:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 08:19:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
06/05/2022 08:19:51 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.295516693163752 on epoch=883
06/05/2022 08:19:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 08:19:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 08:19:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 08:20:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/05/2022 08:20:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 08:20:06 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.2771462254220875 on epoch=899
06/05/2022 08:20:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 08:20:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/05/2022 08:20:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 08:20:17 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 08:20:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 08:20:21 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.3483159117305459 on epoch=916
06/05/2022 08:20:24 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 08:20:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=923
06/05/2022 08:20:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 08:20:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 08:20:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 08:20:36 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.24208754208754205 on epoch=933
06/05/2022 08:20:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
06/05/2022 08:20:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/05/2022 08:20:45 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 08:20:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 08:20:50 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 08:20:52 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.2719344147915576 on epoch=949
06/05/2022 08:20:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 08:20:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 08:21:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 08:21:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 08:21:06 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/05/2022 08:21:07 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.34578073089701 on epoch=966
06/05/2022 08:21:10 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 08:21:13 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 08:21:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
06/05/2022 08:21:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 08:21:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 08:21:23 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.3353439153439153 on epoch=983
06/05/2022 08:21:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 08:21:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 08:21:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 08:21:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 08:21:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 08:21:38 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:21:38 - INFO - __main__ - Printing 3 examples
06/05/2022 08:21:38 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/05/2022 08:21:38 - INFO - __main__ - ['neutral']
06/05/2022 08:21:38 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/05/2022 08:21:38 - INFO - __main__ - ['neutral']
06/05/2022 08:21:38 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/05/2022 08:21:38 - INFO - __main__ - ['neutral']
06/05/2022 08:21:38 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:21:39 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:21:39 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 08:21:39 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:21:39 - INFO - __main__ - Printing 3 examples
06/05/2022 08:21:39 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/05/2022 08:21:39 - INFO - __main__ - ['neutral']
06/05/2022 08:21:39 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/05/2022 08:21:39 - INFO - __main__ - ['neutral']
06/05/2022 08:21:39 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/05/2022 08:21:39 - INFO - __main__ - ['neutral']
06/05/2022 08:21:39 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:21:39 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:21:39 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.2976935268601935 on epoch=999
06/05/2022 08:21:39 - INFO - __main__ - save last model!
06/05/2022 08:21:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 08:21:39 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 08:21:39 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 08:21:39 - INFO - __main__ - Printing 3 examples
06/05/2022 08:21:39 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 08:21:39 - INFO - __main__ - ['contradiction']
06/05/2022 08:21:39 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 08:21:39 - INFO - __main__ - ['entailment']
06/05/2022 08:21:39 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 08:21:39 - INFO - __main__ - ['contradiction']
06/05/2022 08:21:39 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:21:39 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:21:40 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 08:21:56 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 08:21:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 08:21:57 - INFO - __main__ - Starting training!
06/05/2022 08:22:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_42_0.3_8_predictions.txt
06/05/2022 08:22:10 - INFO - __main__ - Classification-F1 on test data: 0.2486
06/05/2022 08:22:11 - INFO - __main__ - prefix=anli_16_42, lr=0.3, bsz=8, dev_performance=0.3803418803418803, test_performance=0.2485764427219957
06/05/2022 08:22:11 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.2, bsz=8 ...
06/05/2022 08:22:12 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:22:12 - INFO - __main__ - Printing 3 examples
06/05/2022 08:22:12 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/05/2022 08:22:12 - INFO - __main__ - ['neutral']
06/05/2022 08:22:12 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/05/2022 08:22:12 - INFO - __main__ - ['neutral']
06/05/2022 08:22:12 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/05/2022 08:22:12 - INFO - __main__ - ['neutral']
06/05/2022 08:22:12 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:22:12 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:22:12 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 08:22:12 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:22:12 - INFO - __main__ - Printing 3 examples
06/05/2022 08:22:12 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
06/05/2022 08:22:12 - INFO - __main__ - ['neutral']
06/05/2022 08:22:12 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
06/05/2022 08:22:12 - INFO - __main__ - ['neutral']
06/05/2022 08:22:12 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
06/05/2022 08:22:12 - INFO - __main__ - ['neutral']
06/05/2022 08:22:12 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:22:12 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:22:12 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 08:22:28 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 08:22:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 08:22:28 - INFO - __main__ - Starting training!
06/05/2022 08:22:32 - INFO - __main__ - Step 10 Global step 10 Train loss 1.16 on epoch=3
06/05/2022 08:22:34 - INFO - __main__ - Step 20 Global step 20 Train loss 0.77 on epoch=6
06/05/2022 08:22:37 - INFO - __main__ - Step 30 Global step 30 Train loss 0.77 on epoch=9
06/05/2022 08:22:40 - INFO - __main__ - Step 40 Global step 40 Train loss 0.60 on epoch=13
06/05/2022 08:22:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.64 on epoch=16
06/05/2022 08:22:44 - INFO - __main__ - Global step 50 Train loss 0.79 Classification-F1 0.16666666666666666 on epoch=16
06/05/2022 08:22:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
06/05/2022 08:22:46 - INFO - __main__ - Step 60 Global step 60 Train loss 0.62 on epoch=19
06/05/2022 08:22:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.57 on epoch=23
06/05/2022 08:22:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=26
06/05/2022 08:22:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=29
06/05/2022 08:22:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.61 on epoch=33
06/05/2022 08:22:58 - INFO - __main__ - Global step 100 Train loss 0.59 Classification-F1 0.16117216117216118 on epoch=33
06/05/2022 08:23:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=36
06/05/2022 08:23:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=39
06/05/2022 08:23:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=43
06/05/2022 08:23:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=46
06/05/2022 08:23:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=49
06/05/2022 08:23:12 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.1630561378989052 on epoch=49
06/05/2022 08:23:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=53
06/05/2022 08:23:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
06/05/2022 08:23:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=59
06/05/2022 08:23:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=63
06/05/2022 08:23:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=66
06/05/2022 08:23:27 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=66
06/05/2022 08:23:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=69
06/05/2022 08:23:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=73
06/05/2022 08:23:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=76
06/05/2022 08:23:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=79
06/05/2022 08:23:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.52 on epoch=83
06/05/2022 08:23:41 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.15873015873015875 on epoch=83
06/05/2022 08:23:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=86
06/05/2022 08:23:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=89
06/05/2022 08:23:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=93
06/05/2022 08:23:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=96
06/05/2022 08:23:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=99
06/05/2022 08:23:56 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.15873015873015875 on epoch=99
06/05/2022 08:23:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=103
06/05/2022 08:24:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
06/05/2022 08:24:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=109
06/05/2022 08:24:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
06/05/2022 08:24:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=116
06/05/2022 08:24:10 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=116
06/05/2022 08:24:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=119
06/05/2022 08:24:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=123
06/05/2022 08:24:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=126
06/05/2022 08:24:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=129
06/05/2022 08:24:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=133
06/05/2022 08:24:24 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.15873015873015875 on epoch=133
06/05/2022 08:24:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=136
06/05/2022 08:24:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=139
06/05/2022 08:24:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=143
06/05/2022 08:24:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=146
06/05/2022 08:24:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=149
06/05/2022 08:24:39 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.21003134796238246 on epoch=149
06/05/2022 08:24:39 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.21003134796238246 on epoch=149, global_step=450
06/05/2022 08:24:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=153
06/05/2022 08:24:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=156
06/05/2022 08:24:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=159
06/05/2022 08:24:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=163
06/05/2022 08:24:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=166
06/05/2022 08:24:53 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.15873015873015875 on epoch=166
06/05/2022 08:24:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=169
06/05/2022 08:24:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=173
06/05/2022 08:25:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=176
06/05/2022 08:25:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=179
06/05/2022 08:25:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=183
06/05/2022 08:25:08 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=183
06/05/2022 08:25:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=186
06/05/2022 08:25:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=189
06/05/2022 08:25:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=193
06/05/2022 08:25:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=196
06/05/2022 08:25:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=199
06/05/2022 08:25:22 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.15300546448087435 on epoch=199
06/05/2022 08:25:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=203
06/05/2022 08:25:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=206
06/05/2022 08:25:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=209
06/05/2022 08:25:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=213
06/05/2022 08:25:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=216
06/05/2022 08:25:37 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.16666666666666666 on epoch=216
06/05/2022 08:25:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=219
06/05/2022 08:25:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=223
06/05/2022 08:25:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=226
06/05/2022 08:25:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=229
06/05/2022 08:25:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=233
06/05/2022 08:25:51 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.1990221455277538 on epoch=233
06/05/2022 08:25:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.32 on epoch=236
06/05/2022 08:25:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=239
06/05/2022 08:25:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=243
06/05/2022 08:26:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=246
06/05/2022 08:26:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=249
06/05/2022 08:26:06 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.2127255460588794 on epoch=249
06/05/2022 08:26:06 - INFO - __main__ - Saving model with best Classification-F1: 0.21003134796238246 -> 0.2127255460588794 on epoch=249, global_step=750
06/05/2022 08:26:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=253
06/05/2022 08:26:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=256
06/05/2022 08:26:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=259
06/05/2022 08:26:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=263
06/05/2022 08:26:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=266
06/05/2022 08:26:20 - INFO - __main__ - Global step 800 Train loss 0.31 Classification-F1 0.2127255460588794 on epoch=266
06/05/2022 08:26:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=269
06/05/2022 08:26:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=273
06/05/2022 08:26:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.29 on epoch=276
06/05/2022 08:26:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=279
06/05/2022 08:26:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=283
06/05/2022 08:26:35 - INFO - __main__ - Global step 850 Train loss 0.28 Classification-F1 0.1990221455277538 on epoch=283
06/05/2022 08:26:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=286
06/05/2022 08:26:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=289
06/05/2022 08:26:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=293
06/05/2022 08:26:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=296
06/05/2022 08:26:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.28 on epoch=299
06/05/2022 08:26:50 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.13172413793103446 on epoch=299
06/05/2022 08:26:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=303
06/05/2022 08:26:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=306
06/05/2022 08:26:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=309
06/05/2022 08:27:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=313
06/05/2022 08:27:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=316
06/05/2022 08:27:06 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.1982924482924483 on epoch=316
06/05/2022 08:27:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=319
06/05/2022 08:27:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=323
06/05/2022 08:27:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=326
06/05/2022 08:27:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.27 on epoch=329
06/05/2022 08:27:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=333
06/05/2022 08:27:21 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.2491073612130105 on epoch=333
06/05/2022 08:27:21 - INFO - __main__ - Saving model with best Classification-F1: 0.2127255460588794 -> 0.2491073612130105 on epoch=333, global_step=1000
06/05/2022 08:27:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=336
06/05/2022 08:27:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=339
06/05/2022 08:27:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=343
06/05/2022 08:27:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=346
06/05/2022 08:27:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=349
06/05/2022 08:27:36 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.19787004167309769 on epoch=349
06/05/2022 08:27:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.23 on epoch=353
06/05/2022 08:27:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=356
06/05/2022 08:27:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=359
06/05/2022 08:27:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=363
06/05/2022 08:27:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=366
06/05/2022 08:27:51 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.1262253743104807 on epoch=366
06/05/2022 08:27:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=369
06/05/2022 08:27:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=373
06/05/2022 08:28:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=376
06/05/2022 08:28:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=379
06/05/2022 08:28:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=383
06/05/2022 08:28:07 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.13242835595776772 on epoch=383
06/05/2022 08:28:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=386
06/05/2022 08:28:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=389
06/05/2022 08:28:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=393
06/05/2022 08:28:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=396
06/05/2022 08:28:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=399
06/05/2022 08:28:21 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.12412698412698413 on epoch=399
06/05/2022 08:28:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=403
06/05/2022 08:28:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=406
06/05/2022 08:28:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=409
06/05/2022 08:28:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=413
06/05/2022 08:28:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=416
06/05/2022 08:28:36 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.14254294093003772 on epoch=416
06/05/2022 08:28:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=419
06/05/2022 08:28:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=423
06/05/2022 08:28:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=426
06/05/2022 08:28:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=429
06/05/2022 08:28:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=433
06/05/2022 08:28:51 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.11885369532428355 on epoch=433
06/05/2022 08:28:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=436
06/05/2022 08:28:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=439
06/05/2022 08:28:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.16 on epoch=443
06/05/2022 08:29:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=446
06/05/2022 08:29:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=449
06/05/2022 08:29:05 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.11047619047619048 on epoch=449
06/05/2022 08:29:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=453
06/05/2022 08:29:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=456
06/05/2022 08:29:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=459
06/05/2022 08:29:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=463
06/05/2022 08:29:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.09 on epoch=466
06/05/2022 08:29:20 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.17896551724137932 on epoch=466
06/05/2022 08:29:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=469
06/05/2022 08:29:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=473
06/05/2022 08:29:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=476
06/05/2022 08:29:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=479
06/05/2022 08:29:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=483
06/05/2022 08:29:35 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.1998639455782313 on epoch=483
06/05/2022 08:29:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=486
06/05/2022 08:29:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=489
06/05/2022 08:29:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=493
06/05/2022 08:29:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=496
06/05/2022 08:29:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=499
06/05/2022 08:29:51 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.21707421707421706 on epoch=499
06/05/2022 08:29:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=503
06/05/2022 08:29:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=506
06/05/2022 08:29:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=509
06/05/2022 08:30:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=513
06/05/2022 08:30:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=516
06/05/2022 08:30:06 - INFO - __main__ - Global step 1550 Train loss 0.08 Classification-F1 0.11574445335471799 on epoch=516
06/05/2022 08:30:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=519
06/05/2022 08:30:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=523
06/05/2022 08:30:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
06/05/2022 08:30:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=529
06/05/2022 08:30:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=533
06/05/2022 08:30:21 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.1341025641025641 on epoch=533
06/05/2022 08:30:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=536
06/05/2022 08:30:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=539
06/05/2022 08:30:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=543
06/05/2022 08:30:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=546
06/05/2022 08:30:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=549
06/05/2022 08:30:36 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.20266116941529236 on epoch=549
06/05/2022 08:30:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=553
06/05/2022 08:30:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=556
06/05/2022 08:30:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=559
06/05/2022 08:30:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=563
06/05/2022 08:30:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=566
06/05/2022 08:30:50 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.17454545454545456 on epoch=566
06/05/2022 08:30:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=569
06/05/2022 08:30:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
06/05/2022 08:30:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
06/05/2022 08:31:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=579
06/05/2022 08:31:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=583
06/05/2022 08:31:06 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.1507143152925587 on epoch=583
06/05/2022 08:31:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=586
06/05/2022 08:31:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
06/05/2022 08:31:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=593
06/05/2022 08:31:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
06/05/2022 08:31:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=599
06/05/2022 08:31:21 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.11904761904761905 on epoch=599
06/05/2022 08:31:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
06/05/2022 08:31:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=606
06/05/2022 08:31:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=609
06/05/2022 08:31:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=613
06/05/2022 08:31:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=616
06/05/2022 08:31:36 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.16218995765275257 on epoch=616
06/05/2022 08:31:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=619
06/05/2022 08:31:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=623
06/05/2022 08:31:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/05/2022 08:31:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=629
06/05/2022 08:31:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=633
06/05/2022 08:31:52 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.2436063856436884 on epoch=633
06/05/2022 08:31:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
06/05/2022 08:31:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
06/05/2022 08:32:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=643
06/05/2022 08:32:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/05/2022 08:32:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
06/05/2022 08:32:07 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.2116883116883117 on epoch=649
06/05/2022 08:32:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=653
06/05/2022 08:32:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
06/05/2022 08:32:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=659
06/05/2022 08:32:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
06/05/2022 08:32:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=666
06/05/2022 08:32:22 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.1425829238329238 on epoch=666
06/05/2022 08:32:24 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=669
06/05/2022 08:32:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
06/05/2022 08:32:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=676
06/05/2022 08:32:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
06/05/2022 08:32:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/05/2022 08:32:37 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.21646551724137933 on epoch=683
06/05/2022 08:32:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
06/05/2022 08:32:42 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=689
06/05/2022 08:32:45 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/05/2022 08:32:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
06/05/2022 08:32:50 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=699
06/05/2022 08:32:52 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.17542242703533026 on epoch=699
06/05/2022 08:32:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
06/05/2022 08:32:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=706
06/05/2022 08:33:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
06/05/2022 08:33:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
06/05/2022 08:33:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/05/2022 08:33:07 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.2088952654232425 on epoch=716
06/05/2022 08:33:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
06/05/2022 08:33:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/05/2022 08:33:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/05/2022 08:33:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=729
06/05/2022 08:33:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/05/2022 08:33:21 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.1109049680478252 on epoch=733
06/05/2022 08:33:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
06/05/2022 08:33:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
06/05/2022 08:33:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
06/05/2022 08:33:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 08:33:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
06/05/2022 08:33:36 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.12474747474747476 on epoch=749
06/05/2022 08:33:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
06/05/2022 08:33:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/05/2022 08:33:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
06/05/2022 08:33:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/05/2022 08:33:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=766
06/05/2022 08:33:50 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.14444971537001897 on epoch=766
06/05/2022 08:33:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
06/05/2022 08:33:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/05/2022 08:33:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=776
06/05/2022 08:34:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/05/2022 08:34:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
06/05/2022 08:34:05 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.16077835208269992 on epoch=783
06/05/2022 08:34:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
06/05/2022 08:34:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=789
06/05/2022 08:34:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
06/05/2022 08:34:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
06/05/2022 08:34:18 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
06/05/2022 08:34:19 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.12544953116381688 on epoch=799
06/05/2022 08:34:22 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 08:34:25 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 08:34:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/05/2022 08:34:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
06/05/2022 08:34:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
06/05/2022 08:34:34 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.17065390749601278 on epoch=816
06/05/2022 08:34:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
06/05/2022 08:34:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
06/05/2022 08:34:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 08:34:45 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=829
06/05/2022 08:34:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
06/05/2022 08:34:49 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.1858517769539003 on epoch=833
06/05/2022 08:34:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 08:34:54 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 08:34:57 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/05/2022 08:34:59 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/05/2022 08:35:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
06/05/2022 08:35:03 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.1812330198537095 on epoch=849
06/05/2022 08:35:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
06/05/2022 08:35:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 08:35:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 08:35:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/05/2022 08:35:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=866
06/05/2022 08:35:18 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.24638795508189681 on epoch=866
06/05/2022 08:35:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
06/05/2022 08:35:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/05/2022 08:35:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 08:35:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/05/2022 08:35:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 08:35:33 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.2090811965811966 on epoch=883
06/05/2022 08:35:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
06/05/2022 08:35:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 08:35:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 08:35:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
06/05/2022 08:35:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 08:35:47 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.18820033955857385 on epoch=899
06/05/2022 08:35:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
06/05/2022 08:35:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 08:35:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=909
06/05/2022 08:35:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
06/05/2022 08:36:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 08:36:02 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.20894539911308205 on epoch=916
06/05/2022 08:36:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
06/05/2022 08:36:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=923
06/05/2022 08:36:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
06/05/2022 08:36:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=929
06/05/2022 08:36:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
06/05/2022 08:36:17 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.21014492753623187 on epoch=933
06/05/2022 08:36:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
06/05/2022 08:36:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
06/05/2022 08:36:25 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/05/2022 08:36:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/05/2022 08:36:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=949
06/05/2022 08:36:31 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.20986622073578595 on epoch=949
06/05/2022 08:36:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 08:36:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 08:36:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 08:36:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/05/2022 08:36:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
06/05/2022 08:36:46 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.1831578947368421 on epoch=966
06/05/2022 08:36:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 08:36:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 08:36:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 08:36:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=979
06/05/2022 08:36:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
06/05/2022 08:37:00 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.22242572242572242 on epoch=983
06/05/2022 08:37:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
06/05/2022 08:37:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 08:37:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
06/05/2022 08:37:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 08:37:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 08:37:15 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.1796638655462185 on epoch=999
06/05/2022 08:37:15 - INFO - __main__ - save last model!
06/05/2022 08:37:15 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:37:15 - INFO - __main__ - Printing 3 examples
06/05/2022 08:37:15 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/05/2022 08:37:15 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:15 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/05/2022 08:37:15 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:15 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/05/2022 08:37:15 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:15 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:37:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 08:37:15 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 08:37:15 - INFO - __main__ - Printing 3 examples
06/05/2022 08:37:15 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 08:37:15 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:15 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 08:37:15 - INFO - __main__ - ['entailment']
06/05/2022 08:37:15 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 08:37:15 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:15 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:37:15 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:37:15 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 08:37:15 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:37:15 - INFO - __main__ - Printing 3 examples
06/05/2022 08:37:15 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
06/05/2022 08:37:15 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:15 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
06/05/2022 08:37:15 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:15 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
06/05/2022 08:37:15 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:15 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:37:15 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:37:15 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 08:37:15 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:37:16 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 08:37:34 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 08:37:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 08:37:34 - INFO - __main__ - Starting training!
06/05/2022 08:37:46 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_42_0.2_8_predictions.txt
06/05/2022 08:37:46 - INFO - __main__ - Classification-F1 on test data: 0.1377
06/05/2022 08:37:46 - INFO - __main__ - prefix=anli_16_42, lr=0.2, bsz=8, dev_performance=0.2491073612130105, test_performance=0.13765767002865478
06/05/2022 08:37:46 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.5, bsz=8 ...
06/05/2022 08:37:47 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:37:47 - INFO - __main__ - Printing 3 examples
06/05/2022 08:37:47 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/05/2022 08:37:47 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:47 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/05/2022 08:37:47 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:47 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/05/2022 08:37:47 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:47 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:37:47 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:37:47 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 08:37:47 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:37:47 - INFO - __main__ - Printing 3 examples
06/05/2022 08:37:47 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
06/05/2022 08:37:47 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:47 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
06/05/2022 08:37:47 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:47 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
06/05/2022 08:37:47 - INFO - __main__ - ['contradiction']
06/05/2022 08:37:47 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:37:48 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:37:48 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 08:38:04 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 08:38:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 08:38:05 - INFO - __main__ - Starting training!
06/05/2022 08:38:08 - INFO - __main__ - Step 10 Global step 10 Train loss 0.84 on epoch=3
06/05/2022 08:38:11 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=6
06/05/2022 08:38:13 - INFO - __main__ - Step 30 Global step 30 Train loss 0.57 on epoch=9
06/05/2022 08:38:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.54 on epoch=13
06/05/2022 08:38:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=16
06/05/2022 08:38:20 - INFO - __main__ - Global step 50 Train loss 0.60 Classification-F1 0.2623951182303585 on epoch=16
06/05/2022 08:38:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2623951182303585 on epoch=16, global_step=50
06/05/2022 08:38:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=19
06/05/2022 08:38:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
06/05/2022 08:38:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=26
06/05/2022 08:38:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
06/05/2022 08:38:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=33
06/05/2022 08:38:35 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.3051365473725722 on epoch=33
06/05/2022 08:38:35 - INFO - __main__ - Saving model with best Classification-F1: 0.2623951182303585 -> 0.3051365473725722 on epoch=33, global_step=100
06/05/2022 08:38:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=36
06/05/2022 08:38:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=39
06/05/2022 08:38:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=43
06/05/2022 08:38:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=46
06/05/2022 08:38:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
06/05/2022 08:38:49 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.1693121693121693 on epoch=49
06/05/2022 08:38:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=53
06/05/2022 08:38:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=56
06/05/2022 08:38:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=59
06/05/2022 08:39:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=63
06/05/2022 08:39:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=66
06/05/2022 08:39:04 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.32391534391534393 on epoch=66
06/05/2022 08:39:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3051365473725722 -> 0.32391534391534393 on epoch=66, global_step=200
06/05/2022 08:39:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=69
06/05/2022 08:39:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=73
06/05/2022 08:39:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=76
06/05/2022 08:39:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
06/05/2022 08:39:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
06/05/2022 08:39:18 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.2333333333333333 on epoch=83
06/05/2022 08:39:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=86
06/05/2022 08:39:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=89
06/05/2022 08:39:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=93
06/05/2022 08:39:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=96
06/05/2022 08:39:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=99
06/05/2022 08:39:33 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.21448527831506556 on epoch=99
06/05/2022 08:39:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=103
06/05/2022 08:39:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=106
06/05/2022 08:39:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.31 on epoch=109
06/05/2022 08:39:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=113
06/05/2022 08:39:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=116
06/05/2022 08:39:48 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.27180527383367137 on epoch=116
06/05/2022 08:39:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=119
06/05/2022 08:39:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=123
06/05/2022 08:39:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=126
06/05/2022 08:39:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=129
06/05/2022 08:40:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=133
06/05/2022 08:40:02 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.27828354670459937 on epoch=133
06/05/2022 08:40:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=136
06/05/2022 08:40:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=139
06/05/2022 08:40:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=143
06/05/2022 08:40:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.16 on epoch=146
06/05/2022 08:40:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=149
06/05/2022 08:40:17 - INFO - __main__ - Global step 450 Train loss 0.19 Classification-F1 0.23137254901960783 on epoch=149
06/05/2022 08:40:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.16 on epoch=153
06/05/2022 08:40:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.14 on epoch=156
06/05/2022 08:40:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.12 on epoch=159
06/05/2022 08:40:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.14 on epoch=163
06/05/2022 08:40:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=166
06/05/2022 08:40:31 - INFO - __main__ - Global step 500 Train loss 0.15 Classification-F1 0.2323232323232323 on epoch=166
06/05/2022 08:40:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=169
06/05/2022 08:40:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=173
06/05/2022 08:40:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=176
06/05/2022 08:40:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=179
06/05/2022 08:40:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.13 on epoch=183
06/05/2022 08:40:46 - INFO - __main__ - Global step 550 Train loss 0.15 Classification-F1 0.32970356926799754 on epoch=183
06/05/2022 08:40:46 - INFO - __main__ - Saving model with best Classification-F1: 0.32391534391534393 -> 0.32970356926799754 on epoch=183, global_step=550
06/05/2022 08:40:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=186
06/05/2022 08:40:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=189
06/05/2022 08:40:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.10 on epoch=193
06/05/2022 08:40:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=196
06/05/2022 08:40:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.10 on epoch=199
06/05/2022 08:41:01 - INFO - __main__ - Global step 600 Train loss 0.09 Classification-F1 0.27690361168622035 on epoch=199
06/05/2022 08:41:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.08 on epoch=203
06/05/2022 08:41:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=206
06/05/2022 08:41:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=209
06/05/2022 08:41:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=213
06/05/2022 08:41:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=216
06/05/2022 08:41:15 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.18308080808080807 on epoch=216
06/05/2022 08:41:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=219
06/05/2022 08:41:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=223
06/05/2022 08:41:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.06 on epoch=226
06/05/2022 08:41:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=229
06/05/2022 08:41:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=233
06/05/2022 08:41:30 - INFO - __main__ - Global step 700 Train loss 0.07 Classification-F1 0.21061492114123692 on epoch=233
06/05/2022 08:41:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=236
06/05/2022 08:41:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=239
06/05/2022 08:41:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=243
06/05/2022 08:41:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=246
06/05/2022 08:41:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=249
06/05/2022 08:41:45 - INFO - __main__ - Global step 750 Train loss 0.05 Classification-F1 0.22346491228070176 on epoch=249
06/05/2022 08:41:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=253
06/05/2022 08:41:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=256
06/05/2022 08:41:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=259
06/05/2022 08:41:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=263
06/05/2022 08:41:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=266
06/05/2022 08:41:59 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.16843971631205673 on epoch=266
06/05/2022 08:42:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=269
06/05/2022 08:42:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=273
06/05/2022 08:42:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=276
06/05/2022 08:42:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=279
06/05/2022 08:42:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=283
06/05/2022 08:42:14 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.15097402597402598 on epoch=283
06/05/2022 08:42:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=286
06/05/2022 08:42:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=289
06/05/2022 08:42:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=293
06/05/2022 08:42:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=296
06/05/2022 08:42:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=299
06/05/2022 08:42:28 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.17786561264822134 on epoch=299
06/05/2022 08:42:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=303
06/05/2022 08:42:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=306
06/05/2022 08:42:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=309
06/05/2022 08:42:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=313
06/05/2022 08:42:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=316
06/05/2022 08:42:43 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.1254355400696864 on epoch=316
06/05/2022 08:42:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=319
06/05/2022 08:42:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=323
06/05/2022 08:42:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=326
06/05/2022 08:42:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=329
06/05/2022 08:42:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=333
06/05/2022 08:42:57 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.19963369963369962 on epoch=333
06/05/2022 08:43:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=336
06/05/2022 08:43:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
06/05/2022 08:43:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
06/05/2022 08:43:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
06/05/2022 08:43:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=349
06/05/2022 08:43:12 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.17712418300653596 on epoch=349
06/05/2022 08:43:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=353
06/05/2022 08:43:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=356
06/05/2022 08:43:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=359
06/05/2022 08:43:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
06/05/2022 08:43:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
06/05/2022 08:43:26 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.1303735024665257 on epoch=366
06/05/2022 08:43:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
06/05/2022 08:43:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=373
06/05/2022 08:43:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=376
06/05/2022 08:43:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
06/05/2022 08:43:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=383
06/05/2022 08:43:41 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.16873858855283003 on epoch=383
06/05/2022 08:43:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
06/05/2022 08:43:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=389
06/05/2022 08:43:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=393
06/05/2022 08:43:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
06/05/2022 08:43:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
06/05/2022 08:43:56 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.19796419796419795 on epoch=399
06/05/2022 08:43:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
06/05/2022 08:44:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=406
06/05/2022 08:44:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=409
06/05/2022 08:44:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=413
06/05/2022 08:44:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=416
06/05/2022 08:44:11 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.2424242424242424 on epoch=416
06/05/2022 08:44:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=419
06/05/2022 08:44:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
06/05/2022 08:44:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
06/05/2022 08:44:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
06/05/2022 08:44:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=433
06/05/2022 08:44:25 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.15724688747944562 on epoch=433
06/05/2022 08:44:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
06/05/2022 08:44:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/05/2022 08:44:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=443
06/05/2022 08:44:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=446
06/05/2022 08:44:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
06/05/2022 08:44:40 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.23781436349494423 on epoch=449
06/05/2022 08:44:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
06/05/2022 08:44:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=456
06/05/2022 08:44:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
06/05/2022 08:44:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
06/05/2022 08:44:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
06/05/2022 08:44:55 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.23689516129032256 on epoch=466
06/05/2022 08:44:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
06/05/2022 08:45:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
06/05/2022 08:45:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
06/05/2022 08:45:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
06/05/2022 08:45:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
06/05/2022 08:45:10 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.3821647734691212 on epoch=483
06/05/2022 08:45:10 - INFO - __main__ - Saving model with best Classification-F1: 0.32970356926799754 -> 0.3821647734691212 on epoch=483, global_step=1450
06/05/2022 08:45:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
06/05/2022 08:45:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=489
06/05/2022 08:45:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
06/05/2022 08:45:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
06/05/2022 08:45:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
06/05/2022 08:45:25 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.10702143385070215 on epoch=499
06/05/2022 08:45:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
06/05/2022 08:45:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
06/05/2022 08:45:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
06/05/2022 08:45:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
06/05/2022 08:45:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
06/05/2022 08:45:39 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.20138888888888887 on epoch=516
06/05/2022 08:45:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
06/05/2022 08:45:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
06/05/2022 08:45:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
06/05/2022 08:45:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/05/2022 08:45:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
06/05/2022 08:45:55 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.20272128892818547 on epoch=533
06/05/2022 08:45:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
06/05/2022 08:46:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/05/2022 08:46:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/05/2022 08:46:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
06/05/2022 08:46:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
06/05/2022 08:46:10 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.18118279569892476 on epoch=549
06/05/2022 08:46:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
06/05/2022 08:46:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=556
06/05/2022 08:46:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
06/05/2022 08:46:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
06/05/2022 08:46:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
06/05/2022 08:46:25 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.201439205955335 on epoch=566
06/05/2022 08:46:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=569
06/05/2022 08:46:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
06/05/2022 08:46:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/05/2022 08:46:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/05/2022 08:46:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/05/2022 08:46:40 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.32482721956406163 on epoch=583
06/05/2022 08:46:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/05/2022 08:46:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
06/05/2022 08:46:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
06/05/2022 08:46:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/05/2022 08:46:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/05/2022 08:46:55 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.25847226762558567 on epoch=599
06/05/2022 08:46:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
06/05/2022 08:47:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
06/05/2022 08:47:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
06/05/2022 08:47:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/05/2022 08:47:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 08:47:10 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.3135449735449735 on epoch=616
06/05/2022 08:47:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/05/2022 08:47:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/05/2022 08:47:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/05/2022 08:47:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/05/2022 08:47:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=633
06/05/2022 08:47:25 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.0976530612244898 on epoch=633
06/05/2022 08:47:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/05/2022 08:47:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/05/2022 08:47:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/05/2022 08:47:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=646
06/05/2022 08:47:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
06/05/2022 08:47:40 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.1854928457869634 on epoch=649
06/05/2022 08:47:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=653
06/05/2022 08:47:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/05/2022 08:47:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=659
06/05/2022 08:47:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 08:47:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/05/2022 08:47:55 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.17350193665983138 on epoch=666
06/05/2022 08:47:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/05/2022 08:48:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
06/05/2022 08:48:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/05/2022 08:48:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/05/2022 08:48:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/05/2022 08:48:10 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.18068924539512776 on epoch=683
06/05/2022 08:48:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 08:48:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/05/2022 08:48:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/05/2022 08:48:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/05/2022 08:48:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
06/05/2022 08:48:25 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.20158963071655658 on epoch=699
06/05/2022 08:48:28 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/05/2022 08:48:31 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
06/05/2022 08:48:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/05/2022 08:48:36 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/05/2022 08:48:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/05/2022 08:48:41 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.3120879120879121 on epoch=716
06/05/2022 08:48:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
06/05/2022 08:48:46 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
06/05/2022 08:48:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/05/2022 08:48:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
06/05/2022 08:48:55 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
06/05/2022 08:48:56 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.2388888888888889 on epoch=733
06/05/2022 08:48:59 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/05/2022 08:49:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/05/2022 08:49:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 08:49:07 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 08:49:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
06/05/2022 08:49:12 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.24563724563724562 on epoch=749
06/05/2022 08:49:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 08:49:17 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/05/2022 08:49:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/05/2022 08:49:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/05/2022 08:49:26 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/05/2022 08:49:27 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.3071428571428571 on epoch=766
06/05/2022 08:49:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 08:49:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/05/2022 08:49:35 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/05/2022 08:49:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 08:49:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/05/2022 08:49:42 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.25846181109339006 on epoch=783
06/05/2022 08:49:45 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 08:49:47 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 08:49:50 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=793
06/05/2022 08:49:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 08:49:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 08:49:57 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.22398190045248872 on epoch=799
06/05/2022 08:49:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 08:50:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=806
06/05/2022 08:50:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 08:50:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 08:50:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
06/05/2022 08:50:12 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.17882380198478587 on epoch=816
06/05/2022 08:50:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
06/05/2022 08:50:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
06/05/2022 08:50:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
06/05/2022 08:50:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 08:50:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 08:50:27 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.2599760270240819 on epoch=833
06/05/2022 08:50:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 08:50:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
06/05/2022 08:50:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
06/05/2022 08:50:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
06/05/2022 08:50:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
06/05/2022 08:50:41 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.26048454469507104 on epoch=849
06/05/2022 08:50:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 08:50:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 08:50:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 08:50:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
06/05/2022 08:50:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 08:50:56 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.19219219219219216 on epoch=866
06/05/2022 08:50:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 08:51:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/05/2022 08:51:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 08:51:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 08:51:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 08:51:11 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.1850284495021337 on epoch=883
06/05/2022 08:51:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 08:51:17 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 08:51:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 08:51:22 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
06/05/2022 08:51:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 08:51:26 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.1793394777265745 on epoch=899
06/05/2022 08:51:29 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 08:51:32 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 08:51:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/05/2022 08:51:37 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 08:51:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 08:51:42 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.27249874308697836 on epoch=916
06/05/2022 08:51:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=919
06/05/2022 08:51:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 08:51:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 08:51:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 08:51:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 08:51:57 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.23973492723492723 on epoch=933
06/05/2022 08:52:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 08:52:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 08:52:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 08:52:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 08:52:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 08:52:12 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.2922779922779922 on epoch=949
06/05/2022 08:52:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 08:52:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 08:52:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 08:52:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 08:52:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/05/2022 08:52:28 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.3148982315648982 on epoch=966
06/05/2022 08:52:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 08:52:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 08:52:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 08:52:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 08:52:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 08:52:43 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.3173789173789174 on epoch=983
06/05/2022 08:52:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 08:52:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 08:52:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 08:52:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 08:52:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 08:52:58 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:52:58 - INFO - __main__ - Printing 3 examples
06/05/2022 08:52:58 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/05/2022 08:52:58 - INFO - __main__ - ['contradiction']
06/05/2022 08:52:58 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/05/2022 08:52:58 - INFO - __main__ - ['contradiction']
06/05/2022 08:52:58 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/05/2022 08:52:58 - INFO - __main__ - ['contradiction']
06/05/2022 08:52:58 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:52:58 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:52:58 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 08:52:58 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:52:58 - INFO - __main__ - Printing 3 examples
06/05/2022 08:52:58 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
06/05/2022 08:52:58 - INFO - __main__ - ['contradiction']
06/05/2022 08:52:58 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
06/05/2022 08:52:58 - INFO - __main__ - ['contradiction']
06/05/2022 08:52:58 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
06/05/2022 08:52:58 - INFO - __main__ - ['contradiction']
06/05/2022 08:52:58 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:52:58 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:52:58 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.3958333333333333 on epoch=999
06/05/2022 08:52:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3821647734691212 -> 0.3958333333333333 on epoch=999, global_step=3000
06/05/2022 08:52:58 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 08:52:58 - INFO - __main__ - save last model!
06/05/2022 08:52:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 08:52:58 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 08:52:58 - INFO - __main__ - Printing 3 examples
06/05/2022 08:52:58 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 08:52:58 - INFO - __main__ - ['contradiction']
06/05/2022 08:52:58 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 08:52:58 - INFO - __main__ - ['entailment']
06/05/2022 08:52:58 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 08:52:58 - INFO - __main__ - ['contradiction']
06/05/2022 08:52:58 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:52:59 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:53:00 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 08:53:17 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 08:53:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 08:53:18 - INFO - __main__ - Starting training!
06/05/2022 08:53:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_87_0.5_8_predictions.txt
06/05/2022 08:53:30 - INFO - __main__ - Classification-F1 on test data: 0.2605
06/05/2022 08:53:31 - INFO - __main__ - prefix=anli_16_87, lr=0.5, bsz=8, dev_performance=0.3958333333333333, test_performance=0.2604845576180288
06/05/2022 08:53:31 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.4, bsz=8 ...
06/05/2022 08:53:31 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:53:31 - INFO - __main__ - Printing 3 examples
06/05/2022 08:53:31 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/05/2022 08:53:31 - INFO - __main__ - ['contradiction']
06/05/2022 08:53:31 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/05/2022 08:53:31 - INFO - __main__ - ['contradiction']
06/05/2022 08:53:31 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/05/2022 08:53:31 - INFO - __main__ - ['contradiction']
06/05/2022 08:53:31 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:53:31 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:53:32 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 08:53:32 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 08:53:32 - INFO - __main__ - Printing 3 examples
06/05/2022 08:53:32 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
06/05/2022 08:53:32 - INFO - __main__ - ['contradiction']
06/05/2022 08:53:32 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
06/05/2022 08:53:32 - INFO - __main__ - ['contradiction']
06/05/2022 08:53:32 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
06/05/2022 08:53:32 - INFO - __main__ - ['contradiction']
06/05/2022 08:53:32 - INFO - __main__ - Tokenizing Input ...
06/05/2022 08:53:32 - INFO - __main__ - Tokenizing Output ...
06/05/2022 08:53:32 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 08:53:51 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 08:53:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 08:53:53 - INFO - __main__ - Starting training!
06/05/2022 08:53:56 - INFO - __main__ - Step 10 Global step 10 Train loss 0.94 on epoch=3
06/05/2022 08:53:59 - INFO - __main__ - Step 20 Global step 20 Train loss 0.58 on epoch=6
06/05/2022 08:54:01 - INFO - __main__ - Step 30 Global step 30 Train loss 0.58 on epoch=9
06/05/2022 08:54:04 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=13
06/05/2022 08:54:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=16
06/05/2022 08:54:08 - INFO - __main__ - Global step 50 Train loss 0.63 Classification-F1 0.2142110762800418 on epoch=16
06/05/2022 08:54:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2142110762800418 on epoch=16, global_step=50
06/05/2022 08:54:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=19
06/05/2022 08:54:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=23
06/05/2022 08:54:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=26
06/05/2022 08:54:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=29
06/05/2022 08:54:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=33
06/05/2022 08:54:23 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=33
06/05/2022 08:54:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
06/05/2022 08:54:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=39
06/05/2022 08:54:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=43
06/05/2022 08:54:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=46
06/05/2022 08:54:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
06/05/2022 08:54:39 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=49
06/05/2022 08:54:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=53
06/05/2022 08:54:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
06/05/2022 08:54:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=59
06/05/2022 08:54:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=63
06/05/2022 08:54:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=66
06/05/2022 08:54:54 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=66
06/05/2022 08:54:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=69
06/05/2022 08:54:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=73
06/05/2022 08:55:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
06/05/2022 08:55:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=79
06/05/2022 08:55:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=83
06/05/2022 08:55:09 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.32464054710995754 on epoch=83
06/05/2022 08:55:09 - INFO - __main__ - Saving model with best Classification-F1: 0.2142110762800418 -> 0.32464054710995754 on epoch=83, global_step=250
06/05/2022 08:55:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=86
06/05/2022 08:55:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=89
06/05/2022 08:55:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=93
06/05/2022 08:55:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.37 on epoch=96
06/05/2022 08:55:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
06/05/2022 08:55:24 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.23809523809523814 on epoch=99
06/05/2022 08:55:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.34 on epoch=103
06/05/2022 08:55:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=106
06/05/2022 08:55:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=109
06/05/2022 08:55:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.31 on epoch=113
06/05/2022 08:55:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=116
06/05/2022 08:55:40 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.27970951343500366 on epoch=116
06/05/2022 08:55:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=119
06/05/2022 08:55:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=123
06/05/2022 08:55:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=126
06/05/2022 08:55:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=129
06/05/2022 08:55:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=133
06/05/2022 08:55:55 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.3343253968253968 on epoch=133
06/05/2022 08:55:55 - INFO - __main__ - Saving model with best Classification-F1: 0.32464054710995754 -> 0.3343253968253968 on epoch=133, global_step=400
06/05/2022 08:55:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=136
06/05/2022 08:56:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=139
06/05/2022 08:56:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=143
06/05/2022 08:56:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=146
06/05/2022 08:56:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=149
06/05/2022 08:56:10 - INFO - __main__ - Global step 450 Train loss 0.21 Classification-F1 0.19015957446808515 on epoch=149
06/05/2022 08:56:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=153
06/05/2022 08:56:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=156
06/05/2022 08:56:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=159
06/05/2022 08:56:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.15 on epoch=163
06/05/2022 08:56:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=166
06/05/2022 08:56:25 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.30606430606430607 on epoch=166
06/05/2022 08:56:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=169
06/05/2022 08:56:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=173
06/05/2022 08:56:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=176
06/05/2022 08:56:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=179
06/05/2022 08:56:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=183
06/05/2022 08:56:40 - INFO - __main__ - Global step 550 Train loss 0.13 Classification-F1 0.3917483660130719 on epoch=183
06/05/2022 08:56:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3343253968253968 -> 0.3917483660130719 on epoch=183, global_step=550
06/05/2022 08:56:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=186
06/05/2022 08:56:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=189
06/05/2022 08:56:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=193
06/05/2022 08:56:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=196
06/05/2022 08:56:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=199
06/05/2022 08:56:55 - INFO - __main__ - Global step 600 Train loss 0.08 Classification-F1 0.4174525745257453 on epoch=199
06/05/2022 08:56:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3917483660130719 -> 0.4174525745257453 on epoch=199, global_step=600
06/05/2022 08:56:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=203
06/05/2022 08:57:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=206
06/05/2022 08:57:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=209
06/05/2022 08:57:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=213
06/05/2022 08:57:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=216
06/05/2022 08:57:10 - INFO - __main__ - Global step 650 Train loss 0.08 Classification-F1 0.3523629148629149 on epoch=216
06/05/2022 08:57:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=219
06/05/2022 08:57:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=223
06/05/2022 08:57:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.06 on epoch=226
06/05/2022 08:57:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=229
06/05/2022 08:57:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=233
06/05/2022 08:57:25 - INFO - __main__ - Global step 700 Train loss 0.07 Classification-F1 0.3395336381988218 on epoch=233
06/05/2022 08:57:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=236
06/05/2022 08:57:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=239
06/05/2022 08:57:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=243
06/05/2022 08:57:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=246
06/05/2022 08:57:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=249
06/05/2022 08:57:40 - INFO - __main__ - Global step 750 Train loss 0.05 Classification-F1 0.35846560846560854 on epoch=249
06/05/2022 08:57:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=253
06/05/2022 08:57:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=256
06/05/2022 08:57:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=259
06/05/2022 08:57:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=263
06/05/2022 08:57:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=266
06/05/2022 08:57:56 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.37128427128427127 on epoch=266
06/05/2022 08:57:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=269
06/05/2022 08:58:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=273
06/05/2022 08:58:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=276
06/05/2022 08:58:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=279
06/05/2022 08:58:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=283
06/05/2022 08:58:11 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.31351214574898784 on epoch=283
06/05/2022 08:58:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=286
06/05/2022 08:58:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=289
06/05/2022 08:58:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=293
06/05/2022 08:58:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=296
06/05/2022 08:58:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=299
06/05/2022 08:58:26 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.3823529411764706 on epoch=299
06/05/2022 08:58:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=303
06/05/2022 08:58:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=306
06/05/2022 08:58:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=309
06/05/2022 08:58:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=313
06/05/2022 08:58:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=316
06/05/2022 08:58:41 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.27247920997920994 on epoch=316
06/05/2022 08:58:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=319
06/05/2022 08:58:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=323
06/05/2022 08:58:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=326
06/05/2022 08:58:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=329
06/05/2022 08:58:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=333
06/05/2022 08:58:56 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.38955722639933166 on epoch=333
06/05/2022 08:58:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=336
06/05/2022 08:59:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=339
06/05/2022 08:59:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
06/05/2022 08:59:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=346
06/05/2022 08:59:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=349
06/05/2022 08:59:11 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.23790322580645162 on epoch=349
06/05/2022 08:59:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=353
06/05/2022 08:59:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=356
06/05/2022 08:59:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=359
06/05/2022 08:59:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
06/05/2022 08:59:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=366
06/05/2022 08:59:27 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.3350636509702138 on epoch=366
06/05/2022 08:59:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
06/05/2022 08:59:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
06/05/2022 08:59:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=376
06/05/2022 08:59:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
06/05/2022 08:59:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
06/05/2022 08:59:42 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.2930875576036866 on epoch=383
06/05/2022 08:59:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=386
06/05/2022 08:59:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=389
06/05/2022 08:59:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
06/05/2022 08:59:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
06/05/2022 08:59:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
06/05/2022 08:59:58 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.2950980392156863 on epoch=399
06/05/2022 09:00:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=403
06/05/2022 09:00:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=406
06/05/2022 09:00:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
06/05/2022 09:00:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
06/05/2022 09:00:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
06/05/2022 09:00:13 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.2950980392156863 on epoch=416
06/05/2022 09:00:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=419
06/05/2022 09:00:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
06/05/2022 09:00:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
06/05/2022 09:00:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
06/05/2022 09:00:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=433
06/05/2022 09:00:28 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.31244239631336407 on epoch=433
06/05/2022 09:00:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=436
06/05/2022 09:00:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=439
06/05/2022 09:00:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=443
06/05/2022 09:00:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
06/05/2022 09:00:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
06/05/2022 09:00:44 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.3356950067476383 on epoch=449
06/05/2022 09:00:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=453
06/05/2022 09:00:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
06/05/2022 09:00:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
06/05/2022 09:00:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=463
06/05/2022 09:00:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.90 on epoch=466
06/05/2022 09:00:59 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.21001779811848462 on epoch=466
06/05/2022 09:01:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.74 on epoch=469
06/05/2022 09:01:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=473
06/05/2022 09:01:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
06/05/2022 09:01:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=479
06/05/2022 09:01:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
06/05/2022 09:01:14 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.2572892040977147 on epoch=483
06/05/2022 09:01:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=486
06/05/2022 09:01:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=489
06/05/2022 09:01:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
06/05/2022 09:01:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
06/05/2022 09:01:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
06/05/2022 09:01:30 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.25598421186656484 on epoch=499
06/05/2022 09:01:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
06/05/2022 09:01:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/05/2022 09:01:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
06/05/2022 09:01:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
06/05/2022 09:01:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
06/05/2022 09:01:45 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.24407327586206895 on epoch=516
06/05/2022 09:01:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
06/05/2022 09:01:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
06/05/2022 09:01:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/05/2022 09:01:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
06/05/2022 09:01:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
06/05/2022 09:02:00 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.20903225806451614 on epoch=533
06/05/2022 09:02:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
06/05/2022 09:02:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
06/05/2022 09:02:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
06/05/2022 09:02:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/05/2022 09:02:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
06/05/2022 09:02:15 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.2235601118359739 on epoch=549
06/05/2022 09:02:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/05/2022 09:02:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
06/05/2022 09:02:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=559
06/05/2022 09:02:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/05/2022 09:02:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
06/05/2022 09:02:31 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.21825268817204302 on epoch=566
06/05/2022 09:02:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
06/05/2022 09:02:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
06/05/2022 09:02:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
06/05/2022 09:02:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/05/2022 09:02:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
06/05/2022 09:02:46 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.21825268817204302 on epoch=583
06/05/2022 09:02:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
06/05/2022 09:02:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
06/05/2022 09:02:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/05/2022 09:02:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
06/05/2022 09:03:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
06/05/2022 09:03:02 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.20606060606060606 on epoch=599
06/05/2022 09:03:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
06/05/2022 09:03:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
06/05/2022 09:03:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/05/2022 09:03:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
06/05/2022 09:03:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 09:03:17 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.21767350928641252 on epoch=616
06/05/2022 09:03:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
06/05/2022 09:03:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
06/05/2022 09:03:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
06/05/2022 09:03:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
06/05/2022 09:03:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/05/2022 09:03:32 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.21935483870967745 on epoch=633
06/05/2022 09:03:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
06/05/2022 09:03:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/05/2022 09:03:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
06/05/2022 09:03:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
06/05/2022 09:03:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/05/2022 09:03:47 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.16858823529411765 on epoch=649
06/05/2022 09:03:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/05/2022 09:03:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
06/05/2022 09:03:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
06/05/2022 09:03:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
06/05/2022 09:04:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
06/05/2022 09:04:02 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.1776908873683067 on epoch=666
06/05/2022 09:04:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/05/2022 09:04:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
06/05/2022 09:04:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
06/05/2022 09:04:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
06/05/2022 09:04:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/05/2022 09:04:18 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.21767241379310348 on epoch=683
06/05/2022 09:04:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 09:04:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
06/05/2022 09:04:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
06/05/2022 09:04:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=696
06/05/2022 09:04:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/05/2022 09:04:33 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.18695014662756596 on epoch=699
06/05/2022 09:04:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
06/05/2022 09:04:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
06/05/2022 09:04:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/05/2022 09:04:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/05/2022 09:04:47 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
06/05/2022 09:04:48 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.16715686274509803 on epoch=716
06/05/2022 09:04:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
06/05/2022 09:04:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
06/05/2022 09:04:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
06/05/2022 09:04:59 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/05/2022 09:05:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
06/05/2022 09:05:03 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.2277310924369748 on epoch=733
06/05/2022 09:05:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
06/05/2022 09:05:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
06/05/2022 09:05:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 09:05:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 09:05:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/05/2022 09:05:19 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.22928641251221898 on epoch=749
06/05/2022 09:05:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
06/05/2022 09:05:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/05/2022 09:05:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/05/2022 09:05:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
06/05/2022 09:05:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/05/2022 09:05:33 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.21725490196078429 on epoch=766
06/05/2022 09:05:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 09:05:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
06/05/2022 09:05:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/05/2022 09:05:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 09:05:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/05/2022 09:05:48 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.21524926686217008 on epoch=783
06/05/2022 09:05:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 09:05:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 09:05:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/05/2022 09:05:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 09:06:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
06/05/2022 09:06:03 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.20466338259441708 on epoch=799
06/05/2022 09:06:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 09:06:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
06/05/2022 09:06:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 09:06:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
06/05/2022 09:06:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/05/2022 09:06:18 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.20746031746031743 on epoch=816
06/05/2022 09:06:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
06/05/2022 09:06:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 09:06:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
06/05/2022 09:06:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
06/05/2022 09:06:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 09:06:33 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.18431899641577063 on epoch=833
06/05/2022 09:06:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
06/05/2022 09:06:39 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/05/2022 09:06:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=843
06/05/2022 09:06:44 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
06/05/2022 09:06:47 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/05/2022 09:06:48 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.2505446623093682 on epoch=849
06/05/2022 09:06:51 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 09:06:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
06/05/2022 09:06:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 09:06:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/05/2022 09:07:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 09:07:04 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.21575757575757576 on epoch=866
06/05/2022 09:07:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 09:07:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=873
06/05/2022 09:07:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=876
06/05/2022 09:07:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
06/05/2022 09:07:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 09:07:18 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.26698211044853515 on epoch=883
06/05/2022 09:07:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
06/05/2022 09:07:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 09:07:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
06/05/2022 09:07:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/05/2022 09:07:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 09:07:33 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.3941837732160313 on epoch=899
06/05/2022 09:07:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 09:07:38 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/05/2022 09:07:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
06/05/2022 09:07:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 09:07:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 09:07:48 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.2700757575757576 on epoch=916
06/05/2022 09:07:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 09:07:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
06/05/2022 09:07:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 09:07:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 09:08:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
06/05/2022 09:08:03 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.1964777680906713 on epoch=933
06/05/2022 09:08:06 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 09:08:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=939
06/05/2022 09:08:11 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 09:08:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
06/05/2022 09:08:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
06/05/2022 09:08:18 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.260752688172043 on epoch=949
06/05/2022 09:08:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
06/05/2022 09:08:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/05/2022 09:08:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=959
06/05/2022 09:08:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
06/05/2022 09:08:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
06/05/2022 09:08:33 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.26205307484506596 on epoch=966
06/05/2022 09:08:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 09:08:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
06/05/2022 09:08:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 09:08:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 09:08:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
06/05/2022 09:08:48 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.22275641025641024 on epoch=983
06/05/2022 09:08:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 09:08:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 09:08:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 09:08:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 09:09:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 09:09:03 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 09:09:03 - INFO - __main__ - Printing 3 examples
06/05/2022 09:09:03 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/05/2022 09:09:03 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:03 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/05/2022 09:09:03 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:03 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/05/2022 09:09:03 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:03 - INFO - __main__ - Tokenizing Input ...
06/05/2022 09:09:03 - INFO - __main__ - Tokenizing Output ...
06/05/2022 09:09:03 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 09:09:03 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 09:09:03 - INFO - __main__ - Printing 3 examples
06/05/2022 09:09:03 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
06/05/2022 09:09:03 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:03 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
06/05/2022 09:09:03 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:03 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
06/05/2022 09:09:03 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:03 - INFO - __main__ - Tokenizing Input ...
06/05/2022 09:09:03 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.25535714285714284 on epoch=999
06/05/2022 09:09:03 - INFO - __main__ - save last model!
06/05/2022 09:09:03 - INFO - __main__ - Tokenizing Output ...
06/05/2022 09:09:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 09:09:03 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 09:09:03 - INFO - __main__ - Printing 3 examples
06/05/2022 09:09:03 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 09:09:03 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:03 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 09:09:03 - INFO - __main__ - ['entailment']
06/05/2022 09:09:03 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 09:09:03 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:03 - INFO - __main__ - Tokenizing Input ...
06/05/2022 09:09:04 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 09:09:04 - INFO - __main__ - Tokenizing Output ...
06/05/2022 09:09:05 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 09:09:23 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 09:09:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 09:09:24 - INFO - __main__ - Starting training!
06/05/2022 09:09:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_87_0.4_8_predictions.txt
06/05/2022 09:09:35 - INFO - __main__ - Classification-F1 on test data: 0.0947
06/05/2022 09:09:35 - INFO - __main__ - prefix=anli_16_87, lr=0.4, bsz=8, dev_performance=0.4174525745257453, test_performance=0.09465095609514618
06/05/2022 09:09:35 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.3, bsz=8 ...
06/05/2022 09:09:36 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 09:09:36 - INFO - __main__ - Printing 3 examples
06/05/2022 09:09:36 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/05/2022 09:09:36 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:36 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/05/2022 09:09:36 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:36 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/05/2022 09:09:36 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:36 - INFO - __main__ - Tokenizing Input ...
06/05/2022 09:09:36 - INFO - __main__ - Tokenizing Output ...
06/05/2022 09:09:36 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 09:09:36 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 09:09:36 - INFO - __main__ - Printing 3 examples
06/05/2022 09:09:36 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
06/05/2022 09:09:36 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:36 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
06/05/2022 09:09:36 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:36 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
06/05/2022 09:09:36 - INFO - __main__ - ['contradiction']
06/05/2022 09:09:36 - INFO - __main__ - Tokenizing Input ...
06/05/2022 09:09:36 - INFO - __main__ - Tokenizing Output ...
06/05/2022 09:09:37 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 09:09:55 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 09:09:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 09:09:56 - INFO - __main__ - Starting training!
06/05/2022 09:10:00 - INFO - __main__ - Step 10 Global step 10 Train loss 0.95 on epoch=3
06/05/2022 09:10:03 - INFO - __main__ - Step 20 Global step 20 Train loss 0.65 on epoch=6
06/05/2022 09:10:05 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=9
06/05/2022 09:10:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=13
06/05/2022 09:10:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=16
06/05/2022 09:10:12 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.2027168234064786 on epoch=16
06/05/2022 09:10:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2027168234064786 on epoch=16, global_step=50
06/05/2022 09:10:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
06/05/2022 09:10:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=23
06/05/2022 09:10:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=26
06/05/2022 09:10:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=29
06/05/2022 09:10:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=33
06/05/2022 09:10:27 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.22582826233941847 on epoch=33
06/05/2022 09:10:27 - INFO - __main__ - Saving model with best Classification-F1: 0.2027168234064786 -> 0.22582826233941847 on epoch=33, global_step=100
06/05/2022 09:10:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
06/05/2022 09:10:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=39
06/05/2022 09:10:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=43
06/05/2022 09:10:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=46
06/05/2022 09:10:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
06/05/2022 09:10:42 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16129032258064516 on epoch=49
06/05/2022 09:10:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=53
06/05/2022 09:10:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=56
06/05/2022 09:10:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=59
06/05/2022 09:10:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=63
06/05/2022 09:10:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=66
06/05/2022 09:10:58 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.15873015873015875 on epoch=66
06/05/2022 09:11:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=69
06/05/2022 09:11:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
06/05/2022 09:11:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
06/05/2022 09:11:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
06/05/2022 09:11:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
06/05/2022 09:11:14 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.1693121693121693 on epoch=83
06/05/2022 09:11:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=86
06/05/2022 09:11:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=89
06/05/2022 09:11:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=93
06/05/2022 09:11:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=96
06/05/2022 09:11:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=99
06/05/2022 09:11:30 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.17204301075268816 on epoch=99
06/05/2022 09:11:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=103
06/05/2022 09:11:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=106
06/05/2022 09:11:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
06/05/2022 09:11:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=113
06/05/2022 09:11:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
06/05/2022 09:11:46 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=116
06/05/2022 09:11:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=119
06/05/2022 09:11:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=123
06/05/2022 09:11:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=126
06/05/2022 09:11:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=129
06/05/2022 09:12:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=133
06/05/2022 09:12:01 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.2619047619047619 on epoch=133
06/05/2022 09:12:01 - INFO - __main__ - Saving model with best Classification-F1: 0.22582826233941847 -> 0.2619047619047619 on epoch=133, global_step=400
06/05/2022 09:12:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=136
06/05/2022 09:12:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=139
06/05/2022 09:12:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=143
06/05/2022 09:12:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=146
06/05/2022 09:12:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=149
06/05/2022 09:12:16 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.20908004778972522 on epoch=149
06/05/2022 09:12:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=153
06/05/2022 09:12:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=156
06/05/2022 09:12:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=159
06/05/2022 09:12:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=163
06/05/2022 09:12:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=166
06/05/2022 09:12:31 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.19999999999999998 on epoch=166
06/05/2022 09:12:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
06/05/2022 09:12:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=173
06/05/2022 09:12:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=176
06/05/2022 09:12:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=179
06/05/2022 09:12:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=183
06/05/2022 09:12:47 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.20656634746922023 on epoch=183
06/05/2022 09:12:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=186
06/05/2022 09:12:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=189
06/05/2022 09:12:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=193
06/05/2022 09:12:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=196
06/05/2022 09:13:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=199
06/05/2022 09:13:02 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.19111111111111112 on epoch=199
06/05/2022 09:13:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=203
06/05/2022 09:13:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.28 on epoch=206
06/05/2022 09:13:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=209
06/05/2022 09:13:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=213
06/05/2022 09:13:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=216
06/05/2022 09:13:17 - INFO - __main__ - Global step 650 Train loss 0.29 Classification-F1 0.21555138628309356 on epoch=216
06/05/2022 09:13:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=219
06/05/2022 09:13:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=223
06/05/2022 09:13:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=226
06/05/2022 09:13:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=229
06/05/2022 09:13:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=233
06/05/2022 09:13:33 - INFO - __main__ - Global step 700 Train loss 0.24 Classification-F1 0.17318938371569947 on epoch=233
06/05/2022 09:13:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=236
06/05/2022 09:13:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=239
06/05/2022 09:13:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=243
06/05/2022 09:13:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=246
06/05/2022 09:13:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=249
06/05/2022 09:13:48 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.22736669015738786 on epoch=249
06/05/2022 09:13:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=253
06/05/2022 09:13:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.18 on epoch=256
06/05/2022 09:13:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=259
06/05/2022 09:13:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=263
06/05/2022 09:14:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=266
06/05/2022 09:14:04 - INFO - __main__ - Global step 800 Train loss 0.17 Classification-F1 0.18486590038314177 on epoch=266
06/05/2022 09:14:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=269
06/05/2022 09:14:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=273
06/05/2022 09:14:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=276
06/05/2022 09:14:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=279
06/05/2022 09:14:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=283
06/05/2022 09:14:19 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.2843976160068114 on epoch=283
06/05/2022 09:14:19 - INFO - __main__ - Saving model with best Classification-F1: 0.2619047619047619 -> 0.2843976160068114 on epoch=283, global_step=850
06/05/2022 09:14:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.15 on epoch=286
06/05/2022 09:14:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=289
06/05/2022 09:14:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=293
06/05/2022 09:14:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=296
06/05/2022 09:14:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=299
06/05/2022 09:14:35 - INFO - __main__ - Global step 900 Train loss 0.14 Classification-F1 0.2914862914862915 on epoch=299
06/05/2022 09:14:35 - INFO - __main__ - Saving model with best Classification-F1: 0.2843976160068114 -> 0.2914862914862915 on epoch=299, global_step=900
06/05/2022 09:14:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=303
06/05/2022 09:14:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=306
06/05/2022 09:14:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=309
06/05/2022 09:14:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=313
06/05/2022 09:14:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=316
06/05/2022 09:14:50 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.18911564625850338 on epoch=316
06/05/2022 09:14:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=319
06/05/2022 09:14:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=323
06/05/2022 09:14:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
06/05/2022 09:15:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=329
06/05/2022 09:15:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
06/05/2022 09:15:06 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.20012507817385863 on epoch=333
06/05/2022 09:15:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=336
06/05/2022 09:15:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=339
06/05/2022 09:15:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=343
06/05/2022 09:15:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=346
06/05/2022 09:15:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=349
06/05/2022 09:15:21 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.1733596664472241 on epoch=349
06/05/2022 09:15:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=353
06/05/2022 09:15:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=356
06/05/2022 09:15:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=359
06/05/2022 09:15:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=363
06/05/2022 09:15:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=366
06/05/2022 09:15:36 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.21245421245421248 on epoch=366
06/05/2022 09:15:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=369
06/05/2022 09:15:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
06/05/2022 09:15:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=376
06/05/2022 09:15:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=379
06/05/2022 09:15:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=383
06/05/2022 09:15:51 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.16541353383458646 on epoch=383
06/05/2022 09:15:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
06/05/2022 09:15:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=389
06/05/2022 09:16:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=393
06/05/2022 09:16:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=396
06/05/2022 09:16:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=399
06/05/2022 09:16:07 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.20920974450386215 on epoch=399
06/05/2022 09:16:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=403
06/05/2022 09:16:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
06/05/2022 09:16:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
06/05/2022 09:16:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
06/05/2022 09:16:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=416
06/05/2022 09:16:21 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.12154150197628458 on epoch=416
06/05/2022 09:16:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
06/05/2022 09:16:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=423
06/05/2022 09:16:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
06/05/2022 09:16:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=429
06/05/2022 09:16:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=433
06/05/2022 09:16:37 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.12323232323232322 on epoch=433
06/05/2022 09:16:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=436
06/05/2022 09:16:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
06/05/2022 09:16:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=443
06/05/2022 09:16:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=446
06/05/2022 09:16:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
06/05/2022 09:16:52 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.1525378151260504 on epoch=449
06/05/2022 09:16:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=453
06/05/2022 09:16:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
06/05/2022 09:17:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
06/05/2022 09:17:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
06/05/2022 09:17:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=466
06/05/2022 09:17:07 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.13194444444444445 on epoch=466
06/05/2022 09:17:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=469
06/05/2022 09:17:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
06/05/2022 09:17:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
06/05/2022 09:17:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
06/05/2022 09:17:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
06/05/2022 09:17:22 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.1126984126984127 on epoch=483
06/05/2022 09:17:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
06/05/2022 09:17:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
06/05/2022 09:17:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
06/05/2022 09:17:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
06/05/2022 09:17:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
06/05/2022 09:17:37 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.16241830065359478 on epoch=499
06/05/2022 09:17:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
06/05/2022 09:17:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
06/05/2022 09:17:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
06/05/2022 09:17:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
06/05/2022 09:17:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=516
06/05/2022 09:17:53 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.1565040650406504 on epoch=516
06/05/2022 09:17:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
06/05/2022 09:17:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
06/05/2022 09:18:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
06/05/2022 09:18:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=529
06/05/2022 09:18:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
06/05/2022 09:18:08 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.1468705547652916 on epoch=533
06/05/2022 09:18:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
06/05/2022 09:18:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
06/05/2022 09:18:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
06/05/2022 09:18:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
06/05/2022 09:18:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
06/05/2022 09:18:23 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.1573111573111573 on epoch=549
06/05/2022 09:18:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
06/05/2022 09:18:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
06/05/2022 09:18:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
06/05/2022 09:18:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
06/05/2022 09:18:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=566
06/05/2022 09:18:39 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.15952034883720928 on epoch=566
06/05/2022 09:18:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
06/05/2022 09:18:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
06/05/2022 09:18:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
06/05/2022 09:18:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
06/05/2022 09:18:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
06/05/2022 09:18:54 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.16760224538893342 on epoch=583
06/05/2022 09:18:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=586
06/05/2022 09:19:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
06/05/2022 09:19:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
06/05/2022 09:19:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
06/05/2022 09:19:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
06/05/2022 09:19:10 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.12908496732026145 on epoch=599
06/05/2022 09:19:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
06/05/2022 09:19:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
06/05/2022 09:19:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
06/05/2022 09:19:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
06/05/2022 09:19:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
06/05/2022 09:19:25 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.16112342941611235 on epoch=616
06/05/2022 09:19:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
06/05/2022 09:19:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
06/05/2022 09:19:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
06/05/2022 09:19:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
06/05/2022 09:19:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
06/05/2022 09:19:41 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.17280820374126662 on epoch=633
06/05/2022 09:19:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
06/05/2022 09:19:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
06/05/2022 09:19:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=643
06/05/2022 09:19:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
06/05/2022 09:19:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
06/05/2022 09:19:56 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.16323529411764703 on epoch=649
06/05/2022 09:19:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
06/05/2022 09:20:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
06/05/2022 09:20:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
06/05/2022 09:20:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
06/05/2022 09:20:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
06/05/2022 09:20:11 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.15116487455197133 on epoch=666
06/05/2022 09:20:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
06/05/2022 09:20:17 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
06/05/2022 09:20:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
06/05/2022 09:20:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=679
06/05/2022 09:20:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
06/05/2022 09:20:27 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.11602787456445993 on epoch=683
06/05/2022 09:20:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
06/05/2022 09:20:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
06/05/2022 09:20:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
06/05/2022 09:20:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
06/05/2022 09:20:41 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
06/05/2022 09:20:42 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.10640113798008535 on epoch=699
06/05/2022 09:20:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
06/05/2022 09:20:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
06/05/2022 09:20:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
06/05/2022 09:20:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
06/05/2022 09:20:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/05/2022 09:20:58 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.15837135837135835 on epoch=716
06/05/2022 09:21:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
06/05/2022 09:21:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=723
06/05/2022 09:21:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
06/05/2022 09:21:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
06/05/2022 09:21:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
06/05/2022 09:21:13 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.1132645803698435 on epoch=733
06/05/2022 09:21:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
06/05/2022 09:21:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
06/05/2022 09:21:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
06/05/2022 09:21:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
06/05/2022 09:21:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
06/05/2022 09:21:29 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.10581395348837211 on epoch=749
06/05/2022 09:21:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
06/05/2022 09:21:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
06/05/2022 09:21:38 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
06/05/2022 09:21:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=763
06/05/2022 09:21:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
06/05/2022 09:21:45 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.21908271908271904 on epoch=766
06/05/2022 09:21:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
06/05/2022 09:21:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=773
06/05/2022 09:21:54 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
06/05/2022 09:21:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
06/05/2022 09:22:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
06/05/2022 09:22:01 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.13710317460317462 on epoch=783
06/05/2022 09:22:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
06/05/2022 09:22:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 09:22:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
06/05/2022 09:22:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
06/05/2022 09:22:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
06/05/2022 09:22:17 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.11794871794871793 on epoch=799
06/05/2022 09:22:20 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
06/05/2022 09:22:23 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
06/05/2022 09:22:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
06/05/2022 09:22:28 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
06/05/2022 09:22:31 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
06/05/2022 09:22:33 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.21678321678321677 on epoch=816
06/05/2022 09:22:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
06/05/2022 09:22:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 09:22:41 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
06/05/2022 09:22:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
06/05/2022 09:22:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
06/05/2022 09:22:49 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.14304915514592934 on epoch=833
06/05/2022 09:22:52 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
06/05/2022 09:22:55 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=839
06/05/2022 09:22:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/05/2022 09:23:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
06/05/2022 09:23:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
06/05/2022 09:23:05 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.10955710955710954 on epoch=849
06/05/2022 09:23:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
06/05/2022 09:23:11 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 09:23:14 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
06/05/2022 09:23:17 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
06/05/2022 09:23:20 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
06/05/2022 09:23:22 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.18392644844257747 on epoch=866
06/05/2022 09:23:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
06/05/2022 09:23:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
06/05/2022 09:23:30 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
06/05/2022 09:23:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=879
06/05/2022 09:23:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
06/05/2022 09:23:38 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.22777434789319337 on epoch=883
06/05/2022 09:23:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=886
06/05/2022 09:23:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
06/05/2022 09:23:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
06/05/2022 09:23:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
06/05/2022 09:23:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=899
06/05/2022 09:23:53 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.1462174940898345 on epoch=899
06/05/2022 09:23:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
06/05/2022 09:23:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
06/05/2022 09:24:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
06/05/2022 09:24:05 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
06/05/2022 09:24:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
06/05/2022 09:24:09 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.20031691770822205 on epoch=916
06/05/2022 09:24:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
06/05/2022 09:24:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 09:24:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
06/05/2022 09:24:20 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
06/05/2022 09:24:23 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=933
06/05/2022 09:24:25 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.23019873271889402 on epoch=933
06/05/2022 09:24:27 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
06/05/2022 09:24:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
06/05/2022 09:24:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
06/05/2022 09:24:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
06/05/2022 09:24:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
06/05/2022 09:24:40 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.1669230769230769 on epoch=949
06/05/2022 09:24:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
06/05/2022 09:24:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
06/05/2022 09:24:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=959
06/05/2022 09:24:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
06/05/2022 09:24:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
06/05/2022 09:24:56 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.24096880131362888 on epoch=966
06/05/2022 09:24:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
06/05/2022 09:25:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
06/05/2022 09:25:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
06/05/2022 09:25:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 09:25:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
06/05/2022 09:25:11 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.24537037037037035 on epoch=983
06/05/2022 09:25:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
06/05/2022 09:25:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
06/05/2022 09:25:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 09:25:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
06/05/2022 09:25:25 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 09:25:26 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 09:25:26 - INFO - __main__ - Printing 3 examples
06/05/2022 09:25:26 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/05/2022 09:25:26 - INFO - __main__ - ['contradiction']
06/05/2022 09:25:26 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/05/2022 09:25:26 - INFO - __main__ - ['contradiction']
06/05/2022 09:25:26 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/05/2022 09:25:26 - INFO - __main__ - ['contradiction']
06/05/2022 09:25:26 - INFO - __main__ - Tokenizing Input ...
06/05/2022 09:25:26 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.20495798319327732 on epoch=999
06/05/2022 09:25:26 - INFO - __main__ - save last model!
06/05/2022 09:25:26 - INFO - __main__ - Tokenizing Output ...
06/05/2022 09:25:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 09:25:27 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 09:25:27 - INFO - __main__ - Printing 3 examples
06/05/2022 09:25:27 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 09:25:27 - INFO - __main__ - ['contradiction']
06/05/2022 09:25:27 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 09:25:27 - INFO - __main__ - ['entailment']
06/05/2022 09:25:27 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 09:25:27 - INFO - __main__ - ['contradiction']
06/05/2022 09:25:27 - INFO - __main__ - Tokenizing Input ...
06/05/2022 09:25:27 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 09:25:27 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 09:25:27 - INFO - __main__ - Printing 3 examples
06/05/2022 09:25:27 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
06/05/2022 09:25:27 - INFO - __main__ - ['contradiction']
06/05/2022 09:25:27 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
06/05/2022 09:25:27 - INFO - __main__ - ['contradiction']
06/05/2022 09:25:27 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
06/05/2022 09:25:27 - INFO - __main__ - ['contradiction']
06/05/2022 09:25:27 - INFO - __main__ - Tokenizing Input ...
06/05/2022 09:25:27 - INFO - __main__ - Tokenizing Output ...
06/05/2022 09:25:27 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 09:25:27 - INFO - __main__ - Tokenizing Output ...
06/05/2022 09:25:28 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 09:25:46 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 09:25:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 09:25:46 - INFO - __main__ - Starting training!
06/05/2022 09:26:00 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_87_0.3_8_predictions.txt
06/05/2022 09:26:00 - INFO - __main__ - Classification-F1 on test data: 0.1929
06/05/2022 09:26:00 - INFO - __main__ - prefix=anli_16_87, lr=0.3, bsz=8, dev_performance=0.2914862914862915, test_performance=0.19290469244723676
06/05/2022 09:26:00 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.2, bsz=8 ...
06/05/2022 09:26:01 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 09:26:01 - INFO - __main__ - Printing 3 examples
06/05/2022 09:26:01 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/05/2022 09:26:01 - INFO - __main__ - ['contradiction']
06/05/2022 09:26:01 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/05/2022 09:26:01 - INFO - __main__ - ['contradiction']
06/05/2022 09:26:01 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/05/2022 09:26:01 - INFO - __main__ - ['contradiction']
06/05/2022 09:26:01 - INFO - __main__ - Tokenizing Input ...
06/05/2022 09:26:01 - INFO - __main__ - Tokenizing Output ...
06/05/2022 09:26:01 - INFO - __main__ - Loaded 48 examples from train data
06/05/2022 09:26:01 - INFO - __main__ - Start tokenizing ... 48 instances
06/05/2022 09:26:01 - INFO - __main__ - Printing 3 examples
06/05/2022 09:26:01 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
06/05/2022 09:26:01 - INFO - __main__ - ['contradiction']
06/05/2022 09:26:01 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
06/05/2022 09:26:01 - INFO - __main__ - ['contradiction']
06/05/2022 09:26:01 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
06/05/2022 09:26:01 - INFO - __main__ - ['contradiction']
06/05/2022 09:26:01 - INFO - __main__ - Tokenizing Input ...
06/05/2022 09:26:02 - INFO - __main__ - Tokenizing Output ...
06/05/2022 09:26:02 - INFO - __main__ - Loaded 48 examples from dev data
06/05/2022 09:26:20 - INFO - __main__ - load prompt embedding from ckpt
06/05/2022 09:26:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/05/2022 09:26:21 - INFO - __main__ - Starting training!
06/05/2022 09:26:25 - INFO - __main__ - Step 10 Global step 10 Train loss 1.07 on epoch=3
06/05/2022 09:26:28 - INFO - __main__ - Step 20 Global step 20 Train loss 0.71 on epoch=6
06/05/2022 09:26:31 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=9
06/05/2022 09:26:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=13
06/05/2022 09:26:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=16
06/05/2022 09:26:37 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.2254545454545455 on epoch=16
06/05/2022 09:26:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2254545454545455 on epoch=16, global_step=50
06/05/2022 09:26:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.62 on epoch=19
06/05/2022 09:26:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=23
06/05/2022 09:26:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=26
06/05/2022 09:26:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=29
06/05/2022 09:26:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=33
06/05/2022 09:26:52 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.18993812214151196 on epoch=33
06/05/2022 09:26:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=36
06/05/2022 09:26:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=39
06/05/2022 09:27:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=43
06/05/2022 09:27:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=46
06/05/2022 09:27:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
06/05/2022 09:27:07 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=49
06/05/2022 09:27:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=53
06/05/2022 09:27:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=56
06/05/2022 09:27:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
06/05/2022 09:27:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=63
06/05/2022 09:27:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=66
06/05/2022 09:27:23 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.20908004778972522 on epoch=66
06/05/2022 09:27:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=69
06/05/2022 09:27:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=73
06/05/2022 09:27:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=76
06/05/2022 09:27:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=79
06/05/2022 09:27:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
06/05/2022 09:27:38 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.24296675191815856 on epoch=83
06/05/2022 09:27:38 - INFO - __main__ - Saving model with best Classification-F1: 0.2254545454545455 -> 0.24296675191815856 on epoch=83, global_step=250
06/05/2022 09:27:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=86
06/05/2022 09:27:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=89
06/05/2022 09:27:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
06/05/2022 09:27:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=96
06/05/2022 09:27:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=99
06/05/2022 09:27:53 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.2109090909090909 on epoch=99
06/05/2022 09:27:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=103
06/05/2022 09:27:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
06/05/2022 09:28:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
06/05/2022 09:28:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=113
06/05/2022 09:28:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=116
06/05/2022 09:28:08 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.20846765527616595 on epoch=116
06/05/2022 09:28:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=119
06/05/2022 09:28:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.54 on epoch=123
06/05/2022 09:28:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=126
06/05/2022 09:28:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=129
06/05/2022 09:28:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=133
06/05/2022 09:28:23 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.2501747030048917 on epoch=133
06/05/2022 09:28:23 - INFO - __main__ - Saving model with best Classification-F1: 0.24296675191815856 -> 0.2501747030048917 on epoch=133, global_step=400
06/05/2022 09:28:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=136
06/05/2022 09:28:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=139
06/05/2022 09:28:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=143
06/05/2022 09:28:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=146
06/05/2022 09:28:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=149
06/05/2022 09:28:38 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.23301985370950887 on epoch=149
06/05/2022 09:28:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=153
06/05/2022 09:28:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=156
06/05/2022 09:28:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=159
06/05/2022 09:28:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=163
06/05/2022 09:28:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=166
06/05/2022 09:28:53 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.24999999999999997 on epoch=166
06/05/2022 09:28:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=169
06/05/2022 09:28:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=173
06/05/2022 09:29:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=176
06/05/2022 09:29:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=179
06/05/2022 09:29:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=183
06/05/2022 09:29:08 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.2111111111111111 on epoch=183
06/05/2022 09:29:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=186
06/05/2022 09:29:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=189
06/05/2022 09:29:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=193
06/05/2022 09:29:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=196
06/05/2022 09:29:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=199
06/05/2022 09:29:24 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.22171370455123318 on epoch=199
06/05/2022 09:29:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=203
06/05/2022 09:29:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=206
06/05/2022 09:29:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=209
06/05/2022 09:29:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=213
06/05/2022 09:29:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=216
06/05/2022 09:29:39 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.19999999999999998 on epoch=216
06/05/2022 09:29:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=219
06/05/2022 09:29:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=223
06/05/2022 09:29:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=226
06/05/2022 09:29:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=229
06/05/2022 09:29:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=233
06/05/2022 09:29:54 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.20148148148148146 on epoch=233
06/05/2022 09:29:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=236
06/05/2022 09:29:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=239
06/05/2022 09:30:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=243
06/05/2022 09:30:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=246
06/05/2022 09:30:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=249
06/05/2022 09:30:09 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.24287056612638006 on epoch=249
06/05/2022 09:30:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=253
06/05/2022 09:30:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=256
06/05/2022 09:30:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=259
06/05/2022 09:30:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=263
06/05/2022 09:30:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=266
06/05/2022 09:30:25 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.18888888888888888 on epoch=266
06/05/2022 09:30:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.35 on epoch=269
06/05/2022 09:30:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.32 on epoch=273
06/05/2022 09:30:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=276
06/05/2022 09:30:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=279
06/05/2022 09:30:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.31 on epoch=283
06/05/2022 09:30:40 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.23847517730496456 on epoch=283
06/05/2022 09:30:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=286
06/05/2022 09:30:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=289
06/05/2022 09:30:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.33 on epoch=293
06/05/2022 09:30:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=296
06/05/2022 09:30:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.29 on epoch=299
06/05/2022 09:30:56 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.23465325790907185 on epoch=299
06/05/2022 09:30:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.30 on epoch=303
06/05/2022 09:31:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=306
06/05/2022 09:31:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=309
06/05/2022 09:31:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=313
06/05/2022 09:31:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=316
06/05/2022 09:31:11 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.24317738791423002 on epoch=316
06/05/2022 09:31:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=319
06/05/2022 09:31:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=323
06/05/2022 09:31:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.28 on epoch=326
06/05/2022 09:31:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=329
06/05/2022 09:31:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=333
06/05/2022 09:31:27 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.22916666666666666 on epoch=333
06/05/2022 09:31:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=336
06/05/2022 09:31:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=339
06/05/2022 09:31:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=343
06/05/2022 09:31:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.28 on epoch=346
06/05/2022 09:31:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=349
06/05/2022 09:31:42 - INFO - __main__ - Global step 1050 Train loss 0.29 Classification-F1 0.27122098174729753 on epoch=349
06/05/2022 09:31:42 - INFO - __main__ - Saving model with best Classification-F1: 0.2501747030048917 -> 0.27122098174729753 on epoch=349, global_step=1050
06/05/2022 09:31:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=353
06/05/2022 09:31:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=356
06/05/2022 09:31:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=359
06/05/2022 09:31:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=363
06/05/2022 09:31:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=366
06/05/2022 09:31:59 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.2873471557682084 on epoch=366
06/05/2022 09:31:59 - INFO - __main__ - Saving model with best Classification-F1: 0.27122098174729753 -> 0.2873471557682084 on epoch=366, global_step=1100
06/05/2022 09:32:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=369
06/05/2022 09:32:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=373
06/05/2022 09:32:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=376
06/05/2022 09:32:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=379
06/05/2022 09:32:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.29 on epoch=383
06/05/2022 09:32:14 - INFO - __main__ - Global step 1150 Train loss 0.24 Classification-F1 0.2427061207549012 on epoch=383
06/05/2022 09:32:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=386
06/05/2022 09:32:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=389
06/05/2022 09:32:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=393
06/05/2022 09:32:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=396
06/05/2022 09:32:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=399
06/05/2022 09:32:30 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.2012987012987013 on epoch=399
06/05/2022 09:32:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=403
06/05/2022 09:32:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=406
06/05/2022 09:32:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=409
06/05/2022 09:32:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=413
06/05/2022 09:32:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=416
06/05/2022 09:32:45 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.20050125313283207 on epoch=416
06/05/2022 09:32:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=419
06/05/2022 09:32:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=423
06/05/2022 09:32:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=426
06/05/2022 09:32:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=429
06/05/2022 09:32:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=433
06/05/2022 09:33:01 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.20030088115194497 on epoch=433
06/05/2022 09:33:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=436
06/05/2022 09:33:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=439
06/05/2022 09:33:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=443
06/05/2022 09:33:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=446
06/05/2022 09:33:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=449
06/05/2022 09:33:16 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.17145135566188197 on epoch=449
06/05/2022 09:33:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=453
06/05/2022 09:33:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=456
06/05/2022 09:33:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=459
06/05/2022 09:33:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=463
06/05/2022 09:33:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=466
06/05/2022 09:33:32 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.22985638699924413 on epoch=466
06/05/2022 09:33:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=469
06/05/2022 09:33:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=473
06/05/2022 09:33:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=476
06/05/2022 09:33:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=479
06/05/2022 09:33:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=483
06/05/2022 09:33:47 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.2517543859649123 on epoch=483
06/05/2022 09:33:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=486
06/05/2022 09:33:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=489
06/05/2022 09:33:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=493
06/05/2022 09:33:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=496
06/05/2022 09:34:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=499
06/05/2022 09:34:03 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.22241545893719805 on epoch=499
06/05/2022 09:34:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=503
06/05/2022 09:34:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=506
06/05/2022 09:34:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=509
06/05/2022 09:34:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=513
06/05/2022 09:34:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=516
06/05/2022 09:34:18 - INFO - __main__ - Global step 1550 Train loss 0.08 Classification-F1 0.22361765218908078 on epoch=516
06/05/2022 09:34:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=519
06/05/2022 09:34:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=523
06/05/2022 09:34:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=526
06/05/2022 09:34:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=529
06/05/2022 09:34:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=533
06/05/2022 09:34:34 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.17523809523809528 on epoch=533
06/05/2022 09:34:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=536
06/05/2022 09:34:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=539
06/05/2022 09:34:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=543
06/05/2022 09:34:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=546
06/05/2022 09:34:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=549
06/05/2022 09:34:50 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.20748458336116124 on epoch=549
06/05/2022 09:34:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=553
06/05/2022 09:34:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=556
06/05/2022 09:34:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=559
06/05/2022 09:35:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=563
06/05/2022 09:35:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
06/05/2022 09:35:06 - INFO - __main__ - Global step 1700 Train loss 0.07 Classification-F1 0.1744881527490223 on epoch=566
06/05/2022 09:35:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=569
06/05/2022 09:35:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=573
06/05/2022 09:35:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=576
06/05/2022 09:35:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
06/05/2022 09:35:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
06/05/2022 09:35:21 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.1602787456445993 on epoch=583
06/05/2022 09:35:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
06/05/2022 09:35:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=589
06/05/2022 09:35:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=593
06/05/2022 09:35:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
06/05/2022 09:35:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
06/05/2022 09:35:37 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.2084279401352572 on epoch=599
06/05/2022 09:35:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=603
06/05/2022 09:35:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=606
06/05/2022 09:35:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=609
06/05/2022 09:35:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=613
06/05/2022 09:35:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
06/05/2022 09:35:53 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.2450793650793651 on epoch=616
06/05/2022 09:35:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=619
06/05/2022 09:35:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=623
06/05/2022 09:36:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=626
06/05/2022 09:36:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
06/05/2022 09:36:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=633
06/05/2022 09:36:08 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.25982589719870014 on epoch=633
06/05/2022 09:36:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=636
06/05/2022 09:36:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=639
06/05/2022 09:36:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=643
06/05/2022 09:36:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=646
06/05/2022 09:36:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=649
06/05/2022 09:36:24 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.20390720390720388 on epoch=649
06/05/2022 09:36:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
06/05/2022 09:36:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=656
06/05/2022 09:36:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=659
06/05/2022 09:36:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
06/05/2022 09:36:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=666
06/05/2022 09:36:40 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.21461675579322637 on epoch=666
06/05/2022 09:36:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=669
06/05/2022 09:36:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
06/05/2022 09:36:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=676
06/05/2022 09:36:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=679
06/05/2022 09:36:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
06/05/2022 09:36:56 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.13736749736749737 on epoch=683
06/05/2022 09:36:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=686
06/05/2022 09:37:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=689
06/05/2022 09:37:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=693
06/05/2022 09:37:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=696
06/05/2022 09:37:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
06/05/2022 09:37:11 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.14709787963161275 on epoch=699
06/05/2022 09:37:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
06/05/2022 09:37:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
06/05/2022 09:37:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=709
06/05/2022 09:37:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
06/05/2022 09:37:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
06/05/2022 09:37:27 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.14864864864864863 on epoch=716
06/05/2022 09:37:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
06/05/2022 09:37:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=723
06/05/2022 09:37:35 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
06/05/2022 09:37:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
06/05/2022 09:37:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=733
06/05/2022 09:37:43 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.15666666666666668 on epoch=733
06/05/2022 09:37:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=736
06/05/2022 09:37:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
06/05/2022 09:37:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
06/05/2022 09:37:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=746
06/05/2022 09:37:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
06/05/2022 09:37:58 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.2039072039072039 on epoch=749
06/05/2022 09:38:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
06/05/2022 09:38:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
06/05/2022 09:38:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=759
06/05/2022 09:38:10 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
06/05/2022 09:38:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=766
06/05/2022 09:38:15 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.15742705570291776 on epoch=766
06/05/2022 09:38:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
06/05/2022 09:38:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
06/05/2022 09:38:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=776
06/05/2022 09:38:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
06/05/2022 09:38:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=783
06/05/2022 09:38:30 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.1966032608695652 on epoch=783
06/05/2022 09:38:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=786
06/05/2022 09:38:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
06/05/2022 09:38:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=793
06/05/2022 09:38:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
06/05/2022 09:38:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=799
06/05/2022 09:38:46 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.17132505175983437 on epoch=799
06/05/2022 09:38:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
06/05/2022 09:38:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=806
06/05/2022 09:38:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
06/05/2022 09:38:58 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=813
06/05/2022 09:39:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
06/05/2022 09:39:02 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.3854332801701223 on epoch=816
06/05/2022 09:39:02 - INFO - __main__ - Saving model with best Classification-F1: 0.2873471557682084 -> 0.3854332801701223 on epoch=816, global_step=2450
06/05/2022 09:39:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=819
06/05/2022 09:39:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
06/05/2022 09:39:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=826
06/05/2022 09:39:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=829
06/05/2022 09:39:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
06/05/2022 09:39:18 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.23707236842105264 on epoch=833
06/05/2022 09:39:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=836
06/05/2022 09:39:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
06/05/2022 09:39:26 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
06/05/2022 09:39:29 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
06/05/2022 09:39:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
06/05/2022 09:39:33 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.27689393939393936 on epoch=849
06/05/2022 09:39:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
06/05/2022 09:39:39 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
06/05/2022 09:39:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
06/05/2022 09:39:44 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
06/05/2022 09:39:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
06/05/2022 09:39:49 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.23946869070208726 on epoch=866
06/05/2022 09:39:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=869
06/05/2022 09:39:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
06/05/2022 09:39:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=876
06/05/2022 09:40:00 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
06/05/2022 09:40:03 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=883
06/05/2022 09:40:04 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.3375682288725767 on epoch=883
06/05/2022 09:40:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=886
06/05/2022 09:40:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
06/05/2022 09:40:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=893
06/05/2022 09:40:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
06/05/2022 09:40:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
06/05/2022 09:40:20 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.33251602357293414 on epoch=899
06/05/2022 09:40:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
06/05/2022 09:40:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
06/05/2022 09:40:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
06/05/2022 09:40:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
06/05/2022 09:40:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
06/05/2022 09:40:35 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.3054095762429096 on epoch=916
06/05/2022 09:40:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
06/05/2022 09:40:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
06/05/2022 09:40:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=926
06/05/2022 09:40:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=929
06/05/2022 09:40:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=933
06/05/2022 09:40:51 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.3235930735930736 on epoch=933
06/05/2022 09:40:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=936
06/05/2022 09:40:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
06/05/2022 09:40:59 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
06/05/2022 09:41:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=946
06/05/2022 09:41:05 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=949
06/05/2022 09:41:06 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.24249512670565299 on epoch=949
06/05/2022 09:41:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
06/05/2022 09:41:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
06/05/2022 09:41:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
06/05/2022 09:41:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
06/05/2022 09:41:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
06/05/2022 09:41:22 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.30650539391958154 on epoch=966
06/05/2022 09:41:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
06/05/2022 09:41:28 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
06/05/2022 09:41:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
06/05/2022 09:41:33 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
06/05/2022 09:41:36 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=983
06/05/2022 09:41:38 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.2306081081081081 on epoch=983
06/05/2022 09:41:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=986
06/05/2022 09:41:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
06/05/2022 09:41:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
06/05/2022 09:41:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
06/05/2022 09:41:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
06/05/2022 09:41:53 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.17793987621573826 on epoch=999
06/05/2022 09:41:53 - INFO - __main__ - save last model!
06/05/2022 09:41:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/05/2022 09:41:53 - INFO - __main__ - Start tokenizing ... 1000 instances
06/05/2022 09:41:53 - INFO - __main__ - Printing 3 examples
06/05/2022 09:41:53 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/05/2022 09:41:53 - INFO - __main__ - ['contradiction']
06/05/2022 09:41:53 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/05/2022 09:41:53 - INFO - __main__ - ['entailment']
06/05/2022 09:41:53 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/05/2022 09:41:53 - INFO - __main__ - ['contradiction']
06/05/2022 09:41:53 - INFO - __main__ - Tokenizing Input ...
06/05/2022 09:41:54 - INFO - __main__ - Tokenizing Output ...
06/05/2022 09:41:55 - INFO - __main__ - Loaded 1000 examples from test data
06/05/2022 09:42:26 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-up64shot/singletask-anli/anli_16_87_0.2_8_predictions.txt
06/05/2022 09:42:26 - INFO - __main__ - Classification-F1 on test data: 0.2326
06/05/2022 09:42:26 - INFO - __main__ - prefix=anli_16_87, lr=0.2, bsz=8, dev_performance=0.3854332801701223, test_performance=0.23257785293927746
