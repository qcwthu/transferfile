05/21/2022 21:22:19 - INFO - __main__ - Namespace(task_dir='data_32/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:22:19 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity
05/21/2022 21:22:19 - INFO - __main__ - Namespace(task_dir='data_32/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:22:19 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity
05/21/2022 21:22:21 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:22:21 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:22:21 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:22:21 - INFO - __main__ - Using 2 gpus
05/21/2022 21:22:21 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:22:21 - INFO - __main__ - Using 2 gpus
05/21/2022 21:22:21 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_32_100', 'amazon_polarity_32_13', 'amazon_polarity_32_21', 'amazon_polarity_32_42', 'amazon_polarity_32_87']
05/21/2022 21:22:21 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_32_100', 'amazon_polarity_32_13', 'amazon_polarity_32_21', 'amazon_polarity_32_42', 'amazon_polarity_32_87']
05/21/2022 21:22:26 - INFO - __main__ - Running ... prefix=amazon_polarity_32_100, lr=0.0005, bsz=8 ...
05/31/2022 21:31:07 - INFO - __main__ - Namespace(task_dir='data_32/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/31/2022 21:31:07 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity
05/31/2022 21:31:07 - INFO - __main__ - Namespace(task_dir='data_32/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/31/2022 21:31:07 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity
05/31/2022 21:31:09 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/31/2022 21:31:09 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/31/2022 21:31:09 - INFO - __main__ - args.device: cuda:1
05/31/2022 21:31:09 - INFO - __main__ - args.device: cuda:0
05/31/2022 21:31:09 - INFO - __main__ - Using 2 gpus
05/31/2022 21:31:09 - INFO - __main__ - Using 2 gpus
05/31/2022 21:31:09 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_32_100', 'amazon_polarity_32_13', 'amazon_polarity_32_21', 'amazon_polarity_32_42', 'amazon_polarity_32_87']
05/31/2022 21:31:09 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_32_100', 'amazon_polarity_32_13', 'amazon_polarity_32_21', 'amazon_polarity_32_42', 'amazon_polarity_32_87']
05/31/2022 21:31:14 - INFO - __main__ - Running ... prefix=amazon_polarity_32_100, lr=0.0005, bsz=8 ...
05/31/2022 21:31:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:31:15 - INFO - __main__ - Printing 3 examples
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:31:15 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:31:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:31:15 - INFO - __main__ - Printing 3 examples
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:31:15 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:31:15 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 21:31:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:31:15 - INFO - __main__ - Printing 3 examples
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: The title is an oxymoron, but they did a good job of making it simple [SEP] content: I actually really like this book, but I think you do need the extra guidance of a professor when using it. It's very easy to look up the pharmaceuticals and all related info, and I was actually able to use it easily when taking the open book section (looking up the pharms) of a pharmacotherapeutics test. I would definitely recommend this book.
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: Incredible value not to be missed [SEP] content: This book is full of priceless information about places you would otherwise never hear about. It is not to be missed.
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: Great movie! [SEP] content: This is the epitome of a "B" western movie, but it's hilarious! Andy Griffith, Tom Berrenger, Marilou Henner and Selma Ward--all of them did a great job on this!
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:31:15 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:31:15 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 21:31:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:31:15 - INFO - __main__ - Printing 3 examples
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: The title is an oxymoron, but they did a good job of making it simple [SEP] content: I actually really like this book, but I think you do need the extra guidance of a professor when using it. It's very easy to look up the pharmaceuticals and all related info, and I was actually able to use it easily when taking the open book section (looking up the pharms) of a pharmacotherapeutics test. I would definitely recommend this book.
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: Incredible value not to be missed [SEP] content: This book is full of priceless information about places you would otherwise never hear about. It is not to be missed.
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ -  [amazon_polarity] title: Great movie! [SEP] content: This is the epitome of a "B" western movie, but it's hilarious! Andy Griffith, Tom Berrenger, Marilou Henner and Selma Ward--all of them did a great job on this!
05/31/2022 21:31:15 - INFO - __main__ - ['positive']
05/31/2022 21:31:15 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:31:15 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:31:15 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 21:31:15 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 21:31:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 21:31:28 - INFO - __main__ - Starting training!
05/31/2022 21:31:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 21:31:28 - INFO - __main__ - Starting training!
05/31/2022 21:31:33 - INFO - __main__ - Step 10 Global step 10 Train loss 22.435602 on epoch=2
05/31/2022 21:31:38 - INFO - __main__ - Step 20 Global step 20 Train loss 16.885752 on epoch=4
05/31/2022 21:31:43 - INFO - __main__ - Step 30 Global step 30 Train loss 14.371289 on epoch=7
05/31/2022 21:31:48 - INFO - __main__ - Step 40 Global step 40 Train loss 13.021730 on epoch=9
05/31/2022 21:31:53 - INFO - __main__ - Step 50 Global step 50 Train loss 11.262527 on epoch=12
05/31/2022 21:31:54 - INFO - __main__ - Global step 50 Train loss 15.595379 Classification-F1 0.0 on epoch=12
05/31/2022 21:32:01 - INFO - __main__ - Step 60 Global step 60 Train loss 6.291714 on epoch=14
05/31/2022 21:32:06 - INFO - __main__ - Step 70 Global step 70 Train loss 2.274574 on epoch=17
05/31/2022 21:32:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.519364 on epoch=19
05/31/2022 21:32:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.538253 on epoch=22
05/31/2022 21:32:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.685198 on epoch=24
05/31/2022 21:32:22 - INFO - __main__ - Global step 100 Train loss 2.061820 Classification-F1 0.3992490613266583 on epoch=24
05/31/2022 21:32:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.536636 on epoch=27
05/31/2022 21:32:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.414464 on epoch=29
05/31/2022 21:32:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.420585 on epoch=32
05/31/2022 21:32:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.393049 on epoch=34
05/31/2022 21:32:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.377397 on epoch=37
05/31/2022 21:33:11 - INFO - __main__ - Global step 150 Train loss 0.428426 Classification-F1 0.21985815602836878 on epoch=37
05/31/2022 21:33:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.385573 on epoch=39
05/31/2022 21:33:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.342360 on epoch=42
05/31/2022 21:33:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.399743 on epoch=44
05/31/2022 21:33:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.411258 on epoch=47
05/31/2022 21:33:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.374788 on epoch=49
05/31/2022 21:33:59 - INFO - __main__ - Global step 200 Train loss 0.382745 Classification-F1 0.21985815602836878 on epoch=49
05/31/2022 21:34:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.425706 on epoch=52
05/31/2022 21:34:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.332713 on epoch=54
05/31/2022 21:34:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.351911 on epoch=57
05/31/2022 21:34:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.326408 on epoch=59
05/31/2022 21:34:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.383199 on epoch=62
05/31/2022 21:34:46 - INFO - __main__ - Global step 250 Train loss 0.363987 Classification-F1 0.24242424242424243 on epoch=62
05/31/2022 21:34:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.360375 on epoch=64
05/31/2022 21:34:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.305930 on epoch=67
05/31/2022 21:35:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.302197 on epoch=69
05/31/2022 21:35:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.302640 on epoch=72
05/31/2022 21:35:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.292417 on epoch=74
05/31/2022 21:35:34 - INFO - __main__ - Global step 300 Train loss 0.312712 Classification-F1 0.3492248062015504 on epoch=74
05/31/2022 21:35:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.210146 on epoch=77
05/31/2022 21:35:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.254469 on epoch=79
05/31/2022 21:35:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.186614 on epoch=82
05/31/2022 21:35:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.270769 on epoch=84
05/31/2022 21:36:01 - INFO - __main__ - Step 350 Global step 350 Train loss 1.772414 on epoch=87
05/31/2022 21:36:02 - INFO - __main__ - Global step 350 Train loss 0.538882 Classification-F1 0.3333333333333333 on epoch=87
05/31/2022 21:36:07 - INFO - __main__ - Step 360 Global step 360 Train loss 1.810025 on epoch=89
05/31/2022 21:36:12 - INFO - __main__ - Step 370 Global step 370 Train loss 1.074479 on epoch=92
05/31/2022 21:36:18 - INFO - __main__ - Step 380 Global step 380 Train loss 1.020682 on epoch=94
05/31/2022 21:36:23 - INFO - __main__ - Step 390 Global step 390 Train loss 1.025161 on epoch=97
05/31/2022 21:36:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.947150 on epoch=99
05/31/2022 21:36:29 - INFO - __main__ - Global step 400 Train loss 1.175499 Classification-F1 0.3333333333333333 on epoch=99
05/31/2022 21:36:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.818923 on epoch=102
05/31/2022 21:36:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.667548 on epoch=104
05/31/2022 21:36:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.357618 on epoch=107
05/31/2022 21:36:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.299640 on epoch=109
05/31/2022 21:36:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.258433 on epoch=112
05/31/2022 21:36:57 - INFO - __main__ - Global step 450 Train loss 0.480433 Classification-F1 0.5622435020519836 on epoch=112
05/31/2022 21:37:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.240488 on epoch=114
05/31/2022 21:37:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.256792 on epoch=117
05/31/2022 21:37:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.144600 on epoch=119
05/31/2022 21:37:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.180790 on epoch=122
05/31/2022 21:37:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.175872 on epoch=124
05/31/2022 21:37:26 - INFO - __main__ - Global step 500 Train loss 0.199708 Classification-F1 0.5972640218878249 on epoch=124
05/31/2022 21:37:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.132700 on epoch=127
05/31/2022 21:37:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.118585 on epoch=129
05/31/2022 21:37:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.079910 on epoch=132
05/31/2022 21:37:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.166780 on epoch=134
05/31/2022 21:37:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.064502 on epoch=137
05/31/2022 21:37:55 - INFO - __main__ - Global step 550 Train loss 0.112495 Classification-F1 0.7117117117117117 on epoch=137
05/31/2022 21:38:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.068634 on epoch=139
05/31/2022 21:38:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.106196 on epoch=142
05/31/2022 21:38:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.033417 on epoch=144
05/31/2022 21:38:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.124097 on epoch=147
05/31/2022 21:38:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.030877 on epoch=149
05/31/2022 21:38:24 - INFO - __main__ - Global step 600 Train loss 0.072644 Classification-F1 0.7085020242914979 on epoch=149
05/31/2022 21:38:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.078110 on epoch=152
05/31/2022 21:38:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.024897 on epoch=154
05/31/2022 21:38:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.043228 on epoch=157
05/31/2022 21:38:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.052140 on epoch=159
05/31/2022 21:38:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.035306 on epoch=162
05/31/2022 21:38:52 - INFO - __main__ - Global step 650 Train loss 0.046736 Classification-F1 0.6532019704433498 on epoch=162
05/31/2022 21:38:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.084945 on epoch=164
05/31/2022 21:39:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.025224 on epoch=167
05/31/2022 21:39:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.064238 on epoch=169
05/31/2022 21:39:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.004310 on epoch=172
05/31/2022 21:39:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.007611 on epoch=174
05/31/2022 21:39:20 - INFO - __main__ - Global step 700 Train loss 0.037266 Classification-F1 0.6679021497405486 on epoch=174
05/31/2022 21:39:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.011322 on epoch=177
05/31/2022 21:39:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.345060 on epoch=179
05/31/2022 21:39:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.379952 on epoch=182
05/31/2022 21:39:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.058267 on epoch=184
05/31/2022 21:39:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.035968 on epoch=187
05/31/2022 21:39:48 - INFO - __main__ - Global step 750 Train loss 0.166114 Classification-F1 0.6559139784946237 on epoch=187
05/31/2022 21:39:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.035072 on epoch=189
05/31/2022 21:39:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.019666 on epoch=192
05/31/2022 21:40:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.034389 on epoch=194
05/31/2022 21:40:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.024350 on epoch=197
05/31/2022 21:40:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.023267 on epoch=199
05/31/2022 21:40:16 - INFO - __main__ - Global step 800 Train loss 0.027349 Classification-F1 0.5151515151515151 on epoch=199
05/31/2022 21:40:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.009284 on epoch=202
05/31/2022 21:40:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.018603 on epoch=204
05/31/2022 21:40:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.019051 on epoch=207
05/31/2022 21:40:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.008559 on epoch=209
05/31/2022 21:40:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.015860 on epoch=212
05/31/2022 21:40:44 - INFO - __main__ - Global step 850 Train loss 0.014271 Classification-F1 0.5622435020519836 on epoch=212
05/31/2022 21:40:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.006255 on epoch=214
05/31/2022 21:40:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.002693 on epoch=217
05/31/2022 21:41:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001476 on epoch=219
05/31/2022 21:41:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.432625 on epoch=222
05/31/2022 21:41:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.123675 on epoch=224
05/31/2022 21:41:12 - INFO - __main__ - Global step 900 Train loss 0.113345 Classification-F1 0.4779299847792999 on epoch=224
05/31/2022 21:41:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.026783 on epoch=227
05/31/2022 21:41:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.135634 on epoch=229
05/31/2022 21:41:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.027118 on epoch=232
05/31/2022 21:41:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.005176 on epoch=234
05/31/2022 21:41:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.006438 on epoch=237
05/31/2022 21:41:40 - INFO - __main__ - Global step 950 Train loss 0.040230 Classification-F1 0.7497556207233627 on epoch=237
05/31/2022 21:41:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.075640 on epoch=239
05/31/2022 21:41:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.081055 on epoch=242
05/31/2022 21:41:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.057352 on epoch=244
05/31/2022 21:42:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.009335 on epoch=247
05/31/2022 21:42:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.036291 on epoch=249
05/31/2022 21:42:09 - INFO - __main__ - Global step 1000 Train loss 0.051934 Classification-F1 0.7176470588235293 on epoch=249
05/31/2022 21:42:09 - INFO - __main__ - save last model!
05/31/2022 21:42:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:42:09 - INFO - __main__ - Printing 3 examples
05/31/2022 21:42:09 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/31/2022 21:42:09 - INFO - __main__ - ['positive']
05/31/2022 21:42:09 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/31/2022 21:42:09 - INFO - __main__ - ['positive']
05/31/2022 21:42:09 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/31/2022 21:42:09 - INFO - __main__ - ['positive']
05/31/2022 21:42:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:42:09 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:42:09 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 21:42:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:42:09 - INFO - __main__ - Printing 3 examples
05/31/2022 21:42:09 - INFO - __main__ -  [amazon_polarity] title: The title is an oxymoron, but they did a good job of making it simple [SEP] content: I actually really like this book, but I think you do need the extra guidance of a professor when using it. It's very easy to look up the pharmaceuticals and all related info, and I was actually able to use it easily when taking the open book section (looking up the pharms) of a pharmacotherapeutics test. I would definitely recommend this book.
05/31/2022 21:42:09 - INFO - __main__ - ['positive']
05/31/2022 21:42:09 - INFO - __main__ -  [amazon_polarity] title: Incredible value not to be missed [SEP] content: This book is full of priceless information about places you would otherwise never hear about. It is not to be missed.
05/31/2022 21:42:09 - INFO - __main__ - ['positive']
05/31/2022 21:42:09 - INFO - __main__ -  [amazon_polarity] title: Great movie! [SEP] content: This is the epitome of a "B" western movie, but it's hilarious! Andy Griffith, Tom Berrenger, Marilou Henner and Selma Ward--all of them did a great job on this!
05/31/2022 21:42:09 - INFO - __main__ - ['positive']
05/31/2022 21:42:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:42:09 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:42:09 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 21:42:16 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 21:42:16 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 21:42:16 - INFO - __main__ - Printing 3 examples
05/31/2022 21:42:16 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 21:42:16 - INFO - __main__ - ['negative']
05/31/2022 21:42:16 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 21:42:16 - INFO - __main__ - ['negative']
05/31/2022 21:42:16 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 21:42:16 - INFO - __main__ - ['negative']
05/31/2022 21:42:16 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:42:17 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:42:18 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 21:42:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 21:42:22 - INFO - __main__ - Starting training!
05/31/2022 21:42:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_100_0.0005_8_predictions.txt
05/31/2022 21:42:34 - INFO - __main__ - Classification-F1 on test data: 0.2418
05/31/2022 21:42:34 - INFO - __main__ - prefix=amazon_polarity_32_100, lr=0.0005, bsz=8, dev_performance=0.7497556207233627, test_performance=0.2417542956984061
05/31/2022 21:42:34 - INFO - __main__ - Running ... prefix=amazon_polarity_32_100, lr=0.0003, bsz=8 ...
05/31/2022 21:42:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:42:35 - INFO - __main__ - Printing 3 examples
05/31/2022 21:42:35 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/31/2022 21:42:35 - INFO - __main__ - ['positive']
05/31/2022 21:42:35 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/31/2022 21:42:35 - INFO - __main__ - ['positive']
05/31/2022 21:42:35 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/31/2022 21:42:35 - INFO - __main__ - ['positive']
05/31/2022 21:42:35 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:42:35 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:42:35 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 21:42:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:42:35 - INFO - __main__ - Printing 3 examples
05/31/2022 21:42:35 - INFO - __main__ -  [amazon_polarity] title: The title is an oxymoron, but they did a good job of making it simple [SEP] content: I actually really like this book, but I think you do need the extra guidance of a professor when using it. It's very easy to look up the pharmaceuticals and all related info, and I was actually able to use it easily when taking the open book section (looking up the pharms) of a pharmacotherapeutics test. I would definitely recommend this book.
05/31/2022 21:42:35 - INFO - __main__ - ['positive']
05/31/2022 21:42:35 - INFO - __main__ -  [amazon_polarity] title: Incredible value not to be missed [SEP] content: This book is full of priceless information about places you would otherwise never hear about. It is not to be missed.
05/31/2022 21:42:35 - INFO - __main__ - ['positive']
05/31/2022 21:42:35 - INFO - __main__ -  [amazon_polarity] title: Great movie! [SEP] content: This is the epitome of a "B" western movie, but it's hilarious! Andy Griffith, Tom Berrenger, Marilou Henner and Selma Ward--all of them did a great job on this!
05/31/2022 21:42:35 - INFO - __main__ - ['positive']
05/31/2022 21:42:35 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:42:35 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:42:35 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 21:42:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 21:42:48 - INFO - __main__ - Starting training!
05/31/2022 21:42:53 - INFO - __main__ - Step 10 Global step 10 Train loss 22.892788 on epoch=2
05/31/2022 21:42:59 - INFO - __main__ - Step 20 Global step 20 Train loss 16.975132 on epoch=4
05/31/2022 21:43:04 - INFO - __main__ - Step 30 Global step 30 Train loss 15.212718 on epoch=7
05/31/2022 21:43:09 - INFO - __main__ - Step 40 Global step 40 Train loss 14.139481 on epoch=9
05/31/2022 21:43:15 - INFO - __main__ - Step 50 Global step 50 Train loss 12.853844 on epoch=12
05/31/2022 21:43:17 - INFO - __main__ - Global step 50 Train loss 16.414793 Classification-F1 0.0 on epoch=12
05/31/2022 21:43:22 - INFO - __main__ - Step 60 Global step 60 Train loss 11.368329 on epoch=14
05/31/2022 21:43:28 - INFO - __main__ - Step 70 Global step 70 Train loss 10.463607 on epoch=17
05/31/2022 21:43:33 - INFO - __main__ - Step 80 Global step 80 Train loss 6.360431 on epoch=19
05/31/2022 21:43:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.687645 on epoch=22
05/31/2022 21:43:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.215554 on epoch=24
05/31/2022 21:43:45 - INFO - __main__ - Global step 100 Train loss 5.819113 Classification-F1 0.8893007165801828 on epoch=24
05/31/2022 21:43:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.154998 on epoch=27
05/31/2022 21:43:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.077396 on epoch=29
05/31/2022 21:44:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.019418 on epoch=32
05/31/2022 21:44:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.032083 on epoch=34
05/31/2022 21:44:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.029723 on epoch=37
05/31/2022 21:44:13 - INFO - __main__ - Global step 150 Train loss 0.062723 Classification-F1 0.9530217763640813 on epoch=37
05/31/2022 21:44:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.013730 on epoch=39
05/31/2022 21:44:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.010581 on epoch=42
05/31/2022 21:44:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.010824 on epoch=44
05/31/2022 21:44:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.001096 on epoch=47
05/31/2022 21:44:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.000788 on epoch=49
05/31/2022 21:44:42 - INFO - __main__ - Global step 200 Train loss 0.007404 Classification-F1 0.9531135531135531 on epoch=49
05/31/2022 21:44:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.001210 on epoch=52
05/31/2022 21:44:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.004612 on epoch=54
05/31/2022 21:44:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.000273 on epoch=57
05/31/2022 21:45:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000899 on epoch=59
05/31/2022 21:45:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.002455 on epoch=62
05/31/2022 21:45:10 - INFO - __main__ - Global step 250 Train loss 0.001890 Classification-F1 0.96875 on epoch=62
05/31/2022 21:45:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000079 on epoch=64
05/31/2022 21:45:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000863 on epoch=67
05/31/2022 21:45:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.044004 on epoch=69
05/31/2022 21:45:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.112035 on epoch=72
05/31/2022 21:45:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.698859 on epoch=74
05/31/2022 21:45:38 - INFO - __main__ - Global step 300 Train loss 0.171168 Classification-F1 0.9531135531135531 on epoch=74
05/31/2022 21:45:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.007215 on epoch=77
05/31/2022 21:45:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.006681 on epoch=79
05/31/2022 21:45:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.003309 on epoch=82
05/31/2022 21:46:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000291 on epoch=84
05/31/2022 21:46:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000240 on epoch=87
05/31/2022 21:46:07 - INFO - __main__ - Global step 350 Train loss 0.003547 Classification-F1 0.9687194525904204 on epoch=87
05/31/2022 21:46:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001115 on epoch=89
05/31/2022 21:46:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001414 on epoch=92
05/31/2022 21:46:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000164 on epoch=94
05/31/2022 21:46:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000117 on epoch=97
05/31/2022 21:46:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000105 on epoch=99
05/31/2022 21:46:35 - INFO - __main__ - Global step 400 Train loss 0.000583 Classification-F1 0.96875 on epoch=99
05/31/2022 21:46:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000298 on epoch=102
05/31/2022 21:46:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000080 on epoch=104
05/31/2022 21:46:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000488 on epoch=107
05/31/2022 21:46:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000050 on epoch=109
05/31/2022 21:47:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000078 on epoch=112
05/31/2022 21:47:03 - INFO - __main__ - Global step 450 Train loss 0.000199 Classification-F1 0.96875 on epoch=112
05/31/2022 21:47:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000035 on epoch=114
05/31/2022 21:47:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000434 on epoch=117
05/31/2022 21:47:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.041274 on epoch=119
05/31/2022 21:47:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000602 on epoch=122
05/31/2022 21:47:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000242 on epoch=124
05/31/2022 21:47:30 - INFO - __main__ - Global step 500 Train loss 0.008518 Classification-F1 0.96875 on epoch=124
05/31/2022 21:47:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000468 on epoch=127
05/31/2022 21:47:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000282 on epoch=129
05/31/2022 21:47:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000189 on epoch=132
05/31/2022 21:47:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000108 on epoch=134
05/31/2022 21:47:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000148 on epoch=137
05/31/2022 21:47:58 - INFO - __main__ - Global step 550 Train loss 0.000239 Classification-F1 0.96875 on epoch=137
05/31/2022 21:48:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000072 on epoch=139
05/31/2022 21:48:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000171 on epoch=142
05/31/2022 21:48:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000033 on epoch=144
05/31/2022 21:48:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000064 on epoch=147
05/31/2022 21:48:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000087 on epoch=149
05/31/2022 21:48:26 - INFO - __main__ - Global step 600 Train loss 0.000086 Classification-F1 0.96875 on epoch=149
05/31/2022 21:48:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000020 on epoch=152
05/31/2022 21:48:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000019 on epoch=154
05/31/2022 21:48:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000126 on epoch=157
05/31/2022 21:48:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000917 on epoch=159
05/31/2022 21:48:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000068 on epoch=162
05/31/2022 21:48:54 - INFO - __main__ - Global step 650 Train loss 0.000230 Classification-F1 0.9531135531135531 on epoch=162
05/31/2022 21:48:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000020 on epoch=164
05/31/2022 21:49:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000021 on epoch=167
05/31/2022 21:49:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000012 on epoch=169
05/31/2022 21:49:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000037 on epoch=172
05/31/2022 21:49:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000019 on epoch=174
05/31/2022 21:49:22 - INFO - __main__ - Global step 700 Train loss 0.000022 Classification-F1 0.9531135531135531 on epoch=174
05/31/2022 21:49:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000025 on epoch=177
05/31/2022 21:49:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000414 on epoch=179
05/31/2022 21:49:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.077876 on epoch=182
05/31/2022 21:49:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000020 on epoch=184
05/31/2022 21:49:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000033 on epoch=187
05/31/2022 21:49:50 - INFO - __main__ - Global step 750 Train loss 0.015673 Classification-F1 0.9531135531135531 on epoch=187
05/31/2022 21:49:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000073 on epoch=189
05/31/2022 21:50:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000087 on epoch=192
05/31/2022 21:50:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000024 on epoch=194
05/31/2022 21:50:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000003 on epoch=197
05/31/2022 21:50:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000031 on epoch=199
05/31/2022 21:50:18 - INFO - __main__ - Global step 800 Train loss 0.000044 Classification-F1 0.9531135531135531 on epoch=199
05/31/2022 21:50:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000007 on epoch=202
05/31/2022 21:50:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000088 on epoch=204
05/31/2022 21:50:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000014 on epoch=207
05/31/2022 21:50:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000003 on epoch=209
05/31/2022 21:50:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000008 on epoch=212
05/31/2022 21:50:46 - INFO - __main__ - Global step 850 Train loss 0.000024 Classification-F1 0.9531135531135531 on epoch=212
05/31/2022 21:50:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000008 on epoch=214
05/31/2022 21:50:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000144 on epoch=217
05/31/2022 21:51:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000073 on epoch=219
05/31/2022 21:51:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000010 on epoch=222
05/31/2022 21:51:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000013 on epoch=224
05/31/2022 21:51:14 - INFO - __main__ - Global step 900 Train loss 0.000050 Classification-F1 0.9531135531135531 on epoch=224
05/31/2022 21:51:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000030 on epoch=227
05/31/2022 21:51:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000002 on epoch=229
05/31/2022 21:51:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000014 on epoch=232
05/31/2022 21:51:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000003 on epoch=234
05/31/2022 21:51:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000008 on epoch=237
05/31/2022 21:51:41 - INFO - __main__ - Global step 950 Train loss 0.000012 Classification-F1 0.9531135531135531 on epoch=237
05/31/2022 21:51:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000005 on epoch=239
05/31/2022 21:51:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000002 on epoch=242
05/31/2022 21:51:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000014 on epoch=244
05/31/2022 21:52:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000002 on epoch=247
05/31/2022 21:52:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000004 on epoch=249
05/31/2022 21:52:09 - INFO - __main__ - Global step 1000 Train loss 0.000005 Classification-F1 0.9531135531135531 on epoch=249
05/31/2022 21:52:09 - INFO - __main__ - save last model!
05/31/2022 21:52:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:52:10 - INFO - __main__ - Printing 3 examples
05/31/2022 21:52:10 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/31/2022 21:52:10 - INFO - __main__ - ['positive']
05/31/2022 21:52:10 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/31/2022 21:52:10 - INFO - __main__ - ['positive']
05/31/2022 21:52:10 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/31/2022 21:52:10 - INFO - __main__ - ['positive']
05/31/2022 21:52:10 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:52:10 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:52:10 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 21:52:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:52:10 - INFO - __main__ - Printing 3 examples
05/31/2022 21:52:10 - INFO - __main__ -  [amazon_polarity] title: The title is an oxymoron, but they did a good job of making it simple [SEP] content: I actually really like this book, but I think you do need the extra guidance of a professor when using it. It's very easy to look up the pharmaceuticals and all related info, and I was actually able to use it easily when taking the open book section (looking up the pharms) of a pharmacotherapeutics test. I would definitely recommend this book.
05/31/2022 21:52:10 - INFO - __main__ - ['positive']
05/31/2022 21:52:10 - INFO - __main__ -  [amazon_polarity] title: Incredible value not to be missed [SEP] content: This book is full of priceless information about places you would otherwise never hear about. It is not to be missed.
05/31/2022 21:52:10 - INFO - __main__ - ['positive']
05/31/2022 21:52:10 - INFO - __main__ -  [amazon_polarity] title: Great movie! [SEP] content: This is the epitome of a "B" western movie, but it's hilarious! Andy Griffith, Tom Berrenger, Marilou Henner and Selma Ward--all of them did a great job on this!
05/31/2022 21:52:10 - INFO - __main__ - ['positive']
05/31/2022 21:52:10 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:52:10 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:52:10 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 21:52:16 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 21:52:17 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 21:52:17 - INFO - __main__ - Printing 3 examples
05/31/2022 21:52:17 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 21:52:17 - INFO - __main__ - ['negative']
05/31/2022 21:52:17 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 21:52:17 - INFO - __main__ - ['negative']
05/31/2022 21:52:17 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 21:52:17 - INFO - __main__ - ['negative']
05/31/2022 21:52:17 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:52:18 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:52:19 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 21:52:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 21:52:21 - INFO - __main__ - Starting training!
05/31/2022 21:52:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_100_0.0003_8_predictions.txt
05/31/2022 21:52:34 - INFO - __main__ - Classification-F1 on test data: 0.9620
05/31/2022 21:52:34 - INFO - __main__ - prefix=amazon_polarity_32_100, lr=0.0003, bsz=8, dev_performance=0.96875, test_performance=0.961984793917567
05/31/2022 21:52:34 - INFO - __main__ - Running ... prefix=amazon_polarity_32_100, lr=0.0002, bsz=8 ...
05/31/2022 21:52:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:52:35 - INFO - __main__ - Printing 3 examples
05/31/2022 21:52:35 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/31/2022 21:52:35 - INFO - __main__ - ['positive']
05/31/2022 21:52:35 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/31/2022 21:52:35 - INFO - __main__ - ['positive']
05/31/2022 21:52:35 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/31/2022 21:52:35 - INFO - __main__ - ['positive']
05/31/2022 21:52:35 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:52:35 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:52:35 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 21:52:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 21:52:35 - INFO - __main__ - Printing 3 examples
05/31/2022 21:52:35 - INFO - __main__ -  [amazon_polarity] title: The title is an oxymoron, but they did a good job of making it simple [SEP] content: I actually really like this book, but I think you do need the extra guidance of a professor when using it. It's very easy to look up the pharmaceuticals and all related info, and I was actually able to use it easily when taking the open book section (looking up the pharms) of a pharmacotherapeutics test. I would definitely recommend this book.
05/31/2022 21:52:35 - INFO - __main__ - ['positive']
05/31/2022 21:52:35 - INFO - __main__ -  [amazon_polarity] title: Incredible value not to be missed [SEP] content: This book is full of priceless information about places you would otherwise never hear about. It is not to be missed.
05/31/2022 21:52:35 - INFO - __main__ - ['positive']
05/31/2022 21:52:35 - INFO - __main__ -  [amazon_polarity] title: Great movie! [SEP] content: This is the epitome of a "B" western movie, but it's hilarious! Andy Griffith, Tom Berrenger, Marilou Henner and Selma Ward--all of them did a great job on this!
05/31/2022 21:52:35 - INFO - __main__ - ['positive']
05/31/2022 21:52:35 - INFO - __main__ - Tokenizing Input ...
05/31/2022 21:52:35 - INFO - __main__ - Tokenizing Output ...
05/31/2022 21:52:36 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 21:52:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 21:52:47 - INFO - __main__ - Starting training!
05/31/2022 21:52:51 - INFO - __main__ - Step 10 Global step 10 Train loss 23.958881 on epoch=2
05/31/2022 21:52:57 - INFO - __main__ - Step 20 Global step 20 Train loss 18.434170 on epoch=4
05/31/2022 21:53:02 - INFO - __main__ - Step 30 Global step 30 Train loss 15.857407 on epoch=7
05/31/2022 21:53:07 - INFO - __main__ - Step 40 Global step 40 Train loss 15.129270 on epoch=9
05/31/2022 21:53:13 - INFO - __main__ - Step 50 Global step 50 Train loss 14.346570 on epoch=12
05/31/2022 21:53:24 - INFO - __main__ - Global step 50 Train loss 17.545259 Classification-F1 0.0 on epoch=12
05/31/2022 21:53:30 - INFO - __main__ - Step 60 Global step 60 Train loss 13.377947 on epoch=14
05/31/2022 21:53:35 - INFO - __main__ - Step 70 Global step 70 Train loss 13.044047 on epoch=17
05/31/2022 21:53:40 - INFO - __main__ - Step 80 Global step 80 Train loss 12.176712 on epoch=19
05/31/2022 21:53:46 - INFO - __main__ - Step 90 Global step 90 Train loss 10.880837 on epoch=22
05/31/2022 21:53:51 - INFO - __main__ - Step 100 Global step 100 Train loss 9.109279 on epoch=24
05/31/2022 21:53:52 - INFO - __main__ - Global step 100 Train loss 11.717765 Classification-F1 0.0 on epoch=24
05/31/2022 21:53:57 - INFO - __main__ - Step 110 Global step 110 Train loss 4.514659 on epoch=27
05/31/2022 21:54:02 - INFO - __main__ - Step 120 Global step 120 Train loss 1.548177 on epoch=29
05/31/2022 21:54:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.409770 on epoch=32
05/31/2022 21:54:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.076392 on epoch=34
05/31/2022 21:54:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.091997 on epoch=37
05/31/2022 21:54:19 - INFO - __main__ - Global step 150 Train loss 1.328199 Classification-F1 0.9531135531135531 on epoch=37
05/31/2022 21:54:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.064204 on epoch=39
05/31/2022 21:54:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.033290 on epoch=42
05/31/2022 21:54:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.024946 on epoch=44
05/31/2022 21:54:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.022822 on epoch=47
05/31/2022 21:54:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.036979 on epoch=49
05/31/2022 21:54:47 - INFO - __main__ - Global step 200 Train loss 0.036448 Classification-F1 0.9531135531135531 on epoch=49
05/31/2022 21:54:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.016444 on epoch=52
05/31/2022 21:54:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.038177 on epoch=54
05/31/2022 21:55:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.002374 on epoch=57
05/31/2022 21:55:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.003757 on epoch=59
05/31/2022 21:55:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.013171 on epoch=62
05/31/2022 21:55:14 - INFO - __main__ - Global step 250 Train loss 0.014785 Classification-F1 0.9372549019607843 on epoch=62
05/31/2022 21:55:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000603 on epoch=64
05/31/2022 21:55:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.002063 on epoch=67
05/31/2022 21:55:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000259 on epoch=69
05/31/2022 21:55:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.025880 on epoch=72
05/31/2022 21:55:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.005193 on epoch=74
05/31/2022 21:55:42 - INFO - __main__ - Global step 300 Train loss 0.006800 Classification-F1 0.9687194525904204 on epoch=74
05/31/2022 21:55:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000604 on epoch=77
05/31/2022 21:55:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.002171 on epoch=79
05/31/2022 21:55:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000436 on epoch=82
05/31/2022 21:56:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.003974 on epoch=84
05/31/2022 21:56:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.045869 on epoch=87
05/31/2022 21:56:10 - INFO - __main__ - Global step 350 Train loss 0.010611 Classification-F1 0.9531135531135531 on epoch=87
05/31/2022 21:56:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.003530 on epoch=89
05/31/2022 21:56:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001589 on epoch=92
05/31/2022 21:56:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.006941 on epoch=94
05/31/2022 21:56:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000454 on epoch=97
05/31/2022 21:56:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001068 on epoch=99
05/31/2022 21:56:38 - INFO - __main__ - Global step 400 Train loss 0.002716 Classification-F1 0.9531135531135531 on epoch=99
05/31/2022 21:56:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000289 on epoch=102
05/31/2022 21:56:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000284 on epoch=104
05/31/2022 21:56:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001325 on epoch=107
05/31/2022 21:57:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000913 on epoch=109
05/31/2022 21:57:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000287 on epoch=112
05/31/2022 21:57:06 - INFO - __main__ - Global step 450 Train loss 0.000620 Classification-F1 0.96875 on epoch=112
05/31/2022 21:57:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002243 on epoch=114
05/31/2022 21:57:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003768 on epoch=117
05/31/2022 21:57:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000486 on epoch=119
05/31/2022 21:57:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.062016 on epoch=122
05/31/2022 21:57:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000286 on epoch=124
05/31/2022 21:57:34 - INFO - __main__ - Global step 500 Train loss 0.013760 Classification-F1 0.9531135531135531 on epoch=124
05/31/2022 21:57:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000672 on epoch=127
05/31/2022 21:57:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000209 on epoch=129
05/31/2022 21:57:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000666 on epoch=132
05/31/2022 21:57:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000340 on epoch=134
05/31/2022 21:58:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000143 on epoch=137
05/31/2022 21:58:01 - INFO - __main__ - Global step 550 Train loss 0.000406 Classification-F1 0.9531135531135531 on epoch=137
05/31/2022 21:58:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000262 on epoch=139
05/31/2022 21:58:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.010076 on epoch=142
05/31/2022 21:58:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000157 on epoch=144
05/31/2022 21:58:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000938 on epoch=147
05/31/2022 21:58:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000123 on epoch=149
05/31/2022 21:58:29 - INFO - __main__ - Global step 600 Train loss 0.002311 Classification-F1 0.9687194525904204 on epoch=149
05/31/2022 21:58:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000077 on epoch=152
05/31/2022 21:58:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000142 on epoch=154
05/31/2022 21:58:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000112 on epoch=157
05/31/2022 21:58:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000074 on epoch=159
05/31/2022 21:58:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000084 on epoch=162
05/31/2022 21:58:56 - INFO - __main__ - Global step 650 Train loss 0.000098 Classification-F1 0.9687194525904204 on epoch=162
05/31/2022 21:59:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000037 on epoch=164
05/31/2022 21:59:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000046 on epoch=167
05/31/2022 21:59:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000034 on epoch=169
05/31/2022 21:59:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000048 on epoch=172
05/31/2022 21:59:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000032 on epoch=174
05/31/2022 21:59:24 - INFO - __main__ - Global step 700 Train loss 0.000039 Classification-F1 0.9843711843711844 on epoch=174
05/31/2022 21:59:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000017 on epoch=177
05/31/2022 21:59:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000103 on epoch=179
05/31/2022 21:59:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000019 on epoch=182
05/31/2022 21:59:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000016 on epoch=184
05/31/2022 21:59:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000027 on epoch=187
05/31/2022 21:59:52 - INFO - __main__ - Global step 750 Train loss 0.000036 Classification-F1 0.9843711843711844 on epoch=187
05/31/2022 21:59:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000045 on epoch=189
05/31/2022 22:00:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000069 on epoch=192
05/31/2022 22:00:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000015 on epoch=194
05/31/2022 22:00:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000013 on epoch=197
05/31/2022 22:00:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000020 on epoch=199
05/31/2022 22:00:19 - INFO - __main__ - Global step 800 Train loss 0.000033 Classification-F1 0.9843711843711844 on epoch=199
05/31/2022 22:00:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000012 on epoch=202
05/31/2022 22:00:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000027 on epoch=204
05/31/2022 22:00:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000054 on epoch=207
05/31/2022 22:00:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000041 on epoch=209
05/31/2022 22:00:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000010 on epoch=212
05/31/2022 22:00:47 - INFO - __main__ - Global step 850 Train loss 0.000029 Classification-F1 0.9843711843711844 on epoch=212
05/31/2022 22:00:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000026 on epoch=214
05/31/2022 22:00:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000032 on epoch=217
05/31/2022 22:01:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.022603 on epoch=219
05/31/2022 22:01:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000273 on epoch=222
05/31/2022 22:01:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.096795 on epoch=224
05/31/2022 22:01:14 - INFO - __main__ - Global step 900 Train loss 0.023946 Classification-F1 0.9687194525904204 on epoch=224
05/31/2022 22:01:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.244475 on epoch=227
05/31/2022 22:01:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.207358 on epoch=229
05/31/2022 22:01:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.002811 on epoch=232
05/31/2022 22:01:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.044454 on epoch=234
05/31/2022 22:01:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.123840 on epoch=237
05/31/2022 22:01:42 - INFO - __main__ - Global step 950 Train loss 0.124588 Classification-F1 0.9843711843711844 on epoch=237
05/31/2022 22:01:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.032592 on epoch=239
05/31/2022 22:01:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.030552 on epoch=242
05/31/2022 22:01:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000206 on epoch=244
05/31/2022 22:02:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001197 on epoch=247
05/31/2022 22:02:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.108531 on epoch=249
05/31/2022 22:02:09 - INFO - __main__ - Global step 1000 Train loss 0.034615 Classification-F1 0.9843711843711844 on epoch=249
05/31/2022 22:02:09 - INFO - __main__ - save last model!
05/31/2022 22:02:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:02:10 - INFO - __main__ - Printing 3 examples
05/31/2022 22:02:10 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/31/2022 22:02:10 - INFO - __main__ - ['positive']
05/31/2022 22:02:10 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/31/2022 22:02:10 - INFO - __main__ - ['positive']
05/31/2022 22:02:10 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/31/2022 22:02:10 - INFO - __main__ - ['positive']
05/31/2022 22:02:10 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:02:10 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:02:10 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:02:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:02:10 - INFO - __main__ - Printing 3 examples
05/31/2022 22:02:10 - INFO - __main__ -  [amazon_polarity] title: The title is an oxymoron, but they did a good job of making it simple [SEP] content: I actually really like this book, but I think you do need the extra guidance of a professor when using it. It's very easy to look up the pharmaceuticals and all related info, and I was actually able to use it easily when taking the open book section (looking up the pharms) of a pharmacotherapeutics test. I would definitely recommend this book.
05/31/2022 22:02:10 - INFO - __main__ - ['positive']
05/31/2022 22:02:10 - INFO - __main__ -  [amazon_polarity] title: Incredible value not to be missed [SEP] content: This book is full of priceless information about places you would otherwise never hear about. It is not to be missed.
05/31/2022 22:02:10 - INFO - __main__ - ['positive']
05/31/2022 22:02:10 - INFO - __main__ -  [amazon_polarity] title: Great movie! [SEP] content: This is the epitome of a "B" western movie, but it's hilarious! Andy Griffith, Tom Berrenger, Marilou Henner and Selma Ward--all of them did a great job on this!
05/31/2022 22:02:10 - INFO - __main__ - ['positive']
05/31/2022 22:02:10 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:02:10 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:02:10 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:02:16 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 22:02:17 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 22:02:17 - INFO - __main__ - Printing 3 examples
05/31/2022 22:02:17 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 22:02:17 - INFO - __main__ - ['negative']
05/31/2022 22:02:17 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 22:02:17 - INFO - __main__ - ['negative']
05/31/2022 22:02:17 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 22:02:17 - INFO - __main__ - ['negative']
05/31/2022 22:02:17 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:02:18 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:02:19 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 22:02:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:02:23 - INFO - __main__ - Starting training!
05/31/2022 22:02:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_100_0.0002_8_predictions.txt
05/31/2022 22:02:34 - INFO - __main__ - Classification-F1 on test data: 0.9520
05/31/2022 22:02:34 - INFO - __main__ - prefix=amazon_polarity_32_100, lr=0.0002, bsz=8, dev_performance=0.9843711843711844, test_performance=0.9519877088534665
05/31/2022 22:02:34 - INFO - __main__ - Running ... prefix=amazon_polarity_32_100, lr=0.0001, bsz=8 ...
05/31/2022 22:02:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:02:35 - INFO - __main__ - Printing 3 examples
05/31/2022 22:02:35 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/31/2022 22:02:35 - INFO - __main__ - ['positive']
05/31/2022 22:02:35 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/31/2022 22:02:35 - INFO - __main__ - ['positive']
05/31/2022 22:02:35 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/31/2022 22:02:35 - INFO - __main__ - ['positive']
05/31/2022 22:02:35 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:02:35 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:02:35 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:02:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:02:35 - INFO - __main__ - Printing 3 examples
05/31/2022 22:02:35 - INFO - __main__ -  [amazon_polarity] title: The title is an oxymoron, but they did a good job of making it simple [SEP] content: I actually really like this book, but I think you do need the extra guidance of a professor when using it. It's very easy to look up the pharmaceuticals and all related info, and I was actually able to use it easily when taking the open book section (looking up the pharms) of a pharmacotherapeutics test. I would definitely recommend this book.
05/31/2022 22:02:35 - INFO - __main__ - ['positive']
05/31/2022 22:02:35 - INFO - __main__ -  [amazon_polarity] title: Incredible value not to be missed [SEP] content: This book is full of priceless information about places you would otherwise never hear about. It is not to be missed.
05/31/2022 22:02:35 - INFO - __main__ - ['positive']
05/31/2022 22:02:35 - INFO - __main__ -  [amazon_polarity] title: Great movie! [SEP] content: This is the epitome of a "B" western movie, but it's hilarious! Andy Griffith, Tom Berrenger, Marilou Henner and Selma Ward--all of them did a great job on this!
05/31/2022 22:02:35 - INFO - __main__ - ['positive']
05/31/2022 22:02:35 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:02:35 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:02:35 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:02:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:02:48 - INFO - __main__ - Starting training!
05/31/2022 22:02:53 - INFO - __main__ - Step 10 Global step 10 Train loss 21.993710 on epoch=2
05/31/2022 22:02:58 - INFO - __main__ - Step 20 Global step 20 Train loss 18.900543 on epoch=4
05/31/2022 22:03:03 - INFO - __main__ - Step 30 Global step 30 Train loss 17.214363 on epoch=7
05/31/2022 22:03:09 - INFO - __main__ - Step 40 Global step 40 Train loss 16.252615 on epoch=9
05/31/2022 22:03:14 - INFO - __main__ - Step 50 Global step 50 Train loss 16.332012 on epoch=12
05/31/2022 22:03:32 - INFO - __main__ - Global step 50 Train loss 18.138649 Classification-F1 0.0 on epoch=12
05/31/2022 22:03:38 - INFO - __main__ - Step 60 Global step 60 Train loss 15.504771 on epoch=14
05/31/2022 22:03:43 - INFO - __main__ - Step 70 Global step 70 Train loss 15.076900 on epoch=17
05/31/2022 22:03:49 - INFO - __main__ - Step 80 Global step 80 Train loss 14.668897 on epoch=19
05/31/2022 22:03:54 - INFO - __main__ - Step 90 Global step 90 Train loss 14.296488 on epoch=22
05/31/2022 22:03:59 - INFO - __main__ - Step 100 Global step 100 Train loss 13.717308 on epoch=24
05/31/2022 22:04:08 - INFO - __main__ - Global step 100 Train loss 14.652874 Classification-F1 0.0 on epoch=24
05/31/2022 22:04:13 - INFO - __main__ - Step 110 Global step 110 Train loss 13.306366 on epoch=27
05/31/2022 22:04:19 - INFO - __main__ - Step 120 Global step 120 Train loss 12.388647 on epoch=29
05/31/2022 22:04:24 - INFO - __main__ - Step 130 Global step 130 Train loss 12.366683 on epoch=32
05/31/2022 22:04:29 - INFO - __main__ - Step 140 Global step 140 Train loss 11.587259 on epoch=34
05/31/2022 22:04:35 - INFO - __main__ - Step 150 Global step 150 Train loss 11.604161 on epoch=37
05/31/2022 22:04:43 - INFO - __main__ - Global step 150 Train loss 12.250622 Classification-F1 0.0053475935828877 on epoch=37
05/31/2022 22:04:49 - INFO - __main__ - Step 160 Global step 160 Train loss 10.754541 on epoch=39
05/31/2022 22:04:54 - INFO - __main__ - Step 170 Global step 170 Train loss 10.029987 on epoch=42
05/31/2022 22:04:59 - INFO - __main__ - Step 180 Global step 180 Train loss 9.324953 on epoch=44
05/31/2022 22:05:05 - INFO - __main__ - Step 190 Global step 190 Train loss 6.939584 on epoch=47
05/31/2022 22:05:10 - INFO - __main__ - Step 200 Global step 200 Train loss 4.845363 on epoch=49
05/31/2022 22:05:11 - INFO - __main__ - Global step 200 Train loss 8.378885 Classification-F1 0.4589371980676329 on epoch=49
05/31/2022 22:05:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.793495 on epoch=52
05/31/2022 22:05:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.424855 on epoch=54
05/31/2022 22:05:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.383516 on epoch=57
05/31/2022 22:05:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.324989 on epoch=59
05/31/2022 22:05:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.494889 on epoch=62
05/31/2022 22:05:39 - INFO - __main__ - Global step 250 Train loss 0.484349 Classification-F1 0.9213952345860967 on epoch=62
05/31/2022 22:05:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.456876 on epoch=64
05/31/2022 22:05:50 - INFO - __main__ - Step 270 Global step 270 Train loss 1.125133 on epoch=67
05/31/2022 22:05:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.389653 on epoch=69
05/31/2022 22:06:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.259189 on epoch=72
05/31/2022 22:06:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.185701 on epoch=74
05/31/2022 22:06:06 - INFO - __main__ - Global step 300 Train loss 0.483311 Classification-F1 0.9531135531135531 on epoch=74
05/31/2022 22:06:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.098190 on epoch=77
05/31/2022 22:06:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.119689 on epoch=79
05/31/2022 22:06:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.194297 on epoch=82
05/31/2022 22:06:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.121039 on epoch=84
05/31/2022 22:06:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.044390 on epoch=87
05/31/2022 22:06:34 - INFO - __main__ - Global step 350 Train loss 0.115521 Classification-F1 0.9530217763640813 on epoch=87
05/31/2022 22:06:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.060978 on epoch=89
05/31/2022 22:06:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.035945 on epoch=92
05/31/2022 22:06:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.157394 on epoch=94
05/31/2022 22:06:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.066964 on epoch=97
05/31/2022 22:07:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.061998 on epoch=99
05/31/2022 22:07:02 - INFO - __main__ - Global step 400 Train loss 0.076656 Classification-F1 0.9531135531135531 on epoch=99
05/31/2022 22:07:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.035307 on epoch=102
05/31/2022 22:07:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.260724 on epoch=104
05/31/2022 22:07:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.023605 on epoch=107
05/31/2022 22:07:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.035580 on epoch=109
05/31/2022 22:07:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.010304 on epoch=112
05/31/2022 22:07:29 - INFO - __main__ - Global step 450 Train loss 0.073104 Classification-F1 0.9531135531135531 on epoch=112
05/31/2022 22:07:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.026356 on epoch=114
05/31/2022 22:07:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.014023 on epoch=117
05/31/2022 22:07:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.009755 on epoch=119
05/31/2022 22:07:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.041363 on epoch=122
05/31/2022 22:07:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013023 on epoch=124
05/31/2022 22:07:57 - INFO - __main__ - Global step 500 Train loss 0.020904 Classification-F1 0.9531135531135531 on epoch=124
05/31/2022 22:08:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.018595 on epoch=127
05/31/2022 22:08:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003602 on epoch=129
05/31/2022 22:08:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.009852 on epoch=132
05/31/2022 22:08:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001179 on epoch=134
05/31/2022 22:08:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.067840 on epoch=137
05/31/2022 22:08:24 - INFO - __main__ - Global step 550 Train loss 0.020213 Classification-F1 0.9372549019607843 on epoch=137
05/31/2022 22:08:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.004387 on epoch=139
05/31/2022 22:08:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000891 on epoch=142
05/31/2022 22:08:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.020149 on epoch=144
05/31/2022 22:08:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.033675 on epoch=147
05/31/2022 22:08:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001454 on epoch=149
05/31/2022 22:08:51 - INFO - __main__ - Global step 600 Train loss 0.012111 Classification-F1 0.9531135531135531 on epoch=149
05/31/2022 22:08:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001215 on epoch=152
05/31/2022 22:09:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000752 on epoch=154
05/31/2022 22:09:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002543 on epoch=157
05/31/2022 22:09:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.028131 on epoch=159
05/31/2022 22:09:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.010396 on epoch=162
05/31/2022 22:09:19 - INFO - __main__ - Global step 650 Train loss 0.008608 Classification-F1 0.9530217763640813 on epoch=162
05/31/2022 22:09:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001739 on epoch=164
05/31/2022 22:09:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.003966 on epoch=167
05/31/2022 22:09:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.008196 on epoch=169
05/31/2022 22:09:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.003206 on epoch=172
05/31/2022 22:09:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.002325 on epoch=174
05/31/2022 22:09:46 - INFO - __main__ - Global step 700 Train loss 0.003886 Classification-F1 0.9687194525904204 on epoch=174
05/31/2022 22:09:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.002855 on epoch=177
05/31/2022 22:09:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.027505 on epoch=179
05/31/2022 22:10:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.005078 on epoch=182
05/31/2022 22:10:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000871 on epoch=184
05/31/2022 22:10:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.012613 on epoch=187
05/31/2022 22:10:14 - INFO - __main__ - Global step 750 Train loss 0.009784 Classification-F1 0.9687194525904204 on epoch=187
05/31/2022 22:10:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.030160 on epoch=189
05/31/2022 22:10:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001787 on epoch=192
05/31/2022 22:10:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003181 on epoch=194
05/31/2022 22:10:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001357 on epoch=197
05/31/2022 22:10:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000806 on epoch=199
05/31/2022 22:10:42 - INFO - __main__ - Global step 800 Train loss 0.007458 Classification-F1 0.9530217763640813 on epoch=199
05/31/2022 22:10:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000690 on epoch=202
05/31/2022 22:10:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.002636 on epoch=204
05/31/2022 22:10:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.010118 on epoch=207
05/31/2022 22:11:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.019100 on epoch=209
05/31/2022 22:11:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000517 on epoch=212
05/31/2022 22:11:09 - INFO - __main__ - Global step 850 Train loss 0.006612 Classification-F1 0.9372549019607843 on epoch=212
05/31/2022 22:11:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.001512 on epoch=214
05/31/2022 22:11:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.015005 on epoch=217
05/31/2022 22:11:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000409 on epoch=219
05/31/2022 22:11:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000745 on epoch=222
05/31/2022 22:11:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000325 on epoch=224
05/31/2022 22:11:37 - INFO - __main__ - Global step 900 Train loss 0.003599 Classification-F1 0.9530217763640813 on epoch=224
05/31/2022 22:11:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000268 on epoch=227
05/31/2022 22:11:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000155 on epoch=229
05/31/2022 22:11:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000340 on epoch=232
05/31/2022 22:11:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000234 on epoch=234
05/31/2022 22:12:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000135 on epoch=237
05/31/2022 22:12:04 - INFO - __main__ - Global step 950 Train loss 0.000226 Classification-F1 0.9372549019607843 on epoch=237
05/31/2022 22:12:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000215 on epoch=239
05/31/2022 22:12:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000230 on epoch=242
05/31/2022 22:12:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.010324 on epoch=244
05/31/2022 22:12:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.033912 on epoch=247
05/31/2022 22:12:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.017730 on epoch=249
05/31/2022 22:12:31 - INFO - __main__ - Global step 1000 Train loss 0.012482 Classification-F1 0.9372549019607843 on epoch=249
05/31/2022 22:12:31 - INFO - __main__ - save last model!
05/31/2022 22:12:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:12:32 - INFO - __main__ - Printing 3 examples
05/31/2022 22:12:32 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/31/2022 22:12:32 - INFO - __main__ - ['negative']
05/31/2022 22:12:32 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/31/2022 22:12:32 - INFO - __main__ - ['negative']
05/31/2022 22:12:32 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/31/2022 22:12:32 - INFO - __main__ - ['negative']
05/31/2022 22:12:32 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:12:32 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:12:32 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:12:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:12:32 - INFO - __main__ - Printing 3 examples
05/31/2022 22:12:32 - INFO - __main__ -  [amazon_polarity] title: Future of Metal, What a joke [SEP] content: These guys are terrible, theyre decent musicians but that doesnt make them a good bandthey are pretty much a second rate metallica clone with some of the worst vocals evermetallcore is becoming the new, nu-metal
05/31/2022 22:12:32 - INFO - __main__ - ['negative']
05/31/2022 22:12:32 - INFO - __main__ -  [amazon_polarity] title: Each book just gets worse... [SEP] content: I remember the "good old days" when picking up a new Clancy book was an event to look forward to. But now I find myself dreading the first few pages on any new Clancy volume, especially after the disaster that was Rainbow Six.It seems that as the threat of the cold war recedes, Tom finds it more difficult to develop interesting storylines which are relevant (and realistic). It was always Tom's portrayal of the battle or espionage, and the realpolitik which motivates governments and the military alike, which captured my imagination.I can't maintain interest in a thousand page book if it reads like a soap opera!
05/31/2022 22:12:32 - INFO - __main__ - ['negative']
05/31/2022 22:12:32 - INFO - __main__ -  [amazon_polarity] title: Huge letdown [SEP] content: I can't understand all of the good reviews for this film, which I found to be a huge disappointment. Bad performances, coupled with awful direction and hackneyed, cliched writing puts this on my list of films to avoid. The film obviously tried to capitalize on Muniz's rising star power among youngsters and families, but offered little substance for this film to endure. There is never an indication at what makes this dog so special. And it' hard to believe that only two dogs were used for Skip; his appearance changes quite frequently throughout the film.If you want a recent "uplifting" family film, opt for "The Sandlot."
05/31/2022 22:12:32 - INFO - __main__ - ['negative']
05/31/2022 22:12:32 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:12:32 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:12:32 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:12:38 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 22:12:39 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 22:12:39 - INFO - __main__ - Printing 3 examples
05/31/2022 22:12:39 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 22:12:39 - INFO - __main__ - ['negative']
05/31/2022 22:12:39 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 22:12:39 - INFO - __main__ - ['negative']
05/31/2022 22:12:39 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 22:12:39 - INFO - __main__ - ['negative']
05/31/2022 22:12:39 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:12:40 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:12:41 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 22:12:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:12:45 - INFO - __main__ - Starting training!
05/31/2022 22:12:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_100_0.0001_8_predictions.txt
05/31/2022 22:12:56 - INFO - __main__ - Classification-F1 on test data: 0.9440
05/31/2022 22:12:56 - INFO - __main__ - prefix=amazon_polarity_32_100, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9439728828753116
05/31/2022 22:12:56 - INFO - __main__ - Running ... prefix=amazon_polarity_32_13, lr=0.0005, bsz=8 ...
05/31/2022 22:12:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:12:57 - INFO - __main__ - Printing 3 examples
05/31/2022 22:12:57 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/31/2022 22:12:57 - INFO - __main__ - ['negative']
05/31/2022 22:12:57 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/31/2022 22:12:57 - INFO - __main__ - ['negative']
05/31/2022 22:12:57 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/31/2022 22:12:57 - INFO - __main__ - ['negative']
05/31/2022 22:12:57 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:12:57 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:12:57 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:12:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:12:57 - INFO - __main__ - Printing 3 examples
05/31/2022 22:12:57 - INFO - __main__ -  [amazon_polarity] title: Future of Metal, What a joke [SEP] content: These guys are terrible, theyre decent musicians but that doesnt make them a good bandthey are pretty much a second rate metallica clone with some of the worst vocals evermetallcore is becoming the new, nu-metal
05/31/2022 22:12:57 - INFO - __main__ - ['negative']
05/31/2022 22:12:57 - INFO - __main__ -  [amazon_polarity] title: Each book just gets worse... [SEP] content: I remember the "good old days" when picking up a new Clancy book was an event to look forward to. But now I find myself dreading the first few pages on any new Clancy volume, especially after the disaster that was Rainbow Six.It seems that as the threat of the cold war recedes, Tom finds it more difficult to develop interesting storylines which are relevant (and realistic). It was always Tom's portrayal of the battle or espionage, and the realpolitik which motivates governments and the military alike, which captured my imagination.I can't maintain interest in a thousand page book if it reads like a soap opera!
05/31/2022 22:12:57 - INFO - __main__ - ['negative']
05/31/2022 22:12:57 - INFO - __main__ -  [amazon_polarity] title: Huge letdown [SEP] content: I can't understand all of the good reviews for this film, which I found to be a huge disappointment. Bad performances, coupled with awful direction and hackneyed, cliched writing puts this on my list of films to avoid. The film obviously tried to capitalize on Muniz's rising star power among youngsters and families, but offered little substance for this film to endure. There is never an indication at what makes this dog so special. And it' hard to believe that only two dogs were used for Skip; his appearance changes quite frequently throughout the film.If you want a recent "uplifting" family film, opt for "The Sandlot."
05/31/2022 22:12:57 - INFO - __main__ - ['negative']
05/31/2022 22:12:57 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:12:57 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:12:57 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:13:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:13:10 - INFO - __main__ - Starting training!
05/31/2022 22:13:15 - INFO - __main__ - Step 10 Global step 10 Train loss 23.688450 on epoch=2
05/31/2022 22:13:19 - INFO - __main__ - Step 20 Global step 20 Train loss 19.086689 on epoch=4
05/31/2022 22:13:24 - INFO - __main__ - Step 30 Global step 30 Train loss 16.435989 on epoch=7
05/31/2022 22:13:30 - INFO - __main__ - Step 40 Global step 40 Train loss 14.244280 on epoch=9
05/31/2022 22:13:35 - INFO - __main__ - Step 50 Global step 50 Train loss 12.128461 on epoch=12
05/31/2022 22:13:46 - INFO - __main__ - Global step 50 Train loss 17.116774 Classification-F1 0.0 on epoch=12
05/31/2022 22:13:52 - INFO - __main__ - Step 60 Global step 60 Train loss 9.057978 on epoch=14
05/31/2022 22:13:57 - INFO - __main__ - Step 70 Global step 70 Train loss 3.205223 on epoch=17
05/31/2022 22:14:02 - INFO - __main__ - Step 80 Global step 80 Train loss 2.912939 on epoch=19
05/31/2022 22:14:08 - INFO - __main__ - Step 90 Global step 90 Train loss 2.661329 on epoch=22
05/31/2022 22:14:13 - INFO - __main__ - Step 100 Global step 100 Train loss 1.811671 on epoch=24
05/31/2022 22:14:14 - INFO - __main__ - Global step 100 Train loss 3.929828 Classification-F1 0.38714090287277697 on epoch=24
05/31/2022 22:14:21 - INFO - __main__ - Step 110 Global step 110 Train loss 1.545512 on epoch=27
05/31/2022 22:14:26 - INFO - __main__ - Step 120 Global step 120 Train loss 1.199255 on epoch=29
05/31/2022 22:14:31 - INFO - __main__ - Step 130 Global step 130 Train loss 1.411250 on epoch=32
05/31/2022 22:14:37 - INFO - __main__ - Step 140 Global step 140 Train loss 1.020931 on epoch=34
05/31/2022 22:14:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.752604 on epoch=37
05/31/2022 22:14:43 - INFO - __main__ - Global step 150 Train loss 1.185910 Classification-F1 0.4102117061021171 on epoch=37
05/31/2022 22:14:49 - INFO - __main__ - Step 160 Global step 160 Train loss 1.098811 on epoch=39
05/31/2022 22:14:54 - INFO - __main__ - Step 170 Global step 170 Train loss 1.133981 on epoch=42
05/31/2022 22:15:00 - INFO - __main__ - Step 180 Global step 180 Train loss 1.049342 on epoch=44
05/31/2022 22:15:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.878324 on epoch=47
05/31/2022 22:15:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.982694 on epoch=49
05/31/2022 22:15:11 - INFO - __main__ - Global step 200 Train loss 1.028630 Classification-F1 0.4202898550724638 on epoch=49
05/31/2022 22:15:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.948510 on epoch=52
05/31/2022 22:15:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.731915 on epoch=54
05/31/2022 22:15:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.877044 on epoch=57
05/31/2022 22:15:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.727598 on epoch=59
05/31/2022 22:15:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.644763 on epoch=62
05/31/2022 22:15:40 - INFO - __main__ - Global step 250 Train loss 0.785966 Classification-F1 0.3333333333333333 on epoch=62
05/31/2022 22:15:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.657531 on epoch=64
05/31/2022 22:15:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.616017 on epoch=67
05/31/2022 22:15:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.463961 on epoch=69
05/31/2022 22:16:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.494776 on epoch=72
05/31/2022 22:16:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.435866 on epoch=74
05/31/2022 22:16:08 - INFO - __main__ - Global step 300 Train loss 0.533630 Classification-F1 0.5272229822161423 on epoch=74
05/31/2022 22:16:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.400988 on epoch=77
05/31/2022 22:16:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.471094 on epoch=79
05/31/2022 22:16:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.407265 on epoch=82
05/31/2022 22:16:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.451770 on epoch=84
05/31/2022 22:16:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.387403 on epoch=87
05/31/2022 22:16:36 - INFO - __main__ - Global step 350 Train loss 0.423704 Classification-F1 0.3333333333333333 on epoch=87
05/31/2022 22:16:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.389785 on epoch=89
05/31/2022 22:16:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.451462 on epoch=92
05/31/2022 22:16:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.409684 on epoch=94
05/31/2022 22:16:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.360546 on epoch=97
05/31/2022 22:17:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.357756 on epoch=99
05/31/2022 22:17:04 - INFO - __main__ - Global step 400 Train loss 0.393847 Classification-F1 0.3333333333333333 on epoch=99
05/31/2022 22:17:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.392890 on epoch=102
05/31/2022 22:17:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.337469 on epoch=104
05/31/2022 22:17:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.319217 on epoch=107
05/31/2022 22:17:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.342124 on epoch=109
05/31/2022 22:17:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.293577 on epoch=112
05/31/2022 22:17:31 - INFO - __main__ - Global step 450 Train loss 0.337055 Classification-F1 0.5373493975903615 on epoch=112
05/31/2022 22:17:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.302831 on epoch=114
05/31/2022 22:17:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.293980 on epoch=117
05/31/2022 22:17:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.296582 on epoch=119
05/31/2022 22:17:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.282875 on epoch=122
05/31/2022 22:17:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.222260 on epoch=124
05/31/2022 22:17:59 - INFO - __main__ - Global step 500 Train loss 0.279706 Classification-F1 0.6652552926525529 on epoch=124
05/31/2022 22:18:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.223084 on epoch=127
05/31/2022 22:18:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.253517 on epoch=129
05/31/2022 22:18:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.233432 on epoch=132
05/31/2022 22:18:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.151241 on epoch=134
05/31/2022 22:18:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.189523 on epoch=137
05/31/2022 22:18:27 - INFO - __main__ - Global step 550 Train loss 0.210159 Classification-F1 0.6717948717948719 on epoch=137
05/31/2022 22:18:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.128489 on epoch=139
05/31/2022 22:18:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.262936 on epoch=142
05/31/2022 22:18:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.221428 on epoch=144
05/31/2022 22:18:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.144447 on epoch=147
05/31/2022 22:18:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.120629 on epoch=149
05/31/2022 22:18:56 - INFO - __main__ - Global step 600 Train loss 0.175586 Classification-F1 0.5789473684210527 on epoch=149
05/31/2022 22:19:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.133174 on epoch=152
05/31/2022 22:19:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.108195 on epoch=154
05/31/2022 22:19:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.061993 on epoch=157
05/31/2022 22:19:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.054928 on epoch=159
05/31/2022 22:19:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.092587 on epoch=162
05/31/2022 22:19:24 - INFO - __main__ - Global step 650 Train loss 0.090175 Classification-F1 0.5666666666666667 on epoch=162
05/31/2022 22:19:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.120571 on epoch=164
05/31/2022 22:19:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.033950 on epoch=167
05/31/2022 22:19:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.028633 on epoch=169
05/31/2022 22:19:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.065456 on epoch=172
05/31/2022 22:19:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.079206 on epoch=174
05/31/2022 22:19:51 - INFO - __main__ - Global step 700 Train loss 0.065563 Classification-F1 0.5933528836754642 on epoch=174
05/31/2022 22:19:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.018042 on epoch=177
05/31/2022 22:20:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.037155 on epoch=179
05/31/2022 22:20:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.008294 on epoch=182
05/31/2022 22:20:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.004746 on epoch=184
05/31/2022 22:20:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.118677 on epoch=187
05/31/2022 22:20:19 - INFO - __main__ - Global step 750 Train loss 0.037383 Classification-F1 0.6437246963562753 on epoch=187
05/31/2022 22:20:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.153642 on epoch=189
05/31/2022 22:20:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.127818 on epoch=192
05/31/2022 22:20:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.015193 on epoch=194
05/31/2022 22:20:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.006806 on epoch=197
05/31/2022 22:20:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.014311 on epoch=199
05/31/2022 22:20:47 - INFO - __main__ - Global step 800 Train loss 0.063554 Classification-F1 0.6251591545709192 on epoch=199
05/31/2022 22:20:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.005071 on epoch=202
05/31/2022 22:20:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.036143 on epoch=204
05/31/2022 22:21:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.011711 on epoch=207
05/31/2022 22:21:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.010969 on epoch=209
05/31/2022 22:21:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.003874 on epoch=212
05/31/2022 22:21:15 - INFO - __main__ - Global step 850 Train loss 0.013554 Classification-F1 0.5497835497835498 on epoch=212
05/31/2022 22:21:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.030305 on epoch=214
05/31/2022 22:21:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003980 on epoch=217
05/31/2022 22:21:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.007809 on epoch=219
05/31/2022 22:21:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002405 on epoch=222
05/31/2022 22:21:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001638 on epoch=224
05/31/2022 22:21:43 - INFO - __main__ - Global step 900 Train loss 0.009227 Classification-F1 0.6235294117647059 on epoch=224
05/31/2022 22:21:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001044 on epoch=227
05/31/2022 22:21:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.001224 on epoch=229
05/31/2022 22:21:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000173 on epoch=232
05/31/2022 22:22:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000650 on epoch=234
05/31/2022 22:22:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000402 on epoch=237
05/31/2022 22:22:10 - INFO - __main__ - Global step 950 Train loss 0.000699 Classification-F1 0.65625 on epoch=237
05/31/2022 22:22:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001167 on epoch=239
05/31/2022 22:22:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000717 on epoch=242
05/31/2022 22:22:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000713 on epoch=244
05/31/2022 22:22:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001881 on epoch=247
05/31/2022 22:22:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.006215 on epoch=249
05/31/2022 22:22:38 - INFO - __main__ - Global step 1000 Train loss 0.002139 Classification-F1 0.6333748443337484 on epoch=249
05/31/2022 22:22:38 - INFO - __main__ - save last model!
05/31/2022 22:22:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:22:38 - INFO - __main__ - Printing 3 examples
05/31/2022 22:22:38 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/31/2022 22:22:38 - INFO - __main__ - ['negative']
05/31/2022 22:22:38 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/31/2022 22:22:38 - INFO - __main__ - ['negative']
05/31/2022 22:22:38 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/31/2022 22:22:38 - INFO - __main__ - ['negative']
05/31/2022 22:22:38 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:22:38 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:22:38 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:22:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:22:38 - INFO - __main__ - Printing 3 examples
05/31/2022 22:22:38 - INFO - __main__ -  [amazon_polarity] title: Future of Metal, What a joke [SEP] content: These guys are terrible, theyre decent musicians but that doesnt make them a good bandthey are pretty much a second rate metallica clone with some of the worst vocals evermetallcore is becoming the new, nu-metal
05/31/2022 22:22:38 - INFO - __main__ - ['negative']
05/31/2022 22:22:38 - INFO - __main__ -  [amazon_polarity] title: Each book just gets worse... [SEP] content: I remember the "good old days" when picking up a new Clancy book was an event to look forward to. But now I find myself dreading the first few pages on any new Clancy volume, especially after the disaster that was Rainbow Six.It seems that as the threat of the cold war recedes, Tom finds it more difficult to develop interesting storylines which are relevant (and realistic). It was always Tom's portrayal of the battle or espionage, and the realpolitik which motivates governments and the military alike, which captured my imagination.I can't maintain interest in a thousand page book if it reads like a soap opera!
05/31/2022 22:22:38 - INFO - __main__ - ['negative']
05/31/2022 22:22:38 - INFO - __main__ -  [amazon_polarity] title: Huge letdown [SEP] content: I can't understand all of the good reviews for this film, which I found to be a huge disappointment. Bad performances, coupled with awful direction and hackneyed, cliched writing puts this on my list of films to avoid. The film obviously tried to capitalize on Muniz's rising star power among youngsters and families, but offered little substance for this film to endure. There is never an indication at what makes this dog so special. And it' hard to believe that only two dogs were used for Skip; his appearance changes quite frequently throughout the film.If you want a recent "uplifting" family film, opt for "The Sandlot."
05/31/2022 22:22:38 - INFO - __main__ - ['negative']
05/31/2022 22:22:38 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:22:38 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:22:38 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:22:45 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 22:22:45 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 22:22:45 - INFO - __main__ - Printing 3 examples
05/31/2022 22:22:45 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 22:22:45 - INFO - __main__ - ['negative']
05/31/2022 22:22:45 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 22:22:45 - INFO - __main__ - ['negative']
05/31/2022 22:22:45 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 22:22:45 - INFO - __main__ - ['negative']
05/31/2022 22:22:45 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:22:46 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:22:47 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 22:22:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:22:49 - INFO - __main__ - Starting training!
05/31/2022 22:23:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_13_0.0005_8_predictions.txt
05/31/2022 22:23:00 - INFO - __main__ - Classification-F1 on test data: 0.6750
05/31/2022 22:23:00 - INFO - __main__ - prefix=amazon_polarity_32_13, lr=0.0005, bsz=8, dev_performance=0.6717948717948719, test_performance=0.674995621046919
05/31/2022 22:23:00 - INFO - __main__ - Running ... prefix=amazon_polarity_32_13, lr=0.0003, bsz=8 ...
05/31/2022 22:23:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:23:01 - INFO - __main__ - Printing 3 examples
05/31/2022 22:23:01 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/31/2022 22:23:01 - INFO - __main__ - ['negative']
05/31/2022 22:23:01 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/31/2022 22:23:01 - INFO - __main__ - ['negative']
05/31/2022 22:23:01 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/31/2022 22:23:01 - INFO - __main__ - ['negative']
05/31/2022 22:23:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:23:01 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:23:01 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:23:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:23:01 - INFO - __main__ - Printing 3 examples
05/31/2022 22:23:01 - INFO - __main__ -  [amazon_polarity] title: Future of Metal, What a joke [SEP] content: These guys are terrible, theyre decent musicians but that doesnt make them a good bandthey are pretty much a second rate metallica clone with some of the worst vocals evermetallcore is becoming the new, nu-metal
05/31/2022 22:23:01 - INFO - __main__ - ['negative']
05/31/2022 22:23:01 - INFO - __main__ -  [amazon_polarity] title: Each book just gets worse... [SEP] content: I remember the "good old days" when picking up a new Clancy book was an event to look forward to. But now I find myself dreading the first few pages on any new Clancy volume, especially after the disaster that was Rainbow Six.It seems that as the threat of the cold war recedes, Tom finds it more difficult to develop interesting storylines which are relevant (and realistic). It was always Tom's portrayal of the battle or espionage, and the realpolitik which motivates governments and the military alike, which captured my imagination.I can't maintain interest in a thousand page book if it reads like a soap opera!
05/31/2022 22:23:01 - INFO - __main__ - ['negative']
05/31/2022 22:23:01 - INFO - __main__ -  [amazon_polarity] title: Huge letdown [SEP] content: I can't understand all of the good reviews for this film, which I found to be a huge disappointment. Bad performances, coupled with awful direction and hackneyed, cliched writing puts this on my list of films to avoid. The film obviously tried to capitalize on Muniz's rising star power among youngsters and families, but offered little substance for this film to endure. There is never an indication at what makes this dog so special. And it' hard to believe that only two dogs were used for Skip; his appearance changes quite frequently throughout the film.If you want a recent "uplifting" family film, opt for "The Sandlot."
05/31/2022 22:23:01 - INFO - __main__ - ['negative']
05/31/2022 22:23:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:23:01 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:23:01 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:23:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:23:12 - INFO - __main__ - Starting training!
05/31/2022 22:23:17 - INFO - __main__ - Step 10 Global step 10 Train loss 22.354305 on epoch=2
05/31/2022 22:23:21 - INFO - __main__ - Step 20 Global step 20 Train loss 19.498653 on epoch=4
05/31/2022 22:23:26 - INFO - __main__ - Step 30 Global step 30 Train loss 17.659937 on epoch=7
05/31/2022 22:23:32 - INFO - __main__ - Step 40 Global step 40 Train loss 16.045021 on epoch=9
05/31/2022 22:23:37 - INFO - __main__ - Step 50 Global step 50 Train loss 14.810992 on epoch=12
05/31/2022 22:23:44 - INFO - __main__ - Global step 50 Train loss 18.073780 Classification-F1 0.0 on epoch=12
05/31/2022 22:23:50 - INFO - __main__ - Step 60 Global step 60 Train loss 12.994696 on epoch=14
05/31/2022 22:23:55 - INFO - __main__ - Step 70 Global step 70 Train loss 12.480433 on epoch=17
05/31/2022 22:24:00 - INFO - __main__ - Step 80 Global step 80 Train loss 10.835783 on epoch=19
05/31/2022 22:24:05 - INFO - __main__ - Step 90 Global step 90 Train loss 6.497362 on epoch=22
05/31/2022 22:24:11 - INFO - __main__ - Step 100 Global step 100 Train loss 2.896401 on epoch=24
05/31/2022 22:24:12 - INFO - __main__ - Global step 100 Train loss 9.140936 Classification-F1 0.3333333333333333 on epoch=24
05/31/2022 22:24:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.566179 on epoch=27
05/31/2022 22:24:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.252048 on epoch=29
05/31/2022 22:24:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.183796 on epoch=32
05/31/2022 22:24:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.080087 on epoch=34
05/31/2022 22:24:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.039777 on epoch=37
05/31/2022 22:24:40 - INFO - __main__ - Global step 150 Train loss 0.224377 Classification-F1 0.9531135531135531 on epoch=37
05/31/2022 22:24:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.010147 on epoch=39
05/31/2022 22:24:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.008680 on epoch=42
05/31/2022 22:24:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.026178 on epoch=44
05/31/2022 22:25:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.029131 on epoch=47
05/31/2022 22:25:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.022355 on epoch=49
05/31/2022 22:25:08 - INFO - __main__ - Global step 200 Train loss 0.019298 Classification-F1 0.9374389051808407 on epoch=49
05/31/2022 22:25:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.030895 on epoch=52
05/31/2022 22:25:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.002599 on epoch=54
05/31/2022 22:25:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.021744 on epoch=57
05/31/2022 22:25:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000859 on epoch=59
05/31/2022 22:25:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.008734 on epoch=62
05/31/2022 22:25:36 - INFO - __main__ - Global step 250 Train loss 0.012966 Classification-F1 0.9372549019607843 on epoch=62
05/31/2022 22:25:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.023290 on epoch=64
05/31/2022 22:25:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000944 on epoch=67
05/31/2022 22:25:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000594 on epoch=69
05/31/2022 22:25:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000278 on epoch=72
05/31/2022 22:26:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.002249 on epoch=74
05/31/2022 22:26:04 - INFO - __main__ - Global step 300 Train loss 0.005471 Classification-F1 0.9687194525904204 on epoch=74
05/31/2022 22:26:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.011260 on epoch=77
05/31/2022 22:26:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.019448 on epoch=79
05/31/2022 22:26:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.032011 on epoch=82
05/31/2022 22:26:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001317 on epoch=84
05/31/2022 22:26:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.003333 on epoch=87
05/31/2022 22:26:34 - INFO - __main__ - Global step 350 Train loss 0.013474 Classification-F1 0.9687194525904204 on epoch=87
05/31/2022 22:26:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001438 on epoch=89
05/31/2022 22:26:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.002439 on epoch=92
05/31/2022 22:26:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000354 on epoch=94
05/31/2022 22:26:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000165 on epoch=97
05/31/2022 22:27:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.025631 on epoch=99
05/31/2022 22:27:02 - INFO - __main__ - Global step 400 Train loss 0.006005 Classification-F1 0.96875 on epoch=99
05/31/2022 22:27:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000579 on epoch=102
05/31/2022 22:27:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000257 on epoch=104
05/31/2022 22:27:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.001109 on epoch=107
05/31/2022 22:27:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000121 on epoch=109
05/31/2022 22:27:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000192 on epoch=112
05/31/2022 22:27:31 - INFO - __main__ - Global step 450 Train loss 0.000452 Classification-F1 0.9374389051808407 on epoch=112
05/31/2022 22:27:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000192 on epoch=114
05/31/2022 22:27:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000432 on epoch=117
05/31/2022 22:27:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000071 on epoch=119
05/31/2022 22:27:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000071 on epoch=122
05/31/2022 22:27:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000139 on epoch=124
05/31/2022 22:27:59 - INFO - __main__ - Global step 500 Train loss 0.000181 Classification-F1 0.9374389051808407 on epoch=124
05/31/2022 22:28:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000060 on epoch=127
05/31/2022 22:28:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003373 on epoch=129
05/31/2022 22:28:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000300 on epoch=132
05/31/2022 22:28:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000083 on epoch=134
05/31/2022 22:28:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000185 on epoch=137
05/31/2022 22:28:27 - INFO - __main__ - Global step 550 Train loss 0.000800 Classification-F1 0.9375 on epoch=137
05/31/2022 22:28:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000224 on epoch=139
05/31/2022 22:28:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.009427 on epoch=142
05/31/2022 22:28:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000046 on epoch=144
05/31/2022 22:28:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000040 on epoch=147
05/31/2022 22:28:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000021 on epoch=149
05/31/2022 22:28:55 - INFO - __main__ - Global step 600 Train loss 0.001952 Classification-F1 0.9374389051808407 on epoch=149
05/31/2022 22:29:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000029 on epoch=152
05/31/2022 22:29:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000019 on epoch=154
05/31/2022 22:29:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000048 on epoch=157
05/31/2022 22:29:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000025 on epoch=159
05/31/2022 22:29:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000021 on epoch=162
05/31/2022 22:29:23 - INFO - __main__ - Global step 650 Train loss 0.000028 Classification-F1 0.9374389051808407 on epoch=162
05/31/2022 22:29:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000015 on epoch=164
05/31/2022 22:29:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000039 on epoch=167
05/31/2022 22:29:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000044 on epoch=169
05/31/2022 22:29:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000201 on epoch=172
05/31/2022 22:29:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000154 on epoch=174
05/31/2022 22:29:52 - INFO - __main__ - Global step 700 Train loss 0.000090 Classification-F1 0.9374389051808407 on epoch=174
05/31/2022 22:29:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000733 on epoch=177
05/31/2022 22:30:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.013002 on epoch=179
05/31/2022 22:30:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.453763 on epoch=182
05/31/2022 22:30:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.375179 on epoch=184
05/31/2022 22:30:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.364980 on epoch=187
05/31/2022 22:30:20 - INFO - __main__ - Global step 750 Train loss 0.241531 Classification-F1 0.5636363636363637 on epoch=187
05/31/2022 22:30:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.383365 on epoch=189
05/31/2022 22:30:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.277085 on epoch=192
05/31/2022 22:30:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.022205 on epoch=194
05/31/2022 22:30:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.010050 on epoch=197
05/31/2022 22:30:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.034438 on epoch=199
05/31/2022 22:30:48 - INFO - __main__ - Global step 800 Train loss 0.145429 Classification-F1 0.9375 on epoch=199
05/31/2022 22:30:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.058984 on epoch=202
05/31/2022 22:30:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.066645 on epoch=204
05/31/2022 22:31:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.054015 on epoch=207
05/31/2022 22:31:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.024435 on epoch=209
05/31/2022 22:31:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.017705 on epoch=212
05/31/2022 22:31:16 - INFO - __main__ - Global step 850 Train loss 0.044357 Classification-F1 0.906158357771261 on epoch=212
05/31/2022 22:31:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000217 on epoch=214
05/31/2022 22:31:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000261 on epoch=217
05/31/2022 22:31:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.005074 on epoch=219
05/31/2022 22:31:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.003626 on epoch=222
05/31/2022 22:31:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000147 on epoch=224
05/31/2022 22:31:44 - INFO - __main__ - Global step 900 Train loss 0.001865 Classification-F1 0.921702960606802 on epoch=224
05/31/2022 22:31:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.004341 on epoch=227
05/31/2022 22:31:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.024900 on epoch=229
05/31/2022 22:32:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.077382 on epoch=232
05/31/2022 22:32:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.077142 on epoch=234
05/31/2022 22:32:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.036319 on epoch=237
05/31/2022 22:32:12 - INFO - __main__ - Global step 950 Train loss 0.044017 Classification-F1 0.906158357771261 on epoch=237
05/31/2022 22:32:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.017790 on epoch=239
05/31/2022 22:32:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.159898 on epoch=242
05/31/2022 22:32:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.297485 on epoch=244
05/31/2022 22:32:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.320620 on epoch=247
05/31/2022 22:32:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.327568 on epoch=249
05/31/2022 22:32:40 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:32:40 - INFO - __main__ - Printing 3 examples
05/31/2022 22:32:40 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/31/2022 22:32:40 - INFO - __main__ - ['negative']
05/31/2022 22:32:40 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/31/2022 22:32:40 - INFO - __main__ - ['negative']
05/31/2022 22:32:40 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/31/2022 22:32:40 - INFO - __main__ - ['negative']
05/31/2022 22:32:40 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:32:40 - INFO - __main__ - Global step 1000 Train loss 0.224672 Classification-F1 0.551443790299972 on epoch=249
05/31/2022 22:32:40 - INFO - __main__ - save last model!
05/31/2022 22:32:40 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:32:40 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:32:40 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:32:40 - INFO - __main__ - Printing 3 examples
05/31/2022 22:32:40 - INFO - __main__ -  [amazon_polarity] title: Future of Metal, What a joke [SEP] content: These guys are terrible, theyre decent musicians but that doesnt make them a good bandthey are pretty much a second rate metallica clone with some of the worst vocals evermetallcore is becoming the new, nu-metal
05/31/2022 22:32:40 - INFO - __main__ - ['negative']
05/31/2022 22:32:40 - INFO - __main__ -  [amazon_polarity] title: Each book just gets worse... [SEP] content: I remember the "good old days" when picking up a new Clancy book was an event to look forward to. But now I find myself dreading the first few pages on any new Clancy volume, especially after the disaster that was Rainbow Six.It seems that as the threat of the cold war recedes, Tom finds it more difficult to develop interesting storylines which are relevant (and realistic). It was always Tom's portrayal of the battle or espionage, and the realpolitik which motivates governments and the military alike, which captured my imagination.I can't maintain interest in a thousand page book if it reads like a soap opera!
05/31/2022 22:32:40 - INFO - __main__ - ['negative']
05/31/2022 22:32:40 - INFO - __main__ -  [amazon_polarity] title: Huge letdown [SEP] content: I can't understand all of the good reviews for this film, which I found to be a huge disappointment. Bad performances, coupled with awful direction and hackneyed, cliched writing puts this on my list of films to avoid. The film obviously tried to capitalize on Muniz's rising star power among youngsters and families, but offered little substance for this film to endure. There is never an indication at what makes this dog so special. And it' hard to believe that only two dogs were used for Skip; his appearance changes quite frequently throughout the film.If you want a recent "uplifting" family film, opt for "The Sandlot."
05/31/2022 22:32:40 - INFO - __main__ - ['negative']
05/31/2022 22:32:40 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:32:40 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:32:40 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:32:47 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 22:32:48 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 22:32:48 - INFO - __main__ - Printing 3 examples
05/31/2022 22:32:48 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 22:32:48 - INFO - __main__ - ['negative']
05/31/2022 22:32:48 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 22:32:48 - INFO - __main__ - ['negative']
05/31/2022 22:32:48 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 22:32:48 - INFO - __main__ - ['negative']
05/31/2022 22:32:48 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:32:48 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:32:49 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 22:32:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:32:53 - INFO - __main__ - Starting training!
05/31/2022 22:33:04 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_13_0.0003_8_predictions.txt
05/31/2022 22:33:04 - INFO - __main__ - Classification-F1 on test data: 0.9439
05/31/2022 22:33:05 - INFO - __main__ - prefix=amazon_polarity_32_13, lr=0.0003, bsz=8, dev_performance=0.96875, test_performance=0.9439273298194459
05/31/2022 22:33:05 - INFO - __main__ - Running ... prefix=amazon_polarity_32_13, lr=0.0002, bsz=8 ...
05/31/2022 22:33:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:33:05 - INFO - __main__ - Printing 3 examples
05/31/2022 22:33:05 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/31/2022 22:33:05 - INFO - __main__ - ['negative']
05/31/2022 22:33:05 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/31/2022 22:33:05 - INFO - __main__ - ['negative']
05/31/2022 22:33:05 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/31/2022 22:33:05 - INFO - __main__ - ['negative']
05/31/2022 22:33:05 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:33:05 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:33:06 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:33:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:33:06 - INFO - __main__ - Printing 3 examples
05/31/2022 22:33:06 - INFO - __main__ -  [amazon_polarity] title: Future of Metal, What a joke [SEP] content: These guys are terrible, theyre decent musicians but that doesnt make them a good bandthey are pretty much a second rate metallica clone with some of the worst vocals evermetallcore is becoming the new, nu-metal
05/31/2022 22:33:06 - INFO - __main__ - ['negative']
05/31/2022 22:33:06 - INFO - __main__ -  [amazon_polarity] title: Each book just gets worse... [SEP] content: I remember the "good old days" when picking up a new Clancy book was an event to look forward to. But now I find myself dreading the first few pages on any new Clancy volume, especially after the disaster that was Rainbow Six.It seems that as the threat of the cold war recedes, Tom finds it more difficult to develop interesting storylines which are relevant (and realistic). It was always Tom's portrayal of the battle or espionage, and the realpolitik which motivates governments and the military alike, which captured my imagination.I can't maintain interest in a thousand page book if it reads like a soap opera!
05/31/2022 22:33:06 - INFO - __main__ - ['negative']
05/31/2022 22:33:06 - INFO - __main__ -  [amazon_polarity] title: Huge letdown [SEP] content: I can't understand all of the good reviews for this film, which I found to be a huge disappointment. Bad performances, coupled with awful direction and hackneyed, cliched writing puts this on my list of films to avoid. The film obviously tried to capitalize on Muniz's rising star power among youngsters and families, but offered little substance for this film to endure. There is never an indication at what makes this dog so special. And it' hard to believe that only two dogs were used for Skip; his appearance changes quite frequently throughout the film.If you want a recent "uplifting" family film, opt for "The Sandlot."
05/31/2022 22:33:06 - INFO - __main__ - ['negative']
05/31/2022 22:33:06 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:33:06 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:33:06 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:33:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:33:18 - INFO - __main__ - Starting training!
05/31/2022 22:33:23 - INFO - __main__ - Step 10 Global step 10 Train loss 24.180161 on epoch=2
05/31/2022 22:33:28 - INFO - __main__ - Step 20 Global step 20 Train loss 18.405321 on epoch=4
05/31/2022 22:33:34 - INFO - __main__ - Step 30 Global step 30 Train loss 16.745174 on epoch=7
05/31/2022 22:33:39 - INFO - __main__ - Step 40 Global step 40 Train loss 16.794363 on epoch=9
05/31/2022 22:33:44 - INFO - __main__ - Step 50 Global step 50 Train loss 15.402618 on epoch=12
05/31/2022 22:33:51 - INFO - __main__ - Global step 50 Train loss 18.305529 Classification-F1 0.0 on epoch=12
05/31/2022 22:33:56 - INFO - __main__ - Step 60 Global step 60 Train loss 14.335777 on epoch=14
05/31/2022 22:34:02 - INFO - __main__ - Step 70 Global step 70 Train loss 13.667753 on epoch=17
05/31/2022 22:34:07 - INFO - __main__ - Step 80 Global step 80 Train loss 13.161191 on epoch=19
05/31/2022 22:34:12 - INFO - __main__ - Step 90 Global step 90 Train loss 12.227452 on epoch=22
05/31/2022 22:34:18 - INFO - __main__ - Step 100 Global step 100 Train loss 10.923574 on epoch=24
05/31/2022 22:34:23 - INFO - __main__ - Global step 100 Train loss 12.863150 Classification-F1 0.0 on epoch=24
05/31/2022 22:34:28 - INFO - __main__ - Step 110 Global step 110 Train loss 9.996830 on epoch=27
05/31/2022 22:34:33 - INFO - __main__ - Step 120 Global step 120 Train loss 7.795824 on epoch=29
05/31/2022 22:34:39 - INFO - __main__ - Step 130 Global step 130 Train loss 2.366144 on epoch=32
05/31/2022 22:34:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.575797 on epoch=34
05/31/2022 22:34:49 - INFO - __main__ - Step 150 Global step 150 Train loss 1.094500 on epoch=37
05/31/2022 22:34:50 - INFO - __main__ - Global step 150 Train loss 4.365819 Classification-F1 0.6133438402941949 on epoch=37
05/31/2022 22:34:56 - INFO - __main__ - Step 160 Global step 160 Train loss 1.084547 on epoch=39
05/31/2022 22:35:01 - INFO - __main__ - Step 170 Global step 170 Train loss 1.359175 on epoch=42
05/31/2022 22:35:06 - INFO - __main__ - Step 180 Global step 180 Train loss 1.000781 on epoch=44
05/31/2022 22:35:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.559795 on epoch=47
05/31/2022 22:35:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.399722 on epoch=49
05/31/2022 22:35:18 - INFO - __main__ - Global step 200 Train loss 0.880804 Classification-F1 0.812316715542522 on epoch=49
05/31/2022 22:35:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.299252 on epoch=52
05/31/2022 22:35:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.215695 on epoch=54
05/31/2022 22:35:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.143132 on epoch=57
05/31/2022 22:35:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.230457 on epoch=59
05/31/2022 22:35:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.182287 on epoch=62
05/31/2022 22:35:46 - INFO - __main__ - Global step 250 Train loss 0.214164 Classification-F1 0.8590653290922436 on epoch=62
05/31/2022 22:35:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.088705 on epoch=64
05/31/2022 22:35:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.076737 on epoch=67
05/31/2022 22:36:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.079572 on epoch=69
05/31/2022 22:36:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.058840 on epoch=72
05/31/2022 22:36:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.055199 on epoch=74
05/31/2022 22:36:15 - INFO - __main__ - Global step 300 Train loss 0.071811 Classification-F1 0.8905982905982905 on epoch=74
05/31/2022 22:36:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.037633 on epoch=77
05/31/2022 22:36:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.043268 on epoch=79
05/31/2022 22:36:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.029499 on epoch=82
05/31/2022 22:36:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.012532 on epoch=84
05/31/2022 22:36:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.010193 on epoch=87
05/31/2022 22:36:44 - INFO - __main__ - Global step 350 Train loss 0.026625 Classification-F1 0.8905982905982905 on epoch=87
05/31/2022 22:36:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.023551 on epoch=89
05/31/2022 22:36:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.016082 on epoch=92
05/31/2022 22:37:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.008448 on epoch=94
05/31/2022 22:37:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.002798 on epoch=97
05/31/2022 22:37:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.007112 on epoch=99
05/31/2022 22:37:12 - INFO - __main__ - Global step 400 Train loss 0.011598 Classification-F1 0.8905982905982905 on epoch=99
05/31/2022 22:37:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.011807 on epoch=102
05/31/2022 22:37:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.022399 on epoch=104
05/31/2022 22:37:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.024242 on epoch=107
05/31/2022 22:37:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.005109 on epoch=109
05/31/2022 22:37:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.007695 on epoch=112
05/31/2022 22:37:40 - INFO - __main__ - Global step 450 Train loss 0.014250 Classification-F1 0.8593406593406593 on epoch=112
05/31/2022 22:37:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.013107 on epoch=114
05/31/2022 22:37:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003667 on epoch=117
05/31/2022 22:37:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.019418 on epoch=119
05/31/2022 22:38:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.041954 on epoch=122
05/31/2022 22:38:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.011010 on epoch=124
05/31/2022 22:38:07 - INFO - __main__ - Global step 500 Train loss 0.017831 Classification-F1 0.8748778103616813 on epoch=124
05/31/2022 22:38:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.006913 on epoch=127
05/31/2022 22:38:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.013791 on epoch=129
05/31/2022 22:38:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.014328 on epoch=132
05/31/2022 22:38:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.009763 on epoch=134
05/31/2022 22:38:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.022752 on epoch=137
05/31/2022 22:38:35 - INFO - __main__ - Global step 550 Train loss 0.013510 Classification-F1 0.84375 on epoch=137
05/31/2022 22:38:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.008866 on epoch=139
05/31/2022 22:38:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.005024 on epoch=142
05/31/2022 22:38:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000373 on epoch=144
05/31/2022 22:38:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000667 on epoch=147
05/31/2022 22:39:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003806 on epoch=149
05/31/2022 22:39:03 - INFO - __main__ - Global step 600 Train loss 0.003747 Classification-F1 0.8748778103616813 on epoch=149
05/31/2022 22:39:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.004600 on epoch=152
05/31/2022 22:39:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000525 on epoch=154
05/31/2022 22:39:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.011551 on epoch=157
05/31/2022 22:39:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000606 on epoch=159
05/31/2022 22:39:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.002614 on epoch=162
05/31/2022 22:39:31 - INFO - __main__ - Global step 650 Train loss 0.003979 Classification-F1 0.9058823529411765 on epoch=162
05/31/2022 22:39:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000898 on epoch=164
05/31/2022 22:39:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000755 on epoch=167
05/31/2022 22:39:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.004787 on epoch=169
05/31/2022 22:39:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.007275 on epoch=172
05/31/2022 22:39:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.002046 on epoch=174
05/31/2022 22:39:59 - INFO - __main__ - Global step 700 Train loss 0.003152 Classification-F1 0.8903841448495229 on epoch=174
05/31/2022 22:40:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000370 on epoch=177
05/31/2022 22:40:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000362 on epoch=179
05/31/2022 22:40:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000233 on epoch=182
05/31/2022 22:40:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.005575 on epoch=184
05/31/2022 22:40:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000626 on epoch=187
05/31/2022 22:40:27 - INFO - __main__ - Global step 750 Train loss 0.001433 Classification-F1 0.8903841448495229 on epoch=187
05/31/2022 22:40:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001952 on epoch=189
05/31/2022 22:40:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.009140 on epoch=192
05/31/2022 22:40:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.024446 on epoch=194
05/31/2022 22:40:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000334 on epoch=197
05/31/2022 22:40:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002524 on epoch=199
05/31/2022 22:40:55 - INFO - __main__ - Global step 800 Train loss 0.007679 Classification-F1 0.8748778103616813 on epoch=199
05/31/2022 22:41:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.008375 on epoch=202
05/31/2022 22:41:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.004042 on epoch=204
05/31/2022 22:41:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.028177 on epoch=207
05/31/2022 22:41:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.007445 on epoch=209
05/31/2022 22:41:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000914 on epoch=212
05/31/2022 22:41:23 - INFO - __main__ - Global step 850 Train loss 0.009791 Classification-F1 0.8905982905982905 on epoch=212
05/31/2022 22:41:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000868 on epoch=214
05/31/2022 22:41:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.006343 on epoch=217
05/31/2022 22:41:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.100058 on epoch=219
05/31/2022 22:41:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.003678 on epoch=222
05/31/2022 22:41:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000121 on epoch=224
05/31/2022 22:41:51 - INFO - __main__ - Global step 900 Train loss 0.022214 Classification-F1 0.8903841448495229 on epoch=224
05/31/2022 22:41:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001253 on epoch=227
05/31/2022 22:42:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000580 on epoch=229
05/31/2022 22:42:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000033 on epoch=232
05/31/2022 22:42:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000222 on epoch=234
05/31/2022 22:42:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.015440 on epoch=237
05/31/2022 22:42:18 - INFO - __main__ - Global step 950 Train loss 0.003505 Classification-F1 0.906158357771261 on epoch=237
05/31/2022 22:42:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000633 on epoch=239
05/31/2022 22:42:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000384 on epoch=242
05/31/2022 22:42:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000030 on epoch=244
05/31/2022 22:42:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000038 on epoch=247
05/31/2022 22:42:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000050 on epoch=249
05/31/2022 22:42:47 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:42:47 - INFO - __main__ - Printing 3 examples
05/31/2022 22:42:47 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/31/2022 22:42:47 - INFO - __main__ - ['negative']
05/31/2022 22:42:47 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/31/2022 22:42:47 - INFO - __main__ - ['negative']
05/31/2022 22:42:47 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/31/2022 22:42:47 - INFO - __main__ - ['negative']
05/31/2022 22:42:47 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:42:47 - INFO - __main__ - Global step 1000 Train loss 0.000227 Classification-F1 0.90625 on epoch=249
05/31/2022 22:42:47 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:42:47 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:42:47 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:42:47 - INFO - __main__ - Printing 3 examples
05/31/2022 22:42:47 - INFO - __main__ -  [amazon_polarity] title: Future of Metal, What a joke [SEP] content: These guys are terrible, theyre decent musicians but that doesnt make them a good bandthey are pretty much a second rate metallica clone with some of the worst vocals evermetallcore is becoming the new, nu-metal
05/31/2022 22:42:47 - INFO - __main__ - ['negative']
05/31/2022 22:42:47 - INFO - __main__ -  [amazon_polarity] title: Each book just gets worse... [SEP] content: I remember the "good old days" when picking up a new Clancy book was an event to look forward to. But now I find myself dreading the first few pages on any new Clancy volume, especially after the disaster that was Rainbow Six.It seems that as the threat of the cold war recedes, Tom finds it more difficult to develop interesting storylines which are relevant (and realistic). It was always Tom's portrayal of the battle or espionage, and the realpolitik which motivates governments and the military alike, which captured my imagination.I can't maintain interest in a thousand page book if it reads like a soap opera!
05/31/2022 22:42:47 - INFO - __main__ - ['negative']
05/31/2022 22:42:47 - INFO - __main__ -  [amazon_polarity] title: Huge letdown [SEP] content: I can't understand all of the good reviews for this film, which I found to be a huge disappointment. Bad performances, coupled with awful direction and hackneyed, cliched writing puts this on my list of films to avoid. The film obviously tried to capitalize on Muniz's rising star power among youngsters and families, but offered little substance for this film to endure. There is never an indication at what makes this dog so special. And it' hard to believe that only two dogs were used for Skip; his appearance changes quite frequently throughout the film.If you want a recent "uplifting" family film, opt for "The Sandlot."
05/31/2022 22:42:47 - INFO - __main__ - ['negative']
05/31/2022 22:42:47 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:42:47 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:42:47 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:42:47 - INFO - __main__ - save last model!
05/31/2022 22:42:54 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 22:42:55 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 22:42:55 - INFO - __main__ - Printing 3 examples
05/31/2022 22:42:55 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 22:42:55 - INFO - __main__ - ['negative']
05/31/2022 22:42:55 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 22:42:55 - INFO - __main__ - ['negative']
05/31/2022 22:42:55 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 22:42:55 - INFO - __main__ - ['negative']
05/31/2022 22:42:55 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:42:55 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:42:57 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 22:42:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:43:00 - INFO - __main__ - Starting training!
05/31/2022 22:43:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_13_0.0002_8_predictions.txt
05/31/2022 22:43:12 - INFO - __main__ - Classification-F1 on test data: 0.9389
05/31/2022 22:43:12 - INFO - __main__ - prefix=amazon_polarity_32_13, lr=0.0002, bsz=8, dev_performance=0.90625, test_performance=0.9389334985799536
05/31/2022 22:43:12 - INFO - __main__ - Running ... prefix=amazon_polarity_32_13, lr=0.0001, bsz=8 ...
05/31/2022 22:43:13 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:43:13 - INFO - __main__ - Printing 3 examples
05/31/2022 22:43:13 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/31/2022 22:43:13 - INFO - __main__ - ['negative']
05/31/2022 22:43:13 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/31/2022 22:43:13 - INFO - __main__ - ['negative']
05/31/2022 22:43:13 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/31/2022 22:43:13 - INFO - __main__ - ['negative']
05/31/2022 22:43:13 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:43:13 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:43:13 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:43:13 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:43:13 - INFO - __main__ - Printing 3 examples
05/31/2022 22:43:13 - INFO - __main__ -  [amazon_polarity] title: Future of Metal, What a joke [SEP] content: These guys are terrible, theyre decent musicians but that doesnt make them a good bandthey are pretty much a second rate metallica clone with some of the worst vocals evermetallcore is becoming the new, nu-metal
05/31/2022 22:43:13 - INFO - __main__ - ['negative']
05/31/2022 22:43:13 - INFO - __main__ -  [amazon_polarity] title: Each book just gets worse... [SEP] content: I remember the "good old days" when picking up a new Clancy book was an event to look forward to. But now I find myself dreading the first few pages on any new Clancy volume, especially after the disaster that was Rainbow Six.It seems that as the threat of the cold war recedes, Tom finds it more difficult to develop interesting storylines which are relevant (and realistic). It was always Tom's portrayal of the battle or espionage, and the realpolitik which motivates governments and the military alike, which captured my imagination.I can't maintain interest in a thousand page book if it reads like a soap opera!
05/31/2022 22:43:13 - INFO - __main__ - ['negative']
05/31/2022 22:43:13 - INFO - __main__ -  [amazon_polarity] title: Huge letdown [SEP] content: I can't understand all of the good reviews for this film, which I found to be a huge disappointment. Bad performances, coupled with awful direction and hackneyed, cliched writing puts this on my list of films to avoid. The film obviously tried to capitalize on Muniz's rising star power among youngsters and families, but offered little substance for this film to endure. There is never an indication at what makes this dog so special. And it' hard to believe that only two dogs were used for Skip; his appearance changes quite frequently throughout the film.If you want a recent "uplifting" family film, opt for "The Sandlot."
05/31/2022 22:43:13 - INFO - __main__ - ['negative']
05/31/2022 22:43:13 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:43:13 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:43:13 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:43:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:43:24 - INFO - __main__ - Starting training!
05/31/2022 22:43:29 - INFO - __main__ - Step 10 Global step 10 Train loss 23.622059 on epoch=2
05/31/2022 22:43:34 - INFO - __main__ - Step 20 Global step 20 Train loss 21.569790 on epoch=4
05/31/2022 22:43:39 - INFO - __main__ - Step 30 Global step 30 Train loss 18.874983 on epoch=7
05/31/2022 22:43:44 - INFO - __main__ - Step 40 Global step 40 Train loss 17.347387 on epoch=9
05/31/2022 22:43:50 - INFO - __main__ - Step 50 Global step 50 Train loss 16.730312 on epoch=12
05/31/2022 22:44:11 - INFO - __main__ - Global step 50 Train loss 19.628908 Classification-F1 0.0 on epoch=12
05/31/2022 22:44:17 - INFO - __main__ - Step 60 Global step 60 Train loss 16.611340 on epoch=14
05/31/2022 22:44:22 - INFO - __main__ - Step 70 Global step 70 Train loss 16.471167 on epoch=17
05/31/2022 22:44:27 - INFO - __main__ - Step 80 Global step 80 Train loss 15.625036 on epoch=19
05/31/2022 22:44:33 - INFO - __main__ - Step 90 Global step 90 Train loss 15.519239 on epoch=22
05/31/2022 22:44:38 - INFO - __main__ - Step 100 Global step 100 Train loss 14.855530 on epoch=24
05/31/2022 22:44:53 - INFO - __main__ - Global step 100 Train loss 15.816462 Classification-F1 0.0 on epoch=24
05/31/2022 22:44:58 - INFO - __main__ - Step 110 Global step 110 Train loss 14.217395 on epoch=27
05/31/2022 22:45:03 - INFO - __main__ - Step 120 Global step 120 Train loss 14.394798 on epoch=29
05/31/2022 22:45:09 - INFO - __main__ - Step 130 Global step 130 Train loss 13.477419 on epoch=32
05/31/2022 22:45:14 - INFO - __main__ - Step 140 Global step 140 Train loss 13.466832 on epoch=34
05/31/2022 22:45:19 - INFO - __main__ - Step 150 Global step 150 Train loss 12.883092 on epoch=37
05/31/2022 22:45:28 - INFO - __main__ - Global step 150 Train loss 13.687906 Classification-F1 0.0 on epoch=37
05/31/2022 22:45:33 - INFO - __main__ - Step 160 Global step 160 Train loss 12.253455 on epoch=39
05/31/2022 22:45:38 - INFO - __main__ - Step 170 Global step 170 Train loss 12.878044 on epoch=42
05/31/2022 22:45:43 - INFO - __main__ - Step 180 Global step 180 Train loss 11.567064 on epoch=44
05/31/2022 22:45:49 - INFO - __main__ - Step 190 Global step 190 Train loss 11.604012 on epoch=47
05/31/2022 22:45:54 - INFO - __main__ - Step 200 Global step 200 Train loss 10.068304 on epoch=49
05/31/2022 22:45:59 - INFO - __main__ - Global step 200 Train loss 11.674176 Classification-F1 0.0 on epoch=49
05/31/2022 22:46:04 - INFO - __main__ - Step 210 Global step 210 Train loss 10.702200 on epoch=52
05/31/2022 22:46:09 - INFO - __main__ - Step 220 Global step 220 Train loss 6.078322 on epoch=54
05/31/2022 22:46:14 - INFO - __main__ - Step 230 Global step 230 Train loss 5.355931 on epoch=57
05/31/2022 22:46:19 - INFO - __main__ - Step 240 Global step 240 Train loss 7.518819 on epoch=59
05/31/2022 22:46:25 - INFO - __main__ - Step 250 Global step 250 Train loss 6.816049 on epoch=62
05/31/2022 22:46:31 - INFO - __main__ - Global step 250 Train loss 7.294264 Classification-F1 0.054824561403508776 on epoch=62
05/31/2022 22:46:38 - INFO - __main__ - Step 260 Global step 260 Train loss 6.112195 on epoch=64
05/31/2022 22:46:43 - INFO - __main__ - Step 270 Global step 270 Train loss 3.536820 on epoch=67
05/31/2022 22:46:48 - INFO - __main__ - Step 280 Global step 280 Train loss 1.647835 on epoch=69
05/31/2022 22:46:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.582154 on epoch=72
05/31/2022 22:46:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.616118 on epoch=74
05/31/2022 22:47:00 - INFO - __main__ - Global step 300 Train loss 2.499024 Classification-F1 0.7608966376089663 on epoch=74
05/31/2022 22:47:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.504853 on epoch=77
05/31/2022 22:47:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.547012 on epoch=79
05/31/2022 22:47:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.591161 on epoch=82
05/31/2022 22:47:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.703687 on epoch=84
05/31/2022 22:47:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.612579 on epoch=87
05/31/2022 22:47:29 - INFO - __main__ - Global step 350 Train loss 0.591858 Classification-F1 0.7964276975776854 on epoch=87
05/31/2022 22:47:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.418371 on epoch=89
05/31/2022 22:47:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.497185 on epoch=92
05/31/2022 22:47:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.544150 on epoch=94
05/31/2022 22:47:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.397636 on epoch=97
05/31/2022 22:47:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.435416 on epoch=99
05/31/2022 22:47:58 - INFO - __main__ - Global step 400 Train loss 0.458552 Classification-F1 0.7262893081761006 on epoch=99
05/31/2022 22:48:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.399674 on epoch=102
05/31/2022 22:48:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.397527 on epoch=104
05/31/2022 22:48:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.365442 on epoch=107
05/31/2022 22:48:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.387501 on epoch=109
05/31/2022 22:48:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.429287 on epoch=112
05/31/2022 22:48:26 - INFO - __main__ - Global step 450 Train loss 0.395886 Classification-F1 0.6389743589743591 on epoch=112
05/31/2022 22:48:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.364457 on epoch=114
05/31/2022 22:48:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.380229 on epoch=117
05/31/2022 22:48:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.471849 on epoch=119
05/31/2022 22:48:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.335545 on epoch=122
05/31/2022 22:48:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.343690 on epoch=124
05/31/2022 22:48:54 - INFO - __main__ - Global step 500 Train loss 0.379154 Classification-F1 0.7117117117117117 on epoch=124
05/31/2022 22:48:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.453627 on epoch=127
05/31/2022 22:49:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.357434 on epoch=129
05/31/2022 22:49:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.398540 on epoch=132
05/31/2022 22:49:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.388770 on epoch=134
05/31/2022 22:49:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.327621 on epoch=137
05/31/2022 22:49:22 - INFO - __main__ - Global step 550 Train loss 0.385198 Classification-F1 0.7608966376089663 on epoch=137
05/31/2022 22:49:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.335681 on epoch=139
05/31/2022 22:49:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.346579 on epoch=142
05/31/2022 22:49:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.330071 on epoch=144
05/31/2022 22:49:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.366447 on epoch=147
05/31/2022 22:49:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.334330 on epoch=149
05/31/2022 22:49:50 - INFO - __main__ - Global step 600 Train loss 0.342622 Classification-F1 0.8593406593406593 on epoch=149
05/31/2022 22:49:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.302959 on epoch=152
05/31/2022 22:50:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.323690 on epoch=154
05/31/2022 22:50:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.310381 on epoch=157
05/31/2022 22:50:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.352268 on epoch=159
05/31/2022 22:50:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.329967 on epoch=162
05/31/2022 22:50:18 - INFO - __main__ - Global step 650 Train loss 0.323853 Classification-F1 0.7777777777777777 on epoch=162
05/31/2022 22:50:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.334513 on epoch=164
05/31/2022 22:50:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.345967 on epoch=167
05/31/2022 22:50:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.339331 on epoch=169
05/31/2022 22:50:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.289055 on epoch=172
05/31/2022 22:50:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.441138 on epoch=174
05/31/2022 22:50:46 - INFO - __main__ - Global step 700 Train loss 0.350001 Classification-F1 0.8435972629521017 on epoch=174
05/31/2022 22:50:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.300504 on epoch=177
05/31/2022 22:50:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.309417 on epoch=179
05/31/2022 22:51:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.325833 on epoch=182
05/31/2022 22:51:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.332171 on epoch=184
05/31/2022 22:51:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.319031 on epoch=187
05/31/2022 22:51:14 - INFO - __main__ - Global step 750 Train loss 0.317391 Classification-F1 0.8905982905982905 on epoch=187
05/31/2022 22:51:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.306612 on epoch=189
05/31/2022 22:51:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.304474 on epoch=192
05/31/2022 22:51:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.468851 on epoch=194
05/31/2022 22:51:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.399666 on epoch=197
05/31/2022 22:51:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.332197 on epoch=199
05/31/2022 22:51:43 - INFO - __main__ - Global step 800 Train loss 0.362360 Classification-F1 0.90625 on epoch=199
05/31/2022 22:51:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.302998 on epoch=202
05/31/2022 22:51:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.303692 on epoch=204
05/31/2022 22:52:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.302506 on epoch=207
05/31/2022 22:52:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.352743 on epoch=209
05/31/2022 22:52:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.307786 on epoch=212
05/31/2022 22:52:12 - INFO - __main__ - Global step 850 Train loss 0.313945 Classification-F1 0.7408906882591093 on epoch=212
05/31/2022 22:52:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.314784 on epoch=214
05/31/2022 22:52:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.310607 on epoch=217
05/31/2022 22:52:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.314276 on epoch=219
05/31/2022 22:52:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.291355 on epoch=222
05/31/2022 22:52:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.314407 on epoch=224
05/31/2022 22:52:40 - INFO - __main__ - Global step 900 Train loss 0.309086 Classification-F1 0.7964276975776854 on epoch=224
05/31/2022 22:52:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.306083 on epoch=227
05/31/2022 22:52:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.313926 on epoch=229
05/31/2022 22:52:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.330433 on epoch=232
05/31/2022 22:53:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.347857 on epoch=234
05/31/2022 22:53:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.295481 on epoch=237
05/31/2022 22:53:08 - INFO - __main__ - Global step 950 Train loss 0.318756 Classification-F1 0.7906918238993711 on epoch=237
05/31/2022 22:53:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.279110 on epoch=239
05/31/2022 22:53:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.286680 on epoch=242
05/31/2022 22:53:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.326696 on epoch=244
05/31/2022 22:53:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.283335 on epoch=247
05/31/2022 22:53:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.300126 on epoch=249
05/31/2022 22:53:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:53:36 - INFO - __main__ - Printing 3 examples
05/31/2022 22:53:36 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/31/2022 22:53:36 - INFO - __main__ - ['positive']
05/31/2022 22:53:36 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/31/2022 22:53:36 - INFO - __main__ - ['positive']
05/31/2022 22:53:36 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/31/2022 22:53:36 - INFO - __main__ - ['positive']
05/31/2022 22:53:36 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:53:36 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:53:36 - INFO - __main__ - Global step 1000 Train loss 0.295189 Classification-F1 0.7176470588235293 on epoch=249
05/31/2022 22:53:36 - INFO - __main__ - save last model!
05/31/2022 22:53:36 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:53:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:53:36 - INFO - __main__ - Printing 3 examples
05/31/2022 22:53:36 - INFO - __main__ -  [amazon_polarity] title: good brother, Bad brother [SEP] content: this was a very interesting read . I learned many things reading this book. I have suggested others should read it.
05/31/2022 22:53:36 - INFO - __main__ - ['positive']
05/31/2022 22:53:36 - INFO - __main__ -  [amazon_polarity] title: ABSOLUTELY BRILLIANT! [SEP] content: This is not only one of the best punk songs ever recorded, it's one of the best songs period. This song is as close to perfection as you can get, both lyrically and musically. You would think that an 18 minute song would be boring at times, but not this one. The changes in tempo and the scathing political commentary keep your interest the entire time. This is a masterpiece.
05/31/2022 22:53:36 - INFO - __main__ - ['positive']
05/31/2022 22:53:36 - INFO - __main__ -  [amazon_polarity] title: Very useful book [SEP] content: Book was recommended for use by Interpreter Training program teacher as an additional tool. I have gotten considerable use out of it over the past year in my transliterating and asl classes. Only one drawback just recently noted. The DVD is not supported by the newer MAC OS Lion systems so I'm forced to use it on a Windows based computer. Other than that, very satisfied with this book.
05/31/2022 22:53:36 - INFO - __main__ - ['positive']
05/31/2022 22:53:36 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:53:36 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:53:36 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:53:43 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 22:53:43 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 22:53:43 - INFO - __main__ - Printing 3 examples
05/31/2022 22:53:43 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 22:53:43 - INFO - __main__ - ['negative']
05/31/2022 22:53:43 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 22:53:43 - INFO - __main__ - ['negative']
05/31/2022 22:53:43 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 22:53:43 - INFO - __main__ - ['negative']
05/31/2022 22:53:43 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:53:44 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:53:45 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 22:53:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:53:47 - INFO - __main__ - Starting training!
05/31/2022 22:54:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_13_0.0001_8_predictions.txt
05/31/2022 22:54:00 - INFO - __main__ - Classification-F1 on test data: 0.9078
05/31/2022 22:54:00 - INFO - __main__ - prefix=amazon_polarity_32_13, lr=0.0001, bsz=8, dev_performance=0.90625, test_performance=0.9077694235588971
05/31/2022 22:54:00 - INFO - __main__ - Running ... prefix=amazon_polarity_32_21, lr=0.0005, bsz=8 ...
05/31/2022 22:54:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:54:01 - INFO - __main__ - Printing 3 examples
05/31/2022 22:54:01 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/31/2022 22:54:01 - INFO - __main__ - ['positive']
05/31/2022 22:54:01 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/31/2022 22:54:01 - INFO - __main__ - ['positive']
05/31/2022 22:54:01 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/31/2022 22:54:01 - INFO - __main__ - ['positive']
05/31/2022 22:54:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:54:01 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:54:01 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 22:54:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 22:54:01 - INFO - __main__ - Printing 3 examples
05/31/2022 22:54:01 - INFO - __main__ -  [amazon_polarity] title: good brother, Bad brother [SEP] content: this was a very interesting read . I learned many things reading this book. I have suggested others should read it.
05/31/2022 22:54:01 - INFO - __main__ - ['positive']
05/31/2022 22:54:01 - INFO - __main__ -  [amazon_polarity] title: ABSOLUTELY BRILLIANT! [SEP] content: This is not only one of the best punk songs ever recorded, it's one of the best songs period. This song is as close to perfection as you can get, both lyrically and musically. You would think that an 18 minute song would be boring at times, but not this one. The changes in tempo and the scathing political commentary keep your interest the entire time. This is a masterpiece.
05/31/2022 22:54:01 - INFO - __main__ - ['positive']
05/31/2022 22:54:01 - INFO - __main__ -  [amazon_polarity] title: Very useful book [SEP] content: Book was recommended for use by Interpreter Training program teacher as an additional tool. I have gotten considerable use out of it over the past year in my transliterating and asl classes. Only one drawback just recently noted. The DVD is not supported by the newer MAC OS Lion systems so I'm forced to use it on a Windows based computer. Other than that, very satisfied with this book.
05/31/2022 22:54:01 - INFO - __main__ - ['positive']
05/31/2022 22:54:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 22:54:01 - INFO - __main__ - Tokenizing Output ...
05/31/2022 22:54:01 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 22:54:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 22:54:12 - INFO - __main__ - Starting training!
05/31/2022 22:54:17 - INFO - __main__ - Step 10 Global step 10 Train loss 21.197229 on epoch=2
05/31/2022 22:54:22 - INFO - __main__ - Step 20 Global step 20 Train loss 16.113688 on epoch=4
05/31/2022 22:54:27 - INFO - __main__ - Step 30 Global step 30 Train loss 13.876165 on epoch=7
05/31/2022 22:54:33 - INFO - __main__ - Step 40 Global step 40 Train loss 11.813249 on epoch=9
05/31/2022 22:54:38 - INFO - __main__ - Step 50 Global step 50 Train loss 9.432220 on epoch=12
05/31/2022 22:54:40 - INFO - __main__ - Global step 50 Train loss 14.486510 Classification-F1 0.09392712550607288 on epoch=12
05/31/2022 22:54:46 - INFO - __main__ - Step 60 Global step 60 Train loss 6.137989 on epoch=14
05/31/2022 22:54:50 - INFO - __main__ - Step 70 Global step 70 Train loss 4.475721 on epoch=17
05/31/2022 22:54:56 - INFO - __main__ - Step 80 Global step 80 Train loss 1.079877 on epoch=19
05/31/2022 22:55:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.513802 on epoch=22
05/31/2022 22:55:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.383511 on epoch=24
05/31/2022 22:55:07 - INFO - __main__ - Global step 100 Train loss 2.518180 Classification-F1 0.5636363636363637 on epoch=24
05/31/2022 22:55:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.352367 on epoch=27
05/31/2022 22:55:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.212843 on epoch=29
05/31/2022 22:55:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.152990 on epoch=32
05/31/2022 22:55:29 - INFO - __main__ - Step 140 Global step 140 Train loss 0.197426 on epoch=34
05/31/2022 22:55:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.083180 on epoch=37
05/31/2022 22:55:35 - INFO - __main__ - Global step 150 Train loss 0.199761 Classification-F1 0.9843711843711844 on epoch=37
05/31/2022 22:55:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.087516 on epoch=39
05/31/2022 22:55:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.069697 on epoch=42
05/31/2022 22:55:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.145163 on epoch=44
05/31/2022 22:55:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.023476 on epoch=47
05/31/2022 22:56:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.014910 on epoch=49
05/31/2022 22:56:03 - INFO - __main__ - Global step 200 Train loss 0.068153 Classification-F1 0.9531135531135531 on epoch=49
05/31/2022 22:56:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.042662 on epoch=52
05/31/2022 22:56:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.045473 on epoch=54
05/31/2022 22:56:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.020960 on epoch=57
05/31/2022 22:56:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.016768 on epoch=59
05/31/2022 22:56:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.019828 on epoch=62
05/31/2022 22:56:30 - INFO - __main__ - Global step 250 Train loss 0.029138 Classification-F1 0.9687194525904204 on epoch=62
05/31/2022 22:56:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.007265 on epoch=64
05/31/2022 22:56:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.017129 on epoch=67
05/31/2022 22:56:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.038296 on epoch=69
05/31/2022 22:56:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002059 on epoch=72
05/31/2022 22:56:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.001490 on epoch=74
05/31/2022 22:56:57 - INFO - __main__ - Global step 300 Train loss 0.013248 Classification-F1 0.9843711843711844 on epoch=74
05/31/2022 22:57:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000626 on epoch=77
05/31/2022 22:57:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000372 on epoch=79
05/31/2022 22:57:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.001218 on epoch=82
05/31/2022 22:57:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000920 on epoch=84
05/31/2022 22:57:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.005433 on epoch=87
05/31/2022 22:57:25 - INFO - __main__ - Global step 350 Train loss 0.001714 Classification-F1 0.9843711843711844 on epoch=87
05/31/2022 22:57:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.011832 on epoch=89
05/31/2022 22:57:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.003816 on epoch=92
05/31/2022 22:57:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000207 on epoch=94
05/31/2022 22:57:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000786 on epoch=97
05/31/2022 22:57:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000176 on epoch=99
05/31/2022 22:57:52 - INFO - __main__ - Global step 400 Train loss 0.003363 Classification-F1 0.9843711843711844 on epoch=99
05/31/2022 22:57:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.016274 on epoch=102
05/31/2022 22:58:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000109 on epoch=104
05/31/2022 22:58:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000089 on epoch=107
05/31/2022 22:58:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000161 on epoch=109
05/31/2022 22:58:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000152 on epoch=112
05/31/2022 22:58:19 - INFO - __main__ - Global step 450 Train loss 0.003357 Classification-F1 0.9843711843711844 on epoch=112
05/31/2022 22:58:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000212 on epoch=114
05/31/2022 22:58:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000183 on epoch=117
05/31/2022 22:58:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000266 on epoch=119
05/31/2022 22:58:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000075 on epoch=122
05/31/2022 22:58:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000041 on epoch=124
05/31/2022 22:58:46 - INFO - __main__ - Global step 500 Train loss 0.000155 Classification-F1 0.9843711843711844 on epoch=124
05/31/2022 22:58:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000087 on epoch=127
05/31/2022 22:58:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000055 on epoch=129
05/31/2022 22:59:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000062 on epoch=132
05/31/2022 22:59:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000036 on epoch=134
05/31/2022 22:59:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000190 on epoch=137
05/31/2022 22:59:14 - INFO - __main__ - Global step 550 Train loss 0.000086 Classification-F1 0.9843711843711844 on epoch=137
05/31/2022 22:59:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000019 on epoch=139
05/31/2022 22:59:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000064 on epoch=142
05/31/2022 22:59:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000010 on epoch=144
05/31/2022 22:59:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.021802 on epoch=147
05/31/2022 22:59:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.017881 on epoch=149
05/31/2022 22:59:41 - INFO - __main__ - Global step 600 Train loss 0.007955 Classification-F1 0.9843711843711844 on epoch=149
05/31/2022 22:59:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000232 on epoch=152
05/31/2022 22:59:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.005137 on epoch=154
05/31/2022 22:59:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000180 on epoch=157
05/31/2022 23:00:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000520 on epoch=159
05/31/2022 23:00:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.012877 on epoch=162
05/31/2022 23:00:08 - INFO - __main__ - Global step 650 Train loss 0.003789 Classification-F1 0.9530217763640813 on epoch=162
05/31/2022 23:00:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000164 on epoch=164
05/31/2022 23:00:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001964 on epoch=167
05/31/2022 23:00:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000499 on epoch=169
05/31/2022 23:00:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.003679 on epoch=172
05/31/2022 23:00:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000251 on epoch=174
05/31/2022 23:00:36 - INFO - __main__ - Global step 700 Train loss 0.001311 Classification-F1 0.9843711843711844 on epoch=174
05/31/2022 23:00:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000133 on epoch=177
05/31/2022 23:00:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000117 on epoch=179
05/31/2022 23:00:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000745 on epoch=182
05/31/2022 23:00:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000032 on epoch=184
05/31/2022 23:01:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000099 on epoch=187
05/31/2022 23:01:03 - INFO - __main__ - Global step 750 Train loss 0.000225 Classification-F1 0.9687194525904204 on epoch=187
05/31/2022 23:01:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000037 on epoch=189
05/31/2022 23:01:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000035 on epoch=192
05/31/2022 23:01:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000067 on epoch=194
05/31/2022 23:01:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000024 on epoch=197
05/31/2022 23:01:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000113 on epoch=199
05/31/2022 23:01:30 - INFO - __main__ - Global step 800 Train loss 0.000055 Classification-F1 0.9530217763640813 on epoch=199
05/31/2022 23:01:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000135 on epoch=202
05/31/2022 23:01:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000018 on epoch=204
05/31/2022 23:01:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000104 on epoch=207
05/31/2022 23:01:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000067 on epoch=209
05/31/2022 23:01:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000031 on epoch=212
05/31/2022 23:01:57 - INFO - __main__ - Global step 850 Train loss 0.000071 Classification-F1 0.9530217763640813 on epoch=212
05/31/2022 23:02:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.024862 on epoch=214
05/31/2022 23:02:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.049538 on epoch=217
05/31/2022 23:02:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000058 on epoch=219
05/31/2022 23:02:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000034 on epoch=222
05/31/2022 23:02:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000032 on epoch=224
05/31/2022 23:02:24 - INFO - __main__ - Global step 900 Train loss 0.014905 Classification-F1 0.9687194525904204 on epoch=224
05/31/2022 23:02:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000061 on epoch=227
05/31/2022 23:02:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000084 on epoch=229
05/31/2022 23:02:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000252 on epoch=232
05/31/2022 23:02:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000025 on epoch=234
05/31/2022 23:02:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000020 on epoch=237
05/31/2022 23:02:51 - INFO - __main__ - Global step 950 Train loss 0.000088 Classification-F1 0.9687194525904204 on epoch=237
05/31/2022 23:02:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000031 on epoch=239
05/31/2022 23:03:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000081 on epoch=242
05/31/2022 23:03:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000152 on epoch=244
05/31/2022 23:03:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000021 on epoch=247
05/31/2022 23:03:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000016 on epoch=249
05/31/2022 23:03:19 - INFO - __main__ - Global step 1000 Train loss 0.000060 Classification-F1 0.9687194525904204 on epoch=249
05/31/2022 23:03:19 - INFO - __main__ - save last model!
05/31/2022 23:03:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:03:19 - INFO - __main__ - Printing 3 examples
05/31/2022 23:03:19 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/31/2022 23:03:19 - INFO - __main__ - ['positive']
05/31/2022 23:03:19 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/31/2022 23:03:19 - INFO - __main__ - ['positive']
05/31/2022 23:03:19 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/31/2022 23:03:19 - INFO - __main__ - ['positive']
05/31/2022 23:03:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:03:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:03:19 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:03:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:03:19 - INFO - __main__ - Printing 3 examples
05/31/2022 23:03:19 - INFO - __main__ -  [amazon_polarity] title: good brother, Bad brother [SEP] content: this was a very interesting read . I learned many things reading this book. I have suggested others should read it.
05/31/2022 23:03:19 - INFO - __main__ - ['positive']
05/31/2022 23:03:19 - INFO - __main__ -  [amazon_polarity] title: ABSOLUTELY BRILLIANT! [SEP] content: This is not only one of the best punk songs ever recorded, it's one of the best songs period. This song is as close to perfection as you can get, both lyrically and musically. You would think that an 18 minute song would be boring at times, but not this one. The changes in tempo and the scathing political commentary keep your interest the entire time. This is a masterpiece.
05/31/2022 23:03:19 - INFO - __main__ - ['positive']
05/31/2022 23:03:19 - INFO - __main__ -  [amazon_polarity] title: Very useful book [SEP] content: Book was recommended for use by Interpreter Training program teacher as an additional tool. I have gotten considerable use out of it over the past year in my transliterating and asl classes. Only one drawback just recently noted. The DVD is not supported by the newer MAC OS Lion systems so I'm forced to use it on a Windows based computer. Other than that, very satisfied with this book.
05/31/2022 23:03:19 - INFO - __main__ - ['positive']
05/31/2022 23:03:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:03:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:03:19 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:03:26 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 23:03:26 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 23:03:26 - INFO - __main__ - Printing 3 examples
05/31/2022 23:03:26 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 23:03:26 - INFO - __main__ - ['negative']
05/31/2022 23:03:26 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 23:03:26 - INFO - __main__ - ['negative']
05/31/2022 23:03:26 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 23:03:26 - INFO - __main__ - ['negative']
05/31/2022 23:03:26 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:03:27 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:03:28 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 23:03:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:03:32 - INFO - __main__ - Starting training!
05/31/2022 23:03:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_21_0.0005_8_predictions.txt
05/31/2022 23:03:44 - INFO - __main__ - Classification-F1 on test data: 0.9330
05/31/2022 23:03:44 - INFO - __main__ - prefix=amazon_polarity_32_21, lr=0.0005, bsz=8, dev_performance=0.9843711843711844, test_performance=0.932998324958124
05/31/2022 23:03:44 - INFO - __main__ - Running ... prefix=amazon_polarity_32_21, lr=0.0003, bsz=8 ...
05/31/2022 23:03:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:03:45 - INFO - __main__ - Printing 3 examples
05/31/2022 23:03:45 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/31/2022 23:03:45 - INFO - __main__ - ['positive']
05/31/2022 23:03:45 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/31/2022 23:03:45 - INFO - __main__ - ['positive']
05/31/2022 23:03:45 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/31/2022 23:03:45 - INFO - __main__ - ['positive']
05/31/2022 23:03:45 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:03:45 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:03:45 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:03:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:03:45 - INFO - __main__ - Printing 3 examples
05/31/2022 23:03:45 - INFO - __main__ -  [amazon_polarity] title: good brother, Bad brother [SEP] content: this was a very interesting read . I learned many things reading this book. I have suggested others should read it.
05/31/2022 23:03:45 - INFO - __main__ - ['positive']
05/31/2022 23:03:45 - INFO - __main__ -  [amazon_polarity] title: ABSOLUTELY BRILLIANT! [SEP] content: This is not only one of the best punk songs ever recorded, it's one of the best songs period. This song is as close to perfection as you can get, both lyrically and musically. You would think that an 18 minute song would be boring at times, but not this one. The changes in tempo and the scathing political commentary keep your interest the entire time. This is a masterpiece.
05/31/2022 23:03:45 - INFO - __main__ - ['positive']
05/31/2022 23:03:45 - INFO - __main__ -  [amazon_polarity] title: Very useful book [SEP] content: Book was recommended for use by Interpreter Training program teacher as an additional tool. I have gotten considerable use out of it over the past year in my transliterating and asl classes. Only one drawback just recently noted. The DVD is not supported by the newer MAC OS Lion systems so I'm forced to use it on a Windows based computer. Other than that, very satisfied with this book.
05/31/2022 23:03:45 - INFO - __main__ - ['positive']
05/31/2022 23:03:45 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:03:45 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:03:45 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:03:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:03:56 - INFO - __main__ - Starting training!
05/31/2022 23:04:00 - INFO - __main__ - Step 10 Global step 10 Train loss 23.340757 on epoch=2
05/31/2022 23:04:06 - INFO - __main__ - Step 20 Global step 20 Train loss 16.387859 on epoch=4
05/31/2022 23:04:11 - INFO - __main__ - Step 30 Global step 30 Train loss 15.844968 on epoch=7
05/31/2022 23:04:17 - INFO - __main__ - Step 40 Global step 40 Train loss 14.309906 on epoch=9
05/31/2022 23:04:22 - INFO - __main__ - Step 50 Global step 50 Train loss 13.200233 on epoch=12
05/31/2022 23:04:29 - INFO - __main__ - Global step 50 Train loss 16.616745 Classification-F1 0.0 on epoch=12
05/31/2022 23:04:35 - INFO - __main__ - Step 60 Global step 60 Train loss 12.098521 on epoch=14
05/31/2022 23:04:40 - INFO - __main__ - Step 70 Global step 70 Train loss 10.479487 on epoch=17
05/31/2022 23:04:45 - INFO - __main__ - Step 80 Global step 80 Train loss 6.105166 on epoch=19
05/31/2022 23:04:51 - INFO - __main__ - Step 90 Global step 90 Train loss 3.135684 on epoch=22
05/31/2022 23:04:56 - INFO - __main__ - Step 100 Global step 100 Train loss 3.023484 on epoch=24
05/31/2022 23:04:57 - INFO - __main__ - Global step 100 Train loss 6.968469 Classification-F1 0.3333333333333333 on epoch=24
05/31/2022 23:05:03 - INFO - __main__ - Step 110 Global step 110 Train loss 2.694242 on epoch=27
05/31/2022 23:05:08 - INFO - __main__ - Step 120 Global step 120 Train loss 1.910629 on epoch=29
05/31/2022 23:05:13 - INFO - __main__ - Step 130 Global step 130 Train loss 1.918813 on epoch=32
05/31/2022 23:05:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.918233 on epoch=34
05/31/2022 23:05:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.444174 on epoch=37
05/31/2022 23:05:25 - INFO - __main__ - Global step 150 Train loss 1.577218 Classification-F1 0.7 on epoch=37
05/31/2022 23:05:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.416158 on epoch=39
05/31/2022 23:05:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.636664 on epoch=42
05/31/2022 23:05:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.846467 on epoch=44
05/31/2022 23:05:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.575730 on epoch=47
05/31/2022 23:05:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.493325 on epoch=49
05/31/2022 23:05:53 - INFO - __main__ - Global step 200 Train loss 0.593669 Classification-F1 0.8903841448495229 on epoch=49
05/31/2022 23:05:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.259014 on epoch=52
05/31/2022 23:06:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.278715 on epoch=54
05/31/2022 23:06:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.181552 on epoch=57
05/31/2022 23:06:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.353949 on epoch=59
05/31/2022 23:06:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.264163 on epoch=62
05/31/2022 23:06:22 - INFO - __main__ - Global step 250 Train loss 0.267479 Classification-F1 0.3671451355661882 on epoch=62
05/31/2022 23:06:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.265598 on epoch=64
05/31/2022 23:06:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.238680 on epoch=67
05/31/2022 23:06:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.196065 on epoch=69
05/31/2022 23:06:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.247720 on epoch=72
05/31/2022 23:06:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.164735 on epoch=74
05/31/2022 23:06:50 - INFO - __main__ - Global step 300 Train loss 0.222560 Classification-F1 0.6825396825396826 on epoch=74
05/31/2022 23:06:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.168231 on epoch=77
05/31/2022 23:07:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.094298 on epoch=79
05/31/2022 23:07:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.090867 on epoch=82
05/31/2022 23:07:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.106287 on epoch=84
05/31/2022 23:07:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.052056 on epoch=87
05/31/2022 23:07:17 - INFO - __main__ - Global step 350 Train loss 0.102348 Classification-F1 0.780392156862745 on epoch=87
05/31/2022 23:07:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.083687 on epoch=89
05/31/2022 23:07:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.060745 on epoch=92
05/31/2022 23:07:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.068820 on epoch=94
05/31/2022 23:07:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.041450 on epoch=97
05/31/2022 23:07:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.018912 on epoch=99
05/31/2022 23:07:45 - INFO - __main__ - Global step 400 Train loss 0.054723 Classification-F1 0.8899533284205354 on epoch=99
05/31/2022 23:07:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.009611 on epoch=102
05/31/2022 23:07:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.021493 on epoch=104
05/31/2022 23:08:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.006192 on epoch=107
05/31/2022 23:08:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.003273 on epoch=109
05/31/2022 23:08:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.003235 on epoch=112
05/31/2022 23:08:13 - INFO - __main__ - Global step 450 Train loss 0.008761 Classification-F1 0.9530217763640813 on epoch=112
05/31/2022 23:08:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001340 on epoch=114
05/31/2022 23:08:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003613 on epoch=117
05/31/2022 23:08:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000734 on epoch=119
05/31/2022 23:08:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002147 on epoch=122
05/31/2022 23:08:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.016336 on epoch=124
05/31/2022 23:08:42 - INFO - __main__ - Global step 500 Train loss 0.004834 Classification-F1 0.9372549019607843 on epoch=124
05/31/2022 23:08:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000773 on epoch=127
05/31/2022 23:08:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000999 on epoch=129
05/31/2022 23:08:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000583 on epoch=132
05/31/2022 23:09:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.041199 on epoch=134
05/31/2022 23:09:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003518 on epoch=137
05/31/2022 23:09:10 - INFO - __main__ - Global step 550 Train loss 0.009414 Classification-F1 0.9687194525904204 on epoch=137
05/31/2022 23:09:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001761 on epoch=139
05/31/2022 23:09:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.010173 on epoch=142
05/31/2022 23:09:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001048 on epoch=144
05/31/2022 23:09:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.020626 on epoch=147
05/31/2022 23:09:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.010811 on epoch=149
05/31/2022 23:09:38 - INFO - __main__ - Global step 600 Train loss 0.008884 Classification-F1 0.9372549019607843 on epoch=149
05/31/2022 23:09:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.021717 on epoch=152
05/31/2022 23:09:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.009531 on epoch=154
05/31/2022 23:09:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000618 on epoch=157
05/31/2022 23:10:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001153 on epoch=159
05/31/2022 23:10:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000671 on epoch=162
05/31/2022 23:10:06 - INFO - __main__ - Global step 650 Train loss 0.006738 Classification-F1 0.921702960606802 on epoch=162
05/31/2022 23:10:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.002505 on epoch=164
05/31/2022 23:10:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000630 on epoch=167
05/31/2022 23:10:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000778 on epoch=169
05/31/2022 23:10:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000225 on epoch=172
05/31/2022 23:10:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000817 on epoch=174
05/31/2022 23:10:33 - INFO - __main__ - Global step 700 Train loss 0.000991 Classification-F1 0.9372549019607843 on epoch=174
05/31/2022 23:10:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001160 on epoch=177
05/31/2022 23:10:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.005138 on epoch=179
05/31/2022 23:10:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.128943 on epoch=182
05/31/2022 23:10:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.007333 on epoch=184
05/31/2022 23:10:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001108 on epoch=187
05/31/2022 23:11:00 - INFO - __main__ - Global step 750 Train loss 0.028736 Classification-F1 0.9530217763640813 on epoch=187
05/31/2022 23:11:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000614 on epoch=189
05/31/2022 23:11:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000906 on epoch=192
05/31/2022 23:11:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000460 on epoch=194
05/31/2022 23:11:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000488 on epoch=197
05/31/2022 23:11:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000250 on epoch=199
05/31/2022 23:11:27 - INFO - __main__ - Global step 800 Train loss 0.000544 Classification-F1 0.9213952345860967 on epoch=199
05/31/2022 23:11:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.002039 on epoch=202
05/31/2022 23:11:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001079 on epoch=204
05/31/2022 23:11:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000487 on epoch=207
05/31/2022 23:11:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.003672 on epoch=209
05/31/2022 23:11:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001338 on epoch=212
05/31/2022 23:11:54 - INFO - __main__ - Global step 850 Train loss 0.001723 Classification-F1 0.9530217763640813 on epoch=212
05/31/2022 23:11:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.001480 on epoch=214
05/31/2022 23:12:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000097 on epoch=217
05/31/2022 23:12:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000658 on epoch=219
05/31/2022 23:12:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001755 on epoch=222
05/31/2022 23:12:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000976 on epoch=224
05/31/2022 23:12:21 - INFO - __main__ - Global step 900 Train loss 0.000993 Classification-F1 0.9372549019607843 on epoch=224
05/31/2022 23:12:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000217 on epoch=227
05/31/2022 23:12:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000420 on epoch=229
05/31/2022 23:12:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000005 on epoch=232
05/31/2022 23:12:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000040 on epoch=234
05/31/2022 23:12:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001366 on epoch=237
05/31/2022 23:12:49 - INFO - __main__ - Global step 950 Train loss 0.000410 Classification-F1 0.9213952345860967 on epoch=237
05/31/2022 23:12:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000010 on epoch=239
05/31/2022 23:12:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000080 on epoch=242
05/31/2022 23:13:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000013 on epoch=244
05/31/2022 23:13:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000086 on epoch=247
05/31/2022 23:13:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000014 on epoch=249
05/31/2022 23:13:16 - INFO - __main__ - Global step 1000 Train loss 0.000041 Classification-F1 0.9372549019607843 on epoch=249
05/31/2022 23:13:16 - INFO - __main__ - save last model!
05/31/2022 23:13:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:13:16 - INFO - __main__ - Printing 3 examples
05/31/2022 23:13:16 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/31/2022 23:13:16 - INFO - __main__ - ['positive']
05/31/2022 23:13:16 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/31/2022 23:13:16 - INFO - __main__ - ['positive']
05/31/2022 23:13:16 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/31/2022 23:13:16 - INFO - __main__ - ['positive']
05/31/2022 23:13:16 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:13:16 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:13:16 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:13:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:13:16 - INFO - __main__ - Printing 3 examples
05/31/2022 23:13:16 - INFO - __main__ -  [amazon_polarity] title: good brother, Bad brother [SEP] content: this was a very interesting read . I learned many things reading this book. I have suggested others should read it.
05/31/2022 23:13:16 - INFO - __main__ - ['positive']
05/31/2022 23:13:16 - INFO - __main__ -  [amazon_polarity] title: ABSOLUTELY BRILLIANT! [SEP] content: This is not only one of the best punk songs ever recorded, it's one of the best songs period. This song is as close to perfection as you can get, both lyrically and musically. You would think that an 18 minute song would be boring at times, but not this one. The changes in tempo and the scathing political commentary keep your interest the entire time. This is a masterpiece.
05/31/2022 23:13:16 - INFO - __main__ - ['positive']
05/31/2022 23:13:16 - INFO - __main__ -  [amazon_polarity] title: Very useful book [SEP] content: Book was recommended for use by Interpreter Training program teacher as an additional tool. I have gotten considerable use out of it over the past year in my transliterating and asl classes. Only one drawback just recently noted. The DVD is not supported by the newer MAC OS Lion systems so I'm forced to use it on a Windows based computer. Other than that, very satisfied with this book.
05/31/2022 23:13:16 - INFO - __main__ - ['positive']
05/31/2022 23:13:16 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:13:16 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:13:16 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:13:23 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 23:13:23 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 23:13:23 - INFO - __main__ - Printing 3 examples
05/31/2022 23:13:23 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 23:13:23 - INFO - __main__ - ['negative']
05/31/2022 23:13:23 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 23:13:23 - INFO - __main__ - ['negative']
05/31/2022 23:13:23 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 23:13:23 - INFO - __main__ - ['negative']
05/31/2022 23:13:23 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:13:24 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:13:25 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 23:13:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:13:29 - INFO - __main__ - Starting training!
05/31/2022 23:13:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_21_0.0003_8_predictions.txt
05/31/2022 23:13:40 - INFO - __main__ - Classification-F1 on test data: 0.9219
05/31/2022 23:13:40 - INFO - __main__ - prefix=amazon_polarity_32_21, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9219097276451578
05/31/2022 23:13:40 - INFO - __main__ - Running ... prefix=amazon_polarity_32_21, lr=0.0002, bsz=8 ...
05/31/2022 23:13:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:13:41 - INFO - __main__ - Printing 3 examples
05/31/2022 23:13:41 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/31/2022 23:13:41 - INFO - __main__ - ['positive']
05/31/2022 23:13:41 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/31/2022 23:13:41 - INFO - __main__ - ['positive']
05/31/2022 23:13:41 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/31/2022 23:13:41 - INFO - __main__ - ['positive']
05/31/2022 23:13:41 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:13:41 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:13:41 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:13:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:13:41 - INFO - __main__ - Printing 3 examples
05/31/2022 23:13:41 - INFO - __main__ -  [amazon_polarity] title: good brother, Bad brother [SEP] content: this was a very interesting read . I learned many things reading this book. I have suggested others should read it.
05/31/2022 23:13:41 - INFO - __main__ - ['positive']
05/31/2022 23:13:41 - INFO - __main__ -  [amazon_polarity] title: ABSOLUTELY BRILLIANT! [SEP] content: This is not only one of the best punk songs ever recorded, it's one of the best songs period. This song is as close to perfection as you can get, both lyrically and musically. You would think that an 18 minute song would be boring at times, but not this one. The changes in tempo and the scathing political commentary keep your interest the entire time. This is a masterpiece.
05/31/2022 23:13:41 - INFO - __main__ - ['positive']
05/31/2022 23:13:41 - INFO - __main__ -  [amazon_polarity] title: Very useful book [SEP] content: Book was recommended for use by Interpreter Training program teacher as an additional tool. I have gotten considerable use out of it over the past year in my transliterating and asl classes. Only one drawback just recently noted. The DVD is not supported by the newer MAC OS Lion systems so I'm forced to use it on a Windows based computer. Other than that, very satisfied with this book.
05/31/2022 23:13:41 - INFO - __main__ - ['positive']
05/31/2022 23:13:41 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:13:41 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:13:42 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:13:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:13:52 - INFO - __main__ - Starting training!
05/31/2022 23:13:57 - INFO - __main__ - Step 10 Global step 10 Train loss 23.186361 on epoch=2
05/31/2022 23:14:02 - INFO - __main__ - Step 20 Global step 20 Train loss 18.198185 on epoch=4
05/31/2022 23:14:07 - INFO - __main__ - Step 30 Global step 30 Train loss 16.612801 on epoch=7
05/31/2022 23:14:13 - INFO - __main__ - Step 40 Global step 40 Train loss 15.515039 on epoch=9
05/31/2022 23:14:18 - INFO - __main__ - Step 50 Global step 50 Train loss 14.698532 on epoch=12
05/31/2022 23:14:26 - INFO - __main__ - Global step 50 Train loss 17.642183 Classification-F1 0.0 on epoch=12
05/31/2022 23:14:32 - INFO - __main__ - Step 60 Global step 60 Train loss 13.529201 on epoch=14
05/31/2022 23:14:37 - INFO - __main__ - Step 70 Global step 70 Train loss 13.452873 on epoch=17
05/31/2022 23:14:42 - INFO - __main__ - Step 80 Global step 80 Train loss 11.584364 on epoch=19
05/31/2022 23:14:47 - INFO - __main__ - Step 90 Global step 90 Train loss 12.075711 on epoch=22
05/31/2022 23:14:52 - INFO - __main__ - Step 100 Global step 100 Train loss 10.149167 on epoch=24
05/31/2022 23:14:55 - INFO - __main__ - Global step 100 Train loss 12.158263 Classification-F1 0.0 on epoch=24
05/31/2022 23:15:00 - INFO - __main__ - Step 110 Global step 110 Train loss 7.502238 on epoch=27
05/31/2022 23:15:05 - INFO - __main__ - Step 120 Global step 120 Train loss 4.268618 on epoch=29
05/31/2022 23:15:10 - INFO - __main__ - Step 130 Global step 130 Train loss 1.035286 on epoch=32
05/31/2022 23:15:16 - INFO - __main__ - Step 140 Global step 140 Train loss 1.989536 on epoch=34
05/31/2022 23:15:21 - INFO - __main__ - Step 150 Global step 150 Train loss 1.290162 on epoch=37
05/31/2022 23:15:22 - INFO - __main__ - Global step 150 Train loss 3.217168 Classification-F1 0.6858692844226298 on epoch=37
05/31/2022 23:15:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.965296 on epoch=39
05/31/2022 23:15:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.730192 on epoch=42
05/31/2022 23:15:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.736531 on epoch=44
05/31/2022 23:15:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.267607 on epoch=47
05/31/2022 23:15:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.305105 on epoch=49
05/31/2022 23:15:50 - INFO - __main__ - Global step 200 Train loss 0.600946 Classification-F1 0.921702960606802 on epoch=49
05/31/2022 23:15:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.259512 on epoch=52
05/31/2022 23:16:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.178249 on epoch=54
05/31/2022 23:16:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.486601 on epoch=57
05/31/2022 23:16:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.266757 on epoch=59
05/31/2022 23:16:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.307217 on epoch=62
05/31/2022 23:16:18 - INFO - __main__ - Global step 250 Train loss 0.299667 Classification-F1 0.8748778103616813 on epoch=62
05/31/2022 23:16:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.207838 on epoch=64
05/31/2022 23:16:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.179200 on epoch=67
05/31/2022 23:16:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.149737 on epoch=69
05/31/2022 23:16:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.210393 on epoch=72
05/31/2022 23:16:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.351161 on epoch=74
05/31/2022 23:16:45 - INFO - __main__ - Global step 300 Train loss 0.219666 Classification-F1 0.9375 on epoch=74
05/31/2022 23:16:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.132297 on epoch=77
05/31/2022 23:16:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.078919 on epoch=79
05/31/2022 23:17:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.089661 on epoch=82
05/31/2022 23:17:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.072531 on epoch=84
05/31/2022 23:17:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.055918 on epoch=87
05/31/2022 23:17:13 - INFO - __main__ - Global step 350 Train loss 0.085865 Classification-F1 0.9531135531135531 on epoch=87
05/31/2022 23:17:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.044713 on epoch=89
05/31/2022 23:17:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.119181 on epoch=92
05/31/2022 23:17:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.018333 on epoch=94
05/31/2022 23:17:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.034451 on epoch=97
05/31/2022 23:17:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.033135 on epoch=99
05/31/2022 23:17:41 - INFO - __main__ - Global step 400 Train loss 0.049963 Classification-F1 0.921702960606802 on epoch=99
05/31/2022 23:17:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.106994 on epoch=102
05/31/2022 23:17:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.146648 on epoch=104
05/31/2022 23:17:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.062280 on epoch=107
05/31/2022 23:18:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.044184 on epoch=109
05/31/2022 23:18:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.038429 on epoch=112
05/31/2022 23:18:08 - INFO - __main__ - Global step 450 Train loss 0.079707 Classification-F1 0.8899533284205354 on epoch=112
05/31/2022 23:18:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.052653 on epoch=114
05/31/2022 23:18:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.082908 on epoch=117
05/31/2022 23:18:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.010255 on epoch=119
05/31/2022 23:18:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.019836 on epoch=122
05/31/2022 23:18:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.008596 on epoch=124
05/31/2022 23:18:36 - INFO - __main__ - Global step 500 Train loss 0.034850 Classification-F1 0.9058823529411765 on epoch=124
05/31/2022 23:18:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.020007 on epoch=127
05/31/2022 23:18:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.029227 on epoch=129
05/31/2022 23:18:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.012808 on epoch=132
05/31/2022 23:18:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.016255 on epoch=134
05/31/2022 23:19:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.023045 on epoch=137
05/31/2022 23:19:03 - INFO - __main__ - Global step 550 Train loss 0.020268 Classification-F1 0.9058823529411765 on epoch=137
05/31/2022 23:19:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.041558 on epoch=139
05/31/2022 23:19:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.073327 on epoch=142
05/31/2022 23:19:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.009526 on epoch=144
05/31/2022 23:19:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.005214 on epoch=147
05/31/2022 23:19:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.003275 on epoch=149
05/31/2022 23:19:30 - INFO - __main__ - Global step 600 Train loss 0.026580 Classification-F1 0.906158357771261 on epoch=149
05/31/2022 23:19:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.022802 on epoch=152
05/31/2022 23:19:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.004427 on epoch=154
05/31/2022 23:19:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.001138 on epoch=157
05/31/2022 23:19:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.003184 on epoch=159
05/31/2022 23:19:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.009651 on epoch=162
05/31/2022 23:19:57 - INFO - __main__ - Global step 650 Train loss 0.008240 Classification-F1 0.921702960606802 on epoch=162
05/31/2022 23:20:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000363 on epoch=164
05/31/2022 23:20:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000573 on epoch=167
05/31/2022 23:20:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.014879 on epoch=169
05/31/2022 23:20:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.020090 on epoch=172
05/31/2022 23:20:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.004114 on epoch=174
05/31/2022 23:20:24 - INFO - __main__ - Global step 700 Train loss 0.008004 Classification-F1 0.9218559218559218 on epoch=174
05/31/2022 23:20:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.042189 on epoch=177
05/31/2022 23:20:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.003150 on epoch=179
05/31/2022 23:20:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001722 on epoch=182
05/31/2022 23:20:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.003952 on epoch=184
05/31/2022 23:20:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000667 on epoch=187
05/31/2022 23:20:51 - INFO - __main__ - Global step 750 Train loss 0.010336 Classification-F1 0.921702960606802 on epoch=187
05/31/2022 23:20:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.009366 on epoch=189
05/31/2022 23:21:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001721 on epoch=192
05/31/2022 23:21:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.014463 on epoch=194
05/31/2022 23:21:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002855 on epoch=197
05/31/2022 23:21:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002636 on epoch=199
05/31/2022 23:21:17 - INFO - __main__ - Global step 800 Train loss 0.006208 Classification-F1 0.9058823529411765 on epoch=199
05/31/2022 23:21:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000309 on epoch=202
05/31/2022 23:21:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000304 on epoch=204
05/31/2022 23:21:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.073009 on epoch=207
05/31/2022 23:21:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.014037 on epoch=209
05/31/2022 23:21:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000341 on epoch=212
05/31/2022 23:21:44 - INFO - __main__ - Global step 850 Train loss 0.017600 Classification-F1 0.9218559218559218 on epoch=212
05/31/2022 23:21:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000407 on epoch=214
05/31/2022 23:21:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000080 on epoch=217
05/31/2022 23:22:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000111 on epoch=219
05/31/2022 23:22:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000122 on epoch=222
05/31/2022 23:22:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000096 on epoch=224
05/31/2022 23:22:11 - INFO - __main__ - Global step 900 Train loss 0.000163 Classification-F1 0.9218559218559218 on epoch=224
05/31/2022 23:22:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.008733 on epoch=227
05/31/2022 23:22:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.010834 on epoch=229
05/31/2022 23:22:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000055 on epoch=232
05/31/2022 23:22:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000448 on epoch=234
05/31/2022 23:22:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001491 on epoch=237
05/31/2022 23:22:38 - INFO - __main__ - Global step 950 Train loss 0.004312 Classification-F1 0.9531135531135531 on epoch=237
05/31/2022 23:22:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000149 on epoch=239
05/31/2022 23:22:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000909 on epoch=242
05/31/2022 23:22:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.058080 on epoch=244
05/31/2022 23:22:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.004487 on epoch=247
05/31/2022 23:23:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000790 on epoch=249
05/31/2022 23:23:04 - INFO - __main__ - Global step 1000 Train loss 0.012883 Classification-F1 0.9374389051808407 on epoch=249
05/31/2022 23:23:04 - INFO - __main__ - save last model!
05/31/2022 23:23:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:23:05 - INFO - __main__ - Printing 3 examples
05/31/2022 23:23:05 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/31/2022 23:23:05 - INFO - __main__ - ['positive']
05/31/2022 23:23:05 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/31/2022 23:23:05 - INFO - __main__ - ['positive']
05/31/2022 23:23:05 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/31/2022 23:23:05 - INFO - __main__ - ['positive']
05/31/2022 23:23:05 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:23:05 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:23:05 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:23:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:23:05 - INFO - __main__ - Printing 3 examples
05/31/2022 23:23:05 - INFO - __main__ -  [amazon_polarity] title: good brother, Bad brother [SEP] content: this was a very interesting read . I learned many things reading this book. I have suggested others should read it.
05/31/2022 23:23:05 - INFO - __main__ - ['positive']
05/31/2022 23:23:05 - INFO - __main__ -  [amazon_polarity] title: ABSOLUTELY BRILLIANT! [SEP] content: This is not only one of the best punk songs ever recorded, it's one of the best songs period. This song is as close to perfection as you can get, both lyrically and musically. You would think that an 18 minute song would be boring at times, but not this one. The changes in tempo and the scathing political commentary keep your interest the entire time. This is a masterpiece.
05/31/2022 23:23:05 - INFO - __main__ - ['positive']
05/31/2022 23:23:05 - INFO - __main__ -  [amazon_polarity] title: Very useful book [SEP] content: Book was recommended for use by Interpreter Training program teacher as an additional tool. I have gotten considerable use out of it over the past year in my transliterating and asl classes. Only one drawback just recently noted. The DVD is not supported by the newer MAC OS Lion systems so I'm forced to use it on a Windows based computer. Other than that, very satisfied with this book.
05/31/2022 23:23:05 - INFO - __main__ - ['positive']
05/31/2022 23:23:05 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:23:05 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:23:05 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:23:12 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 23:23:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 23:23:12 - INFO - __main__ - Printing 3 examples
05/31/2022 23:23:12 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 23:23:12 - INFO - __main__ - ['negative']
05/31/2022 23:23:12 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 23:23:12 - INFO - __main__ - ['negative']
05/31/2022 23:23:12 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 23:23:12 - INFO - __main__ - ['negative']
05/31/2022 23:23:12 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:23:13 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:23:14 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 23:23:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:23:16 - INFO - __main__ - Starting training!
05/31/2022 23:23:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_21_0.0002_8_predictions.txt
05/31/2022 23:23:30 - INFO - __main__ - Classification-F1 on test data: 0.9079
05/31/2022 23:23:30 - INFO - __main__ - prefix=amazon_polarity_32_21, lr=0.0002, bsz=8, dev_performance=0.9531135531135531, test_performance=0.9078525641025641
05/31/2022 23:23:30 - INFO - __main__ - Running ... prefix=amazon_polarity_32_21, lr=0.0001, bsz=8 ...
05/31/2022 23:23:31 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:23:31 - INFO - __main__ - Printing 3 examples
05/31/2022 23:23:31 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/31/2022 23:23:31 - INFO - __main__ - ['positive']
05/31/2022 23:23:31 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/31/2022 23:23:31 - INFO - __main__ - ['positive']
05/31/2022 23:23:31 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/31/2022 23:23:31 - INFO - __main__ - ['positive']
05/31/2022 23:23:31 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:23:31 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:23:31 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:23:31 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:23:31 - INFO - __main__ - Printing 3 examples
05/31/2022 23:23:31 - INFO - __main__ -  [amazon_polarity] title: good brother, Bad brother [SEP] content: this was a very interesting read . I learned many things reading this book. I have suggested others should read it.
05/31/2022 23:23:31 - INFO - __main__ - ['positive']
05/31/2022 23:23:31 - INFO - __main__ -  [amazon_polarity] title: ABSOLUTELY BRILLIANT! [SEP] content: This is not only one of the best punk songs ever recorded, it's one of the best songs period. This song is as close to perfection as you can get, both lyrically and musically. You would think that an 18 minute song would be boring at times, but not this one. The changes in tempo and the scathing political commentary keep your interest the entire time. This is a masterpiece.
05/31/2022 23:23:31 - INFO - __main__ - ['positive']
05/31/2022 23:23:31 - INFO - __main__ -  [amazon_polarity] title: Very useful book [SEP] content: Book was recommended for use by Interpreter Training program teacher as an additional tool. I have gotten considerable use out of it over the past year in my transliterating and asl classes. Only one drawback just recently noted. The DVD is not supported by the newer MAC OS Lion systems so I'm forced to use it on a Windows based computer. Other than that, very satisfied with this book.
05/31/2022 23:23:31 - INFO - __main__ - ['positive']
05/31/2022 23:23:31 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:23:31 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:23:31 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:23:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:23:44 - INFO - __main__ - Starting training!
05/31/2022 23:23:48 - INFO - __main__ - Step 10 Global step 10 Train loss 23.398512 on epoch=2
05/31/2022 23:23:53 - INFO - __main__ - Step 20 Global step 20 Train loss 19.627430 on epoch=4
05/31/2022 23:23:59 - INFO - __main__ - Step 30 Global step 30 Train loss 17.746025 on epoch=7
05/31/2022 23:24:04 - INFO - __main__ - Step 40 Global step 40 Train loss 16.419544 on epoch=9
05/31/2022 23:24:09 - INFO - __main__ - Step 50 Global step 50 Train loss 16.388174 on epoch=12
05/31/2022 23:24:25 - INFO - __main__ - Global step 50 Train loss 18.715937 Classification-F1 0.0 on epoch=12
05/31/2022 23:24:31 - INFO - __main__ - Step 60 Global step 60 Train loss 16.459900 on epoch=14
05/31/2022 23:24:36 - INFO - __main__ - Step 70 Global step 70 Train loss 14.850436 on epoch=17
05/31/2022 23:24:41 - INFO - __main__ - Step 80 Global step 80 Train loss 14.938284 on epoch=19
05/31/2022 23:24:46 - INFO - __main__ - Step 90 Global step 90 Train loss 14.760475 on epoch=22
05/31/2022 23:24:51 - INFO - __main__ - Step 100 Global step 100 Train loss 14.328459 on epoch=24
05/31/2022 23:25:04 - INFO - __main__ - Global step 100 Train loss 15.067511 Classification-F1 0.0 on epoch=24
05/31/2022 23:25:09 - INFO - __main__ - Step 110 Global step 110 Train loss 13.690706 on epoch=27
05/31/2022 23:25:15 - INFO - __main__ - Step 120 Global step 120 Train loss 13.885435 on epoch=29
05/31/2022 23:25:20 - INFO - __main__ - Step 130 Global step 130 Train loss 12.728640 on epoch=32
05/31/2022 23:25:25 - INFO - __main__ - Step 140 Global step 140 Train loss 13.240768 on epoch=34
05/31/2022 23:25:30 - INFO - __main__ - Step 150 Global step 150 Train loss 11.995506 on epoch=37
05/31/2022 23:25:41 - INFO - __main__ - Global step 150 Train loss 13.108212 Classification-F1 0.0 on epoch=37
05/31/2022 23:25:46 - INFO - __main__ - Step 160 Global step 160 Train loss 12.082256 on epoch=39
05/31/2022 23:25:51 - INFO - __main__ - Step 170 Global step 170 Train loss 12.044763 on epoch=42
05/31/2022 23:25:57 - INFO - __main__ - Step 180 Global step 180 Train loss 11.347178 on epoch=44
05/31/2022 23:26:02 - INFO - __main__ - Step 190 Global step 190 Train loss 10.630793 on epoch=47
05/31/2022 23:26:07 - INFO - __main__ - Step 200 Global step 200 Train loss 10.051059 on epoch=49
05/31/2022 23:26:13 - INFO - __main__ - Global step 200 Train loss 11.231211 Classification-F1 0.0 on epoch=49
05/31/2022 23:26:18 - INFO - __main__ - Step 210 Global step 210 Train loss 9.168462 on epoch=52
05/31/2022 23:26:23 - INFO - __main__ - Step 220 Global step 220 Train loss 7.513186 on epoch=54
05/31/2022 23:26:29 - INFO - __main__ - Step 230 Global step 230 Train loss 5.763793 on epoch=57
05/31/2022 23:26:34 - INFO - __main__ - Step 240 Global step 240 Train loss 2.427190 on epoch=59
05/31/2022 23:26:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.844344 on epoch=62
05/31/2022 23:26:40 - INFO - __main__ - Global step 250 Train loss 5.143395 Classification-F1 0.5747508305647842 on epoch=62
05/31/2022 23:26:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.439211 on epoch=64
05/31/2022 23:26:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.638214 on epoch=67
05/31/2022 23:26:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.516348 on epoch=69
05/31/2022 23:27:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.295384 on epoch=72
05/31/2022 23:27:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.211922 on epoch=74
05/31/2022 23:27:07 - INFO - __main__ - Global step 300 Train loss 0.420216 Classification-F1 0.9213952345860967 on epoch=74
05/31/2022 23:27:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.223785 on epoch=77
05/31/2022 23:27:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.301574 on epoch=79
05/31/2022 23:27:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.723218 on epoch=82
05/31/2022 23:27:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.296830 on epoch=84
05/31/2022 23:27:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.547884 on epoch=87
05/31/2022 23:27:35 - INFO - __main__ - Global step 350 Train loss 0.418658 Classification-F1 0.9372549019607843 on epoch=87
05/31/2022 23:27:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.203526 on epoch=89
05/31/2022 23:27:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.200961 on epoch=92
05/31/2022 23:27:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.114470 on epoch=94
05/31/2022 23:27:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.276934 on epoch=97
05/31/2022 23:28:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.107567 on epoch=99
05/31/2022 23:28:02 - INFO - __main__ - Global step 400 Train loss 0.180691 Classification-F1 0.9213952345860967 on epoch=99
05/31/2022 23:28:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.111996 on epoch=102
05/31/2022 23:28:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.263447 on epoch=104
05/31/2022 23:28:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.178932 on epoch=107
05/31/2022 23:28:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.061928 on epoch=109
05/31/2022 23:28:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.076160 on epoch=112
05/31/2022 23:28:29 - INFO - __main__ - Global step 450 Train loss 0.138493 Classification-F1 0.9530217763640813 on epoch=112
05/31/2022 23:28:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.053818 on epoch=114
05/31/2022 23:28:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.212449 on epoch=117
05/31/2022 23:28:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.055883 on epoch=119
05/31/2022 23:28:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.100264 on epoch=122
05/31/2022 23:28:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.088174 on epoch=124
05/31/2022 23:28:57 - INFO - __main__ - Global step 500 Train loss 0.102118 Classification-F1 0.9843711843711844 on epoch=124
05/31/2022 23:29:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.282591 on epoch=127
05/31/2022 23:29:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.132667 on epoch=129
05/31/2022 23:29:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.116997 on epoch=132
05/31/2022 23:29:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.181836 on epoch=134
05/31/2022 23:29:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.213536 on epoch=137
05/31/2022 23:29:25 - INFO - __main__ - Global step 550 Train loss 0.185526 Classification-F1 0.9054187192118226 on epoch=137
05/31/2022 23:29:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.225094 on epoch=139
05/31/2022 23:29:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.191662 on epoch=142
05/31/2022 23:29:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.215596 on epoch=144
05/31/2022 23:29:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.223569 on epoch=147
05/31/2022 23:29:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.326628 on epoch=149
05/31/2022 23:29:52 - INFO - __main__ - Global step 600 Train loss 0.236510 Classification-F1 0.9530217763640813 on epoch=149
05/31/2022 23:29:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.215140 on epoch=152
05/31/2022 23:30:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.321861 on epoch=154
05/31/2022 23:30:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.209683 on epoch=157
05/31/2022 23:30:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.288084 on epoch=159
05/31/2022 23:30:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.268217 on epoch=162
05/31/2022 23:30:20 - INFO - __main__ - Global step 650 Train loss 0.260597 Classification-F1 0.9054187192118226 on epoch=162
05/31/2022 23:30:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.201081 on epoch=164
05/31/2022 23:30:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.244298 on epoch=167
05/31/2022 23:30:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.236436 on epoch=169
05/31/2022 23:30:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.244701 on epoch=172
05/31/2022 23:30:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.265191 on epoch=174
05/31/2022 23:30:48 - INFO - __main__ - Global step 700 Train loss 0.238341 Classification-F1 0.9687194525904204 on epoch=174
05/31/2022 23:30:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.314230 on epoch=177
05/31/2022 23:30:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.264534 on epoch=179
05/31/2022 23:31:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.243215 on epoch=182
05/31/2022 23:31:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.209720 on epoch=184
05/31/2022 23:31:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.221745 on epoch=187
05/31/2022 23:31:16 - INFO - __main__ - Global step 750 Train loss 0.250689 Classification-F1 0.9372549019607843 on epoch=187
05/31/2022 23:31:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.228445 on epoch=189
05/31/2022 23:31:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.248077 on epoch=192
05/31/2022 23:31:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.183310 on epoch=194
05/31/2022 23:31:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.172534 on epoch=197
05/31/2022 23:31:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.145122 on epoch=199
05/31/2022 23:31:44 - INFO - __main__ - Global step 800 Train loss 0.195498 Classification-F1 0.9213952345860967 on epoch=199
05/31/2022 23:31:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.185054 on epoch=202
05/31/2022 23:31:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.176339 on epoch=204
05/31/2022 23:32:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.263515 on epoch=207
05/31/2022 23:32:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.145765 on epoch=209
05/31/2022 23:32:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.167374 on epoch=212
05/31/2022 23:32:11 - INFO - __main__ - Global step 850 Train loss 0.187609 Classification-F1 0.9374389051808407 on epoch=212
05/31/2022 23:32:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.133596 on epoch=214
05/31/2022 23:32:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.234967 on epoch=217
05/31/2022 23:32:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.150384 on epoch=219
05/31/2022 23:32:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.147117 on epoch=222
05/31/2022 23:32:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.152488 on epoch=224
05/31/2022 23:32:39 - INFO - __main__ - Global step 900 Train loss 0.163710 Classification-F1 0.9530217763640813 on epoch=224
05/31/2022 23:32:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.184373 on epoch=227
05/31/2022 23:32:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.181198 on epoch=229
05/31/2022 23:32:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.144183 on epoch=232
05/31/2022 23:33:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.120043 on epoch=234
05/31/2022 23:33:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.150241 on epoch=237
05/31/2022 23:33:07 - INFO - __main__ - Global step 950 Train loss 0.156007 Classification-F1 0.9058823529411765 on epoch=237
05/31/2022 23:33:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.122556 on epoch=239
05/31/2022 23:33:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.236253 on epoch=242
05/31/2022 23:33:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.211754 on epoch=244
05/31/2022 23:33:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.187482 on epoch=247
05/31/2022 23:33:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.153911 on epoch=249
05/31/2022 23:33:35 - INFO - __main__ - Global step 1000 Train loss 0.182391 Classification-F1 0.9054187192118226 on epoch=249
05/31/2022 23:33:35 - INFO - __main__ - save last model!
05/31/2022 23:33:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:33:35 - INFO - __main__ - Printing 3 examples
05/31/2022 23:33:35 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/31/2022 23:33:35 - INFO - __main__ - ['negative']
05/31/2022 23:33:35 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/31/2022 23:33:35 - INFO - __main__ - ['negative']
05/31/2022 23:33:35 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/31/2022 23:33:35 - INFO - __main__ - ['negative']
05/31/2022 23:33:35 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:33:35 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:33:35 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:33:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:33:35 - INFO - __main__ - Printing 3 examples
05/31/2022 23:33:35 - INFO - __main__ -  [amazon_polarity] title: Look Elsewhere [SEP] content: Programming is very time consuming and can get down right frustrating. And the extra price for the useless remote control is silly. Come on, who seriously uses a remote control when you can just reach up & push the buttons or turn a knob, especially while you're trying to drive safely. Quality of the sound however is quite good, but then again most radidios in this price range are worthy of quality speakers, such as the SR60's from BA that are in my car. This radidio really, really needs a SD memory card slot. Even a USB slot would have been acceptable, but I hate having a wire flopping around from my I-Pod.
05/31/2022 23:33:35 - INFO - __main__ - ['negative']
05/31/2022 23:33:35 - INFO - __main__ -  [amazon_polarity] title: MUST READ [SEP] content: Hi! I did not read this book yet but my close friend Brandy said that THIS BOOK IS AN ABSOLUTE MUST READ!I have a baby now who is 6 months old and the method described in this book looks very interesting. Also, sounds like a hilarious read with some good facts about understanding your body. Nothing like a book with some good humor in it.PS- Brandy is pregnant!
05/31/2022 23:33:35 - INFO - __main__ - ['negative']
05/31/2022 23:33:35 - INFO - __main__ -  [amazon_polarity] title: Another American Idol Miscalculation [SEP] content: There is absolutely nothing special about Carrie's voice. Carrie won American Idol because she is young, blonde and pretty. Period. Clearly, she was far from the best vocal in this year's competition. Her voice is extremely average. I predict she MIGHT have one more hit and then fade into the same obscurity that Ruben is in, and Fantasia is headed towards.
05/31/2022 23:33:35 - INFO - __main__ - ['negative']
05/31/2022 23:33:35 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:33:35 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:33:35 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:33:42 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 23:33:43 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 23:33:43 - INFO - __main__ - Printing 3 examples
05/31/2022 23:33:43 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 23:33:43 - INFO - __main__ - ['negative']
05/31/2022 23:33:43 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 23:33:43 - INFO - __main__ - ['negative']
05/31/2022 23:33:43 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 23:33:43 - INFO - __main__ - ['negative']
05/31/2022 23:33:43 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:33:43 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:33:44 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 23:33:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:33:48 - INFO - __main__ - Starting training!
05/31/2022 23:34:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_21_0.0001_8_predictions.txt
05/31/2022 23:34:00 - INFO - __main__ - Classification-F1 on test data: 0.9520
05/31/2022 23:34:00 - INFO - __main__ - prefix=amazon_polarity_32_21, lr=0.0001, bsz=8, dev_performance=0.9843711843711844, test_performance=0.9519992319877117
05/31/2022 23:34:00 - INFO - __main__ - Running ... prefix=amazon_polarity_32_42, lr=0.0005, bsz=8 ...
05/31/2022 23:34:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:34:01 - INFO - __main__ - Printing 3 examples
05/31/2022 23:34:01 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/31/2022 23:34:01 - INFO - __main__ - ['negative']
05/31/2022 23:34:01 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/31/2022 23:34:01 - INFO - __main__ - ['negative']
05/31/2022 23:34:01 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/31/2022 23:34:01 - INFO - __main__ - ['negative']
05/31/2022 23:34:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:34:01 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:34:01 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:34:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:34:01 - INFO - __main__ - Printing 3 examples
05/31/2022 23:34:01 - INFO - __main__ -  [amazon_polarity] title: Look Elsewhere [SEP] content: Programming is very time consuming and can get down right frustrating. And the extra price for the useless remote control is silly. Come on, who seriously uses a remote control when you can just reach up & push the buttons or turn a knob, especially while you're trying to drive safely. Quality of the sound however is quite good, but then again most radidios in this price range are worthy of quality speakers, such as the SR60's from BA that are in my car. This radidio really, really needs a SD memory card slot. Even a USB slot would have been acceptable, but I hate having a wire flopping around from my I-Pod.
05/31/2022 23:34:01 - INFO - __main__ - ['negative']
05/31/2022 23:34:01 - INFO - __main__ -  [amazon_polarity] title: MUST READ [SEP] content: Hi! I did not read this book yet but my close friend Brandy said that THIS BOOK IS AN ABSOLUTE MUST READ!I have a baby now who is 6 months old and the method described in this book looks very interesting. Also, sounds like a hilarious read with some good facts about understanding your body. Nothing like a book with some good humor in it.PS- Brandy is pregnant!
05/31/2022 23:34:01 - INFO - __main__ - ['negative']
05/31/2022 23:34:01 - INFO - __main__ -  [amazon_polarity] title: Another American Idol Miscalculation [SEP] content: There is absolutely nothing special about Carrie's voice. Carrie won American Idol because she is young, blonde and pretty. Period. Clearly, she was far from the best vocal in this year's competition. Her voice is extremely average. I predict she MIGHT have one more hit and then fade into the same obscurity that Ruben is in, and Fantasia is headed towards.
05/31/2022 23:34:01 - INFO - __main__ - ['negative']
05/31/2022 23:34:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:34:01 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:34:01 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:34:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:34:13 - INFO - __main__ - Starting training!
05/31/2022 23:34:17 - INFO - __main__ - Step 10 Global step 10 Train loss 22.843826 on epoch=2
05/31/2022 23:34:22 - INFO - __main__ - Step 20 Global step 20 Train loss 17.778761 on epoch=4
05/31/2022 23:34:27 - INFO - __main__ - Step 30 Global step 30 Train loss 19.590113 on epoch=7
05/31/2022 23:34:32 - INFO - __main__ - Step 40 Global step 40 Train loss 14.418039 on epoch=9
05/31/2022 23:34:38 - INFO - __main__ - Step 50 Global step 50 Train loss 13.391027 on epoch=12
05/31/2022 23:34:41 - INFO - __main__ - Global step 50 Train loss 17.604353 Classification-F1 0.0 on epoch=12
05/31/2022 23:34:47 - INFO - __main__ - Step 60 Global step 60 Train loss 10.549848 on epoch=14
05/31/2022 23:34:52 - INFO - __main__ - Step 70 Global step 70 Train loss 5.111983 on epoch=17
05/31/2022 23:34:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.743668 on epoch=19
05/31/2022 23:35:03 - INFO - __main__ - Step 90 Global step 90 Train loss 0.471590 on epoch=22
05/31/2022 23:35:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.256768 on epoch=24
05/31/2022 23:35:09 - INFO - __main__ - Global step 100 Train loss 3.426771 Classification-F1 0.6190476190476191 on epoch=24
05/31/2022 23:35:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.230746 on epoch=27
05/31/2022 23:35:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.184374 on epoch=29
05/31/2022 23:35:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.067961 on epoch=32
05/31/2022 23:35:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.061160 on epoch=34
05/31/2022 23:35:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.062452 on epoch=37
05/31/2022 23:35:38 - INFO - __main__ - Global step 150 Train loss 0.121338 Classification-F1 0.8748778103616813 on epoch=37
05/31/2022 23:35:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.017506 on epoch=39
05/31/2022 23:35:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.027052 on epoch=42
05/31/2022 23:35:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.021317 on epoch=44
05/31/2022 23:36:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.005062 on epoch=47
05/31/2022 23:36:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.003207 on epoch=49
05/31/2022 23:36:06 - INFO - __main__ - Global step 200 Train loss 0.014829 Classification-F1 0.9218559218559218 on epoch=49
05/31/2022 23:36:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.007308 on epoch=52
05/31/2022 23:36:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.030948 on epoch=54
05/31/2022 23:36:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.008527 on epoch=57
05/31/2022 23:36:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.202877 on epoch=59
05/31/2022 23:36:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.020092 on epoch=62
05/31/2022 23:36:34 - INFO - __main__ - Global step 250 Train loss 0.053951 Classification-F1 0.8431372549019608 on epoch=62
05/31/2022 23:36:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.007173 on epoch=64
05/31/2022 23:36:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.016484 on epoch=67
05/31/2022 23:36:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.002566 on epoch=69
05/31/2022 23:36:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002261 on epoch=72
05/31/2022 23:37:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.021432 on epoch=74
05/31/2022 23:37:02 - INFO - __main__ - Global step 300 Train loss 0.009983 Classification-F1 0.906158357771261 on epoch=74
05/31/2022 23:37:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.001392 on epoch=77
05/31/2022 23:37:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.004796 on epoch=79
05/31/2022 23:37:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.007692 on epoch=82
05/31/2022 23:37:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.001026 on epoch=84
05/31/2022 23:37:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000480 on epoch=87
05/31/2022 23:37:29 - INFO - __main__ - Global step 350 Train loss 0.003077 Classification-F1 0.8905982905982905 on epoch=87
05/31/2022 23:37:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000167 on epoch=89
05/31/2022 23:37:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001004 on epoch=92
05/31/2022 23:37:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.014886 on epoch=94
05/31/2022 23:37:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001847 on epoch=97
05/31/2022 23:37:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001171 on epoch=99
05/31/2022 23:37:57 - INFO - __main__ - Global step 400 Train loss 0.003815 Classification-F1 0.90625 on epoch=99
05/31/2022 23:38:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000514 on epoch=102
05/31/2022 23:38:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000267 on epoch=104
05/31/2022 23:38:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000265 on epoch=107
05/31/2022 23:38:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000147 on epoch=109
05/31/2022 23:38:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000199 on epoch=112
05/31/2022 23:38:24 - INFO - __main__ - Global step 450 Train loss 0.000279 Classification-F1 0.90625 on epoch=112
05/31/2022 23:38:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.048345 on epoch=114
05/31/2022 23:38:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003032 on epoch=117
05/31/2022 23:38:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001812 on epoch=119
05/31/2022 23:38:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000364 on epoch=122
05/31/2022 23:38:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000231 on epoch=124
05/31/2022 23:38:51 - INFO - __main__ - Global step 500 Train loss 0.010757 Classification-F1 0.9374389051808407 on epoch=124
05/31/2022 23:38:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000111 on epoch=127
05/31/2022 23:39:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000159 on epoch=129
05/31/2022 23:39:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000047 on epoch=132
05/31/2022 23:39:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000270 on epoch=134
05/31/2022 23:39:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000032 on epoch=137
05/31/2022 23:39:18 - INFO - __main__ - Global step 550 Train loss 0.000124 Classification-F1 0.9374389051808407 on epoch=137
05/31/2022 23:39:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000041 on epoch=139
05/31/2022 23:39:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000510 on epoch=142
05/31/2022 23:39:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000039 on epoch=144
05/31/2022 23:39:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000114 on epoch=147
05/31/2022 23:39:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000102 on epoch=149
05/31/2022 23:39:45 - INFO - __main__ - Global step 600 Train loss 0.000161 Classification-F1 0.90625 on epoch=149
05/31/2022 23:39:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000314 on epoch=152
05/31/2022 23:39:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000042 on epoch=154
05/31/2022 23:40:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000019 on epoch=157
05/31/2022 23:40:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000039 on epoch=159
05/31/2022 23:40:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000220 on epoch=162
05/31/2022 23:40:12 - INFO - __main__ - Global step 650 Train loss 0.000127 Classification-F1 0.9374389051808407 on epoch=162
05/31/2022 23:40:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000149 on epoch=164
05/31/2022 23:40:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000034 on epoch=167
05/31/2022 23:40:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000383 on epoch=169
05/31/2022 23:40:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000093 on epoch=172
05/31/2022 23:40:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000018 on epoch=174
05/31/2022 23:40:39 - INFO - __main__ - Global step 700 Train loss 0.000135 Classification-F1 0.90625 on epoch=174
05/31/2022 23:40:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000030 on epoch=177
05/31/2022 23:40:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000008 on epoch=179
05/31/2022 23:40:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000077 on epoch=182
05/31/2022 23:41:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000013 on epoch=184
05/31/2022 23:41:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000905 on epoch=187
05/31/2022 23:41:06 - INFO - __main__ - Global step 750 Train loss 0.000207 Classification-F1 0.8905982905982905 on epoch=187
05/31/2022 23:41:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000010 on epoch=189
05/31/2022 23:41:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000014 on epoch=192
05/31/2022 23:41:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000021 on epoch=194
05/31/2022 23:41:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000009 on epoch=197
05/31/2022 23:41:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000008 on epoch=199
05/31/2022 23:41:32 - INFO - __main__ - Global step 800 Train loss 0.000013 Classification-F1 0.8905982905982905 on epoch=199
05/31/2022 23:41:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000005 on epoch=202
05/31/2022 23:41:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000108 on epoch=204
05/31/2022 23:41:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000023 on epoch=207
05/31/2022 23:41:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000487 on epoch=209
05/31/2022 23:41:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000216 on epoch=212
05/31/2022 23:42:00 - INFO - __main__ - Global step 850 Train loss 0.000168 Classification-F1 0.906158357771261 on epoch=212
05/31/2022 23:42:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000015 on epoch=214
05/31/2022 23:42:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000009 on epoch=217
05/31/2022 23:42:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.148154 on epoch=219
05/31/2022 23:42:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.248856 on epoch=222
05/31/2022 23:42:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001684 on epoch=224
05/31/2022 23:42:26 - INFO - __main__ - Global step 900 Train loss 0.079744 Classification-F1 0.7906918238993711 on epoch=224
05/31/2022 23:42:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.007149 on epoch=227
05/31/2022 23:42:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000007 on epoch=229
05/31/2022 23:42:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000012 on epoch=232
05/31/2022 23:42:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000021 on epoch=234
05/31/2022 23:42:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000028 on epoch=237
05/31/2022 23:42:53 - INFO - __main__ - Global step 950 Train loss 0.001443 Classification-F1 0.8905982905982905 on epoch=237
05/31/2022 23:42:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000094 on epoch=239
05/31/2022 23:43:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000008 on epoch=242
05/31/2022 23:43:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000005 on epoch=244
05/31/2022 23:43:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000003 on epoch=247
05/31/2022 23:43:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000011 on epoch=249
05/31/2022 23:43:21 - INFO - __main__ - Global step 1000 Train loss 0.000024 Classification-F1 0.8905982905982905 on epoch=249
05/31/2022 23:43:21 - INFO - __main__ - save last model!
05/31/2022 23:43:21 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:43:21 - INFO - __main__ - Printing 3 examples
05/31/2022 23:43:21 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/31/2022 23:43:21 - INFO - __main__ - ['negative']
05/31/2022 23:43:21 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/31/2022 23:43:21 - INFO - __main__ - ['negative']
05/31/2022 23:43:21 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/31/2022 23:43:21 - INFO - __main__ - ['negative']
05/31/2022 23:43:21 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:43:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:43:21 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:43:21 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:43:21 - INFO - __main__ - Printing 3 examples
05/31/2022 23:43:21 - INFO - __main__ -  [amazon_polarity] title: Look Elsewhere [SEP] content: Programming is very time consuming and can get down right frustrating. And the extra price for the useless remote control is silly. Come on, who seriously uses a remote control when you can just reach up & push the buttons or turn a knob, especially while you're trying to drive safely. Quality of the sound however is quite good, but then again most radidios in this price range are worthy of quality speakers, such as the SR60's from BA that are in my car. This radidio really, really needs a SD memory card slot. Even a USB slot would have been acceptable, but I hate having a wire flopping around from my I-Pod.
05/31/2022 23:43:21 - INFO - __main__ - ['negative']
05/31/2022 23:43:21 - INFO - __main__ -  [amazon_polarity] title: MUST READ [SEP] content: Hi! I did not read this book yet but my close friend Brandy said that THIS BOOK IS AN ABSOLUTE MUST READ!I have a baby now who is 6 months old and the method described in this book looks very interesting. Also, sounds like a hilarious read with some good facts about understanding your body. Nothing like a book with some good humor in it.PS- Brandy is pregnant!
05/31/2022 23:43:21 - INFO - __main__ - ['negative']
05/31/2022 23:43:21 - INFO - __main__ -  [amazon_polarity] title: Another American Idol Miscalculation [SEP] content: There is absolutely nothing special about Carrie's voice. Carrie won American Idol because she is young, blonde and pretty. Period. Clearly, she was far from the best vocal in this year's competition. Her voice is extremely average. I predict she MIGHT have one more hit and then fade into the same obscurity that Ruben is in, and Fantasia is headed towards.
05/31/2022 23:43:21 - INFO - __main__ - ['negative']
05/31/2022 23:43:21 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:43:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:43:21 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:43:27 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 23:43:28 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 23:43:28 - INFO - __main__ - Printing 3 examples
05/31/2022 23:43:28 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 23:43:28 - INFO - __main__ - ['negative']
05/31/2022 23:43:28 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 23:43:28 - INFO - __main__ - ['negative']
05/31/2022 23:43:28 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 23:43:28 - INFO - __main__ - ['negative']
05/31/2022 23:43:28 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:43:29 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:43:30 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 23:43:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:43:34 - INFO - __main__ - Starting training!
05/31/2022 23:43:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_42_0.0005_8_predictions.txt
05/31/2022 23:43:45 - INFO - __main__ - Classification-F1 on test data: 0.9360
05/31/2022 23:43:45 - INFO - __main__ - prefix=amazon_polarity_32_42, lr=0.0005, bsz=8, dev_performance=0.9374389051808407, test_performance=0.936
05/31/2022 23:43:45 - INFO - __main__ - Running ... prefix=amazon_polarity_32_42, lr=0.0003, bsz=8 ...
05/31/2022 23:43:46 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:43:46 - INFO - __main__ - Printing 3 examples
05/31/2022 23:43:46 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/31/2022 23:43:46 - INFO - __main__ - ['negative']
05/31/2022 23:43:46 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/31/2022 23:43:46 - INFO - __main__ - ['negative']
05/31/2022 23:43:46 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/31/2022 23:43:46 - INFO - __main__ - ['negative']
05/31/2022 23:43:46 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:43:46 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:43:46 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:43:46 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:43:46 - INFO - __main__ - Printing 3 examples
05/31/2022 23:43:46 - INFO - __main__ -  [amazon_polarity] title: Look Elsewhere [SEP] content: Programming is very time consuming and can get down right frustrating. And the extra price for the useless remote control is silly. Come on, who seriously uses a remote control when you can just reach up & push the buttons or turn a knob, especially while you're trying to drive safely. Quality of the sound however is quite good, but then again most radidios in this price range are worthy of quality speakers, such as the SR60's from BA that are in my car. This radidio really, really needs a SD memory card slot. Even a USB slot would have been acceptable, but I hate having a wire flopping around from my I-Pod.
05/31/2022 23:43:46 - INFO - __main__ - ['negative']
05/31/2022 23:43:46 - INFO - __main__ -  [amazon_polarity] title: MUST READ [SEP] content: Hi! I did not read this book yet but my close friend Brandy said that THIS BOOK IS AN ABSOLUTE MUST READ!I have a baby now who is 6 months old and the method described in this book looks very interesting. Also, sounds like a hilarious read with some good facts about understanding your body. Nothing like a book with some good humor in it.PS- Brandy is pregnant!
05/31/2022 23:43:46 - INFO - __main__ - ['negative']
05/31/2022 23:43:46 - INFO - __main__ -  [amazon_polarity] title: Another American Idol Miscalculation [SEP] content: There is absolutely nothing special about Carrie's voice. Carrie won American Idol because she is young, blonde and pretty. Period. Clearly, she was far from the best vocal in this year's competition. Her voice is extremely average. I predict she MIGHT have one more hit and then fade into the same obscurity that Ruben is in, and Fantasia is headed towards.
05/31/2022 23:43:46 - INFO - __main__ - ['negative']
05/31/2022 23:43:46 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:43:46 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:43:46 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:43:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:43:57 - INFO - __main__ - Starting training!
05/31/2022 23:44:01 - INFO - __main__ - Step 10 Global step 10 Train loss 22.104462 on epoch=2
05/31/2022 23:44:07 - INFO - __main__ - Step 20 Global step 20 Train loss 19.161036 on epoch=4
05/31/2022 23:44:12 - INFO - __main__ - Step 30 Global step 30 Train loss 16.592493 on epoch=7
05/31/2022 23:44:17 - INFO - __main__ - Step 40 Global step 40 Train loss 15.243208 on epoch=9
05/31/2022 23:44:23 - INFO - __main__ - Step 50 Global step 50 Train loss 14.384888 on epoch=12
05/31/2022 23:44:29 - INFO - __main__ - Global step 50 Train loss 17.497215 Classification-F1 0.0 on epoch=12
05/31/2022 23:44:35 - INFO - __main__ - Step 60 Global step 60 Train loss 13.291804 on epoch=14
05/31/2022 23:44:41 - INFO - __main__ - Step 70 Global step 70 Train loss 11.706974 on epoch=17
05/31/2022 23:44:46 - INFO - __main__ - Step 80 Global step 80 Train loss 9.967631 on epoch=19
05/31/2022 23:44:51 - INFO - __main__ - Step 90 Global step 90 Train loss 4.588936 on epoch=22
05/31/2022 23:44:57 - INFO - __main__ - Step 100 Global step 100 Train loss 1.101516 on epoch=24
05/31/2022 23:44:58 - INFO - __main__ - Global step 100 Train loss 8.131372 Classification-F1 0.3333333333333333 on epoch=24
05/31/2022 23:45:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.431356 on epoch=27
05/31/2022 23:45:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.302583 on epoch=29
05/31/2022 23:45:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.251580 on epoch=32
05/31/2022 23:45:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.208550 on epoch=34
05/31/2022 23:45:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.155517 on epoch=37
05/31/2022 23:45:26 - INFO - __main__ - Global step 150 Train loss 0.269917 Classification-F1 0.90625 on epoch=37
05/31/2022 23:45:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.079528 on epoch=39
05/31/2022 23:45:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.026147 on epoch=42
05/31/2022 23:45:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.006989 on epoch=44
05/31/2022 23:45:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.047006 on epoch=47
05/31/2022 23:45:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.022065 on epoch=49
05/31/2022 23:45:54 - INFO - __main__ - Global step 200 Train loss 0.036347 Classification-F1 0.9058823529411765 on epoch=49
05/31/2022 23:45:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.047726 on epoch=52
05/31/2022 23:46:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.005465 on epoch=54
05/31/2022 23:46:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.008768 on epoch=57
05/31/2022 23:46:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001400 on epoch=59
05/31/2022 23:46:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000629 on epoch=62
05/31/2022 23:46:21 - INFO - __main__ - Global step 250 Train loss 0.012797 Classification-F1 0.8905982905982905 on epoch=62
05/31/2022 23:46:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000861 on epoch=64
05/31/2022 23:46:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.001170 on epoch=67
05/31/2022 23:46:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.054066 on epoch=69
05/31/2022 23:46:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.002054 on epoch=72
05/31/2022 23:46:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.001877 on epoch=74
05/31/2022 23:46:49 - INFO - __main__ - Global step 300 Train loss 0.012006 Classification-F1 0.906158357771261 on epoch=74
05/31/2022 23:46:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000623 on epoch=77
05/31/2022 23:46:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000199 on epoch=79
05/31/2022 23:47:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000240 on epoch=82
05/31/2022 23:47:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000165 on epoch=84
05/31/2022 23:47:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000147 on epoch=87
05/31/2022 23:47:16 - INFO - __main__ - Global step 350 Train loss 0.000275 Classification-F1 0.9218559218559218 on epoch=87
05/31/2022 23:47:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000466 on epoch=89
05/31/2022 23:47:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000268 on epoch=92
05/31/2022 23:47:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000496 on epoch=94
05/31/2022 23:47:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000253 on epoch=97
05/31/2022 23:47:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000188 on epoch=99
05/31/2022 23:47:45 - INFO - __main__ - Global step 400 Train loss 0.000334 Classification-F1 0.90625 on epoch=99
05/31/2022 23:47:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000210 on epoch=102
05/31/2022 23:47:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000150 on epoch=104
05/31/2022 23:48:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000149 on epoch=107
05/31/2022 23:48:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000057 on epoch=109
05/31/2022 23:48:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000054 on epoch=112
05/31/2022 23:48:12 - INFO - __main__ - Global step 450 Train loss 0.000124 Classification-F1 0.906158357771261 on epoch=112
05/31/2022 23:48:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000027 on epoch=114
05/31/2022 23:48:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000086 on epoch=117
05/31/2022 23:48:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000024 on epoch=119
05/31/2022 23:48:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000041 on epoch=122
05/31/2022 23:48:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000055 on epoch=124
05/31/2022 23:48:41 - INFO - __main__ - Global step 500 Train loss 0.000047 Classification-F1 0.90625 on epoch=124
05/31/2022 23:48:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000056 on epoch=127
05/31/2022 23:48:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000052 on epoch=129
05/31/2022 23:48:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000027 on epoch=132
05/31/2022 23:49:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000033 on epoch=134
05/31/2022 23:49:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000067 on epoch=137
05/31/2022 23:49:08 - INFO - __main__ - Global step 550 Train loss 0.000047 Classification-F1 0.906158357771261 on epoch=137
05/31/2022 23:49:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000040 on epoch=139
05/31/2022 23:49:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000015 on epoch=142
05/31/2022 23:49:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000019 on epoch=144
05/31/2022 23:49:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000015 on epoch=147
05/31/2022 23:49:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000013 on epoch=149
05/31/2022 23:49:36 - INFO - __main__ - Global step 600 Train loss 0.000020 Classification-F1 0.906158357771261 on epoch=149
05/31/2022 23:49:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000019 on epoch=152
05/31/2022 23:49:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000094 on epoch=154
05/31/2022 23:49:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000050 on epoch=157
05/31/2022 23:49:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000768 on epoch=159
05/31/2022 23:50:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000015 on epoch=162
05/31/2022 23:50:04 - INFO - __main__ - Global step 650 Train loss 0.000189 Classification-F1 0.9054187192118226 on epoch=162
05/31/2022 23:50:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000013 on epoch=164
05/31/2022 23:50:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000045 on epoch=167
05/31/2022 23:50:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000017 on epoch=169
05/31/2022 23:50:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000005 on epoch=172
05/31/2022 23:50:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.107596 on epoch=174
05/31/2022 23:50:32 - INFO - __main__ - Global step 700 Train loss 0.021535 Classification-F1 0.5181818181818182 on epoch=174
05/31/2022 23:50:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.147593 on epoch=177
05/31/2022 23:50:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000228 on epoch=179
05/31/2022 23:50:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000124 on epoch=182
05/31/2022 23:50:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000134 on epoch=184
05/31/2022 23:50:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000203 on epoch=187
05/31/2022 23:50:59 - INFO - __main__ - Global step 750 Train loss 0.029656 Classification-F1 0.9213952345860967 on epoch=187
05/31/2022 23:51:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000067 on epoch=189
05/31/2022 23:51:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.012782 on epoch=192
05/31/2022 23:51:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000075 on epoch=194
05/31/2022 23:51:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000058 on epoch=197
05/31/2022 23:51:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000042 on epoch=199
05/31/2022 23:51:27 - INFO - __main__ - Global step 800 Train loss 0.002605 Classification-F1 0.9213952345860967 on epoch=199
05/31/2022 23:51:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000101 on epoch=202
05/31/2022 23:51:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000053 on epoch=204
05/31/2022 23:51:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000022 on epoch=207
05/31/2022 23:51:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000048 on epoch=209
05/31/2022 23:51:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000039 on epoch=212
05/31/2022 23:51:55 - INFO - __main__ - Global step 850 Train loss 0.000052 Classification-F1 0.9213952345860967 on epoch=212
05/31/2022 23:52:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000050 on epoch=214
05/31/2022 23:52:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000054 on epoch=217
05/31/2022 23:52:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000030 on epoch=219
05/31/2022 23:52:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000037 on epoch=222
05/31/2022 23:52:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000046 on epoch=224
05/31/2022 23:52:23 - INFO - __main__ - Global step 900 Train loss 0.000043 Classification-F1 0.9213952345860967 on epoch=224
05/31/2022 23:52:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000019 on epoch=227
05/31/2022 23:52:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000024 on epoch=229
05/31/2022 23:52:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000049 on epoch=232
05/31/2022 23:52:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000015 on epoch=234
05/31/2022 23:52:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000029 on epoch=237
05/31/2022 23:52:51 - INFO - __main__ - Global step 950 Train loss 0.000028 Classification-F1 0.9213952345860967 on epoch=237
05/31/2022 23:52:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000034 on epoch=239
05/31/2022 23:53:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000024 on epoch=242
05/31/2022 23:53:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000018 on epoch=244
05/31/2022 23:53:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000023 on epoch=247
05/31/2022 23:53:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000015 on epoch=249
05/31/2022 23:53:19 - INFO - __main__ - Global step 1000 Train loss 0.000023 Classification-F1 0.9213952345860967 on epoch=249
05/31/2022 23:53:19 - INFO - __main__ - save last model!
05/31/2022 23:53:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:53:19 - INFO - __main__ - Printing 3 examples
05/31/2022 23:53:19 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/31/2022 23:53:19 - INFO - __main__ - ['negative']
05/31/2022 23:53:19 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/31/2022 23:53:19 - INFO - __main__ - ['negative']
05/31/2022 23:53:19 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/31/2022 23:53:19 - INFO - __main__ - ['negative']
05/31/2022 23:53:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:53:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:53:19 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:53:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:53:19 - INFO - __main__ - Printing 3 examples
05/31/2022 23:53:19 - INFO - __main__ -  [amazon_polarity] title: Look Elsewhere [SEP] content: Programming is very time consuming and can get down right frustrating. And the extra price for the useless remote control is silly. Come on, who seriously uses a remote control when you can just reach up & push the buttons or turn a knob, especially while you're trying to drive safely. Quality of the sound however is quite good, but then again most radidios in this price range are worthy of quality speakers, such as the SR60's from BA that are in my car. This radidio really, really needs a SD memory card slot. Even a USB slot would have been acceptable, but I hate having a wire flopping around from my I-Pod.
05/31/2022 23:53:19 - INFO - __main__ - ['negative']
05/31/2022 23:53:19 - INFO - __main__ -  [amazon_polarity] title: MUST READ [SEP] content: Hi! I did not read this book yet but my close friend Brandy said that THIS BOOK IS AN ABSOLUTE MUST READ!I have a baby now who is 6 months old and the method described in this book looks very interesting. Also, sounds like a hilarious read with some good facts about understanding your body. Nothing like a book with some good humor in it.PS- Brandy is pregnant!
05/31/2022 23:53:19 - INFO - __main__ - ['negative']
05/31/2022 23:53:19 - INFO - __main__ -  [amazon_polarity] title: Another American Idol Miscalculation [SEP] content: There is absolutely nothing special about Carrie's voice. Carrie won American Idol because she is young, blonde and pretty. Period. Clearly, she was far from the best vocal in this year's competition. Her voice is extremely average. I predict she MIGHT have one more hit and then fade into the same obscurity that Ruben is in, and Fantasia is headed towards.
05/31/2022 23:53:19 - INFO - __main__ - ['negative']
05/31/2022 23:53:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:53:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:53:19 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:53:26 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 23:53:27 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 23:53:27 - INFO - __main__ - Printing 3 examples
05/31/2022 23:53:27 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/31/2022 23:53:27 - INFO - __main__ - ['negative']
05/31/2022 23:53:27 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/31/2022 23:53:27 - INFO - __main__ - ['negative']
05/31/2022 23:53:27 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/31/2022 23:53:27 - INFO - __main__ - ['negative']
05/31/2022 23:53:27 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:53:27 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:53:28 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 23:53:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:53:32 - INFO - __main__ - Starting training!
05/31/2022 23:53:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_42_0.0003_8_predictions.txt
05/31/2022 23:53:43 - INFO - __main__ - Classification-F1 on test data: 0.9460
05/31/2022 23:53:44 - INFO - __main__ - prefix=amazon_polarity_32_42, lr=0.0003, bsz=8, dev_performance=0.9218559218559218, test_performance=0.9459783913565426
05/31/2022 23:53:44 - INFO - __main__ - Running ... prefix=amazon_polarity_32_42, lr=0.0002, bsz=8 ...
05/31/2022 23:53:44 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:53:44 - INFO - __main__ - Printing 3 examples
05/31/2022 23:53:44 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/31/2022 23:53:44 - INFO - __main__ - ['negative']
05/31/2022 23:53:44 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/31/2022 23:53:44 - INFO - __main__ - ['negative']
05/31/2022 23:53:44 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/31/2022 23:53:44 - INFO - __main__ - ['negative']
05/31/2022 23:53:44 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:53:45 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:53:45 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 23:53:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 23:53:45 - INFO - __main__ - Printing 3 examples
05/31/2022 23:53:45 - INFO - __main__ -  [amazon_polarity] title: Look Elsewhere [SEP] content: Programming is very time consuming and can get down right frustrating. And the extra price for the useless remote control is silly. Come on, who seriously uses a remote control when you can just reach up & push the buttons or turn a knob, especially while you're trying to drive safely. Quality of the sound however is quite good, but then again most radidios in this price range are worthy of quality speakers, such as the SR60's from BA that are in my car. This radidio really, really needs a SD memory card slot. Even a USB slot would have been acceptable, but I hate having a wire flopping around from my I-Pod.
05/31/2022 23:53:45 - INFO - __main__ - ['negative']
05/31/2022 23:53:45 - INFO - __main__ -  [amazon_polarity] title: MUST READ [SEP] content: Hi! I did not read this book yet but my close friend Brandy said that THIS BOOK IS AN ABSOLUTE MUST READ!I have a baby now who is 6 months old and the method described in this book looks very interesting. Also, sounds like a hilarious read with some good facts about understanding your body. Nothing like a book with some good humor in it.PS- Brandy is pregnant!
05/31/2022 23:53:45 - INFO - __main__ - ['negative']
05/31/2022 23:53:45 - INFO - __main__ -  [amazon_polarity] title: Another American Idol Miscalculation [SEP] content: There is absolutely nothing special about Carrie's voice. Carrie won American Idol because she is young, blonde and pretty. Period. Clearly, she was far from the best vocal in this year's competition. Her voice is extremely average. I predict she MIGHT have one more hit and then fade into the same obscurity that Ruben is in, and Fantasia is headed towards.
05/31/2022 23:53:45 - INFO - __main__ - ['negative']
05/31/2022 23:53:45 - INFO - __main__ - Tokenizing Input ...
05/31/2022 23:53:45 - INFO - __main__ - Tokenizing Output ...
05/31/2022 23:53:45 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 23:53:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 23:53:56 - INFO - __main__ - Starting training!
05/31/2022 23:54:00 - INFO - __main__ - Step 10 Global step 10 Train loss 22.897635 on epoch=2
05/31/2022 23:54:05 - INFO - __main__ - Step 20 Global step 20 Train loss 18.023575 on epoch=4
05/31/2022 23:54:11 - INFO - __main__ - Step 30 Global step 30 Train loss 18.061804 on epoch=7
05/31/2022 23:54:16 - INFO - __main__ - Step 40 Global step 40 Train loss 16.110121 on epoch=9
05/31/2022 23:54:21 - INFO - __main__ - Step 50 Global step 50 Train loss 14.835169 on epoch=12
05/31/2022 23:54:23 - INFO - __main__ - Global step 50 Train loss 17.985662 Classification-F1 0.0 on epoch=12
05/31/2022 23:54:29 - INFO - __main__ - Step 60 Global step 60 Train loss 15.026403 on epoch=14
05/31/2022 23:54:34 - INFO - __main__ - Step 70 Global step 70 Train loss 13.908368 on epoch=17
05/31/2022 23:54:40 - INFO - __main__ - Step 80 Global step 80 Train loss 13.338046 on epoch=19
05/31/2022 23:54:45 - INFO - __main__ - Step 90 Global step 90 Train loss 12.737264 on epoch=22
05/31/2022 23:54:51 - INFO - __main__ - Step 100 Global step 100 Train loss 11.578627 on epoch=24
05/31/2022 23:54:52 - INFO - __main__ - Global step 100 Train loss 13.317741 Classification-F1 0.0 on epoch=24
05/31/2022 23:54:57 - INFO - __main__ - Step 110 Global step 110 Train loss 10.599163 on epoch=27
05/31/2022 23:55:02 - INFO - __main__ - Step 120 Global step 120 Train loss 6.912745 on epoch=29
05/31/2022 23:55:08 - INFO - __main__ - Step 130 Global step 130 Train loss 2.388657 on epoch=32
05/31/2022 23:55:12 - INFO - __main__ - Step 140 Global step 140 Train loss 2.452791 on epoch=34
05/31/2022 23:55:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.893537 on epoch=37
05/31/2022 23:55:19 - INFO - __main__ - Global step 150 Train loss 4.649379 Classification-F1 0.8745098039215686 on epoch=37
05/31/2022 23:55:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.430133 on epoch=39
05/31/2022 23:55:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.344421 on epoch=42
05/31/2022 23:55:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.164233 on epoch=44
05/31/2022 23:55:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.233957 on epoch=47
05/31/2022 23:55:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.142935 on epoch=49
05/31/2022 23:55:47 - INFO - __main__ - Global step 200 Train loss 0.263136 Classification-F1 0.8423645320197044 on epoch=49
05/31/2022 23:55:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.110156 on epoch=52
05/31/2022 23:55:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.028568 on epoch=54
05/31/2022 23:56:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.089188 on epoch=57
05/31/2022 23:56:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.024444 on epoch=59
05/31/2022 23:56:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.020040 on epoch=62
05/31/2022 23:56:15 - INFO - __main__ - Global step 250 Train loss 0.054479 Classification-F1 0.9375 on epoch=62
05/31/2022 23:56:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.014015 on epoch=64
05/31/2022 23:56:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.007834 on epoch=67
05/31/2022 23:56:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.010330 on epoch=69
05/31/2022 23:56:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.017687 on epoch=72
05/31/2022 23:56:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.020068 on epoch=74
05/31/2022 23:56:43 - INFO - __main__ - Global step 300 Train loss 0.013987 Classification-F1 0.9374389051808407 on epoch=74
05/31/2022 23:56:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.002783 on epoch=77
05/31/2022 23:56:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.002067 on epoch=79
05/31/2022 23:56:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.011511 on epoch=82
05/31/2022 23:57:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.005810 on epoch=84
05/31/2022 23:57:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.001898 on epoch=87
05/31/2022 23:57:11 - INFO - __main__ - Global step 350 Train loss 0.004814 Classification-F1 0.9374389051808407 on epoch=87
05/31/2022 23:57:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001967 on epoch=89
05/31/2022 23:57:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001468 on epoch=92
05/31/2022 23:57:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002224 on epoch=94
05/31/2022 23:57:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.003922 on epoch=97
05/31/2022 23:57:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.003133 on epoch=99
05/31/2022 23:57:39 - INFO - __main__ - Global step 400 Train loss 0.002543 Classification-F1 0.9530217763640813 on epoch=99
05/31/2022 23:57:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.002704 on epoch=102
05/31/2022 23:57:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001568 on epoch=104
05/31/2022 23:57:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000119 on epoch=107
05/31/2022 23:58:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000600 on epoch=109
05/31/2022 23:58:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.018162 on epoch=112
05/31/2022 23:58:07 - INFO - __main__ - Global step 450 Train loss 0.004631 Classification-F1 0.9218559218559218 on epoch=112
05/31/2022 23:58:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002267 on epoch=114
05/31/2022 23:58:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001161 on epoch=117
05/31/2022 23:58:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000670 on epoch=119
05/31/2022 23:58:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.031336 on epoch=122
05/31/2022 23:58:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.003601 on epoch=124
05/31/2022 23:58:35 - INFO - __main__ - Global step 500 Train loss 0.007807 Classification-F1 0.9218559218559218 on epoch=124
05/31/2022 23:58:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000453 on epoch=127
05/31/2022 23:58:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000532 on epoch=129
05/31/2022 23:58:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000231 on epoch=132
05/31/2022 23:58:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001892 on epoch=134
05/31/2022 23:59:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000173 on epoch=137
05/31/2022 23:59:03 - INFO - __main__ - Global step 550 Train loss 0.000656 Classification-F1 0.9375 on epoch=137
05/31/2022 23:59:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000272 on epoch=139
05/31/2022 23:59:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004789 on epoch=142
05/31/2022 23:59:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.011898 on epoch=144
05/31/2022 23:59:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.003321 on epoch=147
05/31/2022 23:59:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000801 on epoch=149
05/31/2022 23:59:31 - INFO - __main__ - Global step 600 Train loss 0.004216 Classification-F1 0.9374389051808407 on epoch=149
05/31/2022 23:59:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001122 on epoch=152
05/31/2022 23:59:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.002217 on epoch=154
05/31/2022 23:59:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000939 on epoch=157
05/31/2022 23:59:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000353 on epoch=159
05/31/2022 23:59:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.039127 on epoch=162
05/31/2022 23:59:59 - INFO - __main__ - Global step 650 Train loss 0.008752 Classification-F1 0.9374389051808407 on epoch=162
06/01/2022 00:00:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000720 on epoch=164
06/01/2022 00:00:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001340 on epoch=167
06/01/2022 00:00:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000118 on epoch=169
06/01/2022 00:00:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000083 on epoch=172
06/01/2022 00:00:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000081 on epoch=174
06/01/2022 00:00:27 - INFO - __main__ - Global step 700 Train loss 0.000468 Classification-F1 0.9530217763640813 on epoch=174
06/01/2022 00:00:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000083 on epoch=177
06/01/2022 00:00:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000534 on epoch=179
06/01/2022 00:00:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000063 on epoch=182
06/01/2022 00:00:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000832 on epoch=184
06/01/2022 00:00:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000056 on epoch=187
06/01/2022 00:00:55 - INFO - __main__ - Global step 750 Train loss 0.000314 Classification-F1 0.9375 on epoch=187
06/01/2022 00:01:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001191 on epoch=189
06/01/2022 00:01:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000098 on epoch=192
06/01/2022 00:01:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000018 on epoch=194
06/01/2022 00:01:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000051 on epoch=197
06/01/2022 00:01:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000246 on epoch=199
06/01/2022 00:01:23 - INFO - __main__ - Global step 800 Train loss 0.000321 Classification-F1 0.9530217763640813 on epoch=199
06/01/2022 00:01:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001660 on epoch=202
06/01/2022 00:01:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000014 on epoch=204
06/01/2022 00:01:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000034 on epoch=207
06/01/2022 00:01:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000180 on epoch=209
06/01/2022 00:01:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000359 on epoch=212
06/01/2022 00:01:50 - INFO - __main__ - Global step 850 Train loss 0.000449 Classification-F1 0.9218559218559218 on epoch=212
06/01/2022 00:01:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000150 on epoch=214
06/01/2022 00:02:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000132 on epoch=217
06/01/2022 00:02:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000068 on epoch=219
06/01/2022 00:02:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000007 on epoch=222
06/01/2022 00:02:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000027 on epoch=224
06/01/2022 00:02:18 - INFO - __main__ - Global step 900 Train loss 0.000077 Classification-F1 0.921702960606802 on epoch=224
06/01/2022 00:02:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000010 on epoch=227
06/01/2022 00:02:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000078 on epoch=229
06/01/2022 00:02:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000005 on epoch=232
06/01/2022 00:02:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000007 on epoch=234
06/01/2022 00:02:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000003 on epoch=237
06/01/2022 00:02:46 - INFO - __main__ - Global step 950 Train loss 0.000020 Classification-F1 0.921702960606802 on epoch=237
06/01/2022 00:02:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000032 on epoch=239
06/01/2022 00:02:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000015 on epoch=242
06/01/2022 00:03:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000010 on epoch=244
06/01/2022 00:03:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000009 on epoch=247
06/01/2022 00:03:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000009 on epoch=249
06/01/2022 00:03:14 - INFO - __main__ - Global step 1000 Train loss 0.000015 Classification-F1 0.9530217763640813 on epoch=249
06/01/2022 00:03:14 - INFO - __main__ - save last model!
06/01/2022 00:03:14 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:03:14 - INFO - __main__ - Printing 3 examples
06/01/2022 00:03:14 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
06/01/2022 00:03:14 - INFO - __main__ - ['negative']
06/01/2022 00:03:14 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
06/01/2022 00:03:14 - INFO - __main__ - ['negative']
06/01/2022 00:03:14 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
06/01/2022 00:03:14 - INFO - __main__ - ['negative']
06/01/2022 00:03:14 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:03:14 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:03:14 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 00:03:14 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:03:14 - INFO - __main__ - Printing 3 examples
06/01/2022 00:03:14 - INFO - __main__ -  [amazon_polarity] title: Look Elsewhere [SEP] content: Programming is very time consuming and can get down right frustrating. And the extra price for the useless remote control is silly. Come on, who seriously uses a remote control when you can just reach up & push the buttons or turn a knob, especially while you're trying to drive safely. Quality of the sound however is quite good, but then again most radidios in this price range are worthy of quality speakers, such as the SR60's from BA that are in my car. This radidio really, really needs a SD memory card slot. Even a USB slot would have been acceptable, but I hate having a wire flopping around from my I-Pod.
06/01/2022 00:03:14 - INFO - __main__ - ['negative']
06/01/2022 00:03:14 - INFO - __main__ -  [amazon_polarity] title: MUST READ [SEP] content: Hi! I did not read this book yet but my close friend Brandy said that THIS BOOK IS AN ABSOLUTE MUST READ!I have a baby now who is 6 months old and the method described in this book looks very interesting. Also, sounds like a hilarious read with some good facts about understanding your body. Nothing like a book with some good humor in it.PS- Brandy is pregnant!
06/01/2022 00:03:14 - INFO - __main__ - ['negative']
06/01/2022 00:03:14 - INFO - __main__ -  [amazon_polarity] title: Another American Idol Miscalculation [SEP] content: There is absolutely nothing special about Carrie's voice. Carrie won American Idol because she is young, blonde and pretty. Period. Clearly, she was far from the best vocal in this year's competition. Her voice is extremely average. I predict she MIGHT have one more hit and then fade into the same obscurity that Ruben is in, and Fantasia is headed towards.
06/01/2022 00:03:14 - INFO - __main__ - ['negative']
06/01/2022 00:03:14 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:03:14 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:03:14 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 00:03:21 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 00:03:22 - INFO - __main__ - Start tokenizing ... 1000 instances
06/01/2022 00:03:22 - INFO - __main__ - Printing 3 examples
06/01/2022 00:03:22 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/01/2022 00:03:22 - INFO - __main__ - ['negative']
06/01/2022 00:03:22 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/01/2022 00:03:22 - INFO - __main__ - ['negative']
06/01/2022 00:03:22 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/01/2022 00:03:22 - INFO - __main__ - ['negative']
06/01/2022 00:03:22 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:03:22 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:03:23 - INFO - __main__ - Loaded 1000 examples from test data
06/01/2022 00:03:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 00:03:26 - INFO - __main__ - Starting training!
06/01/2022 00:03:39 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_42_0.0002_8_predictions.txt
06/01/2022 00:03:39 - INFO - __main__ - Classification-F1 on test data: 0.9420
06/01/2022 00:03:39 - INFO - __main__ - prefix=amazon_polarity_32_42, lr=0.0002, bsz=8, dev_performance=0.9530217763640813, test_performance=0.9419916467971388
06/01/2022 00:03:39 - INFO - __main__ - Running ... prefix=amazon_polarity_32_42, lr=0.0001, bsz=8 ...
06/01/2022 00:03:40 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:03:40 - INFO - __main__ - Printing 3 examples
06/01/2022 00:03:40 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
06/01/2022 00:03:40 - INFO - __main__ - ['negative']
06/01/2022 00:03:40 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
06/01/2022 00:03:40 - INFO - __main__ - ['negative']
06/01/2022 00:03:40 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
06/01/2022 00:03:40 - INFO - __main__ - ['negative']
06/01/2022 00:03:40 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:03:40 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:03:40 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 00:03:40 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:03:40 - INFO - __main__ - Printing 3 examples
06/01/2022 00:03:40 - INFO - __main__ -  [amazon_polarity] title: Look Elsewhere [SEP] content: Programming is very time consuming and can get down right frustrating. And the extra price for the useless remote control is silly. Come on, who seriously uses a remote control when you can just reach up & push the buttons or turn a knob, especially while you're trying to drive safely. Quality of the sound however is quite good, but then again most radidios in this price range are worthy of quality speakers, such as the SR60's from BA that are in my car. This radidio really, really needs a SD memory card slot. Even a USB slot would have been acceptable, but I hate having a wire flopping around from my I-Pod.
06/01/2022 00:03:40 - INFO - __main__ - ['negative']
06/01/2022 00:03:40 - INFO - __main__ -  [amazon_polarity] title: MUST READ [SEP] content: Hi! I did not read this book yet but my close friend Brandy said that THIS BOOK IS AN ABSOLUTE MUST READ!I have a baby now who is 6 months old and the method described in this book looks very interesting. Also, sounds like a hilarious read with some good facts about understanding your body. Nothing like a book with some good humor in it.PS- Brandy is pregnant!
06/01/2022 00:03:40 - INFO - __main__ - ['negative']
06/01/2022 00:03:40 - INFO - __main__ -  [amazon_polarity] title: Another American Idol Miscalculation [SEP] content: There is absolutely nothing special about Carrie's voice. Carrie won American Idol because she is young, blonde and pretty. Period. Clearly, she was far from the best vocal in this year's competition. Her voice is extremely average. I predict she MIGHT have one more hit and then fade into the same obscurity that Ruben is in, and Fantasia is headed towards.
06/01/2022 00:03:40 - INFO - __main__ - ['negative']
06/01/2022 00:03:40 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:03:40 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:03:40 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 00:03:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 00:03:53 - INFO - __main__ - Starting training!
06/01/2022 00:03:58 - INFO - __main__ - Step 10 Global step 10 Train loss 23.366497 on epoch=2
06/01/2022 00:04:03 - INFO - __main__ - Step 20 Global step 20 Train loss 20.484594 on epoch=4
06/01/2022 00:04:08 - INFO - __main__ - Step 30 Global step 30 Train loss 19.332054 on epoch=7
06/01/2022 00:04:13 - INFO - __main__ - Step 40 Global step 40 Train loss 18.641104 on epoch=9
06/01/2022 00:04:18 - INFO - __main__ - Step 50 Global step 50 Train loss 17.669830 on epoch=12
06/01/2022 00:04:40 - INFO - __main__ - Global step 50 Train loss 19.898815 Classification-F1 0.0 on epoch=12
06/01/2022 00:04:46 - INFO - __main__ - Step 60 Global step 60 Train loss 17.378468 on epoch=14
06/01/2022 00:04:52 - INFO - __main__ - Step 70 Global step 70 Train loss 16.943499 on epoch=17
06/01/2022 00:04:57 - INFO - __main__ - Step 80 Global step 80 Train loss 15.386625 on epoch=19
06/01/2022 00:05:02 - INFO - __main__ - Step 90 Global step 90 Train loss 15.116910 on epoch=22
06/01/2022 00:05:07 - INFO - __main__ - Step 100 Global step 100 Train loss 14.273560 on epoch=24
06/01/2022 00:05:24 - INFO - __main__ - Global step 100 Train loss 15.819812 Classification-F1 0.0 on epoch=24
06/01/2022 00:05:29 - INFO - __main__ - Step 110 Global step 110 Train loss 14.643661 on epoch=27
06/01/2022 00:05:34 - INFO - __main__ - Step 120 Global step 120 Train loss 13.713054 on epoch=29
06/01/2022 00:05:40 - INFO - __main__ - Step 130 Global step 130 Train loss 13.979207 on epoch=32
06/01/2022 00:05:45 - INFO - __main__ - Step 140 Global step 140 Train loss 13.319580 on epoch=34
06/01/2022 00:05:50 - INFO - __main__ - Step 150 Global step 150 Train loss 12.869665 on epoch=37
06/01/2022 00:06:11 - INFO - __main__ - Global step 150 Train loss 13.705033 Classification-F1 0.0 on epoch=37
06/01/2022 00:06:17 - INFO - __main__ - Step 160 Global step 160 Train loss 12.727593 on epoch=39
06/01/2022 00:06:22 - INFO - __main__ - Step 170 Global step 170 Train loss 12.027472 on epoch=42
06/01/2022 00:06:27 - INFO - __main__ - Step 180 Global step 180 Train loss 11.466868 on epoch=44
06/01/2022 00:06:32 - INFO - __main__ - Step 190 Global step 190 Train loss 10.898386 on epoch=47
06/01/2022 00:06:38 - INFO - __main__ - Step 200 Global step 200 Train loss 9.543076 on epoch=49
06/01/2022 00:06:43 - INFO - __main__ - Global step 200 Train loss 11.332680 Classification-F1 0.051707957957957955 on epoch=49
06/01/2022 00:06:49 - INFO - __main__ - Step 210 Global step 210 Train loss 7.617636 on epoch=52
06/01/2022 00:06:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.544847 on epoch=54
06/01/2022 00:07:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.908304 on epoch=57
06/01/2022 00:07:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.577327 on epoch=59
06/01/2022 00:07:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.664284 on epoch=62
06/01/2022 00:07:11 - INFO - __main__ - Global step 250 Train loss 2.062479 Classification-F1 0.8590653290922436 on epoch=62
06/01/2022 00:07:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.743323 on epoch=64
06/01/2022 00:07:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.566000 on epoch=67
06/01/2022 00:07:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.437967 on epoch=69
06/01/2022 00:07:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.315932 on epoch=72
06/01/2022 00:07:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.158725 on epoch=74
06/01/2022 00:07:39 - INFO - __main__ - Global step 300 Train loss 0.444389 Classification-F1 0.5983796296296297 on epoch=74
06/01/2022 00:07:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.190949 on epoch=77
06/01/2022 00:07:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.091411 on epoch=79
06/01/2022 00:07:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.270647 on epoch=82
06/01/2022 00:08:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.151026 on epoch=84
06/01/2022 00:08:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.101906 on epoch=87
06/01/2022 00:08:07 - INFO - __main__ - Global step 350 Train loss 0.161188 Classification-F1 0.8593406593406593 on epoch=87
06/01/2022 00:08:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.081137 on epoch=89
06/01/2022 00:08:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.066945 on epoch=92
06/01/2022 00:08:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.042380 on epoch=94
06/01/2022 00:08:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.053816 on epoch=97
06/01/2022 00:08:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.033438 on epoch=99
06/01/2022 00:08:35 - INFO - __main__ - Global step 400 Train loss 0.055543 Classification-F1 0.8905982905982905 on epoch=99
06/01/2022 00:08:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.060413 on epoch=102
06/01/2022 00:08:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.043814 on epoch=104
06/01/2022 00:08:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.240484 on epoch=107
06/01/2022 00:08:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.055742 on epoch=109
06/01/2022 00:09:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.031440 on epoch=112
06/01/2022 00:09:04 - INFO - __main__ - Global step 450 Train loss 0.086379 Classification-F1 0.9218559218559218 on epoch=112
06/01/2022 00:09:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.065089 on epoch=114
06/01/2022 00:09:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.015123 on epoch=117
06/01/2022 00:09:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.038285 on epoch=119
06/01/2022 00:09:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.011863 on epoch=122
06/01/2022 00:09:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.021527 on epoch=124
06/01/2022 00:09:32 - INFO - __main__ - Global step 500 Train loss 0.030377 Classification-F1 0.906158357771261 on epoch=124
06/01/2022 00:09:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.029888 on epoch=127
06/01/2022 00:09:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.058597 on epoch=129
06/01/2022 00:09:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.031247 on epoch=132
06/01/2022 00:09:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.017051 on epoch=134
06/01/2022 00:09:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.042140 on epoch=137
06/01/2022 00:10:00 - INFO - __main__ - Global step 550 Train loss 0.035784 Classification-F1 0.8748778103616813 on epoch=137
06/01/2022 00:10:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.011374 on epoch=139
06/01/2022 00:10:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.024694 on epoch=142
06/01/2022 00:10:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.035052 on epoch=144
06/01/2022 00:10:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.014148 on epoch=147
06/01/2022 00:10:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.014484 on epoch=149
06/01/2022 00:10:28 - INFO - __main__ - Global step 600 Train loss 0.019950 Classification-F1 0.90625 on epoch=149
06/01/2022 00:10:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.003672 on epoch=152
06/01/2022 00:10:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.004142 on epoch=154
06/01/2022 00:10:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.019315 on epoch=157
06/01/2022 00:10:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.079738 on epoch=159
06/01/2022 00:10:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.013604 on epoch=162
06/01/2022 00:10:56 - INFO - __main__ - Global step 650 Train loss 0.024094 Classification-F1 0.8905982905982905 on epoch=162
06/01/2022 00:11:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.007977 on epoch=164
06/01/2022 00:11:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.015250 on epoch=167
06/01/2022 00:11:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.016164 on epoch=169
06/01/2022 00:11:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.035000 on epoch=172
06/01/2022 00:11:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.008650 on epoch=174
06/01/2022 00:11:24 - INFO - __main__ - Global step 700 Train loss 0.016608 Classification-F1 0.5983796296296297 on epoch=174
06/01/2022 00:11:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.015075 on epoch=177
06/01/2022 00:11:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.001010 on epoch=179
06/01/2022 00:11:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.007027 on epoch=182
06/01/2022 00:11:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.001568 on epoch=184
06/01/2022 00:11:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.024295 on epoch=187
06/01/2022 00:11:51 - INFO - __main__ - Global step 750 Train loss 0.009795 Classification-F1 0.619520264681555 on epoch=187
06/01/2022 00:11:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001520 on epoch=189
06/01/2022 00:12:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.002682 on epoch=192
06/01/2022 00:12:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001224 on epoch=194
06/01/2022 00:12:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.015517 on epoch=197
06/01/2022 00:12:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.056044 on epoch=199
06/01/2022 00:12:19 - INFO - __main__ - Global step 800 Train loss 0.015398 Classification-F1 0.9530217763640813 on epoch=199
06/01/2022 00:12:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000217 on epoch=202
06/01/2022 00:12:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001332 on epoch=204
06/01/2022 00:12:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000351 on epoch=207
06/01/2022 00:12:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.004515 on epoch=209
06/01/2022 00:12:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001283 on epoch=212
06/01/2022 00:12:48 - INFO - __main__ - Global step 850 Train loss 0.001540 Classification-F1 0.9374389051808407 on epoch=212
06/01/2022 00:12:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000764 on epoch=214
06/01/2022 00:12:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003288 on epoch=217
06/01/2022 00:13:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.004076 on epoch=219
06/01/2022 00:13:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.003754 on epoch=222
06/01/2022 00:13:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000048 on epoch=224
06/01/2022 00:13:16 - INFO - __main__ - Global step 900 Train loss 0.002386 Classification-F1 0.9530217763640813 on epoch=224
06/01/2022 00:13:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.002031 on epoch=227
06/01/2022 00:13:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000975 on epoch=229
06/01/2022 00:13:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000040 on epoch=232
06/01/2022 00:13:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001798 on epoch=234
06/01/2022 00:13:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000144 on epoch=237
06/01/2022 00:13:44 - INFO - __main__ - Global step 950 Train loss 0.000997 Classification-F1 0.9531135531135531 on epoch=237
06/01/2022 00:13:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000251 on epoch=239
06/01/2022 00:13:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.003698 on epoch=242
06/01/2022 00:14:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.002201 on epoch=244
06/01/2022 00:14:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000141 on epoch=247
06/01/2022 00:14:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000057 on epoch=249
06/01/2022 00:14:12 - INFO - __main__ - Global step 1000 Train loss 0.001269 Classification-F1 0.9530217763640813 on epoch=249
06/01/2022 00:14:12 - INFO - __main__ - save last model!
06/01/2022 00:14:13 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:14:13 - INFO - __main__ - Printing 3 examples
06/01/2022 00:14:13 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/01/2022 00:14:13 - INFO - __main__ - ['negative']
06/01/2022 00:14:13 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/01/2022 00:14:13 - INFO - __main__ - ['negative']
06/01/2022 00:14:13 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/01/2022 00:14:13 - INFO - __main__ - ['negative']
06/01/2022 00:14:13 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:14:13 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:14:13 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 00:14:13 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:14:13 - INFO - __main__ - Printing 3 examples
06/01/2022 00:14:13 - INFO - __main__ -  [amazon_polarity] title: So where are the moves? [SEP] content: This video hypes all the great dance moves it is going to teach you... The salsa and mambo steps are very basic and the rest of the material is just boring. The hand movements are rather comical. The music is OK, but not particularly inspiring. I get much better dance moves at my gym class. Money wasted.
06/01/2022 00:14:13 - INFO - __main__ - ['negative']
06/01/2022 00:14:13 - INFO - __main__ -  [amazon_polarity] title: Not exactly for dummies [SEP] content: I bought this book after purchasing another book on Frontpage that was way over my head. I bought it thinking that it would be basic enough to begin to learn how to use Frontpage but it is not. It is similar to the other book describing features of Frontpage that I do not understand. I have used other "Dummies" books to get a start on a subject but this did not help me at all. I think it would have helped me to have some examples of how the pieces go togather. There are plenty of technical works on Frontpage to go to once one has a basic idea. In the beginning of the book John writes that "it is assummed that the reader knows how to use the keyboard and the mouse" The reader has to know much more than that to get a benefit from this book. It is probably a very good book for someone with some experence with the basic concepts, but it was not helpful at all for me. I went out and hired someone to build my webpage.
06/01/2022 00:14:13 - INFO - __main__ - ['negative']
06/01/2022 00:14:13 - INFO - __main__ -  [amazon_polarity] title: Fell Apart in Less Than 60 Days [SEP] content: I have had good luck with this product in the past, but the latest one tore apart at one of the seams. It was used for maybe 50 hours of exercise. Because Amazon does not allow returns after 30 days, I am out of luck, and out $12.
06/01/2022 00:14:13 - INFO - __main__ - ['negative']
06/01/2022 00:14:13 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:14:13 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:14:13 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 00:14:20 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 00:14:20 - INFO - __main__ - Start tokenizing ... 1000 instances
06/01/2022 00:14:20 - INFO - __main__ - Printing 3 examples
06/01/2022 00:14:20 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/01/2022 00:14:20 - INFO - __main__ - ['negative']
06/01/2022 00:14:20 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/01/2022 00:14:20 - INFO - __main__ - ['negative']
06/01/2022 00:14:20 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/01/2022 00:14:20 - INFO - __main__ - ['negative']
06/01/2022 00:14:20 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:14:21 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:14:22 - INFO - __main__ - Loaded 1000 examples from test data
06/01/2022 00:14:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 00:14:26 - INFO - __main__ - Starting training!
06/01/2022 00:14:37 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_42_0.0001_8_predictions.txt
06/01/2022 00:14:37 - INFO - __main__ - Classification-F1 on test data: 0.9439
06/01/2022 00:14:38 - INFO - __main__ - prefix=amazon_polarity_32_42, lr=0.0001, bsz=8, dev_performance=0.9531135531135531, test_performance=0.9439351890785748
06/01/2022 00:14:38 - INFO - __main__ - Running ... prefix=amazon_polarity_32_87, lr=0.0005, bsz=8 ...
06/01/2022 00:14:39 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:14:39 - INFO - __main__ - Printing 3 examples
06/01/2022 00:14:39 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/01/2022 00:14:39 - INFO - __main__ - ['negative']
06/01/2022 00:14:39 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/01/2022 00:14:39 - INFO - __main__ - ['negative']
06/01/2022 00:14:39 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/01/2022 00:14:39 - INFO - __main__ - ['negative']
06/01/2022 00:14:39 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:14:39 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:14:39 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 00:14:39 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:14:39 - INFO - __main__ - Printing 3 examples
06/01/2022 00:14:39 - INFO - __main__ -  [amazon_polarity] title: So where are the moves? [SEP] content: This video hypes all the great dance moves it is going to teach you... The salsa and mambo steps are very basic and the rest of the material is just boring. The hand movements are rather comical. The music is OK, but not particularly inspiring. I get much better dance moves at my gym class. Money wasted.
06/01/2022 00:14:39 - INFO - __main__ - ['negative']
06/01/2022 00:14:39 - INFO - __main__ -  [amazon_polarity] title: Not exactly for dummies [SEP] content: I bought this book after purchasing another book on Frontpage that was way over my head. I bought it thinking that it would be basic enough to begin to learn how to use Frontpage but it is not. It is similar to the other book describing features of Frontpage that I do not understand. I have used other "Dummies" books to get a start on a subject but this did not help me at all. I think it would have helped me to have some examples of how the pieces go togather. There are plenty of technical works on Frontpage to go to once one has a basic idea. In the beginning of the book John writes that "it is assummed that the reader knows how to use the keyboard and the mouse" The reader has to know much more than that to get a benefit from this book. It is probably a very good book for someone with some experence with the basic concepts, but it was not helpful at all for me. I went out and hired someone to build my webpage.
06/01/2022 00:14:39 - INFO - __main__ - ['negative']
06/01/2022 00:14:39 - INFO - __main__ -  [amazon_polarity] title: Fell Apart in Less Than 60 Days [SEP] content: I have had good luck with this product in the past, but the latest one tore apart at one of the seams. It was used for maybe 50 hours of exercise. Because Amazon does not allow returns after 30 days, I am out of luck, and out $12.
06/01/2022 00:14:39 - INFO - __main__ - ['negative']
06/01/2022 00:14:39 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:14:39 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:14:39 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 00:14:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 00:14:50 - INFO - __main__ - Starting training!
06/01/2022 00:14:54 - INFO - __main__ - Step 10 Global step 10 Train loss 22.665726 on epoch=2
06/01/2022 00:15:00 - INFO - __main__ - Step 20 Global step 20 Train loss 17.813547 on epoch=4
06/01/2022 00:15:05 - INFO - __main__ - Step 30 Global step 30 Train loss 14.983368 on epoch=7
06/01/2022 00:15:11 - INFO - __main__ - Step 40 Global step 40 Train loss 12.739756 on epoch=9
06/01/2022 00:15:16 - INFO - __main__ - Step 50 Global step 50 Train loss 10.745443 on epoch=12
06/01/2022 00:15:17 - INFO - __main__ - Global step 50 Train loss 15.789568 Classification-F1 0.0 on epoch=12
06/01/2022 00:15:22 - INFO - __main__ - Step 60 Global step 60 Train loss 6.475572 on epoch=14
06/01/2022 00:15:27 - INFO - __main__ - Step 70 Global step 70 Train loss 3.514035 on epoch=17
06/01/2022 00:15:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.884290 on epoch=19
06/01/2022 00:15:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.566686 on epoch=22
06/01/2022 00:15:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.510116 on epoch=24
06/01/2022 00:15:44 - INFO - __main__ - Global step 100 Train loss 2.390140 Classification-F1 0.3333333333333333 on epoch=24
06/01/2022 00:15:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.382304 on epoch=27
06/01/2022 00:15:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.717207 on epoch=29
06/01/2022 00:16:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.555641 on epoch=32
06/01/2022 00:16:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.362771 on epoch=34
06/01/2022 00:16:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.392182 on epoch=37
06/01/2022 00:16:13 - INFO - __main__ - Global step 150 Train loss 0.482021 Classification-F1 0.34299516908212563 on epoch=37
06/01/2022 00:16:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.387652 on epoch=39
06/01/2022 00:16:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.338384 on epoch=42
06/01/2022 00:16:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.355603 on epoch=44
06/01/2022 00:16:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.307411 on epoch=47
06/01/2022 00:16:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.388341 on epoch=49
06/01/2022 00:16:41 - INFO - __main__ - Global step 200 Train loss 0.355478 Classification-F1 0.5151515151515151 on epoch=49
06/01/2022 00:16:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.391319 on epoch=52
06/01/2022 00:16:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.373595 on epoch=54
06/01/2022 00:16:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.317512 on epoch=57
06/01/2022 00:17:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.426605 on epoch=59
06/01/2022 00:17:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.311831 on epoch=62
06/01/2022 00:17:10 - INFO - __main__ - Global step 250 Train loss 0.364172 Classification-F1 0.5373493975903615 on epoch=62
06/01/2022 00:17:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.360014 on epoch=64
06/01/2022 00:17:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.256246 on epoch=67
06/01/2022 00:17:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.326656 on epoch=69
06/01/2022 00:17:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.302783 on epoch=72
06/01/2022 00:17:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.344729 on epoch=74
06/01/2022 00:17:39 - INFO - __main__ - Global step 300 Train loss 0.318085 Classification-F1 0.3727353727353727 on epoch=74
06/01/2022 00:17:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.279403 on epoch=77
06/01/2022 00:17:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.366858 on epoch=79
06/01/2022 00:17:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.329185 on epoch=82
06/01/2022 00:18:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.298880 on epoch=84
06/01/2022 00:18:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.218334 on epoch=87
06/01/2022 00:18:07 - INFO - __main__ - Global step 350 Train loss 0.298532 Classification-F1 0.3671451355661882 on epoch=87
06/01/2022 00:18:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.212378 on epoch=89
06/01/2022 00:18:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.267963 on epoch=92
06/01/2022 00:18:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.229720 on epoch=94
06/01/2022 00:18:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.215235 on epoch=97
06/01/2022 00:18:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.145331 on epoch=99
06/01/2022 00:18:35 - INFO - __main__ - Global step 400 Train loss 0.214125 Classification-F1 0.4867834867834868 on epoch=99
06/01/2022 00:18:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.140926 on epoch=102
06/01/2022 00:18:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.257877 on epoch=104
06/01/2022 00:18:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.175265 on epoch=107
06/01/2022 00:18:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.097139 on epoch=109
06/01/2022 00:19:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.166661 on epoch=112
06/01/2022 00:19:03 - INFO - __main__ - Global step 450 Train loss 0.167574 Classification-F1 0.5972640218878249 on epoch=112
06/01/2022 00:19:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.146079 on epoch=114
06/01/2022 00:19:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.058918 on epoch=117
06/01/2022 00:19:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.152196 on epoch=119
06/01/2022 00:19:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.069013 on epoch=122
06/01/2022 00:19:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.063383 on epoch=124
06/01/2022 00:19:31 - INFO - __main__ - Global step 500 Train loss 0.097918 Classification-F1 0.551443790299972 on epoch=124
06/01/2022 00:19:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.156447 on epoch=127
06/01/2022 00:19:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.046739 on epoch=129
06/01/2022 00:19:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.095510 on epoch=132
06/01/2022 00:19:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.049695 on epoch=134
06/01/2022 00:19:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.038547 on epoch=137
06/01/2022 00:19:59 - INFO - __main__ - Global step 550 Train loss 0.077387 Classification-F1 0.5972640218878249 on epoch=137
06/01/2022 00:20:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.041885 on epoch=139
06/01/2022 00:20:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.035454 on epoch=142
06/01/2022 00:20:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.047446 on epoch=144
06/01/2022 00:20:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.022992 on epoch=147
06/01/2022 00:20:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.046330 on epoch=149
06/01/2022 00:20:27 - INFO - __main__ - Global step 600 Train loss 0.038821 Classification-F1 0.6871945259042034 on epoch=149
06/01/2022 00:20:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.048214 on epoch=152
06/01/2022 00:20:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.028946 on epoch=154
06/01/2022 00:20:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.018467 on epoch=157
06/01/2022 00:20:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.023136 on epoch=159
06/01/2022 00:20:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.012389 on epoch=162
06/01/2022 00:20:56 - INFO - __main__ - Global step 650 Train loss 0.026230 Classification-F1 0.680588389808248 on epoch=162
06/01/2022 00:21:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.013917 on epoch=164
06/01/2022 00:21:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.006453 on epoch=167
06/01/2022 00:21:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.027203 on epoch=169
06/01/2022 00:21:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.019915 on epoch=172
06/01/2022 00:21:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001789 on epoch=174
06/01/2022 00:21:23 - INFO - __main__ - Global step 700 Train loss 0.013856 Classification-F1 0.5588547189819725 on epoch=174
06/01/2022 00:21:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.105179 on epoch=177
06/01/2022 00:21:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.118674 on epoch=179
06/01/2022 00:21:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.024708 on epoch=182
06/01/2022 00:21:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.052412 on epoch=184
06/01/2022 00:21:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.024555 on epoch=187
06/01/2022 00:21:51 - INFO - __main__ - Global step 750 Train loss 0.065106 Classification-F1 0.6549019607843137 on epoch=187
06/01/2022 00:21:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.121232 on epoch=189
06/01/2022 00:22:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.025047 on epoch=192
06/01/2022 00:22:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.011501 on epoch=194
06/01/2022 00:22:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.026067 on epoch=197
06/01/2022 00:22:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.025569 on epoch=199
06/01/2022 00:22:18 - INFO - __main__ - Global step 800 Train loss 0.041883 Classification-F1 0.6577540106951872 on epoch=199
06/01/2022 00:22:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.048719 on epoch=202
06/01/2022 00:22:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.025956 on epoch=204
06/01/2022 00:22:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.013924 on epoch=207
06/01/2022 00:22:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.036708 on epoch=209
06/01/2022 00:22:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.010289 on epoch=212
06/01/2022 00:22:46 - INFO - __main__ - Global step 850 Train loss 0.027119 Classification-F1 0.6862745098039216 on epoch=212
06/01/2022 00:22:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.006331 on epoch=214
06/01/2022 00:22:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.004576 on epoch=217
06/01/2022 00:23:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001535 on epoch=219
06/01/2022 00:23:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.118877 on epoch=222
06/01/2022 00:23:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.046397 on epoch=224
06/01/2022 00:23:13 - INFO - __main__ - Global step 900 Train loss 0.035543 Classification-F1 0.5844155844155844 on epoch=224
06/01/2022 00:23:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.003560 on epoch=227
06/01/2022 00:23:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.021211 on epoch=229
06/01/2022 00:23:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.005242 on epoch=232
06/01/2022 00:23:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000676 on epoch=234
06/01/2022 00:23:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.047319 on epoch=237
06/01/2022 00:23:41 - INFO - __main__ - Global step 950 Train loss 0.015602 Classification-F1 0.7290161892901619 on epoch=237
06/01/2022 00:23:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.018962 on epoch=239
06/01/2022 00:23:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.022037 on epoch=242
06/01/2022 00:23:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.013385 on epoch=244
06/01/2022 00:24:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001057 on epoch=247
06/01/2022 00:24:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.002476 on epoch=249
06/01/2022 00:24:09 - INFO - __main__ - Global step 1000 Train loss 0.011583 Classification-F1 0.5974842767295597 on epoch=249
06/01/2022 00:24:09 - INFO - __main__ - save last model!
06/01/2022 00:24:09 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:24:09 - INFO - __main__ - Printing 3 examples
06/01/2022 00:24:09 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/01/2022 00:24:09 - INFO - __main__ - ['negative']
06/01/2022 00:24:09 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/01/2022 00:24:09 - INFO - __main__ - ['negative']
06/01/2022 00:24:09 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/01/2022 00:24:09 - INFO - __main__ - ['negative']
06/01/2022 00:24:09 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:24:09 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:24:09 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 00:24:09 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:24:09 - INFO - __main__ - Printing 3 examples
06/01/2022 00:24:09 - INFO - __main__ -  [amazon_polarity] title: So where are the moves? [SEP] content: This video hypes all the great dance moves it is going to teach you... The salsa and mambo steps are very basic and the rest of the material is just boring. The hand movements are rather comical. The music is OK, but not particularly inspiring. I get much better dance moves at my gym class. Money wasted.
06/01/2022 00:24:09 - INFO - __main__ - ['negative']
06/01/2022 00:24:09 - INFO - __main__ -  [amazon_polarity] title: Not exactly for dummies [SEP] content: I bought this book after purchasing another book on Frontpage that was way over my head. I bought it thinking that it would be basic enough to begin to learn how to use Frontpage but it is not. It is similar to the other book describing features of Frontpage that I do not understand. I have used other "Dummies" books to get a start on a subject but this did not help me at all. I think it would have helped me to have some examples of how the pieces go togather. There are plenty of technical works on Frontpage to go to once one has a basic idea. In the beginning of the book John writes that "it is assummed that the reader knows how to use the keyboard and the mouse" The reader has to know much more than that to get a benefit from this book. It is probably a very good book for someone with some experence with the basic concepts, but it was not helpful at all for me. I went out and hired someone to build my webpage.
06/01/2022 00:24:09 - INFO - __main__ - ['negative']
06/01/2022 00:24:09 - INFO - __main__ -  [amazon_polarity] title: Fell Apart in Less Than 60 Days [SEP] content: I have had good luck with this product in the past, but the latest one tore apart at one of the seams. It was used for maybe 50 hours of exercise. Because Amazon does not allow returns after 30 days, I am out of luck, and out $12.
06/01/2022 00:24:09 - INFO - __main__ - ['negative']
06/01/2022 00:24:09 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:24:09 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:24:10 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 00:24:16 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 00:24:17 - INFO - __main__ - Start tokenizing ... 1000 instances
06/01/2022 00:24:17 - INFO - __main__ - Printing 3 examples
06/01/2022 00:24:17 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/01/2022 00:24:17 - INFO - __main__ - ['negative']
06/01/2022 00:24:17 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/01/2022 00:24:17 - INFO - __main__ - ['negative']
06/01/2022 00:24:17 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/01/2022 00:24:17 - INFO - __main__ - ['negative']
06/01/2022 00:24:17 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:24:18 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:24:19 - INFO - __main__ - Loaded 1000 examples from test data
06/01/2022 00:24:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 00:24:22 - INFO - __main__ - Starting training!
06/01/2022 00:24:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_87_0.0005_8_predictions.txt
06/01/2022 00:24:34 - INFO - __main__ - Classification-F1 on test data: 0.6881
06/01/2022 00:24:34 - INFO - __main__ - prefix=amazon_polarity_32_87, lr=0.0005, bsz=8, dev_performance=0.7290161892901619, test_performance=0.6880650036190517
06/01/2022 00:24:34 - INFO - __main__ - Running ... prefix=amazon_polarity_32_87, lr=0.0003, bsz=8 ...
06/01/2022 00:24:35 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:24:35 - INFO - __main__ - Printing 3 examples
06/01/2022 00:24:35 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/01/2022 00:24:35 - INFO - __main__ - ['negative']
06/01/2022 00:24:35 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/01/2022 00:24:35 - INFO - __main__ - ['negative']
06/01/2022 00:24:35 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/01/2022 00:24:35 - INFO - __main__ - ['negative']
06/01/2022 00:24:35 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:24:35 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:24:35 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 00:24:35 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:24:35 - INFO - __main__ - Printing 3 examples
06/01/2022 00:24:35 - INFO - __main__ -  [amazon_polarity] title: So where are the moves? [SEP] content: This video hypes all the great dance moves it is going to teach you... The salsa and mambo steps are very basic and the rest of the material is just boring. The hand movements are rather comical. The music is OK, but not particularly inspiring. I get much better dance moves at my gym class. Money wasted.
06/01/2022 00:24:35 - INFO - __main__ - ['negative']
06/01/2022 00:24:35 - INFO - __main__ -  [amazon_polarity] title: Not exactly for dummies [SEP] content: I bought this book after purchasing another book on Frontpage that was way over my head. I bought it thinking that it would be basic enough to begin to learn how to use Frontpage but it is not. It is similar to the other book describing features of Frontpage that I do not understand. I have used other "Dummies" books to get a start on a subject but this did not help me at all. I think it would have helped me to have some examples of how the pieces go togather. There are plenty of technical works on Frontpage to go to once one has a basic idea. In the beginning of the book John writes that "it is assummed that the reader knows how to use the keyboard and the mouse" The reader has to know much more than that to get a benefit from this book. It is probably a very good book for someone with some experence with the basic concepts, but it was not helpful at all for me. I went out and hired someone to build my webpage.
06/01/2022 00:24:35 - INFO - __main__ - ['negative']
06/01/2022 00:24:35 - INFO - __main__ -  [amazon_polarity] title: Fell Apart in Less Than 60 Days [SEP] content: I have had good luck with this product in the past, but the latest one tore apart at one of the seams. It was used for maybe 50 hours of exercise. Because Amazon does not allow returns after 30 days, I am out of luck, and out $12.
06/01/2022 00:24:35 - INFO - __main__ - ['negative']
06/01/2022 00:24:35 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:24:35 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:24:35 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 00:24:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 00:24:46 - INFO - __main__ - Starting training!
06/01/2022 00:24:51 - INFO - __main__ - Step 10 Global step 10 Train loss 23.596699 on epoch=2
06/01/2022 00:24:56 - INFO - __main__ - Step 20 Global step 20 Train loss 17.349384 on epoch=4
06/01/2022 00:25:02 - INFO - __main__ - Step 30 Global step 30 Train loss 16.763866 on epoch=7
06/01/2022 00:25:07 - INFO - __main__ - Step 40 Global step 40 Train loss 15.348938 on epoch=9
06/01/2022 00:25:13 - INFO - __main__ - Step 50 Global step 50 Train loss 14.114477 on epoch=12
06/01/2022 00:25:23 - INFO - __main__ - Global step 50 Train loss 17.434673 Classification-F1 0.0 on epoch=12
06/01/2022 00:25:30 - INFO - __main__ - Step 60 Global step 60 Train loss 12.560262 on epoch=14
06/01/2022 00:25:35 - INFO - __main__ - Step 70 Global step 70 Train loss 11.604739 on epoch=17
06/01/2022 00:25:40 - INFO - __main__ - Step 80 Global step 80 Train loss 7.094813 on epoch=19
06/01/2022 00:25:45 - INFO - __main__ - Step 90 Global step 90 Train loss 2.555585 on epoch=22
06/01/2022 00:25:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.421619 on epoch=24
06/01/2022 00:25:51 - INFO - __main__ - Global step 100 Train loss 6.847404 Classification-F1 0.3333333333333333 on epoch=24
06/01/2022 00:25:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.403861 on epoch=27
06/01/2022 00:26:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.375812 on epoch=29
06/01/2022 00:26:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.521127 on epoch=32
06/01/2022 00:26:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.387821 on epoch=34
06/01/2022 00:26:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.291187 on epoch=37
06/01/2022 00:26:20 - INFO - __main__ - Global step 150 Train loss 0.395962 Classification-F1 1.0 on epoch=37
06/01/2022 00:26:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.134050 on epoch=39
06/01/2022 00:26:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.067353 on epoch=42
06/01/2022 00:26:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.029028 on epoch=44
06/01/2022 00:26:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.687145 on epoch=47
06/01/2022 00:26:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.116769 on epoch=49
06/01/2022 00:26:48 - INFO - __main__ - Global step 200 Train loss 0.206869 Classification-F1 0.96875 on epoch=49
06/01/2022 00:26:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.402849 on epoch=52
06/01/2022 00:26:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.244946 on epoch=54
06/01/2022 00:27:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.063813 on epoch=57
06/01/2022 00:27:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.133697 on epoch=59
06/01/2022 00:27:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.057458 on epoch=62
06/01/2022 00:27:14 - INFO - __main__ - Global step 250 Train loss 0.180553 Classification-F1 1.0 on epoch=62
06/01/2022 00:27:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.085405 on epoch=64
06/01/2022 00:27:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.038094 on epoch=67
06/01/2022 00:27:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.011359 on epoch=69
06/01/2022 00:27:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.037590 on epoch=72
06/01/2022 00:27:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.041855 on epoch=74
06/01/2022 00:27:41 - INFO - __main__ - Global step 300 Train loss 0.042861 Classification-F1 0.9374389051808407 on epoch=74
06/01/2022 00:27:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.081538 on epoch=77
06/01/2022 00:27:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.049422 on epoch=79
06/01/2022 00:27:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.162759 on epoch=82
06/01/2022 00:28:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.004040 on epoch=84
06/01/2022 00:28:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.004032 on epoch=87
06/01/2022 00:28:07 - INFO - __main__ - Global step 350 Train loss 0.060358 Classification-F1 0.921702960606802 on epoch=87
06/01/2022 00:28:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.006954 on epoch=89
06/01/2022 00:28:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.003941 on epoch=92
06/01/2022 00:28:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.001682 on epoch=94
06/01/2022 00:28:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000969 on epoch=97
06/01/2022 00:28:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.000681 on epoch=99
06/01/2022 00:28:33 - INFO - __main__ - Global step 400 Train loss 0.002845 Classification-F1 0.9531135531135531 on epoch=99
06/01/2022 00:28:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000988 on epoch=102
06/01/2022 00:28:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000772 on epoch=104
06/01/2022 00:28:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.004745 on epoch=107
06/01/2022 00:28:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.003827 on epoch=109
06/01/2022 00:28:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.002172 on epoch=112
06/01/2022 00:29:00 - INFO - __main__ - Global step 450 Train loss 0.002501 Classification-F1 0.9531135531135531 on epoch=112
06/01/2022 00:29:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000889 on epoch=114
06/01/2022 00:29:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000414 on epoch=117
06/01/2022 00:29:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001004 on epoch=119
06/01/2022 00:29:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.025208 on epoch=122
06/01/2022 00:29:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000907 on epoch=124
06/01/2022 00:29:26 - INFO - __main__ - Global step 500 Train loss 0.005685 Classification-F1 0.9687194525904204 on epoch=124
06/01/2022 00:29:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000147 on epoch=127
06/01/2022 00:29:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000197 on epoch=129
06/01/2022 00:29:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000945 on epoch=132
06/01/2022 00:29:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000831 on epoch=134
06/01/2022 00:29:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000429 on epoch=137
06/01/2022 00:29:52 - INFO - __main__ - Global step 550 Train loss 0.000510 Classification-F1 0.9843711843711844 on epoch=137
06/01/2022 00:29:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000112 on epoch=139
06/01/2022 00:30:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000147 on epoch=142
06/01/2022 00:30:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000097 on epoch=144
06/01/2022 00:30:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000105 on epoch=147
06/01/2022 00:30:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000095 on epoch=149
06/01/2022 00:30:18 - INFO - __main__ - Global step 600 Train loss 0.000111 Classification-F1 0.9843711843711844 on epoch=149
06/01/2022 00:30:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000302 on epoch=152
06/01/2022 00:30:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.002227 on epoch=154
06/01/2022 00:30:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.037644 on epoch=157
06/01/2022 00:30:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000268 on epoch=159
06/01/2022 00:30:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.035361 on epoch=162
06/01/2022 00:30:44 - INFO - __main__ - Global step 650 Train loss 0.015161 Classification-F1 0.9530217763640813 on epoch=162
06/01/2022 00:30:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.006697 on epoch=164
06/01/2022 00:30:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.011587 on epoch=167
06/01/2022 00:30:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.168533 on epoch=169
06/01/2022 00:31:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.064949 on epoch=172
06/01/2022 00:31:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.402730 on epoch=174
06/01/2022 00:31:10 - INFO - __main__ - Global step 700 Train loss 0.130899 Classification-F1 0.9843711843711844 on epoch=174
06/01/2022 00:31:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.015123 on epoch=177
06/01/2022 00:31:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.004248 on epoch=179
06/01/2022 00:31:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000977 on epoch=182
06/01/2022 00:31:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.017810 on epoch=184
06/01/2022 00:31:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.013059 on epoch=187
06/01/2022 00:31:37 - INFO - __main__ - Global step 750 Train loss 0.010243 Classification-F1 0.9843711843711844 on epoch=187
06/01/2022 00:31:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000389 on epoch=189
06/01/2022 00:31:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000492 on epoch=192
06/01/2022 00:31:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.004111 on epoch=194
06/01/2022 00:31:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000408 on epoch=197
06/01/2022 00:32:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000226 on epoch=199
06/01/2022 00:32:03 - INFO - __main__ - Global step 800 Train loss 0.001125 Classification-F1 0.96875 on epoch=199
06/01/2022 00:32:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000102 on epoch=202
06/01/2022 00:32:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000439 on epoch=204
06/01/2022 00:32:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.004508 on epoch=207
06/01/2022 00:32:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000107 on epoch=209
06/01/2022 00:32:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000144 on epoch=212
06/01/2022 00:32:29 - INFO - __main__ - Global step 850 Train loss 0.001060 Classification-F1 0.96875 on epoch=212
06/01/2022 00:32:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000142 on epoch=214
06/01/2022 00:32:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000130 on epoch=217
06/01/2022 00:32:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000130 on epoch=219
06/01/2022 00:32:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000178 on epoch=222
06/01/2022 00:32:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000339 on epoch=224
06/01/2022 00:32:56 - INFO - __main__ - Global step 900 Train loss 0.000184 Classification-F1 0.9843711843711844 on epoch=224
06/01/2022 00:33:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000073 on epoch=227
06/01/2022 00:33:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000088 on epoch=229
06/01/2022 00:33:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.013093 on epoch=232
06/01/2022 00:33:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000112 on epoch=234
06/01/2022 00:33:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000070 on epoch=237
06/01/2022 00:33:23 - INFO - __main__ - Global step 950 Train loss 0.002687 Classification-F1 0.96875 on epoch=237
06/01/2022 00:33:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000088 on epoch=239
06/01/2022 00:33:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000064 on epoch=242
06/01/2022 00:33:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000045 on epoch=244
06/01/2022 00:33:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000068 on epoch=247
06/01/2022 00:33:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000082 on epoch=249
06/01/2022 00:33:51 - INFO - __main__ - Global step 1000 Train loss 0.000069 Classification-F1 0.96875 on epoch=249
06/01/2022 00:33:51 - INFO - __main__ - save last model!
06/01/2022 00:33:51 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:33:51 - INFO - __main__ - Printing 3 examples
06/01/2022 00:33:51 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/01/2022 00:33:51 - INFO - __main__ - ['negative']
06/01/2022 00:33:51 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/01/2022 00:33:51 - INFO - __main__ - ['negative']
06/01/2022 00:33:51 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/01/2022 00:33:51 - INFO - __main__ - ['negative']
06/01/2022 00:33:51 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:33:51 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:33:51 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 00:33:51 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:33:51 - INFO - __main__ - Printing 3 examples
06/01/2022 00:33:51 - INFO - __main__ -  [amazon_polarity] title: So where are the moves? [SEP] content: This video hypes all the great dance moves it is going to teach you... The salsa and mambo steps are very basic and the rest of the material is just boring. The hand movements are rather comical. The music is OK, but not particularly inspiring. I get much better dance moves at my gym class. Money wasted.
06/01/2022 00:33:51 - INFO - __main__ - ['negative']
06/01/2022 00:33:51 - INFO - __main__ -  [amazon_polarity] title: Not exactly for dummies [SEP] content: I bought this book after purchasing another book on Frontpage that was way over my head. I bought it thinking that it would be basic enough to begin to learn how to use Frontpage but it is not. It is similar to the other book describing features of Frontpage that I do not understand. I have used other "Dummies" books to get a start on a subject but this did not help me at all. I think it would have helped me to have some examples of how the pieces go togather. There are plenty of technical works on Frontpage to go to once one has a basic idea. In the beginning of the book John writes that "it is assummed that the reader knows how to use the keyboard and the mouse" The reader has to know much more than that to get a benefit from this book. It is probably a very good book for someone with some experence with the basic concepts, but it was not helpful at all for me. I went out and hired someone to build my webpage.
06/01/2022 00:33:51 - INFO - __main__ - ['negative']
06/01/2022 00:33:51 - INFO - __main__ -  [amazon_polarity] title: Fell Apart in Less Than 60 Days [SEP] content: I have had good luck with this product in the past, but the latest one tore apart at one of the seams. It was used for maybe 50 hours of exercise. Because Amazon does not allow returns after 30 days, I am out of luck, and out $12.
06/01/2022 00:33:51 - INFO - __main__ - ['negative']
06/01/2022 00:33:51 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:33:51 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:33:51 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 00:33:59 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 00:33:59 - INFO - __main__ - Start tokenizing ... 1000 instances
06/01/2022 00:33:59 - INFO - __main__ - Printing 3 examples
06/01/2022 00:33:59 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/01/2022 00:33:59 - INFO - __main__ - ['negative']
06/01/2022 00:33:59 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/01/2022 00:33:59 - INFO - __main__ - ['negative']
06/01/2022 00:33:59 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/01/2022 00:33:59 - INFO - __main__ - ['negative']
06/01/2022 00:33:59 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:34:00 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:34:01 - INFO - __main__ - Loaded 1000 examples from test data
06/01/2022 00:34:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 00:34:04 - INFO - __main__ - Starting training!
06/01/2022 00:34:16 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_87_0.0003_8_predictions.txt
06/01/2022 00:34:16 - INFO - __main__ - Classification-F1 on test data: 0.9198
06/01/2022 00:34:17 - INFO - __main__ - prefix=amazon_polarity_32_87, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.9198448195706889
06/01/2022 00:34:17 - INFO - __main__ - Running ... prefix=amazon_polarity_32_87, lr=0.0002, bsz=8 ...
06/01/2022 00:34:18 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:34:18 - INFO - __main__ - Printing 3 examples
06/01/2022 00:34:18 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/01/2022 00:34:18 - INFO - __main__ - ['negative']
06/01/2022 00:34:18 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/01/2022 00:34:18 - INFO - __main__ - ['negative']
06/01/2022 00:34:18 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/01/2022 00:34:18 - INFO - __main__ - ['negative']
06/01/2022 00:34:18 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:34:18 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:34:18 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 00:34:18 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:34:18 - INFO - __main__ - Printing 3 examples
06/01/2022 00:34:18 - INFO - __main__ -  [amazon_polarity] title: So where are the moves? [SEP] content: This video hypes all the great dance moves it is going to teach you... The salsa and mambo steps are very basic and the rest of the material is just boring. The hand movements are rather comical. The music is OK, but not particularly inspiring. I get much better dance moves at my gym class. Money wasted.
06/01/2022 00:34:18 - INFO - __main__ - ['negative']
06/01/2022 00:34:18 - INFO - __main__ -  [amazon_polarity] title: Not exactly for dummies [SEP] content: I bought this book after purchasing another book on Frontpage that was way over my head. I bought it thinking that it would be basic enough to begin to learn how to use Frontpage but it is not. It is similar to the other book describing features of Frontpage that I do not understand. I have used other "Dummies" books to get a start on a subject but this did not help me at all. I think it would have helped me to have some examples of how the pieces go togather. There are plenty of technical works on Frontpage to go to once one has a basic idea. In the beginning of the book John writes that "it is assummed that the reader knows how to use the keyboard and the mouse" The reader has to know much more than that to get a benefit from this book. It is probably a very good book for someone with some experence with the basic concepts, but it was not helpful at all for me. I went out and hired someone to build my webpage.
06/01/2022 00:34:18 - INFO - __main__ - ['negative']
06/01/2022 00:34:18 - INFO - __main__ -  [amazon_polarity] title: Fell Apart in Less Than 60 Days [SEP] content: I have had good luck with this product in the past, but the latest one tore apart at one of the seams. It was used for maybe 50 hours of exercise. Because Amazon does not allow returns after 30 days, I am out of luck, and out $12.
06/01/2022 00:34:18 - INFO - __main__ - ['negative']
06/01/2022 00:34:18 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:34:18 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:34:18 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 00:34:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 00:34:30 - INFO - __main__ - Starting training!
06/01/2022 00:34:35 - INFO - __main__ - Step 10 Global step 10 Train loss 23.267488 on epoch=2
06/01/2022 00:34:40 - INFO - __main__ - Step 20 Global step 20 Train loss 17.836760 on epoch=4
06/01/2022 00:34:46 - INFO - __main__ - Step 30 Global step 30 Train loss 16.459585 on epoch=7
06/01/2022 00:34:51 - INFO - __main__ - Step 40 Global step 40 Train loss 16.223557 on epoch=9
06/01/2022 00:34:57 - INFO - __main__ - Step 50 Global step 50 Train loss 15.154196 on epoch=12
06/01/2022 00:35:07 - INFO - __main__ - Global step 50 Train loss 17.788319 Classification-F1 0.0 on epoch=12
06/01/2022 00:35:13 - INFO - __main__ - Step 60 Global step 60 Train loss 14.579593 on epoch=14
06/01/2022 00:35:18 - INFO - __main__ - Step 70 Global step 70 Train loss 13.620657 on epoch=17
06/01/2022 00:35:24 - INFO - __main__ - Step 80 Global step 80 Train loss 13.387642 on epoch=19
06/01/2022 00:35:29 - INFO - __main__ - Step 90 Global step 90 Train loss 10.301069 on epoch=22
06/01/2022 00:35:35 - INFO - __main__ - Step 100 Global step 100 Train loss 1.054979 on epoch=24
06/01/2022 00:35:36 - INFO - __main__ - Global step 100 Train loss 10.588788 Classification-F1 0.3333333333333333 on epoch=24
06/01/2022 00:35:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.532669 on epoch=27
06/01/2022 00:35:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.383442 on epoch=29
06/01/2022 00:35:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.495864 on epoch=32
06/01/2022 00:35:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.393267 on epoch=34
06/01/2022 00:36:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.541239 on epoch=37
06/01/2022 00:36:04 - INFO - __main__ - Global step 150 Train loss 0.469296 Classification-F1 0.8576723498888065 on epoch=37
06/01/2022 00:36:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.213996 on epoch=39
06/01/2022 00:36:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.151558 on epoch=42
06/01/2022 00:36:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.184455 on epoch=44
06/01/2022 00:36:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.168181 on epoch=47
06/01/2022 00:36:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.126923 on epoch=49
06/01/2022 00:36:33 - INFO - __main__ - Global step 200 Train loss 0.169023 Classification-F1 0.9374389051808407 on epoch=49
06/01/2022 00:36:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.106222 on epoch=52
06/01/2022 00:36:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.055771 on epoch=54
06/01/2022 00:36:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.116020 on epoch=57
06/01/2022 00:36:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.036463 on epoch=59
06/01/2022 00:37:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.017170 on epoch=62
06/01/2022 00:37:01 - INFO - __main__ - Global step 250 Train loss 0.066329 Classification-F1 0.9531135531135531 on epoch=62
06/01/2022 00:37:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.019421 on epoch=64
06/01/2022 00:37:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.071212 on epoch=67
06/01/2022 00:37:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.053255 on epoch=69
06/01/2022 00:37:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.009623 on epoch=72
06/01/2022 00:37:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.011610 on epoch=74
06/01/2022 00:37:30 - INFO - __main__ - Global step 300 Train loss 0.033024 Classification-F1 0.9531135531135531 on epoch=74
06/01/2022 00:37:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.050186 on epoch=77
06/01/2022 00:37:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.015073 on epoch=79
06/01/2022 00:37:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.032698 on epoch=82
06/01/2022 00:37:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.033404 on epoch=84
06/01/2022 00:37:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.007070 on epoch=87
06/01/2022 00:37:58 - INFO - __main__ - Global step 350 Train loss 0.027686 Classification-F1 0.9531135531135531 on epoch=87
06/01/2022 00:38:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.037005 on epoch=89
06/01/2022 00:38:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.016264 on epoch=92
06/01/2022 00:38:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.014981 on epoch=94
06/01/2022 00:38:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.024472 on epoch=97
06/01/2022 00:38:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.028818 on epoch=99
06/01/2022 00:38:26 - INFO - __main__ - Global step 400 Train loss 0.024308 Classification-F1 0.6858692844226298 on epoch=99
06/01/2022 00:38:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.138682 on epoch=102
06/01/2022 00:38:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.048095 on epoch=104
06/01/2022 00:38:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.012021 on epoch=107
06/01/2022 00:38:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.020171 on epoch=109
06/01/2022 00:38:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.088572 on epoch=112
06/01/2022 00:38:54 - INFO - __main__ - Global step 450 Train loss 0.061508 Classification-F1 0.8108374384236454 on epoch=112
06/01/2022 00:39:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.042436 on epoch=114
06/01/2022 00:39:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.012335 on epoch=117
06/01/2022 00:39:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.023746 on epoch=119
06/01/2022 00:39:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.172528 on epoch=122
06/01/2022 00:39:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.028310 on epoch=124
06/01/2022 00:39:23 - INFO - __main__ - Global step 500 Train loss 0.055871 Classification-F1 0.7477832512315271 on epoch=124
06/01/2022 00:39:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.009616 on epoch=127
06/01/2022 00:39:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.062131 on epoch=129
06/01/2022 00:39:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.019804 on epoch=132
06/01/2022 00:39:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.021598 on epoch=134
06/01/2022 00:39:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.009636 on epoch=137
06/01/2022 00:39:51 - INFO - __main__ - Global step 550 Train loss 0.024557 Classification-F1 0.8270695160894128 on epoch=137
06/01/2022 00:39:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.017966 on epoch=139
06/01/2022 00:40:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003265 on epoch=142
06/01/2022 00:40:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.005535 on epoch=144
06/01/2022 00:40:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.013997 on epoch=147
06/01/2022 00:40:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.054268 on epoch=149
06/01/2022 00:40:19 - INFO - __main__ - Global step 600 Train loss 0.019006 Classification-F1 0.8117647058823529 on epoch=149
06/01/2022 00:40:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.010625 on epoch=152
06/01/2022 00:40:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.004267 on epoch=154
06/01/2022 00:40:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002437 on epoch=157
06/01/2022 00:40:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001289 on epoch=159
06/01/2022 00:40:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000340 on epoch=162
06/01/2022 00:40:47 - INFO - __main__ - Global step 650 Train loss 0.003792 Classification-F1 0.7956276099238515 on epoch=162
06/01/2022 00:40:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.006225 on epoch=164
06/01/2022 00:40:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001186 on epoch=167
06/01/2022 00:41:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.002715 on epoch=169
06/01/2022 00:41:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.016001 on epoch=172
06/01/2022 00:41:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000355 on epoch=174
06/01/2022 00:41:15 - INFO - __main__ - Global step 700 Train loss 0.005296 Classification-F1 0.8260439831974302 on epoch=174
06/01/2022 00:41:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000676 on epoch=177
06/01/2022 00:41:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000226 on epoch=179
06/01/2022 00:41:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000775 on epoch=182
06/01/2022 00:41:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.002898 on epoch=184
06/01/2022 00:41:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.017674 on epoch=187
06/01/2022 00:41:42 - INFO - __main__ - Global step 750 Train loss 0.004450 Classification-F1 0.805668016194332 on epoch=187
06/01/2022 00:41:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.006873 on epoch=189
06/01/2022 00:41:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000297 on epoch=192
06/01/2022 00:41:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000603 on epoch=194
06/01/2022 00:42:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000048 on epoch=197
06/01/2022 00:42:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000024 on epoch=199
06/01/2022 00:42:10 - INFO - __main__ - Global step 800 Train loss 0.001569 Classification-F1 0.8738916256157635 on epoch=199
06/01/2022 00:42:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000048 on epoch=202
06/01/2022 00:42:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000151 on epoch=204
06/01/2022 00:42:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.003054 on epoch=207
06/01/2022 00:42:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000181 on epoch=209
06/01/2022 00:42:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000074 on epoch=212
06/01/2022 00:42:38 - INFO - __main__ - Global step 850 Train loss 0.000701 Classification-F1 0.873015873015873 on epoch=212
06/01/2022 00:42:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.337797 on epoch=214
06/01/2022 00:42:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003809 on epoch=217
06/01/2022 00:42:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000176 on epoch=219
06/01/2022 00:42:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000074 on epoch=222
06/01/2022 00:43:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000214 on epoch=224
06/01/2022 00:43:05 - INFO - __main__ - Global step 900 Train loss 0.068414 Classification-F1 0.9213952345860967 on epoch=224
06/01/2022 00:43:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000116 on epoch=227
06/01/2022 00:43:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000164 on epoch=229
06/01/2022 00:43:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000075 on epoch=232
06/01/2022 00:43:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000034 on epoch=234
06/01/2022 00:43:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000300 on epoch=237
06/01/2022 00:43:33 - INFO - __main__ - Global step 950 Train loss 0.000138 Classification-F1 0.9058823529411765 on epoch=237
06/01/2022 00:43:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000063 on epoch=239
06/01/2022 00:43:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000086 on epoch=242
06/01/2022 00:43:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000057 on epoch=244
06/01/2022 00:43:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000040 on epoch=247
06/01/2022 00:43:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000125 on epoch=249
06/01/2022 00:44:00 - INFO - __main__ - Global step 1000 Train loss 0.000074 Classification-F1 0.9218559218559218 on epoch=249
06/01/2022 00:44:00 - INFO - __main__ - save last model!
06/01/2022 00:44:01 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:44:01 - INFO - __main__ - Printing 3 examples
06/01/2022 00:44:01 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/01/2022 00:44:01 - INFO - __main__ - ['negative']
06/01/2022 00:44:01 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/01/2022 00:44:01 - INFO - __main__ - ['negative']
06/01/2022 00:44:01 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/01/2022 00:44:01 - INFO - __main__ - ['negative']
06/01/2022 00:44:01 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:44:01 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:44:01 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 00:44:01 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:44:01 - INFO - __main__ - Printing 3 examples
06/01/2022 00:44:01 - INFO - __main__ -  [amazon_polarity] title: So where are the moves? [SEP] content: This video hypes all the great dance moves it is going to teach you... The salsa and mambo steps are very basic and the rest of the material is just boring. The hand movements are rather comical. The music is OK, but not particularly inspiring. I get much better dance moves at my gym class. Money wasted.
06/01/2022 00:44:01 - INFO - __main__ - ['negative']
06/01/2022 00:44:01 - INFO - __main__ -  [amazon_polarity] title: Not exactly for dummies [SEP] content: I bought this book after purchasing another book on Frontpage that was way over my head. I bought it thinking that it would be basic enough to begin to learn how to use Frontpage but it is not. It is similar to the other book describing features of Frontpage that I do not understand. I have used other "Dummies" books to get a start on a subject but this did not help me at all. I think it would have helped me to have some examples of how the pieces go togather. There are plenty of technical works on Frontpage to go to once one has a basic idea. In the beginning of the book John writes that "it is assummed that the reader knows how to use the keyboard and the mouse" The reader has to know much more than that to get a benefit from this book. It is probably a very good book for someone with some experence with the basic concepts, but it was not helpful at all for me. I went out and hired someone to build my webpage.
06/01/2022 00:44:01 - INFO - __main__ - ['negative']
06/01/2022 00:44:01 - INFO - __main__ -  [amazon_polarity] title: Fell Apart in Less Than 60 Days [SEP] content: I have had good luck with this product in the past, but the latest one tore apart at one of the seams. It was used for maybe 50 hours of exercise. Because Amazon does not allow returns after 30 days, I am out of luck, and out $12.
06/01/2022 00:44:01 - INFO - __main__ - ['negative']
06/01/2022 00:44:01 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:44:01 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:44:01 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 00:44:08 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 00:44:08 - INFO - __main__ - Start tokenizing ... 1000 instances
06/01/2022 00:44:08 - INFO - __main__ - Printing 3 examples
06/01/2022 00:44:08 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/01/2022 00:44:08 - INFO - __main__ - ['negative']
06/01/2022 00:44:08 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/01/2022 00:44:08 - INFO - __main__ - ['negative']
06/01/2022 00:44:08 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/01/2022 00:44:08 - INFO - __main__ - ['negative']
06/01/2022 00:44:08 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:44:09 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:44:10 - INFO - __main__ - Loaded 1000 examples from test data
06/01/2022 00:44:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 00:44:12 - INFO - __main__ - Starting training!
06/01/2022 00:44:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_87_0.0002_8_predictions.txt
06/01/2022 00:44:34 - INFO - __main__ - Classification-F1 on test data: 0.9329
06/01/2022 00:44:34 - INFO - __main__ - prefix=amazon_polarity_32_87, lr=0.0002, bsz=8, dev_performance=0.9531135531135531, test_performance=0.932935551064573
06/01/2022 00:44:34 - INFO - __main__ - Running ... prefix=amazon_polarity_32_87, lr=0.0001, bsz=8 ...
06/01/2022 00:44:35 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:44:35 - INFO - __main__ - Printing 3 examples
06/01/2022 00:44:35 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/01/2022 00:44:35 - INFO - __main__ - ['negative']
06/01/2022 00:44:35 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/01/2022 00:44:35 - INFO - __main__ - ['negative']
06/01/2022 00:44:35 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/01/2022 00:44:35 - INFO - __main__ - ['negative']
06/01/2022 00:44:35 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:44:35 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:44:35 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 00:44:35 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 00:44:35 - INFO - __main__ - Printing 3 examples
06/01/2022 00:44:35 - INFO - __main__ -  [amazon_polarity] title: So where are the moves? [SEP] content: This video hypes all the great dance moves it is going to teach you... The salsa and mambo steps are very basic and the rest of the material is just boring. The hand movements are rather comical. The music is OK, but not particularly inspiring. I get much better dance moves at my gym class. Money wasted.
06/01/2022 00:44:35 - INFO - __main__ - ['negative']
06/01/2022 00:44:35 - INFO - __main__ -  [amazon_polarity] title: Not exactly for dummies [SEP] content: I bought this book after purchasing another book on Frontpage that was way over my head. I bought it thinking that it would be basic enough to begin to learn how to use Frontpage but it is not. It is similar to the other book describing features of Frontpage that I do not understand. I have used other "Dummies" books to get a start on a subject but this did not help me at all. I think it would have helped me to have some examples of how the pieces go togather. There are plenty of technical works on Frontpage to go to once one has a basic idea. In the beginning of the book John writes that "it is assummed that the reader knows how to use the keyboard and the mouse" The reader has to know much more than that to get a benefit from this book. It is probably a very good book for someone with some experence with the basic concepts, but it was not helpful at all for me. I went out and hired someone to build my webpage.
06/01/2022 00:44:35 - INFO - __main__ - ['negative']
06/01/2022 00:44:35 - INFO - __main__ -  [amazon_polarity] title: Fell Apart in Less Than 60 Days [SEP] content: I have had good luck with this product in the past, but the latest one tore apart at one of the seams. It was used for maybe 50 hours of exercise. Because Amazon does not allow returns after 30 days, I am out of luck, and out $12.
06/01/2022 00:44:35 - INFO - __main__ - ['negative']
06/01/2022 00:44:35 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:44:35 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:44:35 - INFO - __main__ - Loaded 64 examples from dev data
06/01/2022 00:44:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 00:44:48 - INFO - __main__ - Starting training!
06/01/2022 00:44:52 - INFO - __main__ - Step 10 Global step 10 Train loss 23.782736 on epoch=2
06/01/2022 00:44:58 - INFO - __main__ - Step 20 Global step 20 Train loss 19.989885 on epoch=4
06/01/2022 00:45:03 - INFO - __main__ - Step 30 Global step 30 Train loss 18.883816 on epoch=7
06/01/2022 00:45:08 - INFO - __main__ - Step 40 Global step 40 Train loss 18.388235 on epoch=9
06/01/2022 00:45:14 - INFO - __main__ - Step 50 Global step 50 Train loss 16.907751 on epoch=12
06/01/2022 00:45:33 - INFO - __main__ - Global step 50 Train loss 19.590485 Classification-F1 0.0 on epoch=12
06/01/2022 00:45:39 - INFO - __main__ - Step 60 Global step 60 Train loss 16.334621 on epoch=14
06/01/2022 00:45:44 - INFO - __main__ - Step 70 Global step 70 Train loss 16.329870 on epoch=17
06/01/2022 00:45:49 - INFO - __main__ - Step 80 Global step 80 Train loss 16.184141 on epoch=19
06/01/2022 00:45:54 - INFO - __main__ - Step 90 Global step 90 Train loss 15.596739 on epoch=22
06/01/2022 00:45:59 - INFO - __main__ - Step 100 Global step 100 Train loss 15.703877 on epoch=24
06/01/2022 00:46:02 - INFO - __main__ - Global step 100 Train loss 16.029850 Classification-F1 0.0 on epoch=24
06/01/2022 00:46:07 - INFO - __main__ - Step 110 Global step 110 Train loss 14.953596 on epoch=27
06/01/2022 00:46:12 - INFO - __main__ - Step 120 Global step 120 Train loss 14.840200 on epoch=29
06/01/2022 00:46:18 - INFO - __main__ - Step 130 Global step 130 Train loss 14.245804 on epoch=32
06/01/2022 00:46:23 - INFO - __main__ - Step 140 Global step 140 Train loss 14.090248 on epoch=34
06/01/2022 00:46:29 - INFO - __main__ - Step 150 Global step 150 Train loss 12.794539 on epoch=37
06/01/2022 00:46:30 - INFO - __main__ - Global step 150 Train loss 14.184876 Classification-F1 0.0 on epoch=37
06/01/2022 00:46:36 - INFO - __main__ - Step 160 Global step 160 Train loss 13.195627 on epoch=39
06/01/2022 00:46:41 - INFO - __main__ - Step 170 Global step 170 Train loss 13.127722 on epoch=42
06/01/2022 00:46:46 - INFO - __main__ - Step 180 Global step 180 Train loss 12.478584 on epoch=44
06/01/2022 00:46:52 - INFO - __main__ - Step 190 Global step 190 Train loss 12.099740 on epoch=47
06/01/2022 00:46:57 - INFO - __main__ - Step 200 Global step 200 Train loss 12.035352 on epoch=49
06/01/2022 00:46:59 - INFO - __main__ - Global step 200 Train loss 12.587403 Classification-F1 0.0 on epoch=49
06/01/2022 00:47:04 - INFO - __main__ - Step 210 Global step 210 Train loss 10.699987 on epoch=52
06/01/2022 00:47:10 - INFO - __main__ - Step 220 Global step 220 Train loss 10.585657 on epoch=54
06/01/2022 00:47:15 - INFO - __main__ - Step 230 Global step 230 Train loss 10.312155 on epoch=57
06/01/2022 00:47:21 - INFO - __main__ - Step 240 Global step 240 Train loss 9.774022 on epoch=59
06/01/2022 00:47:26 - INFO - __main__ - Step 250 Global step 250 Train loss 9.328093 on epoch=62
06/01/2022 00:47:30 - INFO - __main__ - Global step 250 Train loss 10.139982 Classification-F1 0.0 on epoch=62
06/01/2022 00:47:35 - INFO - __main__ - Step 260 Global step 260 Train loss 7.996552 on epoch=64
06/01/2022 00:47:41 - INFO - __main__ - Step 270 Global step 270 Train loss 7.413452 on epoch=67
06/01/2022 00:47:46 - INFO - __main__ - Step 280 Global step 280 Train loss 7.263999 on epoch=69
06/01/2022 00:47:52 - INFO - __main__ - Step 290 Global step 290 Train loss 5.056716 on epoch=72
06/01/2022 00:47:57 - INFO - __main__ - Step 300 Global step 300 Train loss 4.473119 on epoch=74
06/01/2022 00:47:59 - INFO - __main__ - Global step 300 Train loss 6.440767 Classification-F1 0.0 on epoch=74
06/01/2022 00:48:05 - INFO - __main__ - Step 310 Global step 310 Train loss 3.634818 on epoch=77
06/01/2022 00:48:10 - INFO - __main__ - Step 320 Global step 320 Train loss 3.230414 on epoch=79
06/01/2022 00:48:15 - INFO - __main__ - Step 330 Global step 330 Train loss 2.531108 on epoch=82
06/01/2022 00:48:21 - INFO - __main__ - Step 340 Global step 340 Train loss 2.887889 on epoch=84
06/01/2022 00:48:26 - INFO - __main__ - Step 350 Global step 350 Train loss 2.620878 on epoch=87
06/01/2022 00:48:28 - INFO - __main__ - Global step 350 Train loss 2.981021 Classification-F1 0.03243243243243243 on epoch=87
06/01/2022 00:48:34 - INFO - __main__ - Step 360 Global step 360 Train loss 2.415195 on epoch=89
06/01/2022 00:48:40 - INFO - __main__ - Step 370 Global step 370 Train loss 3.512559 on epoch=92
06/01/2022 00:48:45 - INFO - __main__ - Step 380 Global step 380 Train loss 2.351122 on epoch=94
06/01/2022 00:48:50 - INFO - __main__ - Step 390 Global step 390 Train loss 1.943219 on epoch=97
06/01/2022 00:48:55 - INFO - __main__ - Step 400 Global step 400 Train loss 2.876646 on epoch=99
06/01/2022 00:48:57 - INFO - __main__ - Global step 400 Train loss 2.619748 Classification-F1 0.041666666666666664 on epoch=99
06/01/2022 00:49:03 - INFO - __main__ - Step 410 Global step 410 Train loss 2.565921 on epoch=102
06/01/2022 00:49:08 - INFO - __main__ - Step 420 Global step 420 Train loss 1.906818 on epoch=104
06/01/2022 00:49:13 - INFO - __main__ - Step 430 Global step 430 Train loss 1.821211 on epoch=107
06/01/2022 00:49:19 - INFO - __main__ - Step 440 Global step 440 Train loss 2.452087 on epoch=109
06/01/2022 00:49:24 - INFO - __main__ - Step 450 Global step 450 Train loss 1.485083 on epoch=112
06/01/2022 00:49:25 - INFO - __main__ - Global step 450 Train loss 2.046224 Classification-F1 0.06 on epoch=112
06/01/2022 00:49:31 - INFO - __main__ - Step 460 Global step 460 Train loss 2.050061 on epoch=114
06/01/2022 00:49:36 - INFO - __main__ - Step 470 Global step 470 Train loss 1.918435 on epoch=117
06/01/2022 00:49:42 - INFO - __main__ - Step 480 Global step 480 Train loss 1.692842 on epoch=119
06/01/2022 00:49:47 - INFO - __main__ - Step 490 Global step 490 Train loss 1.674585 on epoch=122
06/01/2022 00:49:52 - INFO - __main__ - Step 500 Global step 500 Train loss 1.424802 on epoch=124
06/01/2022 00:49:54 - INFO - __main__ - Global step 500 Train loss 1.752145 Classification-F1 0.3333333333333333 on epoch=124
06/01/2022 00:50:00 - INFO - __main__ - Step 510 Global step 510 Train loss 1.781109 on epoch=127
06/01/2022 00:50:05 - INFO - __main__ - Step 520 Global step 520 Train loss 1.376954 on epoch=129
06/01/2022 00:50:10 - INFO - __main__ - Step 530 Global step 530 Train loss 1.878795 on epoch=132
06/01/2022 00:50:15 - INFO - __main__ - Step 540 Global step 540 Train loss 1.792582 on epoch=134
06/01/2022 00:50:21 - INFO - __main__ - Step 550 Global step 550 Train loss 1.870545 on epoch=137
06/01/2022 00:50:22 - INFO - __main__ - Global step 550 Train loss 1.739997 Classification-F1 0.3333333333333333 on epoch=137
06/01/2022 00:50:27 - INFO - __main__ - Step 560 Global step 560 Train loss 1.823566 on epoch=139
06/01/2022 00:50:32 - INFO - __main__ - Step 570 Global step 570 Train loss 1.821944 on epoch=142
06/01/2022 00:50:37 - INFO - __main__ - Step 580 Global step 580 Train loss 1.709419 on epoch=144
06/01/2022 00:50:42 - INFO - __main__ - Step 590 Global step 590 Train loss 1.200708 on epoch=147
06/01/2022 00:50:48 - INFO - __main__ - Step 600 Global step 600 Train loss 2.050250 on epoch=149
06/01/2022 00:50:49 - INFO - __main__ - Global step 600 Train loss 1.721178 Classification-F1 0.3333333333333333 on epoch=149
06/01/2022 00:50:54 - INFO - __main__ - Step 610 Global step 610 Train loss 1.577815 on epoch=152
06/01/2022 00:51:00 - INFO - __main__ - Step 620 Global step 620 Train loss 1.851680 on epoch=154
06/01/2022 00:51:05 - INFO - __main__ - Step 630 Global step 630 Train loss 1.414037 on epoch=157
06/01/2022 00:51:10 - INFO - __main__ - Step 640 Global step 640 Train loss 1.423752 on epoch=159
06/01/2022 00:51:15 - INFO - __main__ - Step 650 Global step 650 Train loss 1.715318 on epoch=162
06/01/2022 00:51:17 - INFO - __main__ - Global step 650 Train loss 1.596520 Classification-F1 0.3333333333333333 on epoch=162
06/01/2022 00:51:22 - INFO - __main__ - Step 660 Global step 660 Train loss 1.255334 on epoch=164
06/01/2022 00:51:27 - INFO - __main__ - Step 670 Global step 670 Train loss 1.399568 on epoch=167
06/01/2022 00:51:32 - INFO - __main__ - Step 680 Global step 680 Train loss 1.257391 on epoch=169
06/01/2022 00:51:37 - INFO - __main__ - Step 690 Global step 690 Train loss 1.453166 on epoch=172
06/01/2022 00:51:43 - INFO - __main__ - Step 700 Global step 700 Train loss 1.146125 on epoch=174
06/01/2022 00:51:45 - INFO - __main__ - Global step 700 Train loss 1.302317 Classification-F1 0.3333333333333333 on epoch=174
06/01/2022 00:51:50 - INFO - __main__ - Step 710 Global step 710 Train loss 1.107052 on epoch=177
06/01/2022 00:51:55 - INFO - __main__ - Step 720 Global step 720 Train loss 1.143304 on epoch=179
06/01/2022 00:52:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.729041 on epoch=182
06/01/2022 00:52:05 - INFO - __main__ - Step 740 Global step 740 Train loss 1.138662 on epoch=184
06/01/2022 00:52:11 - INFO - __main__ - Step 750 Global step 750 Train loss 1.360917 on epoch=187
06/01/2022 00:52:13 - INFO - __main__ - Global step 750 Train loss 1.095795 Classification-F1 0.3333333333333333 on epoch=187
06/01/2022 00:52:18 - INFO - __main__ - Step 760 Global step 760 Train loss 1.290751 on epoch=189
06/01/2022 00:52:24 - INFO - __main__ - Step 770 Global step 770 Train loss 1.267536 on epoch=192
06/01/2022 00:52:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.741622 on epoch=194
06/01/2022 00:52:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.754710 on epoch=197
06/01/2022 00:52:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.612397 on epoch=199
06/01/2022 00:52:41 - INFO - __main__ - Global step 800 Train loss 0.933403 Classification-F1 0.41125541125541126 on epoch=199
06/01/2022 00:52:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.479958 on epoch=202
06/01/2022 00:52:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.456933 on epoch=204
06/01/2022 00:52:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.441012 on epoch=207
06/01/2022 00:53:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.504851 on epoch=209
06/01/2022 00:53:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.374407 on epoch=212
06/01/2022 00:53:10 - INFO - __main__ - Global step 850 Train loss 0.451432 Classification-F1 0.43333333333333335 on epoch=212
06/01/2022 00:53:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.514980 on epoch=214
06/01/2022 00:53:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.404249 on epoch=217
06/01/2022 00:53:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.385971 on epoch=219
06/01/2022 00:53:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.428771 on epoch=222
06/01/2022 00:53:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.379306 on epoch=224
06/01/2022 00:53:40 - INFO - __main__ - Global step 900 Train loss 0.422655 Classification-F1 0.4812085482682388 on epoch=224
06/01/2022 00:53:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.416590 on epoch=227
06/01/2022 00:53:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.358324 on epoch=229
06/01/2022 00:53:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.433145 on epoch=232
06/01/2022 00:54:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.398674 on epoch=234
06/01/2022 00:54:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.456288 on epoch=237
06/01/2022 00:54:08 - INFO - __main__ - Global step 950 Train loss 0.412604 Classification-F1 0.44976664210267747 on epoch=237
06/01/2022 00:54:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.387895 on epoch=239
06/01/2022 00:54:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.465484 on epoch=242
06/01/2022 00:54:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.446847 on epoch=244
06/01/2022 00:54:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.372195 on epoch=247
06/01/2022 00:54:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.511657 on epoch=249
06/01/2022 00:54:36 - INFO - __main__ - Global step 1000 Train loss 0.436816 Classification-F1 0.4102117061021171 on epoch=249
06/01/2022 00:54:36 - INFO - __main__ - save last model!
06/01/2022 00:54:43 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 00:54:44 - INFO - __main__ - Start tokenizing ... 1000 instances
06/01/2022 00:54:44 - INFO - __main__ - Printing 3 examples
06/01/2022 00:54:44 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/01/2022 00:54:44 - INFO - __main__ - ['negative']
06/01/2022 00:54:44 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/01/2022 00:54:44 - INFO - __main__ - ['negative']
06/01/2022 00:54:44 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/01/2022 00:54:44 - INFO - __main__ - ['negative']
06/01/2022 00:54:44 - INFO - __main__ - Tokenizing Input ...
06/01/2022 00:54:45 - INFO - __main__ - Tokenizing Output ...
06/01/2022 00:54:46 - INFO - __main__ - Loaded 1000 examples from test data
06/01/2022 00:55:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-amazon_polarity/amazon_polarity_32_87_0.0001_8_predictions.txt
06/01/2022 00:55:12 - INFO - __main__ - Classification-F1 on test data: 0.2512
06/01/2022 00:55:12 - INFO - __main__ - prefix=amazon_polarity_32_87, lr=0.0001, bsz=8, dev_performance=0.4812085482682388, test_performance=0.25120267474864094
