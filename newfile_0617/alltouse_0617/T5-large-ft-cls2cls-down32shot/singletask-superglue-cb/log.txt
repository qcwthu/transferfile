05/21/2022 21:19:22 - INFO - __main__ - Namespace(task_dir='data_32/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:19:22 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb
05/21/2022 21:19:22 - INFO - __main__ - Namespace(task_dir='data_32/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:19:22 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb
05/21/2022 21:19:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:19:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:19:23 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:19:23 - INFO - __main__ - Using 2 gpus
05/21/2022 21:19:23 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:19:23 - INFO - __main__ - Using 2 gpus
05/21/2022 21:19:23 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_32_100', 'superglue-cb_32_13', 'superglue-cb_32_21', 'superglue-cb_32_42', 'superglue-cb_32_87']
05/21/2022 21:19:23 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_32_100', 'superglue-cb_32_13', 'superglue-cb_32_21', 'superglue-cb_32_42', 'superglue-cb_32_87']
05/21/2022 21:19:28 - INFO - __main__ - Running ... prefix=superglue-cb_32_100, lr=0.0005, bsz=8 ...
05/30/2022 22:44:17 - INFO - __main__ - Namespace(task_dir='data_32/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/30/2022 22:44:17 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb
05/30/2022 22:44:17 - INFO - __main__ - Namespace(task_dir='data_32/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/30/2022 22:44:17 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb
05/30/2022 22:44:17 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/30/2022 22:44:17 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/30/2022 22:44:17 - INFO - __main__ - args.device: cuda:0
05/30/2022 22:44:17 - INFO - __main__ - Using 2 gpus
05/30/2022 22:44:17 - INFO - __main__ - args.device: cuda:1
05/30/2022 22:44:17 - INFO - __main__ - Using 2 gpus
05/30/2022 22:44:17 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_32_100', 'superglue-cb_32_13', 'superglue-cb_32_21', 'superglue-cb_32_42', 'superglue-cb_32_87']
05/30/2022 22:44:17 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_32_100', 'superglue-cb_32_13', 'superglue-cb_32_21', 'superglue-cb_32_42', 'superglue-cb_32_87']
05/30/2022 22:44:22 - INFO - __main__ - Running ... prefix=superglue-cb_32_100, lr=0.0005, bsz=8 ...
05/30/2022 22:44:23 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 22:44:23 - INFO - __main__ - Printing 3 examples
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:44:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:44:23 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 22:44:23 - INFO - __main__ - Printing 3 examples
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:44:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:44:23 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 22:44:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:44:23 - INFO - __main__ - Printing 3 examples
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: Meh ' Lindi did not develop the lower set of arms nor the bony, sinuous tail. Too much to expect a new pair of arms to grow out of her ribs, or her coccyx to elongate so enormously. Nor could Jaq imagine that she could attain the full strength of a purestrain Stealer - though her own strength was formidable even when unenhanced. [SEP] hypothesis: Meh ' Lindi could attain the full strength of a purestrain Stealer
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: He's weird enough to have undressed me without thinking, according to some mad notion of the ``proper'' thing to do. Perhaps he thought I couldn't lie in bed with my clothes on. [SEP] hypothesis: she couldn't lie in bed with her clothes on
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: B: And I don't think that rehabilitation is effective. A: Right. Have to agree with you, and I'm kind of in favor of capital punishment also. I just don't think that it acts much as a deterrent to these people because, uh, you still see them committing the same crimes, [SEP] hypothesis: it acts much as a deterrent to these people
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:44:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:44:23 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 22:44:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:44:23 - INFO - __main__ - Printing 3 examples
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: Meh ' Lindi did not develop the lower set of arms nor the bony, sinuous tail. Too much to expect a new pair of arms to grow out of her ribs, or her coccyx to elongate so enormously. Nor could Jaq imagine that she could attain the full strength of a purestrain Stealer - though her own strength was formidable even when unenhanced. [SEP] hypothesis: Meh ' Lindi could attain the full strength of a purestrain Stealer
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: He's weird enough to have undressed me without thinking, according to some mad notion of the ``proper'' thing to do. Perhaps he thought I couldn't lie in bed with my clothes on. [SEP] hypothesis: she couldn't lie in bed with her clothes on
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ -  [superglue-cb] premise: B: And I don't think that rehabilitation is effective. A: Right. Have to agree with you, and I'm kind of in favor of capital punishment also. I just don't think that it acts much as a deterrent to these people because, uh, you still see them committing the same crimes, [SEP] hypothesis: it acts much as a deterrent to these people
05/30/2022 22:44:23 - INFO - __main__ - ['contradiction']
05/30/2022 22:44:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:44:24 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:44:24 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:44:24 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:44:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 22:44:36 - INFO - __main__ - Starting training!
05/30/2022 22:44:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 22:44:37 - INFO - __main__ - Starting training!
05/30/2022 22:44:41 - INFO - __main__ - Step 10 Global step 10 Train loss 24.255579 on epoch=1
05/30/2022 22:44:46 - INFO - __main__ - Step 20 Global step 20 Train loss 19.694582 on epoch=3
05/30/2022 22:44:51 - INFO - __main__ - Step 30 Global step 30 Train loss 16.413164 on epoch=5
05/30/2022 22:44:56 - INFO - __main__ - Step 40 Global step 40 Train loss 10.763373 on epoch=7
05/30/2022 22:45:01 - INFO - __main__ - Step 50 Global step 50 Train loss 9.712717 on epoch=9
05/30/2022 22:45:02 - INFO - __main__ - Global step 50 Train loss 16.167883 ACC 0.0 on epoch=9
05/30/2022 22:45:09 - INFO - __main__ - Step 60 Global step 60 Train loss 8.300911 on epoch=11
05/30/2022 22:45:14 - INFO - __main__ - Step 70 Global step 70 Train loss 7.809079 on epoch=13
05/30/2022 22:45:20 - INFO - __main__ - Step 80 Global step 80 Train loss 6.590566 on epoch=15
05/30/2022 22:45:25 - INFO - __main__ - Step 90 Global step 90 Train loss 4.474981 on epoch=17
05/30/2022 22:45:30 - INFO - __main__ - Step 100 Global step 100 Train loss 3.788874 on epoch=19
05/30/2022 22:45:31 - INFO - __main__ - Global step 100 Train loss 6.192882 ACC 0.484375 on epoch=19
05/30/2022 22:45:39 - INFO - __main__ - Step 110 Global step 110 Train loss 2.669778 on epoch=21
05/30/2022 22:45:44 - INFO - __main__ - Step 120 Global step 120 Train loss 2.659234 on epoch=23
05/30/2022 22:45:50 - INFO - __main__ - Step 130 Global step 130 Train loss 2.095521 on epoch=25
05/30/2022 22:45:55 - INFO - __main__ - Step 140 Global step 140 Train loss 1.963847 on epoch=27
05/30/2022 22:46:00 - INFO - __main__ - Step 150 Global step 150 Train loss 2.044225 on epoch=29
05/30/2022 22:46:01 - INFO - __main__ - Global step 150 Train loss 2.286521 ACC 0.5 on epoch=29
05/30/2022 22:46:07 - INFO - __main__ - Step 160 Global step 160 Train loss 2.356644 on epoch=31
05/30/2022 22:46:13 - INFO - __main__ - Step 170 Global step 170 Train loss 1.707552 on epoch=33
05/30/2022 22:46:18 - INFO - __main__ - Step 180 Global step 180 Train loss 1.478994 on epoch=35
05/30/2022 22:46:23 - INFO - __main__ - Step 190 Global step 190 Train loss 1.653969 on epoch=37
05/30/2022 22:46:28 - INFO - __main__ - Step 200 Global step 200 Train loss 1.021017 on epoch=39
05/30/2022 22:46:29 - INFO - __main__ - Global step 200 Train loss 1.643635 ACC 0.5 on epoch=39
05/30/2022 22:46:35 - INFO - __main__ - Step 210 Global step 210 Train loss 1.495912 on epoch=41
05/30/2022 22:46:40 - INFO - __main__ - Step 220 Global step 220 Train loss 1.458545 on epoch=43
05/30/2022 22:46:45 - INFO - __main__ - Step 230 Global step 230 Train loss 1.079555 on epoch=45
05/30/2022 22:46:50 - INFO - __main__ - Step 240 Global step 240 Train loss 1.537016 on epoch=47
05/30/2022 22:46:55 - INFO - __main__ - Step 250 Global step 250 Train loss 1.039142 on epoch=49
05/30/2022 22:46:57 - INFO - __main__ - Global step 250 Train loss 1.322034 ACC 0.5 on epoch=49
05/30/2022 22:47:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.962570 on epoch=51
05/30/2022 22:47:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.994806 on epoch=53
05/30/2022 22:47:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.868076 on epoch=55
05/30/2022 22:47:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.985113 on epoch=57
05/30/2022 22:47:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.946040 on epoch=59
05/30/2022 22:47:24 - INFO - __main__ - Global step 300 Train loss 0.951321 ACC 0.5 on epoch=59
05/30/2022 22:47:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.943255 on epoch=61
05/30/2022 22:47:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.885977 on epoch=63
05/30/2022 22:47:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.811721 on epoch=65
05/30/2022 22:47:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.529114 on epoch=67
05/30/2022 22:47:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.690671 on epoch=69
05/30/2022 22:47:51 - INFO - __main__ - Global step 350 Train loss 0.772148 ACC 0.5 on epoch=69
05/30/2022 22:47:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.625728 on epoch=71
05/30/2022 22:48:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.678085 on epoch=73
05/30/2022 22:48:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.699221 on epoch=75
05/30/2022 22:48:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.555903 on epoch=77
05/30/2022 22:48:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.595226 on epoch=79
05/30/2022 22:48:18 - INFO - __main__ - Global step 400 Train loss 0.630832 ACC 0.5 on epoch=79
05/30/2022 22:48:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.607206 on epoch=81
05/30/2022 22:48:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.538345 on epoch=83
05/30/2022 22:48:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.466887 on epoch=85
05/30/2022 22:48:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.500171 on epoch=87
05/30/2022 22:48:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.528692 on epoch=89
05/30/2022 22:48:46 - INFO - __main__ - Global step 450 Train loss 0.528260 ACC 0.5 on epoch=89
05/30/2022 22:48:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.527598 on epoch=91
05/30/2022 22:48:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.532124 on epoch=93
05/30/2022 22:49:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.452022 on epoch=95
05/30/2022 22:49:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.543015 on epoch=97
05/30/2022 22:49:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.570774 on epoch=99
05/30/2022 22:49:13 - INFO - __main__ - Global step 500 Train loss 0.525107 ACC 0.5 on epoch=99
05/30/2022 22:49:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.504656 on epoch=101
05/30/2022 22:49:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.555563 on epoch=103
05/30/2022 22:49:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.501527 on epoch=105
05/30/2022 22:49:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.408314 on epoch=107
05/30/2022 22:49:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.446120 on epoch=109
05/30/2022 22:49:40 - INFO - __main__ - Global step 550 Train loss 0.483236 ACC 0.5 on epoch=109
05/30/2022 22:49:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.407840 on epoch=111
05/30/2022 22:49:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.372416 on epoch=113
05/30/2022 22:49:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.504537 on epoch=115
05/30/2022 22:50:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.381984 on epoch=117
05/30/2022 22:50:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.481497 on epoch=119
05/30/2022 22:50:08 - INFO - __main__ - Global step 600 Train loss 0.429655 ACC 0.5 on epoch=119
05/30/2022 22:50:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.375849 on epoch=121
05/30/2022 22:50:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.436684 on epoch=123
05/30/2022 22:50:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.425388 on epoch=125
05/30/2022 22:50:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.420265 on epoch=127
05/30/2022 22:50:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.443650 on epoch=129
05/30/2022 22:50:35 - INFO - __main__ - Global step 650 Train loss 0.420367 ACC 0.5 on epoch=129
05/30/2022 22:50:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.411569 on epoch=131
05/30/2022 22:50:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.398851 on epoch=133
05/30/2022 22:50:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.405108 on epoch=135
05/30/2022 22:50:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.419724 on epoch=137
05/30/2022 22:51:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.475451 on epoch=139
05/30/2022 22:51:02 - INFO - __main__ - Global step 700 Train loss 0.422141 ACC 0.5 on epoch=139
05/30/2022 22:51:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.407809 on epoch=141
05/30/2022 22:51:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.426033 on epoch=143
05/30/2022 22:51:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.402129 on epoch=145
05/30/2022 22:51:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.433842 on epoch=147
05/30/2022 22:51:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.385609 on epoch=149
05/30/2022 22:51:30 - INFO - __main__ - Global step 750 Train loss 0.411084 ACC 0.5 on epoch=149
05/30/2022 22:51:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.420584 on epoch=151
05/30/2022 22:51:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.428632 on epoch=153
05/30/2022 22:51:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.487178 on epoch=155
05/30/2022 22:51:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.383026 on epoch=157
05/30/2022 22:51:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.617157 on epoch=159
05/30/2022 22:51:57 - INFO - __main__ - Global step 800 Train loss 0.467315 ACC 0.5 on epoch=159
05/30/2022 22:52:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.381589 on epoch=161
05/30/2022 22:52:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.408723 on epoch=163
05/30/2022 22:52:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.359590 on epoch=165
05/30/2022 22:52:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.374493 on epoch=167
05/30/2022 22:52:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.383407 on epoch=169
05/30/2022 22:52:25 - INFO - __main__ - Global step 850 Train loss 0.381560 ACC 0.5 on epoch=169
05/30/2022 22:52:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.368066 on epoch=171
05/30/2022 22:52:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.412488 on epoch=173
05/30/2022 22:52:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.405025 on epoch=175
05/30/2022 22:52:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.324222 on epoch=177
05/30/2022 22:52:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.405934 on epoch=179
05/30/2022 22:52:52 - INFO - __main__ - Global step 900 Train loss 0.383147 ACC 0.5 on epoch=179
05/30/2022 22:52:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.417523 on epoch=181
05/30/2022 22:53:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.384443 on epoch=183
05/30/2022 22:53:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.363947 on epoch=185
05/30/2022 22:53:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.369683 on epoch=187
05/30/2022 22:53:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.376052 on epoch=189
05/30/2022 22:53:19 - INFO - __main__ - Global step 950 Train loss 0.382330 ACC 0.5 on epoch=189
05/30/2022 22:53:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.363134 on epoch=191
05/30/2022 22:53:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.392603 on epoch=193
05/30/2022 22:53:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.375450 on epoch=195
05/30/2022 22:53:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.383330 on epoch=197
05/30/2022 22:53:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.403964 on epoch=199
05/30/2022 22:53:47 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 22:53:47 - INFO - __main__ - Printing 3 examples
05/30/2022 22:53:47 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/30/2022 22:53:47 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:47 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/30/2022 22:53:47 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:47 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/30/2022 22:53:47 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:47 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:53:47 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:53:47 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 22:53:47 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:53:47 - INFO - __main__ - Printing 3 examples
05/30/2022 22:53:47 - INFO - __main__ -  [superglue-cb] premise: Meh ' Lindi did not develop the lower set of arms nor the bony, sinuous tail. Too much to expect a new pair of arms to grow out of her ribs, or her coccyx to elongate so enormously. Nor could Jaq imagine that she could attain the full strength of a purestrain Stealer - though her own strength was formidable even when unenhanced. [SEP] hypothesis: Meh ' Lindi could attain the full strength of a purestrain Stealer
05/30/2022 22:53:47 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:47 - INFO - __main__ -  [superglue-cb] premise: He's weird enough to have undressed me without thinking, according to some mad notion of the ``proper'' thing to do. Perhaps he thought I couldn't lie in bed with my clothes on. [SEP] hypothesis: she couldn't lie in bed with her clothes on
05/30/2022 22:53:47 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:47 - INFO - __main__ -  [superglue-cb] premise: B: And I don't think that rehabilitation is effective. A: Right. Have to agree with you, and I'm kind of in favor of capital punishment also. I just don't think that it acts much as a deterrent to these people because, uh, you still see them committing the same crimes, [SEP] hypothesis: it acts much as a deterrent to these people
05/30/2022 22:53:47 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:47 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:53:47 - INFO - __main__ - Global step 1000 Train loss 0.383696 ACC 0.5 on epoch=199
05/30/2022 22:53:47 - INFO - __main__ - save last model!
05/30/2022 22:53:47 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:53:47 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:53:54 - INFO - __main__ - Loading checkpoint on the fly
05/30/2022 22:53:55 - INFO - __main__ - Start tokenizing ... 56 instances
05/30/2022 22:53:55 - INFO - __main__ - Printing 3 examples
05/30/2022 22:53:55 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/30/2022 22:53:55 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:55 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/30/2022 22:53:55 - INFO - __main__ - ['neutral']
05/30/2022 22:53:55 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/30/2022 22:53:55 - INFO - __main__ - ['entailment']
05/30/2022 22:53:55 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:53:55 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:53:55 - INFO - __main__ - Loaded 56 examples from test data
05/30/2022 22:53:57 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_100_0.0005_8_predictions.txt
05/30/2022 22:53:57 - INFO - __main__ - ACC on test data: 0.5000
05/30/2022 22:53:57 - INFO - __main__ - prefix=superglue-cb_32_100, lr=0.0005, bsz=8, dev_performance=0.5, test_performance=0.5
05/30/2022 22:53:57 - INFO - __main__ - Running ... prefix=superglue-cb_32_100, lr=0.0003, bsz=8 ...
05/30/2022 22:53:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 22:53:57 - INFO - __main__ - Starting training!
05/30/2022 22:53:58 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 22:53:58 - INFO - __main__ - Printing 3 examples
05/30/2022 22:53:58 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/30/2022 22:53:58 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:58 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/30/2022 22:53:58 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:58 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/30/2022 22:53:58 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:53:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:53:58 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 22:53:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:53:58 - INFO - __main__ - Printing 3 examples
05/30/2022 22:53:58 - INFO - __main__ -  [superglue-cb] premise: Meh ' Lindi did not develop the lower set of arms nor the bony, sinuous tail. Too much to expect a new pair of arms to grow out of her ribs, or her coccyx to elongate so enormously. Nor could Jaq imagine that she could attain the full strength of a purestrain Stealer - though her own strength was formidable even when unenhanced. [SEP] hypothesis: Meh ' Lindi could attain the full strength of a purestrain Stealer
05/30/2022 22:53:58 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:58 - INFO - __main__ -  [superglue-cb] premise: He's weird enough to have undressed me without thinking, according to some mad notion of the ``proper'' thing to do. Perhaps he thought I couldn't lie in bed with my clothes on. [SEP] hypothesis: she couldn't lie in bed with her clothes on
05/30/2022 22:53:58 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:58 - INFO - __main__ -  [superglue-cb] premise: B: And I don't think that rehabilitation is effective. A: Right. Have to agree with you, and I'm kind of in favor of capital punishment also. I just don't think that it acts much as a deterrent to these people because, uh, you still see them committing the same crimes, [SEP] hypothesis: it acts much as a deterrent to these people
05/30/2022 22:53:58 - INFO - __main__ - ['contradiction']
05/30/2022 22:53:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:53:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:53:58 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:54:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 22:54:09 - INFO - __main__ - Starting training!
05/30/2022 22:54:13 - INFO - __main__ - Step 10 Global step 10 Train loss 23.284023 on epoch=1
05/30/2022 22:54:18 - INFO - __main__ - Step 20 Global step 20 Train loss 15.375636 on epoch=3
05/30/2022 22:54:23 - INFO - __main__ - Step 30 Global step 30 Train loss 11.996281 on epoch=5
05/30/2022 22:54:28 - INFO - __main__ - Step 40 Global step 40 Train loss 10.869279 on epoch=7
05/30/2022 22:54:33 - INFO - __main__ - Step 50 Global step 50 Train loss 9.506884 on epoch=9
05/30/2022 22:54:35 - INFO - __main__ - Global step 50 Train loss 14.206420 ACC 0.0 on epoch=9
05/30/2022 22:54:41 - INFO - __main__ - Step 60 Global step 60 Train loss 8.942504 on epoch=11
05/30/2022 22:54:46 - INFO - __main__ - Step 70 Global step 70 Train loss 8.609632 on epoch=13
05/30/2022 22:54:51 - INFO - __main__ - Step 80 Global step 80 Train loss 7.851437 on epoch=15
05/30/2022 22:54:56 - INFO - __main__ - Step 90 Global step 90 Train loss 6.898136 on epoch=17
05/30/2022 22:55:01 - INFO - __main__ - Step 100 Global step 100 Train loss 5.874821 on epoch=19
05/30/2022 22:55:03 - INFO - __main__ - Global step 100 Train loss 7.635306 ACC 0.0 on epoch=19
05/30/2022 22:55:08 - INFO - __main__ - Step 110 Global step 110 Train loss 5.639639 on epoch=21
05/30/2022 22:55:13 - INFO - __main__ - Step 120 Global step 120 Train loss 3.808573 on epoch=23
05/30/2022 22:55:18 - INFO - __main__ - Step 130 Global step 130 Train loss 3.474786 on epoch=25
05/30/2022 22:55:24 - INFO - __main__ - Step 140 Global step 140 Train loss 2.894911 on epoch=27
05/30/2022 22:55:29 - INFO - __main__ - Step 150 Global step 150 Train loss 2.872684 on epoch=29
05/30/2022 22:55:30 - INFO - __main__ - Global step 150 Train loss 3.738119 ACC 0.5 on epoch=29
05/30/2022 22:55:36 - INFO - __main__ - Step 160 Global step 160 Train loss 3.432538 on epoch=31
05/30/2022 22:55:41 - INFO - __main__ - Step 170 Global step 170 Train loss 2.845237 on epoch=33
05/30/2022 22:55:46 - INFO - __main__ - Step 180 Global step 180 Train loss 2.545235 on epoch=35
05/30/2022 22:55:51 - INFO - __main__ - Step 190 Global step 190 Train loss 2.226707 on epoch=37
05/30/2022 22:55:57 - INFO - __main__ - Step 200 Global step 200 Train loss 1.995336 on epoch=39
05/30/2022 22:55:58 - INFO - __main__ - Global step 200 Train loss 2.609011 ACC 0.5 on epoch=39
05/30/2022 22:56:03 - INFO - __main__ - Step 210 Global step 210 Train loss 2.055464 on epoch=41
05/30/2022 22:56:08 - INFO - __main__ - Step 220 Global step 220 Train loss 1.734050 on epoch=43
05/30/2022 22:56:13 - INFO - __main__ - Step 230 Global step 230 Train loss 1.564211 on epoch=45
05/30/2022 22:56:18 - INFO - __main__ - Step 240 Global step 240 Train loss 1.667966 on epoch=47
05/30/2022 22:56:23 - INFO - __main__ - Step 250 Global step 250 Train loss 1.533260 on epoch=49
05/30/2022 22:56:25 - INFO - __main__ - Global step 250 Train loss 1.710990 ACC 0.5 on epoch=49
05/30/2022 22:56:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.979050 on epoch=51
05/30/2022 22:56:35 - INFO - __main__ - Step 270 Global step 270 Train loss 1.490649 on epoch=53
05/30/2022 22:56:40 - INFO - __main__ - Step 280 Global step 280 Train loss 1.532730 on epoch=55
05/30/2022 22:56:45 - INFO - __main__ - Step 290 Global step 290 Train loss 1.508322 on epoch=57
05/30/2022 22:56:50 - INFO - __main__ - Step 300 Global step 300 Train loss 1.421253 on epoch=59
05/30/2022 22:56:52 - INFO - __main__ - Global step 300 Train loss 1.586401 ACC 0.5 on epoch=59
05/30/2022 22:56:57 - INFO - __main__ - Step 310 Global step 310 Train loss 1.191041 on epoch=61
05/30/2022 22:57:02 - INFO - __main__ - Step 320 Global step 320 Train loss 1.350427 on epoch=63
05/30/2022 22:57:07 - INFO - __main__ - Step 330 Global step 330 Train loss 1.306406 on epoch=65
05/30/2022 22:57:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.946180 on epoch=67
05/30/2022 22:57:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.909554 on epoch=69
05/30/2022 22:57:19 - INFO - __main__ - Global step 350 Train loss 1.140722 ACC 0.6875 on epoch=69
05/30/2022 22:57:25 - INFO - __main__ - Step 360 Global step 360 Train loss 1.270070 on epoch=71
05/30/2022 22:57:30 - INFO - __main__ - Step 370 Global step 370 Train loss 1.098260 on epoch=73
05/30/2022 22:57:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.857211 on epoch=75
05/30/2022 22:57:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.708267 on epoch=77
05/30/2022 22:57:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.612786 on epoch=79
05/30/2022 22:57:46 - INFO - __main__ - Global step 400 Train loss 0.909319 ACC 0.84375 on epoch=79
05/30/2022 22:57:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.731611 on epoch=81
05/30/2022 22:57:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.776158 on epoch=83
05/30/2022 22:58:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.666005 on epoch=85
05/30/2022 22:58:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.931943 on epoch=87
05/30/2022 22:58:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.428214 on epoch=89
05/30/2022 22:58:14 - INFO - __main__ - Global step 450 Train loss 0.706786 ACC 0.78125 on epoch=89
05/30/2022 22:58:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.448302 on epoch=91
05/30/2022 22:58:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.295197 on epoch=93
05/30/2022 22:58:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.278689 on epoch=95
05/30/2022 22:58:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.265557 on epoch=97
05/30/2022 22:58:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.165693 on epoch=99
05/30/2022 22:58:41 - INFO - __main__ - Global step 500 Train loss 0.290688 ACC 0.703125 on epoch=99
05/30/2022 22:58:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.076710 on epoch=101
05/30/2022 22:58:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.042837 on epoch=103
05/30/2022 22:58:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.030992 on epoch=105
05/30/2022 22:59:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.038560 on epoch=107
05/30/2022 22:59:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.016116 on epoch=109
05/30/2022 22:59:08 - INFO - __main__ - Global step 550 Train loss 0.041043 ACC 0.953125 on epoch=109
05/30/2022 22:59:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.022738 on epoch=111
05/30/2022 22:59:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.041294 on epoch=113
05/30/2022 22:59:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.005126 on epoch=115
05/30/2022 22:59:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.026869 on epoch=117
05/30/2022 22:59:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.016905 on epoch=119
05/30/2022 22:59:36 - INFO - __main__ - Global step 600 Train loss 0.022586 ACC 0.90625 on epoch=119
05/30/2022 22:59:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.004842 on epoch=121
05/30/2022 22:59:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.005939 on epoch=123
05/30/2022 22:59:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.003805 on epoch=125
05/30/2022 22:59:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000417 on epoch=127
05/30/2022 23:00:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001675 on epoch=129
05/30/2022 23:00:03 - INFO - __main__ - Global step 650 Train loss 0.003336 ACC 0.9375 on epoch=129
05/30/2022 23:00:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001359 on epoch=131
05/30/2022 23:00:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.104749 on epoch=133
05/30/2022 23:00:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.001149 on epoch=135
05/30/2022 23:00:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.022202 on epoch=137
05/30/2022 23:00:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.003529 on epoch=139
05/30/2022 23:00:30 - INFO - __main__ - Global step 700 Train loss 0.026598 ACC 0.9375 on epoch=139
05/30/2022 23:00:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000433 on epoch=141
05/30/2022 23:00:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.001075 on epoch=143
05/30/2022 23:00:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001047 on epoch=145
05/30/2022 23:00:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000891 on epoch=147
05/30/2022 23:00:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001223 on epoch=149
05/30/2022 23:00:57 - INFO - __main__ - Global step 750 Train loss 0.000934 ACC 0.9375 on epoch=149
05/30/2022 23:01:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000130 on epoch=151
05/30/2022 23:01:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000256 on epoch=153
05/30/2022 23:01:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000210 on epoch=155
05/30/2022 23:01:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000411 on epoch=157
05/30/2022 23:01:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.001201 on epoch=159
05/30/2022 23:01:24 - INFO - __main__ - Global step 800 Train loss 0.000442 ACC 0.90625 on epoch=159
05/30/2022 23:01:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.005772 on epoch=161
05/30/2022 23:01:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000364 on epoch=163
05/30/2022 23:01:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000113 on epoch=165
05/30/2022 23:01:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000083 on epoch=167
05/30/2022 23:01:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000121 on epoch=169
05/30/2022 23:01:51 - INFO - __main__ - Global step 850 Train loss 0.001291 ACC 0.9375 on epoch=169
05/30/2022 23:01:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000083 on epoch=171
05/30/2022 23:02:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000854 on epoch=173
05/30/2022 23:02:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001473 on epoch=175
05/30/2022 23:02:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000077 on epoch=177
05/30/2022 23:02:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000367 on epoch=179
05/30/2022 23:02:19 - INFO - __main__ - Global step 900 Train loss 0.000571 ACC 0.9375 on epoch=179
05/30/2022 23:02:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.010310 on epoch=181
05/30/2022 23:02:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000619 on epoch=183
05/30/2022 23:02:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.018447 on epoch=185
05/30/2022 23:02:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.002019 on epoch=187
05/30/2022 23:02:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000404 on epoch=189
05/30/2022 23:02:46 - INFO - __main__ - Global step 950 Train loss 0.006360 ACC 0.921875 on epoch=189
05/30/2022 23:02:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.002127 on epoch=191
05/30/2022 23:02:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000347 on epoch=193
05/30/2022 23:03:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.019862 on epoch=195
05/30/2022 23:03:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.002088 on epoch=197
05/30/2022 23:03:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.002115 on epoch=199
05/30/2022 23:03:13 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:03:13 - INFO - __main__ - Printing 3 examples
05/30/2022 23:03:13 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/30/2022 23:03:13 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:13 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/30/2022 23:03:13 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:13 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/30/2022 23:03:13 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:13 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:03:13 - INFO - __main__ - Global step 1000 Train loss 0.005308 ACC 0.9375 on epoch=199
05/30/2022 23:03:13 - INFO - __main__ - save last model!
05/30/2022 23:03:13 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:03:13 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:03:13 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:03:13 - INFO - __main__ - Printing 3 examples
05/30/2022 23:03:13 - INFO - __main__ -  [superglue-cb] premise: Meh ' Lindi did not develop the lower set of arms nor the bony, sinuous tail. Too much to expect a new pair of arms to grow out of her ribs, or her coccyx to elongate so enormously. Nor could Jaq imagine that she could attain the full strength of a purestrain Stealer - though her own strength was formidable even when unenhanced. [SEP] hypothesis: Meh ' Lindi could attain the full strength of a purestrain Stealer
05/30/2022 23:03:13 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:13 - INFO - __main__ -  [superglue-cb] premise: He's weird enough to have undressed me without thinking, according to some mad notion of the ``proper'' thing to do. Perhaps he thought I couldn't lie in bed with my clothes on. [SEP] hypothesis: she couldn't lie in bed with her clothes on
05/30/2022 23:03:13 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:13 - INFO - __main__ -  [superglue-cb] premise: B: And I don't think that rehabilitation is effective. A: Right. Have to agree with you, and I'm kind of in favor of capital punishment also. I just don't think that it acts much as a deterrent to these people because, uh, you still see them committing the same crimes, [SEP] hypothesis: it acts much as a deterrent to these people
05/30/2022 23:03:13 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:13 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:03:13 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:03:13 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:03:20 - INFO - __main__ - Loading checkpoint on the fly
05/30/2022 23:03:20 - INFO - __main__ - Start tokenizing ... 56 instances
05/30/2022 23:03:20 - INFO - __main__ - Printing 3 examples
05/30/2022 23:03:20 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/30/2022 23:03:20 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:20 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/30/2022 23:03:20 - INFO - __main__ - ['neutral']
05/30/2022 23:03:20 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/30/2022 23:03:20 - INFO - __main__ - ['entailment']
05/30/2022 23:03:20 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:03:20 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:03:20 - INFO - __main__ - Loaded 56 examples from test data
05/30/2022 23:03:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_100_0.0003_8_predictions.txt
05/30/2022 23:03:21 - INFO - __main__ - ACC on test data: 0.8929
05/30/2022 23:03:22 - INFO - __main__ - prefix=superglue-cb_32_100, lr=0.0003, bsz=8, dev_performance=0.953125, test_performance=0.8928571428571429
05/30/2022 23:03:22 - INFO - __main__ - Running ... prefix=superglue-cb_32_100, lr=0.0002, bsz=8 ...
05/30/2022 23:03:23 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:03:23 - INFO - __main__ - Printing 3 examples
05/30/2022 23:03:23 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/30/2022 23:03:23 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:23 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/30/2022 23:03:23 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:23 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/30/2022 23:03:23 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:03:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:03:23 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:03:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:03:23 - INFO - __main__ - Printing 3 examples
05/30/2022 23:03:23 - INFO - __main__ -  [superglue-cb] premise: Meh ' Lindi did not develop the lower set of arms nor the bony, sinuous tail. Too much to expect a new pair of arms to grow out of her ribs, or her coccyx to elongate so enormously. Nor could Jaq imagine that she could attain the full strength of a purestrain Stealer - though her own strength was formidable even when unenhanced. [SEP] hypothesis: Meh ' Lindi could attain the full strength of a purestrain Stealer
05/30/2022 23:03:23 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:23 - INFO - __main__ -  [superglue-cb] premise: He's weird enough to have undressed me without thinking, according to some mad notion of the ``proper'' thing to do. Perhaps he thought I couldn't lie in bed with my clothes on. [SEP] hypothesis: she couldn't lie in bed with her clothes on
05/30/2022 23:03:23 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:23 - INFO - __main__ -  [superglue-cb] premise: B: And I don't think that rehabilitation is effective. A: Right. Have to agree with you, and I'm kind of in favor of capital punishment also. I just don't think that it acts much as a deterrent to these people because, uh, you still see them committing the same crimes, [SEP] hypothesis: it acts much as a deterrent to these people
05/30/2022 23:03:23 - INFO - __main__ - ['contradiction']
05/30/2022 23:03:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:03:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:03:23 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:03:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:03:23 - INFO - __main__ - Starting training!
05/30/2022 23:03:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:03:36 - INFO - __main__ - Starting training!
05/30/2022 23:03:40 - INFO - __main__ - Step 10 Global step 10 Train loss 25.007811 on epoch=1
05/30/2022 23:03:45 - INFO - __main__ - Step 20 Global step 20 Train loss 17.959215 on epoch=3
05/30/2022 23:03:50 - INFO - __main__ - Step 30 Global step 30 Train loss 11.759418 on epoch=5
05/30/2022 23:03:56 - INFO - __main__ - Step 40 Global step 40 Train loss 10.649616 on epoch=7
05/30/2022 23:04:01 - INFO - __main__ - Step 50 Global step 50 Train loss 10.466768 on epoch=9
05/30/2022 23:04:02 - INFO - __main__ - Global step 50 Train loss 15.168567 ACC 0.0 on epoch=9
05/30/2022 23:04:08 - INFO - __main__ - Step 60 Global step 60 Train loss 10.492109 on epoch=11
05/30/2022 23:04:13 - INFO - __main__ - Step 70 Global step 70 Train loss 9.870824 on epoch=13
05/30/2022 23:04:18 - INFO - __main__ - Step 80 Global step 80 Train loss 9.307386 on epoch=15
05/30/2022 23:04:23 - INFO - __main__ - Step 90 Global step 90 Train loss 9.179634 on epoch=17
05/30/2022 23:04:29 - INFO - __main__ - Step 100 Global step 100 Train loss 8.219693 on epoch=19
05/30/2022 23:04:30 - INFO - __main__ - Global step 100 Train loss 9.413929 ACC 0.0 on epoch=19
05/30/2022 23:04:35 - INFO - __main__ - Step 110 Global step 110 Train loss 7.996203 on epoch=21
05/30/2022 23:04:40 - INFO - __main__ - Step 120 Global step 120 Train loss 7.233298 on epoch=23
05/30/2022 23:04:46 - INFO - __main__ - Step 130 Global step 130 Train loss 6.987913 on epoch=25
05/30/2022 23:04:51 - INFO - __main__ - Step 140 Global step 140 Train loss 6.144531 on epoch=27
05/30/2022 23:04:56 - INFO - __main__ - Step 150 Global step 150 Train loss 5.111751 on epoch=29
05/30/2022 23:04:57 - INFO - __main__ - Global step 150 Train loss 6.694739 ACC 0.0 on epoch=29
05/30/2022 23:05:02 - INFO - __main__ - Step 160 Global step 160 Train loss 4.034647 on epoch=31
05/30/2022 23:05:08 - INFO - __main__ - Step 170 Global step 170 Train loss 3.857364 on epoch=33
05/30/2022 23:05:13 - INFO - __main__ - Step 180 Global step 180 Train loss 3.388914 on epoch=35
05/30/2022 23:05:18 - INFO - __main__ - Step 190 Global step 190 Train loss 3.231439 on epoch=37
05/30/2022 23:05:23 - INFO - __main__ - Step 200 Global step 200 Train loss 3.353878 on epoch=39
05/30/2022 23:05:24 - INFO - __main__ - Global step 200 Train loss 3.573249 ACC 0.5 on epoch=39
05/30/2022 23:05:30 - INFO - __main__ - Step 210 Global step 210 Train loss 2.174588 on epoch=41
05/30/2022 23:05:35 - INFO - __main__ - Step 220 Global step 220 Train loss 2.729109 on epoch=43
05/30/2022 23:05:40 - INFO - __main__ - Step 230 Global step 230 Train loss 2.310253 on epoch=45
05/30/2022 23:05:46 - INFO - __main__ - Step 240 Global step 240 Train loss 2.350976 on epoch=47
05/30/2022 23:05:51 - INFO - __main__ - Step 250 Global step 250 Train loss 2.594239 on epoch=49
05/30/2022 23:05:52 - INFO - __main__ - Global step 250 Train loss 2.431833 ACC 0.5 on epoch=49
05/30/2022 23:05:57 - INFO - __main__ - Step 260 Global step 260 Train loss 2.420866 on epoch=51
05/30/2022 23:06:02 - INFO - __main__ - Step 270 Global step 270 Train loss 1.841338 on epoch=53
05/30/2022 23:06:07 - INFO - __main__ - Step 280 Global step 280 Train loss 2.157876 on epoch=55
05/30/2022 23:06:13 - INFO - __main__ - Step 290 Global step 290 Train loss 2.326894 on epoch=57
05/30/2022 23:06:17 - INFO - __main__ - Step 300 Global step 300 Train loss 2.130599 on epoch=59
05/30/2022 23:06:18 - INFO - __main__ - Global step 300 Train loss 2.175514 ACC 0.5 on epoch=59
05/30/2022 23:06:24 - INFO - __main__ - Step 310 Global step 310 Train loss 2.000782 on epoch=61
05/30/2022 23:06:29 - INFO - __main__ - Step 320 Global step 320 Train loss 1.543853 on epoch=63
05/30/2022 23:06:34 - INFO - __main__ - Step 330 Global step 330 Train loss 1.956066 on epoch=65
05/30/2022 23:06:39 - INFO - __main__ - Step 340 Global step 340 Train loss 2.141491 on epoch=67
05/30/2022 23:06:44 - INFO - __main__ - Step 350 Global step 350 Train loss 1.911961 on epoch=69
05/30/2022 23:06:45 - INFO - __main__ - Global step 350 Train loss 1.910831 ACC 0.5 on epoch=69
05/30/2022 23:06:51 - INFO - __main__ - Step 360 Global step 360 Train loss 1.608517 on epoch=71
05/30/2022 23:06:56 - INFO - __main__ - Step 370 Global step 370 Train loss 1.579114 on epoch=73
05/30/2022 23:07:01 - INFO - __main__ - Step 380 Global step 380 Train loss 2.031198 on epoch=75
05/30/2022 23:07:06 - INFO - __main__ - Step 390 Global step 390 Train loss 1.266369 on epoch=77
05/30/2022 23:07:11 - INFO - __main__ - Step 400 Global step 400 Train loss 1.505558 on epoch=79
05/30/2022 23:07:13 - INFO - __main__ - Global step 400 Train loss 1.598151 ACC 0.5 on epoch=79
05/30/2022 23:07:18 - INFO - __main__ - Step 410 Global step 410 Train loss 1.684233 on epoch=81
05/30/2022 23:07:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.966913 on epoch=83
05/30/2022 23:07:28 - INFO - __main__ - Step 430 Global step 430 Train loss 1.604745 on epoch=85
05/30/2022 23:07:33 - INFO - __main__ - Step 440 Global step 440 Train loss 1.245246 on epoch=87
05/30/2022 23:07:38 - INFO - __main__ - Step 450 Global step 450 Train loss 1.227463 on epoch=89
05/30/2022 23:07:40 - INFO - __main__ - Global step 450 Train loss 1.345720 ACC 0.5 on epoch=89
05/30/2022 23:07:45 - INFO - __main__ - Step 460 Global step 460 Train loss 1.160958 on epoch=91
05/30/2022 23:07:50 - INFO - __main__ - Step 470 Global step 470 Train loss 1.140267 on epoch=93
05/30/2022 23:07:55 - INFO - __main__ - Step 480 Global step 480 Train loss 1.174727 on epoch=95
05/30/2022 23:08:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.948051 on epoch=97
05/30/2022 23:08:06 - INFO - __main__ - Step 500 Global step 500 Train loss 1.013853 on epoch=99
05/30/2022 23:08:07 - INFO - __main__ - Global step 500 Train loss 1.087571 ACC 0.5 on epoch=99
05/30/2022 23:08:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.984636 on epoch=101
05/30/2022 23:08:17 - INFO - __main__ - Step 520 Global step 520 Train loss 1.057740 on epoch=103
05/30/2022 23:08:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.948565 on epoch=105
05/30/2022 23:08:28 - INFO - __main__ - Step 540 Global step 540 Train loss 1.116456 on epoch=107
05/30/2022 23:08:33 - INFO - __main__ - Step 550 Global step 550 Train loss 1.144010 on epoch=109
05/30/2022 23:08:34 - INFO - __main__ - Global step 550 Train loss 1.050281 ACC 0.640625 on epoch=109
05/30/2022 23:08:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.962707 on epoch=111
05/30/2022 23:08:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.852020 on epoch=113
05/30/2022 23:08:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.849615 on epoch=115
05/30/2022 23:08:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.806469 on epoch=117
05/30/2022 23:09:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.821239 on epoch=119
05/30/2022 23:09:02 - INFO - __main__ - Global step 600 Train loss 0.858410 ACC 0.515625 on epoch=119
05/30/2022 23:09:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.804268 on epoch=121
05/30/2022 23:09:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.737550 on epoch=123
05/30/2022 23:09:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.635141 on epoch=125
05/30/2022 23:09:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.618873 on epoch=127
05/30/2022 23:09:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.519392 on epoch=129
05/30/2022 23:09:29 - INFO - __main__ - Global step 650 Train loss 0.663045 ACC 0.828125 on epoch=129
05/30/2022 23:09:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.584419 on epoch=131
05/30/2022 23:09:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.443755 on epoch=133
05/30/2022 23:09:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.580851 on epoch=135
05/30/2022 23:09:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.516898 on epoch=137
05/30/2022 23:09:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.345402 on epoch=139
05/30/2022 23:09:57 - INFO - __main__ - Global step 700 Train loss 0.494265 ACC 0.875 on epoch=139
05/30/2022 23:10:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.347642 on epoch=141
05/30/2022 23:10:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.254109 on epoch=143
05/30/2022 23:10:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.257788 on epoch=145
05/30/2022 23:10:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.252447 on epoch=147
05/30/2022 23:10:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.165298 on epoch=149
05/30/2022 23:10:24 - INFO - __main__ - Global step 750 Train loss 0.255457 ACC 0.90625 on epoch=149
05/30/2022 23:10:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.081949 on epoch=151
05/30/2022 23:10:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.067890 on epoch=153
05/30/2022 23:10:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.087738 on epoch=155
05/30/2022 23:10:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.070224 on epoch=157
05/30/2022 23:10:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.038411 on epoch=159
05/30/2022 23:10:52 - INFO - __main__ - Global step 800 Train loss 0.069242 ACC 0.890625 on epoch=159
05/30/2022 23:10:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.039247 on epoch=161
05/30/2022 23:11:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.018530 on epoch=163
05/30/2022 23:11:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.009510 on epoch=165
05/30/2022 23:11:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.041106 on epoch=167
05/30/2022 23:11:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.010916 on epoch=169
05/30/2022 23:11:19 - INFO - __main__ - Global step 850 Train loss 0.023862 ACC 0.953125 on epoch=169
05/30/2022 23:11:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.010312 on epoch=171
05/30/2022 23:11:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.034738 on epoch=173
05/30/2022 23:11:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.012054 on epoch=175
05/30/2022 23:11:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.018434 on epoch=177
05/30/2022 23:11:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.005613 on epoch=179
05/30/2022 23:11:47 - INFO - __main__ - Global step 900 Train loss 0.016230 ACC 0.875 on epoch=179
05/30/2022 23:11:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.009223 on epoch=181
05/30/2022 23:11:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.004134 on epoch=183
05/30/2022 23:12:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.035205 on epoch=185
05/30/2022 23:12:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.003795 on epoch=187
05/30/2022 23:12:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.007991 on epoch=189
05/30/2022 23:12:14 - INFO - __main__ - Global step 950 Train loss 0.012070 ACC 0.90625 on epoch=189
05/30/2022 23:12:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.028189 on epoch=191
05/30/2022 23:12:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.020074 on epoch=193
05/30/2022 23:12:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.020529 on epoch=195
05/30/2022 23:12:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.002617 on epoch=197
05/30/2022 23:12:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.011358 on epoch=199
05/30/2022 23:12:41 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:12:41 - INFO - __main__ - Printing 3 examples
05/30/2022 23:12:41 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/30/2022 23:12:41 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:41 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/30/2022 23:12:41 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:41 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/30/2022 23:12:41 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:41 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:12:41 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:12:41 - INFO - __main__ - Global step 1000 Train loss 0.016553 ACC 0.828125 on epoch=199
05/30/2022 23:12:41 - INFO - __main__ - save last model!
05/30/2022 23:12:41 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:12:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:12:41 - INFO - __main__ - Printing 3 examples
05/30/2022 23:12:41 - INFO - __main__ -  [superglue-cb] premise: Meh ' Lindi did not develop the lower set of arms nor the bony, sinuous tail. Too much to expect a new pair of arms to grow out of her ribs, or her coccyx to elongate so enormously. Nor could Jaq imagine that she could attain the full strength of a purestrain Stealer - though her own strength was formidable even when unenhanced. [SEP] hypothesis: Meh ' Lindi could attain the full strength of a purestrain Stealer
05/30/2022 23:12:41 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:41 - INFO - __main__ -  [superglue-cb] premise: He's weird enough to have undressed me without thinking, according to some mad notion of the ``proper'' thing to do. Perhaps he thought I couldn't lie in bed with my clothes on. [SEP] hypothesis: she couldn't lie in bed with her clothes on
05/30/2022 23:12:41 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:41 - INFO - __main__ -  [superglue-cb] premise: B: And I don't think that rehabilitation is effective. A: Right. Have to agree with you, and I'm kind of in favor of capital punishment also. I just don't think that it acts much as a deterrent to these people because, uh, you still see them committing the same crimes, [SEP] hypothesis: it acts much as a deterrent to these people
05/30/2022 23:12:41 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:41 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:12:42 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:12:42 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:12:48 - INFO - __main__ - Loading checkpoint on the fly
05/30/2022 23:12:49 - INFO - __main__ - Start tokenizing ... 56 instances
05/30/2022 23:12:49 - INFO - __main__ - Printing 3 examples
05/30/2022 23:12:49 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/30/2022 23:12:49 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:49 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/30/2022 23:12:49 - INFO - __main__ - ['neutral']
05/30/2022 23:12:49 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/30/2022 23:12:49 - INFO - __main__ - ['entailment']
05/30/2022 23:12:49 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:12:49 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:12:49 - INFO - __main__ - Loaded 56 examples from test data
05/30/2022 23:12:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_100_0.0002_8_predictions.txt
05/30/2022 23:12:51 - INFO - __main__ - ACC on test data: 0.8750
05/30/2022 23:12:51 - INFO - __main__ - prefix=superglue-cb_32_100, lr=0.0002, bsz=8, dev_performance=0.953125, test_performance=0.875
05/30/2022 23:12:51 - INFO - __main__ - Running ... prefix=superglue-cb_32_100, lr=0.0001, bsz=8 ...
05/30/2022 23:12:52 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:12:52 - INFO - __main__ - Printing 3 examples
05/30/2022 23:12:52 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/30/2022 23:12:52 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:52 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/30/2022 23:12:52 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:52 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/30/2022 23:12:52 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:52 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:12:52 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:12:52 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:12:52 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:12:52 - INFO - __main__ - Printing 3 examples
05/30/2022 23:12:52 - INFO - __main__ -  [superglue-cb] premise: Meh ' Lindi did not develop the lower set of arms nor the bony, sinuous tail. Too much to expect a new pair of arms to grow out of her ribs, or her coccyx to elongate so enormously. Nor could Jaq imagine that she could attain the full strength of a purestrain Stealer - though her own strength was formidable even when unenhanced. [SEP] hypothesis: Meh ' Lindi could attain the full strength of a purestrain Stealer
05/30/2022 23:12:52 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:52 - INFO - __main__ -  [superglue-cb] premise: He's weird enough to have undressed me without thinking, according to some mad notion of the ``proper'' thing to do. Perhaps he thought I couldn't lie in bed with my clothes on. [SEP] hypothesis: she couldn't lie in bed with her clothes on
05/30/2022 23:12:52 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:52 - INFO - __main__ -  [superglue-cb] premise: B: And I don't think that rehabilitation is effective. A: Right. Have to agree with you, and I'm kind of in favor of capital punishment also. I just don't think that it acts much as a deterrent to these people because, uh, you still see them committing the same crimes, [SEP] hypothesis: it acts much as a deterrent to these people
05/30/2022 23:12:52 - INFO - __main__ - ['contradiction']
05/30/2022 23:12:52 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:12:52 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:12:52 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:12:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:12:53 - INFO - __main__ - Starting training!
05/30/2022 23:13:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:13:05 - INFO - __main__ - Starting training!
05/30/2022 23:13:09 - INFO - __main__ - Step 10 Global step 10 Train loss 24.759338 on epoch=1
05/30/2022 23:13:14 - INFO - __main__ - Step 20 Global step 20 Train loss 23.039448 on epoch=3
05/30/2022 23:13:19 - INFO - __main__ - Step 30 Global step 30 Train loss 18.509146 on epoch=5
05/30/2022 23:13:24 - INFO - __main__ - Step 40 Global step 40 Train loss 14.854342 on epoch=7
05/30/2022 23:13:29 - INFO - __main__ - Step 50 Global step 50 Train loss 13.977008 on epoch=9
05/30/2022 23:13:37 - INFO - __main__ - Global step 50 Train loss 19.027857 ACC 0.140625 on epoch=9
05/30/2022 23:13:43 - INFO - __main__ - Step 60 Global step 60 Train loss 13.401297 on epoch=11
05/30/2022 23:13:48 - INFO - __main__ - Step 70 Global step 70 Train loss 12.862555 on epoch=13
05/30/2022 23:13:53 - INFO - __main__ - Step 80 Global step 80 Train loss 11.678218 on epoch=15
05/30/2022 23:13:58 - INFO - __main__ - Step 90 Global step 90 Train loss 11.708345 on epoch=17
05/30/2022 23:14:04 - INFO - __main__ - Step 100 Global step 100 Train loss 10.629847 on epoch=19
05/30/2022 23:14:05 - INFO - __main__ - Global step 100 Train loss 12.056052 ACC 0.0 on epoch=19
05/30/2022 23:14:10 - INFO - __main__ - Step 110 Global step 110 Train loss 10.681534 on epoch=21
05/30/2022 23:14:15 - INFO - __main__ - Step 120 Global step 120 Train loss 10.321474 on epoch=23
05/30/2022 23:14:20 - INFO - __main__ - Step 130 Global step 130 Train loss 10.223143 on epoch=25
05/30/2022 23:14:26 - INFO - __main__ - Step 140 Global step 140 Train loss 10.245176 on epoch=27
05/30/2022 23:14:31 - INFO - __main__ - Step 150 Global step 150 Train loss 9.838064 on epoch=29
05/30/2022 23:14:32 - INFO - __main__ - Global step 150 Train loss 10.261879 ACC 0.0 on epoch=29
05/30/2022 23:14:37 - INFO - __main__ - Step 160 Global step 160 Train loss 9.061427 on epoch=31
05/30/2022 23:14:43 - INFO - __main__ - Step 170 Global step 170 Train loss 8.907742 on epoch=33
05/30/2022 23:14:48 - INFO - __main__ - Step 180 Global step 180 Train loss 8.792087 on epoch=35
05/30/2022 23:14:53 - INFO - __main__ - Step 190 Global step 190 Train loss 8.418810 on epoch=37
05/30/2022 23:14:58 - INFO - __main__ - Step 200 Global step 200 Train loss 7.724550 on epoch=39
05/30/2022 23:14:59 - INFO - __main__ - Global step 200 Train loss 8.580923 ACC 0.0 on epoch=39
05/30/2022 23:15:05 - INFO - __main__ - Step 210 Global step 210 Train loss 7.800076 on epoch=41
05/30/2022 23:15:10 - INFO - __main__ - Step 220 Global step 220 Train loss 8.185107 on epoch=43
05/30/2022 23:15:15 - INFO - __main__ - Step 230 Global step 230 Train loss 7.790102 on epoch=45
05/30/2022 23:15:20 - INFO - __main__ - Step 240 Global step 240 Train loss 7.434108 on epoch=47
05/30/2022 23:15:25 - INFO - __main__ - Step 250 Global step 250 Train loss 7.110407 on epoch=49
05/30/2022 23:15:26 - INFO - __main__ - Global step 250 Train loss 7.663960 ACC 0.0 on epoch=49
05/30/2022 23:15:32 - INFO - __main__ - Step 260 Global step 260 Train loss 6.473402 on epoch=51
05/30/2022 23:15:37 - INFO - __main__ - Step 270 Global step 270 Train loss 6.501981 on epoch=53
05/30/2022 23:15:42 - INFO - __main__ - Step 280 Global step 280 Train loss 6.740843 on epoch=55
05/30/2022 23:15:47 - INFO - __main__ - Step 290 Global step 290 Train loss 6.071262 on epoch=57
05/30/2022 23:15:52 - INFO - __main__ - Step 300 Global step 300 Train loss 5.496497 on epoch=59
05/30/2022 23:15:54 - INFO - __main__ - Global step 300 Train loss 6.256797 ACC 0.0 on epoch=59
05/30/2022 23:15:59 - INFO - __main__ - Step 310 Global step 310 Train loss 5.401651 on epoch=61
05/30/2022 23:16:04 - INFO - __main__ - Step 320 Global step 320 Train loss 5.612926 on epoch=63
05/30/2022 23:16:09 - INFO - __main__ - Step 330 Global step 330 Train loss 4.755998 on epoch=65
05/30/2022 23:16:14 - INFO - __main__ - Step 340 Global step 340 Train loss 4.540236 on epoch=67
05/30/2022 23:16:20 - INFO - __main__ - Step 350 Global step 350 Train loss 4.098854 on epoch=69
05/30/2022 23:16:21 - INFO - __main__ - Global step 350 Train loss 4.881933 ACC 0.0 on epoch=69
05/30/2022 23:16:26 - INFO - __main__ - Step 360 Global step 360 Train loss 3.681477 on epoch=71
05/30/2022 23:16:31 - INFO - __main__ - Step 370 Global step 370 Train loss 3.331874 on epoch=73
05/30/2022 23:16:36 - INFO - __main__ - Step 380 Global step 380 Train loss 3.091654 on epoch=75
05/30/2022 23:16:42 - INFO - __main__ - Step 390 Global step 390 Train loss 2.557808 on epoch=77
05/30/2022 23:16:47 - INFO - __main__ - Step 400 Global step 400 Train loss 3.084430 on epoch=79
05/30/2022 23:16:48 - INFO - __main__ - Global step 400 Train loss 3.149449 ACC 0.015625 on epoch=79
05/30/2022 23:16:53 - INFO - __main__ - Step 410 Global step 410 Train loss 2.349218 on epoch=81
05/30/2022 23:16:58 - INFO - __main__ - Step 420 Global step 420 Train loss 2.363251 on epoch=83
05/30/2022 23:17:04 - INFO - __main__ - Step 430 Global step 430 Train loss 2.406629 on epoch=85
05/30/2022 23:17:09 - INFO - __main__ - Step 440 Global step 440 Train loss 2.391510 on epoch=87
05/30/2022 23:17:14 - INFO - __main__ - Step 450 Global step 450 Train loss 2.492188 on epoch=89
05/30/2022 23:17:15 - INFO - __main__ - Global step 450 Train loss 2.400559 ACC 0.5 on epoch=89
05/30/2022 23:17:21 - INFO - __main__ - Step 460 Global step 460 Train loss 1.937157 on epoch=91
05/30/2022 23:17:26 - INFO - __main__ - Step 470 Global step 470 Train loss 2.010177 on epoch=93
05/30/2022 23:17:31 - INFO - __main__ - Step 480 Global step 480 Train loss 2.445729 on epoch=95
05/30/2022 23:17:36 - INFO - __main__ - Step 490 Global step 490 Train loss 1.798006 on epoch=97
05/30/2022 23:17:41 - INFO - __main__ - Step 500 Global step 500 Train loss 1.632803 on epoch=99
05/30/2022 23:17:43 - INFO - __main__ - Global step 500 Train loss 1.964775 ACC 0.5 on epoch=99
05/30/2022 23:17:48 - INFO - __main__ - Step 510 Global step 510 Train loss 2.437650 on epoch=101
05/30/2022 23:17:53 - INFO - __main__ - Step 520 Global step 520 Train loss 2.523113 on epoch=103
05/30/2022 23:17:58 - INFO - __main__ - Step 530 Global step 530 Train loss 1.899427 on epoch=105
05/30/2022 23:18:03 - INFO - __main__ - Step 540 Global step 540 Train loss 2.143895 on epoch=107
05/30/2022 23:18:09 - INFO - __main__ - Step 550 Global step 550 Train loss 1.969317 on epoch=109
05/30/2022 23:18:10 - INFO - __main__ - Global step 550 Train loss 2.194680 ACC 0.5 on epoch=109
05/30/2022 23:18:15 - INFO - __main__ - Step 560 Global step 560 Train loss 2.111335 on epoch=111
05/30/2022 23:18:20 - INFO - __main__ - Step 570 Global step 570 Train loss 2.065564 on epoch=113
05/30/2022 23:18:25 - INFO - __main__ - Step 580 Global step 580 Train loss 2.192384 on epoch=115
05/30/2022 23:18:31 - INFO - __main__ - Step 590 Global step 590 Train loss 1.823439 on epoch=117
05/30/2022 23:18:36 - INFO - __main__ - Step 600 Global step 600 Train loss 1.771711 on epoch=119
05/30/2022 23:18:37 - INFO - __main__ - Global step 600 Train loss 1.992887 ACC 0.5 on epoch=119
05/30/2022 23:18:42 - INFO - __main__ - Step 610 Global step 610 Train loss 1.982296 on epoch=121
05/30/2022 23:18:47 - INFO - __main__ - Step 620 Global step 620 Train loss 1.744178 on epoch=123
05/30/2022 23:18:53 - INFO - __main__ - Step 630 Global step 630 Train loss 1.523442 on epoch=125
05/30/2022 23:18:58 - INFO - __main__ - Step 640 Global step 640 Train loss 1.697533 on epoch=127
05/30/2022 23:19:03 - INFO - __main__ - Step 650 Global step 650 Train loss 1.620432 on epoch=129
05/30/2022 23:19:04 - INFO - __main__ - Global step 650 Train loss 1.713576 ACC 0.5 on epoch=129
05/30/2022 23:19:09 - INFO - __main__ - Step 660 Global step 660 Train loss 1.603279 on epoch=131
05/30/2022 23:19:15 - INFO - __main__ - Step 670 Global step 670 Train loss 1.725219 on epoch=133
05/30/2022 23:19:20 - INFO - __main__ - Step 680 Global step 680 Train loss 1.947317 on epoch=135
05/30/2022 23:19:25 - INFO - __main__ - Step 690 Global step 690 Train loss 1.819192 on epoch=137
05/30/2022 23:19:30 - INFO - __main__ - Step 700 Global step 700 Train loss 1.528372 on epoch=139
05/30/2022 23:19:31 - INFO - __main__ - Global step 700 Train loss 1.724676 ACC 0.5 on epoch=139
05/30/2022 23:19:36 - INFO - __main__ - Step 710 Global step 710 Train loss 1.037515 on epoch=141
05/30/2022 23:19:42 - INFO - __main__ - Step 720 Global step 720 Train loss 1.444605 on epoch=143
05/30/2022 23:19:47 - INFO - __main__ - Step 730 Global step 730 Train loss 1.400692 on epoch=145
05/30/2022 23:19:52 - INFO - __main__ - Step 740 Global step 740 Train loss 1.343467 on epoch=147
05/30/2022 23:19:57 - INFO - __main__ - Step 750 Global step 750 Train loss 1.495162 on epoch=149
05/30/2022 23:19:58 - INFO - __main__ - Global step 750 Train loss 1.344288 ACC 0.5 on epoch=149
05/30/2022 23:20:03 - INFO - __main__ - Step 760 Global step 760 Train loss 1.570766 on epoch=151
05/30/2022 23:20:08 - INFO - __main__ - Step 770 Global step 770 Train loss 1.382927 on epoch=153
05/30/2022 23:20:14 - INFO - __main__ - Step 780 Global step 780 Train loss 1.330135 on epoch=155
05/30/2022 23:20:19 - INFO - __main__ - Step 790 Global step 790 Train loss 1.250958 on epoch=157
05/30/2022 23:20:24 - INFO - __main__ - Step 800 Global step 800 Train loss 1.421656 on epoch=159
05/30/2022 23:20:25 - INFO - __main__ - Global step 800 Train loss 1.391288 ACC 0.5 on epoch=159
05/30/2022 23:20:30 - INFO - __main__ - Step 810 Global step 810 Train loss 1.369219 on epoch=161
05/30/2022 23:20:36 - INFO - __main__ - Step 820 Global step 820 Train loss 1.439987 on epoch=163
05/30/2022 23:20:41 - INFO - __main__ - Step 830 Global step 830 Train loss 1.229194 on epoch=165
05/30/2022 23:20:46 - INFO - __main__ - Step 840 Global step 840 Train loss 1.208255 on epoch=167
05/30/2022 23:20:51 - INFO - __main__ - Step 850 Global step 850 Train loss 1.473133 on epoch=169
05/30/2022 23:20:53 - INFO - __main__ - Global step 850 Train loss 1.343958 ACC 0.5 on epoch=169
05/30/2022 23:20:58 - INFO - __main__ - Step 860 Global step 860 Train loss 1.114657 on epoch=171
05/30/2022 23:21:03 - INFO - __main__ - Step 870 Global step 870 Train loss 1.049181 on epoch=173
05/30/2022 23:21:08 - INFO - __main__ - Step 880 Global step 880 Train loss 1.208848 on epoch=175
05/30/2022 23:21:13 - INFO - __main__ - Step 890 Global step 890 Train loss 1.075079 on epoch=177
05/30/2022 23:21:19 - INFO - __main__ - Step 900 Global step 900 Train loss 1.094400 on epoch=179
05/30/2022 23:21:20 - INFO - __main__ - Global step 900 Train loss 1.108433 ACC 0.5 on epoch=179
05/30/2022 23:21:25 - INFO - __main__ - Step 910 Global step 910 Train loss 1.182734 on epoch=181
05/30/2022 23:21:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.960948 on epoch=183
05/30/2022 23:21:36 - INFO - __main__ - Step 930 Global step 930 Train loss 1.043531 on epoch=185
05/30/2022 23:21:41 - INFO - __main__ - Step 940 Global step 940 Train loss 1.085618 on epoch=187
05/30/2022 23:21:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.820355 on epoch=189
05/30/2022 23:21:47 - INFO - __main__ - Global step 950 Train loss 1.018637 ACC 0.5 on epoch=189
05/30/2022 23:21:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.975489 on epoch=191
05/30/2022 23:21:58 - INFO - __main__ - Step 970 Global step 970 Train loss 1.226217 on epoch=193
05/30/2022 23:22:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.833243 on epoch=195
05/30/2022 23:22:08 - INFO - __main__ - Step 990 Global step 990 Train loss 1.051457 on epoch=197
05/30/2022 23:22:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.718193 on epoch=199
05/30/2022 23:22:14 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:22:14 - INFO - __main__ - Printing 3 examples
05/30/2022 23:22:14 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/30/2022 23:22:14 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:14 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/30/2022 23:22:14 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:14 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/30/2022 23:22:14 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:14 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:22:14 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:22:15 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:22:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:22:15 - INFO - __main__ - Printing 3 examples
05/30/2022 23:22:15 - INFO - __main__ -  [superglue-cb] premise: A: How do you feel about that. B: I don't really, I more, I don't know about the government as much as, uh, the people, uh, I wouldn't consider to be a threat at all and I really don't feel much like the Soviet Union itself is a threat anymore. [SEP] hypothesis: the Soviet Union itself is a threat still
05/30/2022 23:22:15 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:15 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/30/2022 23:22:15 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:15 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
05/30/2022 23:22:15 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:15 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:22:15 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:22:15 - INFO - __main__ - Global step 1000 Train loss 0.960920 ACC 0.578125 on epoch=199
05/30/2022 23:22:15 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:22:15 - INFO - __main__ - save last model!
05/30/2022 23:22:22 - INFO - __main__ - Loading checkpoint on the fly
05/30/2022 23:22:23 - INFO - __main__ - Start tokenizing ... 56 instances
05/30/2022 23:22:23 - INFO - __main__ - Printing 3 examples
05/30/2022 23:22:23 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/30/2022 23:22:23 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:23 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/30/2022 23:22:23 - INFO - __main__ - ['neutral']
05/30/2022 23:22:23 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/30/2022 23:22:23 - INFO - __main__ - ['entailment']
05/30/2022 23:22:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:22:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:22:23 - INFO - __main__ - Loaded 56 examples from test data
05/30/2022 23:22:24 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_100_0.0001_8_predictions.txt
05/30/2022 23:22:24 - INFO - __main__ - ACC on test data: 0.5357
05/30/2022 23:22:24 - INFO - __main__ - prefix=superglue-cb_32_100, lr=0.0001, bsz=8, dev_performance=0.578125, test_performance=0.5357142857142857
05/30/2022 23:22:24 - INFO - __main__ - Running ... prefix=superglue-cb_32_13, lr=0.0005, bsz=8 ...
05/30/2022 23:22:25 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:22:25 - INFO - __main__ - Printing 3 examples
05/30/2022 23:22:25 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/30/2022 23:22:25 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:25 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/30/2022 23:22:25 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:25 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/30/2022 23:22:25 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:25 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:22:25 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:22:25 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:22:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:22:25 - INFO - __main__ - Printing 3 examples
05/30/2022 23:22:25 - INFO - __main__ -  [superglue-cb] premise: A: How do you feel about that. B: I don't really, I more, I don't know about the government as much as, uh, the people, uh, I wouldn't consider to be a threat at all and I really don't feel much like the Soviet Union itself is a threat anymore. [SEP] hypothesis: the Soviet Union itself is a threat still
05/30/2022 23:22:25 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:25 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/30/2022 23:22:25 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:25 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
05/30/2022 23:22:25 - INFO - __main__ - ['contradiction']
05/30/2022 23:22:25 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:22:25 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:22:26 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:22:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:22:28 - INFO - __main__ - Starting training!
05/30/2022 23:22:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:22:38 - INFO - __main__ - Starting training!
05/30/2022 23:22:43 - INFO - __main__ - Step 10 Global step 10 Train loss 22.803730 on epoch=1
05/30/2022 23:22:48 - INFO - __main__ - Step 20 Global step 20 Train loss 16.606718 on epoch=3
05/30/2022 23:22:53 - INFO - __main__ - Step 30 Global step 30 Train loss 11.714197 on epoch=5
05/30/2022 23:22:58 - INFO - __main__ - Step 40 Global step 40 Train loss 10.044451 on epoch=7
05/30/2022 23:23:04 - INFO - __main__ - Step 50 Global step 50 Train loss 8.282336 on epoch=9
05/30/2022 23:23:05 - INFO - __main__ - Global step 50 Train loss 13.890286 ACC 0.0 on epoch=9
05/30/2022 23:23:11 - INFO - __main__ - Step 60 Global step 60 Train loss 7.840623 on epoch=11
05/30/2022 23:23:16 - INFO - __main__ - Step 70 Global step 70 Train loss 6.625355 on epoch=13
05/30/2022 23:23:21 - INFO - __main__ - Step 80 Global step 80 Train loss 4.805480 on epoch=15
05/30/2022 23:23:26 - INFO - __main__ - Step 90 Global step 90 Train loss 3.421380 on epoch=17
05/30/2022 23:23:31 - INFO - __main__ - Step 100 Global step 100 Train loss 2.483966 on epoch=19
05/30/2022 23:23:33 - INFO - __main__ - Global step 100 Train loss 5.035361 ACC 0.484375 on epoch=19
05/30/2022 23:23:38 - INFO - __main__ - Step 110 Global step 110 Train loss 1.922480 on epoch=21
05/30/2022 23:23:43 - INFO - __main__ - Step 120 Global step 120 Train loss 2.173642 on epoch=23
05/30/2022 23:23:49 - INFO - __main__ - Step 130 Global step 130 Train loss 1.911245 on epoch=25
05/30/2022 23:23:54 - INFO - __main__ - Step 140 Global step 140 Train loss 1.577883 on epoch=27
05/30/2022 23:23:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.750013 on epoch=29
05/30/2022 23:24:00 - INFO - __main__ - Global step 150 Train loss 1.867053 ACC 0.5 on epoch=29
05/30/2022 23:24:06 - INFO - __main__ - Step 160 Global step 160 Train loss 1.503515 on epoch=31
05/30/2022 23:24:11 - INFO - __main__ - Step 170 Global step 170 Train loss 1.568863 on epoch=33
05/30/2022 23:24:16 - INFO - __main__ - Step 180 Global step 180 Train loss 1.683035 on epoch=35
05/30/2022 23:24:21 - INFO - __main__ - Step 190 Global step 190 Train loss 1.306937 on epoch=37
05/30/2022 23:24:26 - INFO - __main__ - Step 200 Global step 200 Train loss 1.793367 on epoch=39
05/30/2022 23:24:27 - INFO - __main__ - Global step 200 Train loss 1.571143 ACC 0.5 on epoch=39
05/30/2022 23:24:33 - INFO - __main__ - Step 210 Global step 210 Train loss 1.255699 on epoch=41
05/30/2022 23:24:38 - INFO - __main__ - Step 220 Global step 220 Train loss 1.055503 on epoch=43
05/30/2022 23:24:43 - INFO - __main__ - Step 230 Global step 230 Train loss 1.335747 on epoch=45
05/30/2022 23:24:49 - INFO - __main__ - Step 240 Global step 240 Train loss 1.035558 on epoch=47
05/30/2022 23:24:54 - INFO - __main__ - Step 250 Global step 250 Train loss 1.134717 on epoch=49
05/30/2022 23:24:55 - INFO - __main__ - Global step 250 Train loss 1.163445 ACC 0.5 on epoch=49
05/30/2022 23:25:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.830439 on epoch=51
05/30/2022 23:25:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.915781 on epoch=53
05/30/2022 23:25:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.719993 on epoch=55
05/30/2022 23:25:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.774514 on epoch=57
05/30/2022 23:25:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.633875 on epoch=59
05/30/2022 23:25:22 - INFO - __main__ - Global step 300 Train loss 0.774920 ACC 0.5 on epoch=59
05/30/2022 23:25:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.567139 on epoch=61
05/30/2022 23:25:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.527810 on epoch=63
05/30/2022 23:25:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.659576 on epoch=65
05/30/2022 23:25:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.419340 on epoch=67
05/30/2022 23:25:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.571541 on epoch=69
05/30/2022 23:25:49 - INFO - __main__ - Global step 350 Train loss 0.549081 ACC 0.53125 on epoch=69
05/30/2022 23:25:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.443231 on epoch=71
05/30/2022 23:26:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.421159 on epoch=73
05/30/2022 23:26:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.485789 on epoch=75
05/30/2022 23:26:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.434069 on epoch=77
05/30/2022 23:26:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.349686 on epoch=79
05/30/2022 23:26:17 - INFO - __main__ - Global step 400 Train loss 0.426787 ACC 0.890625 on epoch=79
05/30/2022 23:26:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.175091 on epoch=81
05/30/2022 23:26:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.257323 on epoch=83
05/30/2022 23:26:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.124232 on epoch=85
05/30/2022 23:26:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.071693 on epoch=87
05/30/2022 23:26:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.082388 on epoch=89
05/30/2022 23:26:46 - INFO - __main__ - Global step 450 Train loss 0.142145 ACC 0.890625 on epoch=89
05/30/2022 23:26:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.008488 on epoch=91
05/30/2022 23:26:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.033316 on epoch=93
05/30/2022 23:27:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.011517 on epoch=95
05/30/2022 23:27:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.106343 on epoch=97
05/30/2022 23:27:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.006631 on epoch=99
05/30/2022 23:27:13 - INFO - __main__ - Global step 500 Train loss 0.033259 ACC 0.9375 on epoch=99
05/30/2022 23:27:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.007156 on epoch=101
05/30/2022 23:27:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.002745 on epoch=103
05/30/2022 23:27:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003735 on epoch=105
05/30/2022 23:27:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001486 on epoch=107
05/30/2022 23:27:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003368 on epoch=109
05/30/2022 23:27:41 - INFO - __main__ - Global step 550 Train loss 0.003698 ACC 0.953125 on epoch=109
05/30/2022 23:27:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001697 on epoch=111
05/30/2022 23:27:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002222 on epoch=113
05/30/2022 23:27:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000780 on epoch=115
05/30/2022 23:28:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.599778 on epoch=117
05/30/2022 23:28:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.025292 on epoch=119
05/30/2022 23:28:09 - INFO - __main__ - Global step 600 Train loss 0.125954 ACC 0.90625 on epoch=119
05/30/2022 23:28:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.012257 on epoch=121
05/30/2022 23:28:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.011040 on epoch=123
05/30/2022 23:28:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000310 on epoch=125
05/30/2022 23:28:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001728 on epoch=127
05/30/2022 23:28:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.002220 on epoch=129
05/30/2022 23:28:36 - INFO - __main__ - Global step 650 Train loss 0.005511 ACC 0.8125 on epoch=129
05/30/2022 23:28:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.399946 on epoch=131
05/30/2022 23:28:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.502500 on epoch=133
05/30/2022 23:28:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.404212 on epoch=135
05/30/2022 23:28:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.451283 on epoch=137
05/30/2022 23:29:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.427271 on epoch=139
05/30/2022 23:29:03 - INFO - __main__ - Global step 700 Train loss 0.437043 ACC 0.5 on epoch=139
05/30/2022 23:29:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.409490 on epoch=141
05/30/2022 23:29:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.435359 on epoch=143
05/30/2022 23:29:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.353383 on epoch=145
05/30/2022 23:29:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.356841 on epoch=147
05/30/2022 23:29:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.276195 on epoch=149
05/30/2022 23:29:30 - INFO - __main__ - Global step 750 Train loss 0.366254 ACC 0.5625 on epoch=149
05/30/2022 23:29:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.241738 on epoch=151
05/30/2022 23:29:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.329483 on epoch=153
05/30/2022 23:29:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.282878 on epoch=155
05/30/2022 23:29:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.279090 on epoch=157
05/30/2022 23:29:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.171180 on epoch=159
05/30/2022 23:29:57 - INFO - __main__ - Global step 800 Train loss 0.260874 ACC 0.84375 on epoch=159
05/30/2022 23:30:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.112819 on epoch=161
05/30/2022 23:30:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.142602 on epoch=163
05/30/2022 23:30:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.130967 on epoch=165
05/30/2022 23:30:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.062498 on epoch=167
05/30/2022 23:30:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.157664 on epoch=169
05/30/2022 23:30:24 - INFO - __main__ - Global step 850 Train loss 0.121310 ACC 0.859375 on epoch=169
05/30/2022 23:30:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.134490 on epoch=171
05/30/2022 23:30:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.194569 on epoch=173
05/30/2022 23:30:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.048505 on epoch=175
05/30/2022 23:30:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.100477 on epoch=177
05/30/2022 23:30:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.399843 on epoch=179
05/30/2022 23:30:51 - INFO - __main__ - Global step 900 Train loss 0.175577 ACC 0.890625 on epoch=179
05/30/2022 23:30:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.159161 on epoch=181
05/30/2022 23:31:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.106807 on epoch=183
05/30/2022 23:31:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.218876 on epoch=185
05/30/2022 23:31:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.091681 on epoch=187
05/30/2022 23:31:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.071246 on epoch=189
05/30/2022 23:31:19 - INFO - __main__ - Global step 950 Train loss 0.129554 ACC 0.765625 on epoch=189
05/30/2022 23:31:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.082023 on epoch=191
05/30/2022 23:31:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.096278 on epoch=193
05/30/2022 23:31:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.082484 on epoch=195
05/30/2022 23:31:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.052817 on epoch=197
05/30/2022 23:31:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.027111 on epoch=199
05/30/2022 23:31:46 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:31:46 - INFO - __main__ - Printing 3 examples
05/30/2022 23:31:46 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/30/2022 23:31:46 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:46 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/30/2022 23:31:46 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:46 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/30/2022 23:31:46 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:46 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:31:46 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:31:46 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:31:46 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:31:46 - INFO - __main__ - Printing 3 examples
05/30/2022 23:31:46 - INFO - __main__ -  [superglue-cb] premise: A: How do you feel about that. B: I don't really, I more, I don't know about the government as much as, uh, the people, uh, I wouldn't consider to be a threat at all and I really don't feel much like the Soviet Union itself is a threat anymore. [SEP] hypothesis: the Soviet Union itself is a threat still
05/30/2022 23:31:46 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:46 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/30/2022 23:31:46 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:46 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
05/30/2022 23:31:46 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:46 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:31:46 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:31:46 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:31:46 - INFO - __main__ - Global step 1000 Train loss 0.068143 ACC 0.859375 on epoch=199
05/30/2022 23:31:46 - INFO - __main__ - save last model!
05/30/2022 23:31:53 - INFO - __main__ - Loading checkpoint on the fly
05/30/2022 23:31:53 - INFO - __main__ - Start tokenizing ... 56 instances
05/30/2022 23:31:53 - INFO - __main__ - Printing 3 examples
05/30/2022 23:31:53 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/30/2022 23:31:53 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:53 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/30/2022 23:31:53 - INFO - __main__ - ['neutral']
05/30/2022 23:31:53 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/30/2022 23:31:53 - INFO - __main__ - ['entailment']
05/30/2022 23:31:53 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:31:53 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:31:53 - INFO - __main__ - Loaded 56 examples from test data
05/30/2022 23:31:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_13_0.0005_8_predictions.txt
05/30/2022 23:31:55 - INFO - __main__ - ACC on test data: 0.8750
05/30/2022 23:31:55 - INFO - __main__ - prefix=superglue-cb_32_13, lr=0.0005, bsz=8, dev_performance=0.953125, test_performance=0.875
05/30/2022 23:31:55 - INFO - __main__ - Running ... prefix=superglue-cb_32_13, lr=0.0003, bsz=8 ...
05/30/2022 23:31:56 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:31:56 - INFO - __main__ - Printing 3 examples
05/30/2022 23:31:56 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/30/2022 23:31:56 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:56 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/30/2022 23:31:56 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:56 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/30/2022 23:31:56 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:56 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:31:56 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:31:56 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:31:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:31:56 - INFO - __main__ - Printing 3 examples
05/30/2022 23:31:56 - INFO - __main__ -  [superglue-cb] premise: A: How do you feel about that. B: I don't really, I more, I don't know about the government as much as, uh, the people, uh, I wouldn't consider to be a threat at all and I really don't feel much like the Soviet Union itself is a threat anymore. [SEP] hypothesis: the Soviet Union itself is a threat still
05/30/2022 23:31:56 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:56 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/30/2022 23:31:56 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:56 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
05/30/2022 23:31:56 - INFO - __main__ - ['contradiction']
05/30/2022 23:31:56 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:31:56 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:31:56 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:31:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:31:59 - INFO - __main__ - Starting training!
05/30/2022 23:32:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:32:09 - INFO - __main__ - Starting training!
05/30/2022 23:32:13 - INFO - __main__ - Step 10 Global step 10 Train loss 22.924156 on epoch=1
05/30/2022 23:32:18 - INFO - __main__ - Step 20 Global step 20 Train loss 21.330135 on epoch=3
05/30/2022 23:32:23 - INFO - __main__ - Step 30 Global step 30 Train loss 15.078341 on epoch=5
05/30/2022 23:32:28 - INFO - __main__ - Step 40 Global step 40 Train loss 12.186386 on epoch=7
05/30/2022 23:32:33 - INFO - __main__ - Step 50 Global step 50 Train loss 10.982675 on epoch=9
05/30/2022 23:32:35 - INFO - __main__ - Global step 50 Train loss 16.500338 ACC 0.21875 on epoch=9
05/30/2022 23:32:41 - INFO - __main__ - Step 60 Global step 60 Train loss 9.402929 on epoch=11
05/30/2022 23:32:46 - INFO - __main__ - Step 70 Global step 70 Train loss 9.046420 on epoch=13
05/30/2022 23:32:51 - INFO - __main__ - Step 80 Global step 80 Train loss 8.746458 on epoch=15
05/30/2022 23:32:56 - INFO - __main__ - Step 90 Global step 90 Train loss 8.122954 on epoch=17
05/30/2022 23:33:01 - INFO - __main__ - Step 100 Global step 100 Train loss 7.765834 on epoch=19
05/30/2022 23:33:03 - INFO - __main__ - Global step 100 Train loss 8.616920 ACC 0.171875 on epoch=19
05/30/2022 23:33:08 - INFO - __main__ - Step 110 Global step 110 Train loss 7.135508 on epoch=21
05/30/2022 23:33:13 - INFO - __main__ - Step 120 Global step 120 Train loss 5.286163 on epoch=23
05/30/2022 23:33:19 - INFO - __main__ - Step 130 Global step 130 Train loss 4.649679 on epoch=25
05/30/2022 23:33:24 - INFO - __main__ - Step 140 Global step 140 Train loss 3.700897 on epoch=27
05/30/2022 23:33:29 - INFO - __main__ - Step 150 Global step 150 Train loss 2.890139 on epoch=29
05/30/2022 23:33:31 - INFO - __main__ - Global step 150 Train loss 4.732477 ACC 0.328125 on epoch=29
05/30/2022 23:33:36 - INFO - __main__ - Step 160 Global step 160 Train loss 2.725231 on epoch=31
05/30/2022 23:33:42 - INFO - __main__ - Step 170 Global step 170 Train loss 2.371886 on epoch=33
05/30/2022 23:33:47 - INFO - __main__ - Step 180 Global step 180 Train loss 2.933504 on epoch=35
05/30/2022 23:33:52 - INFO - __main__ - Step 190 Global step 190 Train loss 2.514705 on epoch=37
05/30/2022 23:33:57 - INFO - __main__ - Step 200 Global step 200 Train loss 2.693875 on epoch=39
05/30/2022 23:33:59 - INFO - __main__ - Global step 200 Train loss 2.647840 ACC 0.5625 on epoch=39
05/30/2022 23:34:04 - INFO - __main__ - Step 210 Global step 210 Train loss 2.362323 on epoch=41
05/30/2022 23:34:10 - INFO - __main__ - Step 220 Global step 220 Train loss 2.061793 on epoch=43
05/30/2022 23:34:15 - INFO - __main__ - Step 230 Global step 230 Train loss 2.023420 on epoch=45
05/30/2022 23:34:20 - INFO - __main__ - Step 240 Global step 240 Train loss 2.122512 on epoch=47
05/30/2022 23:34:25 - INFO - __main__ - Step 250 Global step 250 Train loss 1.881740 on epoch=49
05/30/2022 23:34:27 - INFO - __main__ - Global step 250 Train loss 2.090358 ACC 0.5 on epoch=49
05/30/2022 23:34:32 - INFO - __main__ - Step 260 Global step 260 Train loss 1.902706 on epoch=51
05/30/2022 23:34:37 - INFO - __main__ - Step 270 Global step 270 Train loss 2.054372 on epoch=53
05/30/2022 23:34:43 - INFO - __main__ - Step 280 Global step 280 Train loss 1.157208 on epoch=55
05/30/2022 23:34:48 - INFO - __main__ - Step 290 Global step 290 Train loss 1.765632 on epoch=57
05/30/2022 23:34:53 - INFO - __main__ - Step 300 Global step 300 Train loss 1.548553 on epoch=59
05/30/2022 23:34:54 - INFO - __main__ - Global step 300 Train loss 1.685694 ACC 0.5 on epoch=59
05/30/2022 23:34:59 - INFO - __main__ - Step 310 Global step 310 Train loss 1.466071 on epoch=61
05/30/2022 23:35:05 - INFO - __main__ - Step 320 Global step 320 Train loss 1.407341 on epoch=63
05/30/2022 23:35:10 - INFO - __main__ - Step 330 Global step 330 Train loss 1.456747 on epoch=65
05/30/2022 23:35:15 - INFO - __main__ - Step 340 Global step 340 Train loss 1.292684 on epoch=67
05/30/2022 23:35:20 - INFO - __main__ - Step 350 Global step 350 Train loss 1.291005 on epoch=69
05/30/2022 23:35:21 - INFO - __main__ - Global step 350 Train loss 1.382770 ACC 0.5 on epoch=69
05/30/2022 23:35:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.892295 on epoch=71
05/30/2022 23:35:32 - INFO - __main__ - Step 370 Global step 370 Train loss 1.218789 on epoch=73
05/30/2022 23:35:37 - INFO - __main__ - Step 380 Global step 380 Train loss 1.083997 on epoch=75
05/30/2022 23:35:42 - INFO - __main__ - Step 390 Global step 390 Train loss 1.022617 on epoch=77
05/30/2022 23:35:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.966023 on epoch=79
05/30/2022 23:35:48 - INFO - __main__ - Global step 400 Train loss 1.036744 ACC 0.5 on epoch=79
05/30/2022 23:35:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.939002 on epoch=81
05/30/2022 23:35:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.893299 on epoch=83
05/30/2022 23:36:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.987584 on epoch=85
05/30/2022 23:36:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.780912 on epoch=87
05/30/2022 23:36:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.622023 on epoch=89
05/30/2022 23:36:16 - INFO - __main__ - Global step 450 Train loss 0.844564 ACC 0.5 on epoch=89
05/30/2022 23:36:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.789481 on epoch=91
05/30/2022 23:36:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.762523 on epoch=93
05/30/2022 23:36:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.736992 on epoch=95
05/30/2022 23:36:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.579832 on epoch=97
05/30/2022 23:36:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.465947 on epoch=99
05/30/2022 23:36:43 - INFO - __main__ - Global step 500 Train loss 0.666955 ACC 0.5 on epoch=99
05/30/2022 23:36:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.600212 on epoch=101
05/30/2022 23:36:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.461409 on epoch=103
05/30/2022 23:36:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.412675 on epoch=105
05/30/2022 23:37:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.451830 on epoch=107
05/30/2022 23:37:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.418591 on epoch=109
05/30/2022 23:37:10 - INFO - __main__ - Global step 550 Train loss 0.468943 ACC 0.515625 on epoch=109
05/30/2022 23:37:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.398970 on epoch=111
05/30/2022 23:37:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.438514 on epoch=113
05/30/2022 23:37:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.441530 on epoch=115
05/30/2022 23:37:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.408243 on epoch=117
05/30/2022 23:37:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.482446 on epoch=119
05/30/2022 23:37:38 - INFO - __main__ - Global step 600 Train loss 0.433941 ACC 0.5 on epoch=119
05/30/2022 23:37:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.374546 on epoch=121
05/30/2022 23:37:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.446489 on epoch=123
05/30/2022 23:37:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.416446 on epoch=125
05/30/2022 23:37:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.413612 on epoch=127
05/30/2022 23:38:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.415541 on epoch=129
05/30/2022 23:38:05 - INFO - __main__ - Global step 650 Train loss 0.413327 ACC 0.5 on epoch=129
05/30/2022 23:38:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.362549 on epoch=131
05/30/2022 23:38:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.394917 on epoch=133
05/30/2022 23:38:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.387927 on epoch=135
05/30/2022 23:38:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.385495 on epoch=137
05/30/2022 23:38:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.404121 on epoch=139
05/30/2022 23:38:32 - INFO - __main__ - Global step 700 Train loss 0.387002 ACC 0.5 on epoch=139
05/30/2022 23:38:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.355513 on epoch=141
05/30/2022 23:38:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.369534 on epoch=143
05/30/2022 23:38:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.418281 on epoch=145
05/30/2022 23:38:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.405122 on epoch=147
05/30/2022 23:38:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.382174 on epoch=149
05/30/2022 23:38:58 - INFO - __main__ - Global step 750 Train loss 0.386125 ACC 0.5 on epoch=149
05/30/2022 23:39:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.411234 on epoch=151
05/30/2022 23:39:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.377829 on epoch=153
05/30/2022 23:39:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.321280 on epoch=155
05/30/2022 23:39:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.355596 on epoch=157
05/30/2022 23:39:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.354525 on epoch=159
05/30/2022 23:39:25 - INFO - __main__ - Global step 800 Train loss 0.364093 ACC 0.515625 on epoch=159
05/30/2022 23:39:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.326348 on epoch=161
05/30/2022 23:39:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.343090 on epoch=163
05/30/2022 23:39:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.383899 on epoch=165
05/30/2022 23:39:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.374616 on epoch=167
05/30/2022 23:39:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.344382 on epoch=169
05/30/2022 23:39:51 - INFO - __main__ - Global step 850 Train loss 0.354467 ACC 0.515625 on epoch=169
05/30/2022 23:39:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.371650 on epoch=171
05/30/2022 23:40:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.360631 on epoch=173
05/30/2022 23:40:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.382718 on epoch=175
05/30/2022 23:40:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.326096 on epoch=177
05/30/2022 23:40:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.370472 on epoch=179
05/30/2022 23:40:18 - INFO - __main__ - Global step 900 Train loss 0.362313 ACC 0.515625 on epoch=179
05/30/2022 23:40:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.342754 on epoch=181
05/30/2022 23:40:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.369130 on epoch=183
05/30/2022 23:40:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.352420 on epoch=185
05/30/2022 23:40:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.369427 on epoch=187
05/30/2022 23:40:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.337750 on epoch=189
05/30/2022 23:40:44 - INFO - __main__ - Global step 950 Train loss 0.354296 ACC 0.5 on epoch=189
05/30/2022 23:40:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.379547 on epoch=191
05/30/2022 23:40:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.379083 on epoch=193
05/30/2022 23:41:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.384341 on epoch=195
05/30/2022 23:41:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.357143 on epoch=197
05/30/2022 23:41:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.383796 on epoch=199
05/30/2022 23:41:11 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:41:11 - INFO - __main__ - Printing 3 examples
05/30/2022 23:41:11 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/30/2022 23:41:11 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:11 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/30/2022 23:41:11 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:11 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/30/2022 23:41:11 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:11 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:41:11 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:41:11 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:41:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:41:11 - INFO - __main__ - Printing 3 examples
05/30/2022 23:41:11 - INFO - __main__ -  [superglue-cb] premise: A: How do you feel about that. B: I don't really, I more, I don't know about the government as much as, uh, the people, uh, I wouldn't consider to be a threat at all and I really don't feel much like the Soviet Union itself is a threat anymore. [SEP] hypothesis: the Soviet Union itself is a threat still
05/30/2022 23:41:11 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:11 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/30/2022 23:41:11 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:11 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
05/30/2022 23:41:11 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:11 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:41:11 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:41:11 - INFO - __main__ - Global step 1000 Train loss 0.376782 ACC 0.5 on epoch=199
05/30/2022 23:41:11 - INFO - __main__ - save last model!
05/30/2022 23:41:11 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:41:18 - INFO - __main__ - Loading checkpoint on the fly
05/30/2022 23:41:18 - INFO - __main__ - Start tokenizing ... 56 instances
05/30/2022 23:41:18 - INFO - __main__ - Printing 3 examples
05/30/2022 23:41:18 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/30/2022 23:41:18 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:18 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/30/2022 23:41:18 - INFO - __main__ - ['neutral']
05/30/2022 23:41:18 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/30/2022 23:41:18 - INFO - __main__ - ['entailment']
05/30/2022 23:41:18 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:41:18 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:41:18 - INFO - __main__ - Loaded 56 examples from test data
05/30/2022 23:41:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_13_0.0003_8_predictions.txt
05/30/2022 23:41:20 - INFO - __main__ - ACC on test data: 0.5000
05/30/2022 23:41:20 - INFO - __main__ - prefix=superglue-cb_32_13, lr=0.0003, bsz=8, dev_performance=0.5625, test_performance=0.5
05/30/2022 23:41:20 - INFO - __main__ - Running ... prefix=superglue-cb_32_13, lr=0.0002, bsz=8 ...
05/30/2022 23:41:21 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:41:21 - INFO - __main__ - Printing 3 examples
05/30/2022 23:41:21 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/30/2022 23:41:21 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:21 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/30/2022 23:41:21 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:21 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/30/2022 23:41:21 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:21 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:41:21 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:41:21 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:41:21 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:41:21 - INFO - __main__ - Printing 3 examples
05/30/2022 23:41:21 - INFO - __main__ -  [superglue-cb] premise: A: How do you feel about that. B: I don't really, I more, I don't know about the government as much as, uh, the people, uh, I wouldn't consider to be a threat at all and I really don't feel much like the Soviet Union itself is a threat anymore. [SEP] hypothesis: the Soviet Union itself is a threat still
05/30/2022 23:41:21 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:21 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/30/2022 23:41:21 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:21 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
05/30/2022 23:41:21 - INFO - __main__ - ['contradiction']
05/30/2022 23:41:21 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:41:21 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:41:21 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:41:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:41:24 - INFO - __main__ - Starting training!
05/30/2022 23:41:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:41:31 - INFO - __main__ - Starting training!
05/30/2022 23:41:36 - INFO - __main__ - Step 10 Global step 10 Train loss 24.087997 on epoch=1
05/30/2022 23:41:41 - INFO - __main__ - Step 20 Global step 20 Train loss 17.257477 on epoch=3
05/30/2022 23:41:46 - INFO - __main__ - Step 30 Global step 30 Train loss 11.497961 on epoch=5
05/30/2022 23:41:51 - INFO - __main__ - Step 40 Global step 40 Train loss 11.470341 on epoch=7
05/30/2022 23:41:56 - INFO - __main__ - Step 50 Global step 50 Train loss 10.145967 on epoch=9
05/30/2022 23:41:58 - INFO - __main__ - Global step 50 Train loss 14.891949 ACC 0.015625 on epoch=9
05/30/2022 23:42:04 - INFO - __main__ - Step 60 Global step 60 Train loss 10.026889 on epoch=11
05/30/2022 23:42:09 - INFO - __main__ - Step 70 Global step 70 Train loss 9.726581 on epoch=13
05/30/2022 23:42:14 - INFO - __main__ - Step 80 Global step 80 Train loss 8.544518 on epoch=15
05/30/2022 23:42:19 - INFO - __main__ - Step 90 Global step 90 Train loss 8.600904 on epoch=17
05/30/2022 23:42:24 - INFO - __main__ - Step 100 Global step 100 Train loss 8.314764 on epoch=19
05/30/2022 23:42:26 - INFO - __main__ - Global step 100 Train loss 9.042731 ACC 0.0 on epoch=19
05/30/2022 23:42:31 - INFO - __main__ - Step 110 Global step 110 Train loss 7.539989 on epoch=21
05/30/2022 23:42:36 - INFO - __main__ - Step 120 Global step 120 Train loss 7.457593 on epoch=23
05/30/2022 23:42:41 - INFO - __main__ - Step 130 Global step 130 Train loss 6.968608 on epoch=25
05/30/2022 23:42:46 - INFO - __main__ - Step 140 Global step 140 Train loss 6.070016 on epoch=27
05/30/2022 23:42:51 - INFO - __main__ - Step 150 Global step 150 Train loss 4.921023 on epoch=29
05/30/2022 23:42:52 - INFO - __main__ - Global step 150 Train loss 6.591446 ACC 0.0 on epoch=29
05/30/2022 23:42:57 - INFO - __main__ - Step 160 Global step 160 Train loss 4.764842 on epoch=31
05/30/2022 23:43:02 - INFO - __main__ - Step 170 Global step 170 Train loss 4.370298 on epoch=33
05/30/2022 23:43:07 - INFO - __main__ - Step 180 Global step 180 Train loss 2.812992 on epoch=35
05/30/2022 23:43:12 - INFO - __main__ - Step 190 Global step 190 Train loss 2.161344 on epoch=37
05/30/2022 23:43:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.961404 on epoch=39
05/30/2022 23:43:19 - INFO - __main__ - Global step 200 Train loss 3.014176 ACC 0.515625 on epoch=39
05/30/2022 23:43:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.562390 on epoch=41
05/30/2022 23:43:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.395683 on epoch=43
05/30/2022 23:43:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.631553 on epoch=45
05/30/2022 23:43:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.404039 on epoch=47
05/30/2022 23:43:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.305061 on epoch=49
05/30/2022 23:43:46 - INFO - __main__ - Global step 250 Train loss 0.459745 ACC 0.484375 on epoch=49
05/30/2022 23:43:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.478423 on epoch=51
05/30/2022 23:43:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.298921 on epoch=53
05/30/2022 23:44:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.214577 on epoch=55
05/30/2022 23:44:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.124134 on epoch=57
05/30/2022 23:44:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.112708 on epoch=59
05/30/2022 23:44:13 - INFO - __main__ - Global step 300 Train loss 0.245753 ACC 0.671875 on epoch=59
05/30/2022 23:44:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.148271 on epoch=61
05/30/2022 23:44:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.056479 on epoch=63
05/30/2022 23:44:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.060489 on epoch=65
05/30/2022 23:44:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.018495 on epoch=67
05/30/2022 23:44:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.072866 on epoch=69
05/30/2022 23:44:41 - INFO - __main__ - Global step 350 Train loss 0.071320 ACC 0.78125 on epoch=69
05/30/2022 23:44:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.016087 on epoch=71
05/30/2022 23:44:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.006787 on epoch=73
05/30/2022 23:44:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.009529 on epoch=75
05/30/2022 23:45:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.018152 on epoch=77
05/30/2022 23:45:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.017157 on epoch=79
05/30/2022 23:45:09 - INFO - __main__ - Global step 400 Train loss 0.013542 ACC 0.703125 on epoch=79
05/30/2022 23:45:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.010275 on epoch=81
05/30/2022 23:45:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.041686 on epoch=83
05/30/2022 23:45:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.013711 on epoch=85
05/30/2022 23:45:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.021391 on epoch=87
05/30/2022 23:45:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.003489 on epoch=89
05/30/2022 23:45:36 - INFO - __main__ - Global step 450 Train loss 0.018110 ACC 0.78125 on epoch=89
05/30/2022 23:45:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.004180 on epoch=91
05/30/2022 23:45:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.028487 on epoch=93
05/30/2022 23:45:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.002641 on epoch=95
05/30/2022 23:45:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002486 on epoch=97
05/30/2022 23:46:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002473 on epoch=99
05/30/2022 23:46:03 - INFO - __main__ - Global step 500 Train loss 0.008054 ACC 0.78125 on epoch=99
05/30/2022 23:46:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001103 on epoch=101
05/30/2022 23:46:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001895 on epoch=103
05/30/2022 23:46:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000588 on epoch=105
05/30/2022 23:46:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001524 on epoch=107
05/30/2022 23:46:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003261 on epoch=109
05/30/2022 23:46:30 - INFO - __main__ - Global step 550 Train loss 0.001674 ACC 0.828125 on epoch=109
05/30/2022 23:46:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000876 on epoch=111
05/30/2022 23:46:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.007830 on epoch=113
05/30/2022 23:46:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000908 on epoch=115
05/30/2022 23:46:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001511 on epoch=117
05/30/2022 23:46:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000705 on epoch=119
05/30/2022 23:46:58 - INFO - __main__ - Global step 600 Train loss 0.002366 ACC 0.765625 on epoch=119
05/30/2022 23:47:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000542 on epoch=121
05/30/2022 23:47:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.009852 on epoch=123
05/30/2022 23:47:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.005015 on epoch=125
05/30/2022 23:47:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001649 on epoch=127
05/30/2022 23:47:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000693 on epoch=129
05/30/2022 23:47:26 - INFO - __main__ - Global step 650 Train loss 0.003550 ACC 0.78125 on epoch=129
05/30/2022 23:47:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.007548 on epoch=131
05/30/2022 23:47:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001718 on epoch=133
05/30/2022 23:47:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000162 on epoch=135
05/30/2022 23:47:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.002654 on epoch=137
05/30/2022 23:47:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000301 on epoch=139
05/30/2022 23:47:53 - INFO - __main__ - Global step 700 Train loss 0.002477 ACC 0.734375 on epoch=139
05/30/2022 23:47:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001481 on epoch=141
05/30/2022 23:48:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000119 on epoch=143
05/30/2022 23:48:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000505 on epoch=145
05/30/2022 23:48:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000048 on epoch=147
05/30/2022 23:48:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000589 on epoch=149
05/30/2022 23:48:20 - INFO - __main__ - Global step 750 Train loss 0.000548 ACC 0.796875 on epoch=149
05/30/2022 23:48:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000229 on epoch=151
05/30/2022 23:48:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000071 on epoch=153
05/30/2022 23:48:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000048 on epoch=155
05/30/2022 23:48:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000373 on epoch=157
05/30/2022 23:48:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002650 on epoch=159
05/30/2022 23:48:48 - INFO - __main__ - Global step 800 Train loss 0.000674 ACC 0.765625 on epoch=159
05/30/2022 23:48:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000347 on epoch=161
05/30/2022 23:48:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000097 on epoch=163
05/30/2022 23:49:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000184 on epoch=165
05/30/2022 23:49:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.003595 on epoch=167
05/30/2022 23:49:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.003055 on epoch=169
05/30/2022 23:49:15 - INFO - __main__ - Global step 850 Train loss 0.001456 ACC 0.796875 on epoch=169
05/30/2022 23:49:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000096 on epoch=171
05/30/2022 23:49:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000056 on epoch=173
05/30/2022 23:49:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000227 on epoch=175
05/30/2022 23:49:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000133 on epoch=177
05/30/2022 23:49:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000078 on epoch=179
05/30/2022 23:49:42 - INFO - __main__ - Global step 900 Train loss 0.000118 ACC 0.78125 on epoch=179
05/30/2022 23:49:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000235 on epoch=181
05/30/2022 23:49:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000747 on epoch=183
05/30/2022 23:49:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000242 on epoch=185
05/30/2022 23:50:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000362 on epoch=187
05/30/2022 23:50:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000122 on epoch=189
05/30/2022 23:50:09 - INFO - __main__ - Global step 950 Train loss 0.000341 ACC 0.8125 on epoch=189
05/30/2022 23:50:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000109 on epoch=191
05/30/2022 23:50:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000190 on epoch=193
05/30/2022 23:50:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000717 on epoch=195
05/30/2022 23:50:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000343 on epoch=197
05/30/2022 23:50:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000026 on epoch=199
05/30/2022 23:50:36 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:50:36 - INFO - __main__ - Printing 3 examples
05/30/2022 23:50:36 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/30/2022 23:50:36 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:36 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/30/2022 23:50:36 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:36 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/30/2022 23:50:36 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:36 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:50:36 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:50:36 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:50:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:50:36 - INFO - __main__ - Printing 3 examples
05/30/2022 23:50:36 - INFO - __main__ -  [superglue-cb] premise: A: How do you feel about that. B: I don't really, I more, I don't know about the government as much as, uh, the people, uh, I wouldn't consider to be a threat at all and I really don't feel much like the Soviet Union itself is a threat anymore. [SEP] hypothesis: the Soviet Union itself is a threat still
05/30/2022 23:50:36 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:36 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/30/2022 23:50:36 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:36 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
05/30/2022 23:50:36 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:36 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:50:36 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:50:36 - INFO - __main__ - Global step 1000 Train loss 0.000277 ACC 0.84375 on epoch=199
05/30/2022 23:50:36 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:50:37 - INFO - __main__ - save last model!
05/30/2022 23:50:44 - INFO - __main__ - Loading checkpoint on the fly
05/30/2022 23:50:44 - INFO - __main__ - Start tokenizing ... 56 instances
05/30/2022 23:50:44 - INFO - __main__ - Printing 3 examples
05/30/2022 23:50:44 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/30/2022 23:50:44 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:44 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/30/2022 23:50:44 - INFO - __main__ - ['neutral']
05/30/2022 23:50:44 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/30/2022 23:50:44 - INFO - __main__ - ['entailment']
05/30/2022 23:50:44 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:50:44 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:50:44 - INFO - __main__ - Loaded 56 examples from test data
05/30/2022 23:50:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_13_0.0002_8_predictions.txt
05/30/2022 23:50:46 - INFO - __main__ - ACC on test data: 0.7321
05/30/2022 23:50:46 - INFO - __main__ - prefix=superglue-cb_32_13, lr=0.0002, bsz=8, dev_performance=0.84375, test_performance=0.7321428571428571
05/30/2022 23:50:46 - INFO - __main__ - Running ... prefix=superglue-cb_32_13, lr=0.0001, bsz=8 ...
05/30/2022 23:50:47 - INFO - __main__ - Start tokenizing ... 80 instances
05/30/2022 23:50:47 - INFO - __main__ - Printing 3 examples
05/30/2022 23:50:47 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/30/2022 23:50:47 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:47 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/30/2022 23:50:47 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:47 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/30/2022 23:50:47 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:47 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:50:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:50:47 - INFO - __main__ - Starting training!
05/30/2022 23:50:47 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:50:47 - INFO - __main__ - Loaded 80 examples from train data
05/30/2022 23:50:47 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:50:47 - INFO - __main__ - Printing 3 examples
05/30/2022 23:50:47 - INFO - __main__ -  [superglue-cb] premise: A: How do you feel about that. B: I don't really, I more, I don't know about the government as much as, uh, the people, uh, I wouldn't consider to be a threat at all and I really don't feel much like the Soviet Union itself is a threat anymore. [SEP] hypothesis: the Soviet Union itself is a threat still
05/30/2022 23:50:47 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:47 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/30/2022 23:50:47 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:47 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
05/30/2022 23:50:47 - INFO - __main__ - ['contradiction']
05/30/2022 23:50:47 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:50:47 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:50:47 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:50:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/30/2022 23:50:58 - INFO - __main__ - Starting training!
05/30/2022 23:51:02 - INFO - __main__ - Step 10 Global step 10 Train loss 23.162159 on epoch=1
05/30/2022 23:51:07 - INFO - __main__ - Step 20 Global step 20 Train loss 21.084667 on epoch=3
05/30/2022 23:51:12 - INFO - __main__ - Step 30 Global step 30 Train loss 17.486853 on epoch=5
05/30/2022 23:51:17 - INFO - __main__ - Step 40 Global step 40 Train loss 14.401024 on epoch=7
05/30/2022 23:51:22 - INFO - __main__ - Step 50 Global step 50 Train loss 12.089797 on epoch=9
05/30/2022 23:51:29 - INFO - __main__ - Global step 50 Train loss 17.644901 ACC 0.0 on epoch=9
05/30/2022 23:51:35 - INFO - __main__ - Step 60 Global step 60 Train loss 13.235532 on epoch=11
05/30/2022 23:51:40 - INFO - __main__ - Step 70 Global step 70 Train loss 11.992860 on epoch=13
05/30/2022 23:51:45 - INFO - __main__ - Step 80 Global step 80 Train loss 11.084872 on epoch=15
05/30/2022 23:51:50 - INFO - __main__ - Step 90 Global step 90 Train loss 10.599166 on epoch=17
05/30/2022 23:51:55 - INFO - __main__ - Step 100 Global step 100 Train loss 10.835203 on epoch=19
05/30/2022 23:51:58 - INFO - __main__ - Global step 100 Train loss 11.549527 ACC 0.046875 on epoch=19
05/30/2022 23:52:04 - INFO - __main__ - Step 110 Global step 110 Train loss 10.232542 on epoch=21
05/30/2022 23:52:09 - INFO - __main__ - Step 120 Global step 120 Train loss 9.389887 on epoch=23
05/30/2022 23:52:14 - INFO - __main__ - Step 130 Global step 130 Train loss 10.313082 on epoch=25
05/30/2022 23:52:19 - INFO - __main__ - Step 140 Global step 140 Train loss 9.319590 on epoch=27
05/30/2022 23:52:24 - INFO - __main__ - Step 150 Global step 150 Train loss 9.452624 on epoch=29
05/30/2022 23:52:26 - INFO - __main__ - Global step 150 Train loss 9.741546 ACC 0.0625 on epoch=29
05/30/2022 23:52:32 - INFO - __main__ - Step 160 Global step 160 Train loss 9.335861 on epoch=31
05/30/2022 23:52:37 - INFO - __main__ - Step 170 Global step 170 Train loss 8.885676 on epoch=33
05/30/2022 23:52:42 - INFO - __main__ - Step 180 Global step 180 Train loss 8.354481 on epoch=35
05/30/2022 23:52:47 - INFO - __main__ - Step 190 Global step 190 Train loss 8.068336 on epoch=37
05/30/2022 23:52:53 - INFO - __main__ - Step 200 Global step 200 Train loss 8.328655 on epoch=39
05/30/2022 23:52:54 - INFO - __main__ - Global step 200 Train loss 8.594602 ACC 0.0 on epoch=39
05/30/2022 23:53:00 - INFO - __main__ - Step 210 Global step 210 Train loss 7.847195 on epoch=41
05/30/2022 23:53:05 - INFO - __main__ - Step 220 Global step 220 Train loss 7.994241 on epoch=43
05/30/2022 23:53:10 - INFO - __main__ - Step 230 Global step 230 Train loss 7.664632 on epoch=45
05/30/2022 23:53:15 - INFO - __main__ - Step 240 Global step 240 Train loss 7.264894 on epoch=47
05/30/2022 23:53:20 - INFO - __main__ - Step 250 Global step 250 Train loss 6.747411 on epoch=49
05/30/2022 23:53:22 - INFO - __main__ - Global step 250 Train loss 7.503675 ACC 0.0 on epoch=49
05/30/2022 23:53:27 - INFO - __main__ - Step 260 Global step 260 Train loss 6.604338 on epoch=51
05/30/2022 23:53:33 - INFO - __main__ - Step 270 Global step 270 Train loss 6.791928 on epoch=53
05/30/2022 23:53:38 - INFO - __main__ - Step 280 Global step 280 Train loss 5.931979 on epoch=55
05/30/2022 23:53:43 - INFO - __main__ - Step 290 Global step 290 Train loss 6.363653 on epoch=57
05/30/2022 23:53:48 - INFO - __main__ - Step 300 Global step 300 Train loss 5.507311 on epoch=59
05/30/2022 23:53:50 - INFO - __main__ - Global step 300 Train loss 6.239842 ACC 0.015625 on epoch=59
05/30/2022 23:53:55 - INFO - __main__ - Step 310 Global step 310 Train loss 4.803204 on epoch=61
05/30/2022 23:54:00 - INFO - __main__ - Step 320 Global step 320 Train loss 4.718616 on epoch=63
05/30/2022 23:54:05 - INFO - __main__ - Step 330 Global step 330 Train loss 4.434804 on epoch=65
05/30/2022 23:54:10 - INFO - __main__ - Step 340 Global step 340 Train loss 3.814345 on epoch=67
05/30/2022 23:54:15 - INFO - __main__ - Step 350 Global step 350 Train loss 3.683974 on epoch=69
05/30/2022 23:54:17 - INFO - __main__ - Global step 350 Train loss 4.290988 ACC 0.015625 on epoch=69
05/30/2022 23:54:22 - INFO - __main__ - Step 360 Global step 360 Train loss 3.546544 on epoch=71
05/30/2022 23:54:27 - INFO - __main__ - Step 370 Global step 370 Train loss 3.112681 on epoch=73
05/30/2022 23:54:32 - INFO - __main__ - Step 380 Global step 380 Train loss 2.688175 on epoch=75
05/30/2022 23:54:37 - INFO - __main__ - Step 390 Global step 390 Train loss 2.878037 on epoch=77
05/30/2022 23:54:43 - INFO - __main__ - Step 400 Global step 400 Train loss 2.277287 on epoch=79
05/30/2022 23:54:44 - INFO - __main__ - Global step 400 Train loss 2.900545 ACC 0.484375 on epoch=79
05/30/2022 23:54:50 - INFO - __main__ - Step 410 Global step 410 Train loss 2.321285 on epoch=81
05/30/2022 23:54:55 - INFO - __main__ - Step 420 Global step 420 Train loss 2.652718 on epoch=83
05/30/2022 23:55:00 - INFO - __main__ - Step 430 Global step 430 Train loss 2.186822 on epoch=85
05/30/2022 23:55:05 - INFO - __main__ - Step 440 Global step 440 Train loss 2.681202 on epoch=87
05/30/2022 23:55:10 - INFO - __main__ - Step 450 Global step 450 Train loss 2.268155 on epoch=89
05/30/2022 23:55:12 - INFO - __main__ - Global step 450 Train loss 2.422037 ACC 0.5 on epoch=89
05/30/2022 23:55:18 - INFO - __main__ - Step 460 Global step 460 Train loss 1.904528 on epoch=91
05/30/2022 23:55:23 - INFO - __main__ - Step 470 Global step 470 Train loss 2.915371 on epoch=93
05/30/2022 23:55:28 - INFO - __main__ - Step 480 Global step 480 Train loss 2.271242 on epoch=95
05/30/2022 23:55:33 - INFO - __main__ - Step 490 Global step 490 Train loss 2.387673 on epoch=97
05/30/2022 23:55:38 - INFO - __main__ - Step 500 Global step 500 Train loss 1.814457 on epoch=99
05/30/2022 23:55:40 - INFO - __main__ - Global step 500 Train loss 2.258654 ACC 0.5 on epoch=99
05/30/2022 23:55:45 - INFO - __main__ - Step 510 Global step 510 Train loss 1.978684 on epoch=101
05/30/2022 23:55:50 - INFO - __main__ - Step 520 Global step 520 Train loss 1.930342 on epoch=103
05/30/2022 23:55:55 - INFO - __main__ - Step 530 Global step 530 Train loss 1.712970 on epoch=105
05/30/2022 23:56:00 - INFO - __main__ - Step 540 Global step 540 Train loss 2.265502 on epoch=107
05/30/2022 23:56:06 - INFO - __main__ - Step 550 Global step 550 Train loss 1.872039 on epoch=109
05/30/2022 23:56:06 - INFO - __main__ - Global step 550 Train loss 1.951907 ACC 0.5 on epoch=109
05/30/2022 23:56:12 - INFO - __main__ - Step 560 Global step 560 Train loss 1.977233 on epoch=111
05/30/2022 23:56:17 - INFO - __main__ - Step 570 Global step 570 Train loss 1.761314 on epoch=113
05/30/2022 23:56:22 - INFO - __main__ - Step 580 Global step 580 Train loss 1.762350 on epoch=115
05/30/2022 23:56:27 - INFO - __main__ - Step 590 Global step 590 Train loss 2.216306 on epoch=117
05/30/2022 23:56:32 - INFO - __main__ - Step 600 Global step 600 Train loss 1.649837 on epoch=119
05/30/2022 23:56:34 - INFO - __main__ - Global step 600 Train loss 1.873408 ACC 0.5 on epoch=119
05/30/2022 23:56:39 - INFO - __main__ - Step 610 Global step 610 Train loss 1.641671 on epoch=121
05/30/2022 23:56:44 - INFO - __main__ - Step 620 Global step 620 Train loss 1.621764 on epoch=123
05/30/2022 23:56:49 - INFO - __main__ - Step 630 Global step 630 Train loss 1.628942 on epoch=125
05/30/2022 23:56:54 - INFO - __main__ - Step 640 Global step 640 Train loss 1.160574 on epoch=127
05/30/2022 23:56:59 - INFO - __main__ - Step 650 Global step 650 Train loss 2.220781 on epoch=129
05/30/2022 23:57:01 - INFO - __main__ - Global step 650 Train loss 1.654747 ACC 0.5 on epoch=129
05/30/2022 23:57:06 - INFO - __main__ - Step 660 Global step 660 Train loss 1.579138 on epoch=131
05/30/2022 23:57:11 - INFO - __main__ - Step 670 Global step 670 Train loss 1.667344 on epoch=133
05/30/2022 23:57:16 - INFO - __main__ - Step 680 Global step 680 Train loss 1.669160 on epoch=135
05/30/2022 23:57:21 - INFO - __main__ - Step 690 Global step 690 Train loss 1.586273 on epoch=137
05/30/2022 23:57:26 - INFO - __main__ - Step 700 Global step 700 Train loss 1.424786 on epoch=139
05/30/2022 23:57:28 - INFO - __main__ - Global step 700 Train loss 1.585340 ACC 0.5 on epoch=139
05/30/2022 23:57:33 - INFO - __main__ - Step 710 Global step 710 Train loss 1.101376 on epoch=141
05/30/2022 23:57:38 - INFO - __main__ - Step 720 Global step 720 Train loss 1.555042 on epoch=143
05/30/2022 23:57:43 - INFO - __main__ - Step 730 Global step 730 Train loss 1.206572 on epoch=145
05/30/2022 23:57:48 - INFO - __main__ - Step 740 Global step 740 Train loss 1.767650 on epoch=147
05/30/2022 23:57:54 - INFO - __main__ - Step 750 Global step 750 Train loss 1.483099 on epoch=149
05/30/2022 23:57:55 - INFO - __main__ - Global step 750 Train loss 1.422748 ACC 0.5 on epoch=149
05/30/2022 23:58:00 - INFO - __main__ - Step 760 Global step 760 Train loss 1.319313 on epoch=151
05/30/2022 23:58:05 - INFO - __main__ - Step 770 Global step 770 Train loss 1.379306 on epoch=153
05/30/2022 23:58:10 - INFO - __main__ - Step 780 Global step 780 Train loss 1.124672 on epoch=155
05/30/2022 23:58:15 - INFO - __main__ - Step 790 Global step 790 Train loss 1.141243 on epoch=157
05/30/2022 23:58:20 - INFO - __main__ - Step 800 Global step 800 Train loss 1.257695 on epoch=159
05/30/2022 23:58:22 - INFO - __main__ - Global step 800 Train loss 1.244446 ACC 0.5 on epoch=159
05/30/2022 23:58:27 - INFO - __main__ - Step 810 Global step 810 Train loss 1.344873 on epoch=161
05/30/2022 23:58:32 - INFO - __main__ - Step 820 Global step 820 Train loss 1.199622 on epoch=163
05/30/2022 23:58:37 - INFO - __main__ - Step 830 Global step 830 Train loss 1.369567 on epoch=165
05/30/2022 23:58:42 - INFO - __main__ - Step 840 Global step 840 Train loss 1.070052 on epoch=167
05/30/2022 23:58:48 - INFO - __main__ - Step 850 Global step 850 Train loss 1.337317 on epoch=169
05/30/2022 23:58:49 - INFO - __main__ - Global step 850 Train loss 1.264286 ACC 0.703125 on epoch=169
05/30/2022 23:58:55 - INFO - __main__ - Step 860 Global step 860 Train loss 1.035401 on epoch=171
05/30/2022 23:59:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.906841 on epoch=173
05/30/2022 23:59:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.972962 on epoch=175
05/30/2022 23:59:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.855990 on epoch=177
05/30/2022 23:59:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.744952 on epoch=179
05/30/2022 23:59:17 - INFO - __main__ - Global step 900 Train loss 0.903229 ACC 0.5625 on epoch=179
05/30/2022 23:59:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.498571 on epoch=181
05/30/2022 23:59:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.316791 on epoch=183
05/30/2022 23:59:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.252271 on epoch=185
05/30/2022 23:59:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.220695 on epoch=187
05/30/2022 23:59:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.232085 on epoch=189
05/30/2022 23:59:44 - INFO - __main__ - Global step 950 Train loss 0.304083 ACC 0.890625 on epoch=189
05/30/2022 23:59:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.156687 on epoch=191
05/30/2022 23:59:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.129265 on epoch=193
05/31/2022 00:00:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.080475 on epoch=195
05/31/2022 00:00:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.076276 on epoch=197
05/31/2022 00:00:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.091020 on epoch=199
05/31/2022 00:00:12 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:00:12 - INFO - __main__ - Printing 3 examples
05/31/2022 00:00:12 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/31/2022 00:00:12 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:12 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/31/2022 00:00:12 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:12 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/31/2022 00:00:12 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:12 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:00:12 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:00:12 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:00:12 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:00:12 - INFO - __main__ - Printing 3 examples
05/31/2022 00:00:12 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
05/31/2022 00:00:12 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:12 - INFO - __main__ -  [superglue-cb] premise: A:  nanny, sort of? Uh-huh. Uh-huh. B: and you know, I could envision a society where that would happen and make an interesting, uh, uh, story or whatever. A: Yeah. B: I don't think I have a philosophical problem with that. [SEP] hypothesis: she has a philosophical problem with that
05/31/2022 00:00:12 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:12 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, you're probably right, two years might be a little too long. B: Yeah, and there will be a lot of rebellion in that and when you get people who have no desire to be there in the first place, I don't think that they're going to be serving anybody. [SEP] hypothesis: they're going to be serving somebody
05/31/2022 00:00:12 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:12 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:00:12 - INFO - __main__ - Global step 1000 Train loss 0.106744 ACC 0.765625 on epoch=199
05/31/2022 00:00:12 - INFO - __main__ - save last model!
05/31/2022 00:00:12 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:00:12 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:00:19 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 00:00:20 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 00:00:20 - INFO - __main__ - Printing 3 examples
05/31/2022 00:00:20 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 00:00:20 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:20 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 00:00:20 - INFO - __main__ - ['neutral']
05/31/2022 00:00:20 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 00:00:20 - INFO - __main__ - ['entailment']
05/31/2022 00:00:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:00:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:00:20 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 00:00:22 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_13_0.0001_8_predictions.txt
05/31/2022 00:00:22 - INFO - __main__ - ACC on test data: 0.8214
05/31/2022 00:00:22 - INFO - __main__ - prefix=superglue-cb_32_13, lr=0.0001, bsz=8, dev_performance=0.890625, test_performance=0.8214285714285714
05/31/2022 00:00:22 - INFO - __main__ - Running ... prefix=superglue-cb_32_21, lr=0.0005, bsz=8 ...
05/31/2022 00:00:23 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:00:23 - INFO - __main__ - Printing 3 examples
05/31/2022 00:00:23 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/31/2022 00:00:23 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:23 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/31/2022 00:00:23 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:23 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/31/2022 00:00:23 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:23 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:00:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:00:23 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:00:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:00:23 - INFO - __main__ - Printing 3 examples
05/31/2022 00:00:23 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
05/31/2022 00:00:23 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:23 - INFO - __main__ -  [superglue-cb] premise: A:  nanny, sort of? Uh-huh. Uh-huh. B: and you know, I could envision a society where that would happen and make an interesting, uh, uh, story or whatever. A: Yeah. B: I don't think I have a philosophical problem with that. [SEP] hypothesis: she has a philosophical problem with that
05/31/2022 00:00:23 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:23 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, you're probably right, two years might be a little too long. B: Yeah, and there will be a lot of rebellion in that and when you get people who have no desire to be there in the first place, I don't think that they're going to be serving anybody. [SEP] hypothesis: they're going to be serving somebody
05/31/2022 00:00:23 - INFO - __main__ - ['contradiction']
05/31/2022 00:00:23 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:00:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:00:23 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:00:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:00:25 - INFO - __main__ - Starting training!
05/31/2022 00:00:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:00:34 - INFO - __main__ - Starting training!
05/31/2022 00:00:38 - INFO - __main__ - Step 10 Global step 10 Train loss 23.923540 on epoch=1
05/31/2022 00:00:43 - INFO - __main__ - Step 20 Global step 20 Train loss 15.565264 on epoch=3
05/31/2022 00:00:47 - INFO - __main__ - Step 30 Global step 30 Train loss 12.852533 on epoch=5
05/31/2022 00:00:52 - INFO - __main__ - Step 40 Global step 40 Train loss 12.721674 on epoch=7
05/31/2022 00:00:57 - INFO - __main__ - Step 50 Global step 50 Train loss 10.022707 on epoch=9
05/31/2022 00:00:59 - INFO - __main__ - Global step 50 Train loss 15.017143 ACC 0.09375 on epoch=9
05/31/2022 00:01:05 - INFO - __main__ - Step 60 Global step 60 Train loss 8.977293 on epoch=11
05/31/2022 00:01:10 - INFO - __main__ - Step 70 Global step 70 Train loss 7.605936 on epoch=13
05/31/2022 00:01:15 - INFO - __main__ - Step 80 Global step 80 Train loss 6.462370 on epoch=15
05/31/2022 00:01:21 - INFO - __main__ - Step 90 Global step 90 Train loss 4.769724 on epoch=17
05/31/2022 00:01:26 - INFO - __main__ - Step 100 Global step 100 Train loss 3.285620 on epoch=19
05/31/2022 00:01:27 - INFO - __main__ - Global step 100 Train loss 6.220189 ACC 0.21875 on epoch=19
05/31/2022 00:01:33 - INFO - __main__ - Step 110 Global step 110 Train loss 2.497976 on epoch=21
05/31/2022 00:01:38 - INFO - __main__ - Step 120 Global step 120 Train loss 1.973050 on epoch=23
05/31/2022 00:01:43 - INFO - __main__ - Step 130 Global step 130 Train loss 4.001393 on epoch=25
05/31/2022 00:01:48 - INFO - __main__ - Step 140 Global step 140 Train loss 2.450372 on epoch=27
05/31/2022 00:01:54 - INFO - __main__ - Step 150 Global step 150 Train loss 2.205144 on epoch=29
05/31/2022 00:01:55 - INFO - __main__ - Global step 150 Train loss 2.625587 ACC 0.5 on epoch=29
05/31/2022 00:02:01 - INFO - __main__ - Step 160 Global step 160 Train loss 1.845472 on epoch=31
05/31/2022 00:02:06 - INFO - __main__ - Step 170 Global step 170 Train loss 2.136248 on epoch=33
05/31/2022 00:02:11 - INFO - __main__ - Step 180 Global step 180 Train loss 1.520875 on epoch=35
05/31/2022 00:02:16 - INFO - __main__ - Step 190 Global step 190 Train loss 1.418482 on epoch=37
05/31/2022 00:02:22 - INFO - __main__ - Step 200 Global step 200 Train loss 1.582941 on epoch=39
05/31/2022 00:02:23 - INFO - __main__ - Global step 200 Train loss 1.700804 ACC 0.5 on epoch=39
05/31/2022 00:02:28 - INFO - __main__ - Step 210 Global step 210 Train loss 1.464455 on epoch=41
05/31/2022 00:02:33 - INFO - __main__ - Step 220 Global step 220 Train loss 1.637423 on epoch=43
05/31/2022 00:02:38 - INFO - __main__ - Step 230 Global step 230 Train loss 1.383198 on epoch=45
05/31/2022 00:02:44 - INFO - __main__ - Step 240 Global step 240 Train loss 1.018295 on epoch=47
05/31/2022 00:02:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.999684 on epoch=49
05/31/2022 00:02:50 - INFO - __main__ - Global step 250 Train loss 1.300611 ACC 0.5 on epoch=49
05/31/2022 00:02:55 - INFO - __main__ - Step 260 Global step 260 Train loss 1.131115 on epoch=51
05/31/2022 00:03:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.970918 on epoch=53
05/31/2022 00:03:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.998450 on epoch=55
05/31/2022 00:03:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.918239 on epoch=57
05/31/2022 00:03:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.811774 on epoch=59
05/31/2022 00:03:18 - INFO - __main__ - Global step 300 Train loss 0.966099 ACC 0.5 on epoch=59
05/31/2022 00:03:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.788760 on epoch=61
05/31/2022 00:03:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.788910 on epoch=63
05/31/2022 00:03:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.798032 on epoch=65
05/31/2022 00:03:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.676517 on epoch=67
05/31/2022 00:03:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.803056 on epoch=69
05/31/2022 00:03:45 - INFO - __main__ - Global step 350 Train loss 0.771055 ACC 0.5 on epoch=69
05/31/2022 00:03:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.775123 on epoch=71
05/31/2022 00:03:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.672830 on epoch=73
05/31/2022 00:04:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.760899 on epoch=75
05/31/2022 00:04:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.543859 on epoch=77
05/31/2022 00:04:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.643709 on epoch=79
05/31/2022 00:04:12 - INFO - __main__ - Global step 400 Train loss 0.679284 ACC 0.6875 on epoch=79
05/31/2022 00:04:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.622796 on epoch=81
05/31/2022 00:04:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.549190 on epoch=83
05/31/2022 00:04:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.577002 on epoch=85
05/31/2022 00:04:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.558616 on epoch=87
05/31/2022 00:04:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.516402 on epoch=89
05/31/2022 00:04:41 - INFO - __main__ - Global step 450 Train loss 0.564801 ACC 0.53125 on epoch=89
05/31/2022 00:04:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.554520 on epoch=91
05/31/2022 00:04:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.495215 on epoch=93
05/31/2022 00:04:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.471815 on epoch=95
05/31/2022 00:05:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.516229 on epoch=97
05/31/2022 00:05:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.530055 on epoch=99
05/31/2022 00:05:08 - INFO - __main__ - Global step 500 Train loss 0.513567 ACC 0.5 on epoch=99
05/31/2022 00:05:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.568486 on epoch=101
05/31/2022 00:05:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.420609 on epoch=103
05/31/2022 00:05:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.490143 on epoch=105
05/31/2022 00:05:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.511406 on epoch=107
05/31/2022 00:05:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.500782 on epoch=109
05/31/2022 00:05:36 - INFO - __main__ - Global step 550 Train loss 0.498285 ACC 0.640625 on epoch=109
05/31/2022 00:05:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.488092 on epoch=111
05/31/2022 00:05:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.507182 on epoch=113
05/31/2022 00:05:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.454058 on epoch=115
05/31/2022 00:05:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.461287 on epoch=117
05/31/2022 00:06:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.417844 on epoch=119
05/31/2022 00:06:03 - INFO - __main__ - Global step 600 Train loss 0.465693 ACC 0.5 on epoch=119
05/31/2022 00:06:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.474628 on epoch=121
05/31/2022 00:06:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.428142 on epoch=123
05/31/2022 00:06:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.428898 on epoch=125
05/31/2022 00:06:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.452130 on epoch=127
05/31/2022 00:06:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.478672 on epoch=129
05/31/2022 00:06:31 - INFO - __main__ - Global step 650 Train loss 0.452494 ACC 0.5 on epoch=129
05/31/2022 00:06:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.452046 on epoch=131
05/31/2022 00:06:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.384527 on epoch=133
05/31/2022 00:06:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.439097 on epoch=135
05/31/2022 00:06:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.455690 on epoch=137
05/31/2022 00:06:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.439843 on epoch=139
05/31/2022 00:06:58 - INFO - __main__ - Global step 700 Train loss 0.434240 ACC 0.5 on epoch=139
05/31/2022 00:07:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.386996 on epoch=141
05/31/2022 00:07:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.419571 on epoch=143
05/31/2022 00:07:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.350731 on epoch=145
05/31/2022 00:07:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.425745 on epoch=147
05/31/2022 00:07:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.456033 on epoch=149
05/31/2022 00:07:26 - INFO - __main__ - Global step 750 Train loss 0.407815 ACC 0.5 on epoch=149
05/31/2022 00:07:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.385365 on epoch=151
05/31/2022 00:07:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.401347 on epoch=153
05/31/2022 00:07:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.370919 on epoch=155
05/31/2022 00:07:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.405575 on epoch=157
05/31/2022 00:07:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.446591 on epoch=159
05/31/2022 00:07:53 - INFO - __main__ - Global step 800 Train loss 0.401959 ACC 0.5 on epoch=159
05/31/2022 00:07:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.410027 on epoch=161
05/31/2022 00:08:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.342887 on epoch=163
05/31/2022 00:08:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.394418 on epoch=165
05/31/2022 00:08:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.402707 on epoch=167
05/31/2022 00:08:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.384094 on epoch=169
05/31/2022 00:08:21 - INFO - __main__ - Global step 850 Train loss 0.386827 ACC 0.5 on epoch=169
05/31/2022 00:08:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.374789 on epoch=171
05/31/2022 00:08:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.370714 on epoch=173
05/31/2022 00:08:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.369699 on epoch=175
05/31/2022 00:08:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.368790 on epoch=177
05/31/2022 00:08:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.356032 on epoch=179
05/31/2022 00:08:49 - INFO - __main__ - Global step 900 Train loss 0.368005 ACC 0.5 on epoch=179
05/31/2022 00:08:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.360338 on epoch=181
05/31/2022 00:08:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.351349 on epoch=183
05/31/2022 00:09:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.387683 on epoch=185
05/31/2022 00:09:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.377912 on epoch=187
05/31/2022 00:09:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.364126 on epoch=189
05/31/2022 00:09:16 - INFO - __main__ - Global step 950 Train loss 0.368282 ACC 0.515625 on epoch=189
05/31/2022 00:09:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.349983 on epoch=191
05/31/2022 00:09:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.379328 on epoch=193
05/31/2022 00:09:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.346126 on epoch=195
05/31/2022 00:09:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.374799 on epoch=197
05/31/2022 00:09:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.362814 on epoch=199
05/31/2022 00:09:42 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:09:42 - INFO - __main__ - Printing 3 examples
05/31/2022 00:09:42 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/31/2022 00:09:42 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:42 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/31/2022 00:09:42 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:42 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/31/2022 00:09:42 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:42 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:09:42 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:09:42 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:09:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:09:42 - INFO - __main__ - Printing 3 examples
05/31/2022 00:09:42 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
05/31/2022 00:09:42 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:42 - INFO - __main__ -  [superglue-cb] premise: A:  nanny, sort of? Uh-huh. Uh-huh. B: and you know, I could envision a society where that would happen and make an interesting, uh, uh, story or whatever. A: Yeah. B: I don't think I have a philosophical problem with that. [SEP] hypothesis: she has a philosophical problem with that
05/31/2022 00:09:42 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:42 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, you're probably right, two years might be a little too long. B: Yeah, and there will be a lot of rebellion in that and when you get people who have no desire to be there in the first place, I don't think that they're going to be serving anybody. [SEP] hypothesis: they're going to be serving somebody
05/31/2022 00:09:42 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:42 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:09:42 - INFO - __main__ - Global step 1000 Train loss 0.362610 ACC 0.5 on epoch=199
05/31/2022 00:09:42 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:09:42 - INFO - __main__ - save last model!
05/31/2022 00:09:42 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:09:49 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 00:09:50 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 00:09:50 - INFO - __main__ - Printing 3 examples
05/31/2022 00:09:50 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 00:09:50 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:50 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 00:09:50 - INFO - __main__ - ['neutral']
05/31/2022 00:09:50 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 00:09:50 - INFO - __main__ - ['entailment']
05/31/2022 00:09:50 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:09:50 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:09:50 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 00:09:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_21_0.0005_8_predictions.txt
05/31/2022 00:09:52 - INFO - __main__ - ACC on test data: 0.5893
05/31/2022 00:09:52 - INFO - __main__ - prefix=superglue-cb_32_21, lr=0.0005, bsz=8, dev_performance=0.6875, test_performance=0.5892857142857143
05/31/2022 00:09:52 - INFO - __main__ - Running ... prefix=superglue-cb_32_21, lr=0.0003, bsz=8 ...
05/31/2022 00:09:53 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:09:53 - INFO - __main__ - Printing 3 examples
05/31/2022 00:09:53 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/31/2022 00:09:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:53 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/31/2022 00:09:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:53 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/31/2022 00:09:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:53 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:09:53 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:09:53 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:09:53 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:09:53 - INFO - __main__ - Printing 3 examples
05/31/2022 00:09:53 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
05/31/2022 00:09:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:53 - INFO - __main__ -  [superglue-cb] premise: A:  nanny, sort of? Uh-huh. Uh-huh. B: and you know, I could envision a society where that would happen and make an interesting, uh, uh, story or whatever. A: Yeah. B: I don't think I have a philosophical problem with that. [SEP] hypothesis: she has a philosophical problem with that
05/31/2022 00:09:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:53 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, you're probably right, two years might be a little too long. B: Yeah, and there will be a lot of rebellion in that and when you get people who have no desire to be there in the first place, I don't think that they're going to be serving anybody. [SEP] hypothesis: they're going to be serving somebody
05/31/2022 00:09:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:09:53 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:09:53 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:09:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:09:53 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:09:53 - INFO - __main__ - Starting training!
05/31/2022 00:10:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:10:04 - INFO - __main__ - Starting training!
05/31/2022 00:10:08 - INFO - __main__ - Step 10 Global step 10 Train loss 24.312969 on epoch=1
05/31/2022 00:10:13 - INFO - __main__ - Step 20 Global step 20 Train loss 19.297750 on epoch=3
05/31/2022 00:10:19 - INFO - __main__ - Step 30 Global step 30 Train loss 13.644989 on epoch=5
05/31/2022 00:10:24 - INFO - __main__ - Step 40 Global step 40 Train loss 12.080570 on epoch=7
05/31/2022 00:10:29 - INFO - __main__ - Step 50 Global step 50 Train loss 10.837624 on epoch=9
05/31/2022 00:10:32 - INFO - __main__ - Global step 50 Train loss 16.034782 ACC 0.0 on epoch=9
05/31/2022 00:10:38 - INFO - __main__ - Step 60 Global step 60 Train loss 10.032155 on epoch=11
05/31/2022 00:10:43 - INFO - __main__ - Step 70 Global step 70 Train loss 9.067999 on epoch=13
05/31/2022 00:10:48 - INFO - __main__ - Step 80 Global step 80 Train loss 8.005266 on epoch=15
05/31/2022 00:10:53 - INFO - __main__ - Step 90 Global step 90 Train loss 7.146314 on epoch=17
05/31/2022 00:10:59 - INFO - __main__ - Step 100 Global step 100 Train loss 6.527308 on epoch=19
05/31/2022 00:11:00 - INFO - __main__ - Global step 100 Train loss 8.155809 ACC 0.0 on epoch=19
05/31/2022 00:11:05 - INFO - __main__ - Step 110 Global step 110 Train loss 5.845784 on epoch=21
05/31/2022 00:11:10 - INFO - __main__ - Step 120 Global step 120 Train loss 4.785109 on epoch=23
05/31/2022 00:11:15 - INFO - __main__ - Step 130 Global step 130 Train loss 4.802188 on epoch=25
05/31/2022 00:11:20 - INFO - __main__ - Step 140 Global step 140 Train loss 9.851030 on epoch=27
05/31/2022 00:11:25 - INFO - __main__ - Step 150 Global step 150 Train loss 9.474952 on epoch=29
05/31/2022 00:11:27 - INFO - __main__ - Global step 150 Train loss 6.951812 ACC 0.0 on epoch=29
05/31/2022 00:11:32 - INFO - __main__ - Step 160 Global step 160 Train loss 7.767231 on epoch=31
05/31/2022 00:11:37 - INFO - __main__ - Step 170 Global step 170 Train loss 6.809190 on epoch=33
05/31/2022 00:11:42 - INFO - __main__ - Step 180 Global step 180 Train loss 3.654082 on epoch=35
05/31/2022 00:11:47 - INFO - __main__ - Step 190 Global step 190 Train loss 1.903604 on epoch=37
05/31/2022 00:11:53 - INFO - __main__ - Step 200 Global step 200 Train loss 2.062522 on epoch=39
05/31/2022 00:11:54 - INFO - __main__ - Global step 200 Train loss 4.439326 ACC 0.5 on epoch=39
05/31/2022 00:12:00 - INFO - __main__ - Step 210 Global step 210 Train loss 1.758224 on epoch=41
05/31/2022 00:12:05 - INFO - __main__ - Step 220 Global step 220 Train loss 1.657883 on epoch=43
05/31/2022 00:12:10 - INFO - __main__ - Step 230 Global step 230 Train loss 1.689282 on epoch=45
05/31/2022 00:12:15 - INFO - __main__ - Step 240 Global step 240 Train loss 1.617560 on epoch=47
05/31/2022 00:12:21 - INFO - __main__ - Step 250 Global step 250 Train loss 1.680432 on epoch=49
05/31/2022 00:12:22 - INFO - __main__ - Global step 250 Train loss 1.680676 ACC 0.5 on epoch=49
05/31/2022 00:12:27 - INFO - __main__ - Step 260 Global step 260 Train loss 1.233376 on epoch=51
05/31/2022 00:12:32 - INFO - __main__ - Step 270 Global step 270 Train loss 1.631582 on epoch=53
05/31/2022 00:12:37 - INFO - __main__ - Step 280 Global step 280 Train loss 1.216977 on epoch=55
05/31/2022 00:12:43 - INFO - __main__ - Step 290 Global step 290 Train loss 1.485572 on epoch=57
05/31/2022 00:12:48 - INFO - __main__ - Step 300 Global step 300 Train loss 1.448677 on epoch=59
05/31/2022 00:12:49 - INFO - __main__ - Global step 300 Train loss 1.403237 ACC 0.5 on epoch=59
05/31/2022 00:12:54 - INFO - __main__ - Step 310 Global step 310 Train loss 1.161663 on epoch=61
05/31/2022 00:12:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.981161 on epoch=63
05/31/2022 00:13:05 - INFO - __main__ - Step 330 Global step 330 Train loss 1.520764 on epoch=65
05/31/2022 00:13:10 - INFO - __main__ - Step 340 Global step 340 Train loss 1.081937 on epoch=67
05/31/2022 00:13:15 - INFO - __main__ - Step 350 Global step 350 Train loss 1.027792 on epoch=69
05/31/2022 00:13:16 - INFO - __main__ - Global step 350 Train loss 1.154663 ACC 0.671875 on epoch=69
05/31/2022 00:13:22 - INFO - __main__ - Step 360 Global step 360 Train loss 1.028631 on epoch=71
05/31/2022 00:13:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.826722 on epoch=73
05/31/2022 00:13:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.838021 on epoch=75
05/31/2022 00:13:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.794841 on epoch=77
05/31/2022 00:13:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.756362 on epoch=79
05/31/2022 00:13:44 - INFO - __main__ - Global step 400 Train loss 0.848915 ACC 0.78125 on epoch=79
05/31/2022 00:13:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.626438 on epoch=81
05/31/2022 00:13:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.466026 on epoch=83
05/31/2022 00:14:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.570788 on epoch=85
05/31/2022 00:14:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.360081 on epoch=87
05/31/2022 00:14:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.509379 on epoch=89
05/31/2022 00:14:12 - INFO - __main__ - Global step 450 Train loss 0.506542 ACC 0.78125 on epoch=89
05/31/2022 00:14:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.662071 on epoch=91
05/31/2022 00:14:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.353117 on epoch=93
05/31/2022 00:14:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.250830 on epoch=95
05/31/2022 00:14:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.224352 on epoch=97
05/31/2022 00:14:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.203178 on epoch=99
05/31/2022 00:14:40 - INFO - __main__ - Global step 500 Train loss 0.338710 ACC 0.8125 on epoch=99
05/31/2022 00:14:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.215899 on epoch=101
05/31/2022 00:14:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.113328 on epoch=103
05/31/2022 00:14:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.183393 on epoch=105
05/31/2022 00:15:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.099134 on epoch=107
05/31/2022 00:15:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.071726 on epoch=109
05/31/2022 00:15:08 - INFO - __main__ - Global step 550 Train loss 0.136696 ACC 0.71875 on epoch=109
05/31/2022 00:15:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.055071 on epoch=111
05/31/2022 00:15:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.038805 on epoch=113
05/31/2022 00:15:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.063543 on epoch=115
05/31/2022 00:15:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.017675 on epoch=117
05/31/2022 00:15:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.024316 on epoch=119
05/31/2022 00:15:35 - INFO - __main__ - Global step 600 Train loss 0.039882 ACC 0.859375 on epoch=119
05/31/2022 00:15:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.012090 on epoch=121
05/31/2022 00:15:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.004462 on epoch=123
05/31/2022 00:15:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.027926 on epoch=125
05/31/2022 00:15:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.017219 on epoch=127
05/31/2022 00:16:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.003902 on epoch=129
05/31/2022 00:16:03 - INFO - __main__ - Global step 650 Train loss 0.013120 ACC 0.859375 on epoch=129
05/31/2022 00:16:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.004902 on epoch=131
05/31/2022 00:16:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001949 on epoch=133
05/31/2022 00:16:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000929 on epoch=135
05/31/2022 00:16:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000821 on epoch=137
05/31/2022 00:16:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001943 on epoch=139
05/31/2022 00:16:31 - INFO - __main__ - Global step 700 Train loss 0.002109 ACC 0.84375 on epoch=139
05/31/2022 00:16:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000741 on epoch=141
05/31/2022 00:16:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.003152 on epoch=143
05/31/2022 00:16:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.016775 on epoch=145
05/31/2022 00:16:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.001844 on epoch=147
05/31/2022 00:16:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000584 on epoch=149
05/31/2022 00:16:58 - INFO - __main__ - Global step 750 Train loss 0.004619 ACC 0.828125 on epoch=149
05/31/2022 00:17:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.005965 on epoch=151
05/31/2022 00:17:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001749 on epoch=153
05/31/2022 00:17:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000340 on epoch=155
05/31/2022 00:17:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002306 on epoch=157
05/31/2022 00:17:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000458 on epoch=159
05/31/2022 00:17:25 - INFO - __main__ - Global step 800 Train loss 0.002164 ACC 0.84375 on epoch=159
05/31/2022 00:17:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000292 on epoch=161
05/31/2022 00:17:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001464 on epoch=163
05/31/2022 00:17:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000504 on epoch=165
05/31/2022 00:17:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.010493 on epoch=167
05/31/2022 00:17:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.003276 on epoch=169
05/31/2022 00:17:53 - INFO - __main__ - Global step 850 Train loss 0.003206 ACC 0.90625 on epoch=169
05/31/2022 00:17:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.003131 on epoch=171
05/31/2022 00:18:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000399 on epoch=173
05/31/2022 00:18:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.004282 on epoch=175
05/31/2022 00:18:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002279 on epoch=177
05/31/2022 00:18:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001347 on epoch=179
05/31/2022 00:18:21 - INFO - __main__ - Global step 900 Train loss 0.002288 ACC 0.890625 on epoch=179
05/31/2022 00:18:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.003684 on epoch=181
05/31/2022 00:18:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.028115 on epoch=183
05/31/2022 00:18:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000243 on epoch=185
05/31/2022 00:18:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000311 on epoch=187
05/31/2022 00:18:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000394 on epoch=189
05/31/2022 00:18:49 - INFO - __main__ - Global step 950 Train loss 0.006550 ACC 0.90625 on epoch=189
05/31/2022 00:18:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000586 on epoch=191
05/31/2022 00:18:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000074 on epoch=193
05/31/2022 00:19:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000403 on epoch=195
05/31/2022 00:19:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000220 on epoch=197
05/31/2022 00:19:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000046 on epoch=199
05/31/2022 00:19:16 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:19:16 - INFO - __main__ - Printing 3 examples
05/31/2022 00:19:16 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/31/2022 00:19:16 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:16 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/31/2022 00:19:16 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:16 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/31/2022 00:19:16 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:16 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:19:16 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:19:16 - INFO - __main__ - Global step 1000 Train loss 0.000266 ACC 0.90625 on epoch=199
05/31/2022 00:19:16 - INFO - __main__ - save last model!
05/31/2022 00:19:16 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:19:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:19:16 - INFO - __main__ - Printing 3 examples
05/31/2022 00:19:16 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
05/31/2022 00:19:16 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:16 - INFO - __main__ -  [superglue-cb] premise: A:  nanny, sort of? Uh-huh. Uh-huh. B: and you know, I could envision a society where that would happen and make an interesting, uh, uh, story or whatever. A: Yeah. B: I don't think I have a philosophical problem with that. [SEP] hypothesis: she has a philosophical problem with that
05/31/2022 00:19:16 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:16 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, you're probably right, two years might be a little too long. B: Yeah, and there will be a lot of rebellion in that and when you get people who have no desire to be there in the first place, I don't think that they're going to be serving anybody. [SEP] hypothesis: they're going to be serving somebody
05/31/2022 00:19:16 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:16 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:19:16 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:19:16 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:19:23 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 00:19:24 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 00:19:24 - INFO - __main__ - Printing 3 examples
05/31/2022 00:19:24 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 00:19:24 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:24 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 00:19:24 - INFO - __main__ - ['neutral']
05/31/2022 00:19:24 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 00:19:24 - INFO - __main__ - ['entailment']
05/31/2022 00:19:24 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:19:24 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:19:24 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 00:19:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_21_0.0003_8_predictions.txt
05/31/2022 00:19:25 - INFO - __main__ - ACC on test data: 0.8393
05/31/2022 00:19:25 - INFO - __main__ - prefix=superglue-cb_32_21, lr=0.0003, bsz=8, dev_performance=0.90625, test_performance=0.8392857142857143
05/31/2022 00:19:25 - INFO - __main__ - Running ... prefix=superglue-cb_32_21, lr=0.0002, bsz=8 ...
05/31/2022 00:19:26 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:19:26 - INFO - __main__ - Printing 3 examples
05/31/2022 00:19:26 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/31/2022 00:19:26 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:26 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/31/2022 00:19:26 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:26 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/31/2022 00:19:26 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:26 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:19:26 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:19:26 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:19:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:19:26 - INFO - __main__ - Printing 3 examples
05/31/2022 00:19:26 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
05/31/2022 00:19:26 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:26 - INFO - __main__ -  [superglue-cb] premise: A:  nanny, sort of? Uh-huh. Uh-huh. B: and you know, I could envision a society where that would happen and make an interesting, uh, uh, story or whatever. A: Yeah. B: I don't think I have a philosophical problem with that. [SEP] hypothesis: she has a philosophical problem with that
05/31/2022 00:19:26 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:26 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, you're probably right, two years might be a little too long. B: Yeah, and there will be a lot of rebellion in that and when you get people who have no desire to be there in the first place, I don't think that they're going to be serving anybody. [SEP] hypothesis: they're going to be serving somebody
05/31/2022 00:19:26 - INFO - __main__ - ['contradiction']
05/31/2022 00:19:26 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:19:26 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:19:27 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:19:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:19:29 - INFO - __main__ - Starting training!
05/31/2022 00:19:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:19:39 - INFO - __main__ - Starting training!
05/31/2022 00:19:44 - INFO - __main__ - Step 10 Global step 10 Train loss 23.348141 on epoch=1
05/31/2022 00:19:49 - INFO - __main__ - Step 20 Global step 20 Train loss 19.641968 on epoch=3
05/31/2022 00:19:54 - INFO - __main__ - Step 30 Global step 30 Train loss 13.302542 on epoch=5
05/31/2022 00:19:59 - INFO - __main__ - Step 40 Global step 40 Train loss 11.119962 on epoch=7
05/31/2022 00:20:05 - INFO - __main__ - Step 50 Global step 50 Train loss 9.795737 on epoch=9
05/31/2022 00:20:06 - INFO - __main__ - Global step 50 Train loss 15.441671 ACC 0.0 on epoch=9
05/31/2022 00:20:12 - INFO - __main__ - Step 60 Global step 60 Train loss 10.018243 on epoch=11
05/31/2022 00:20:17 - INFO - __main__ - Step 70 Global step 70 Train loss 9.704334 on epoch=13
05/31/2022 00:20:22 - INFO - __main__ - Step 80 Global step 80 Train loss 9.503388 on epoch=15
05/31/2022 00:20:27 - INFO - __main__ - Step 90 Global step 90 Train loss 8.821281 on epoch=17
05/31/2022 00:20:33 - INFO - __main__ - Step 100 Global step 100 Train loss 8.360663 on epoch=19
05/31/2022 00:20:35 - INFO - __main__ - Global step 100 Train loss 9.281582 ACC 0.015625 on epoch=19
05/31/2022 00:20:40 - INFO - __main__ - Step 110 Global step 110 Train loss 8.114362 on epoch=21
05/31/2022 00:20:46 - INFO - __main__ - Step 120 Global step 120 Train loss 7.405797 on epoch=23
05/31/2022 00:20:51 - INFO - __main__ - Step 130 Global step 130 Train loss 7.128007 on epoch=25
05/31/2022 00:20:56 - INFO - __main__ - Step 140 Global step 140 Train loss 6.579279 on epoch=27
05/31/2022 00:21:01 - INFO - __main__ - Step 150 Global step 150 Train loss 5.335278 on epoch=29
05/31/2022 00:21:03 - INFO - __main__ - Global step 150 Train loss 6.912544 ACC 0.0 on epoch=29
05/31/2022 00:21:08 - INFO - __main__ - Step 160 Global step 160 Train loss 5.051865 on epoch=31
05/31/2022 00:21:14 - INFO - __main__ - Step 170 Global step 170 Train loss 4.379027 on epoch=33
05/31/2022 00:21:19 - INFO - __main__ - Step 180 Global step 180 Train loss 3.467804 on epoch=35
05/31/2022 00:21:24 - INFO - __main__ - Step 190 Global step 190 Train loss 2.944606 on epoch=37
05/31/2022 00:21:29 - INFO - __main__ - Step 200 Global step 200 Train loss 2.485272 on epoch=39
05/31/2022 00:21:30 - INFO - __main__ - Global step 200 Train loss 3.665715 ACC 0.5 on epoch=39
05/31/2022 00:21:36 - INFO - __main__ - Step 210 Global step 210 Train loss 2.756103 on epoch=41
05/31/2022 00:21:41 - INFO - __main__ - Step 220 Global step 220 Train loss 2.793614 on epoch=43
05/31/2022 00:21:46 - INFO - __main__ - Step 230 Global step 230 Train loss 3.024873 on epoch=45
05/31/2022 00:21:52 - INFO - __main__ - Step 240 Global step 240 Train loss 1.973627 on epoch=47
05/31/2022 00:21:57 - INFO - __main__ - Step 250 Global step 250 Train loss 2.627927 on epoch=49
05/31/2022 00:21:58 - INFO - __main__ - Global step 250 Train loss 2.635229 ACC 0.5 on epoch=49
05/31/2022 00:22:03 - INFO - __main__ - Step 260 Global step 260 Train loss 2.210075 on epoch=51
05/31/2022 00:22:09 - INFO - __main__ - Step 270 Global step 270 Train loss 1.735826 on epoch=53
05/31/2022 00:22:14 - INFO - __main__ - Step 280 Global step 280 Train loss 1.801168 on epoch=55
05/31/2022 00:22:19 - INFO - __main__ - Step 290 Global step 290 Train loss 1.999930 on epoch=57
05/31/2022 00:22:24 - INFO - __main__ - Step 300 Global step 300 Train loss 2.210981 on epoch=59
05/31/2022 00:22:25 - INFO - __main__ - Global step 300 Train loss 1.991596 ACC 0.5 on epoch=59
05/31/2022 00:22:31 - INFO - __main__ - Step 310 Global step 310 Train loss 2.096370 on epoch=61
05/31/2022 00:22:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.850957 on epoch=63
05/31/2022 00:22:41 - INFO - __main__ - Step 330 Global step 330 Train loss 1.263281 on epoch=65
05/31/2022 00:22:46 - INFO - __main__ - Step 340 Global step 340 Train loss 2.125157 on epoch=67
05/31/2022 00:22:52 - INFO - __main__ - Step 350 Global step 350 Train loss 1.391216 on epoch=69
05/31/2022 00:22:53 - INFO - __main__ - Global step 350 Train loss 1.745396 ACC 0.5 on epoch=69
05/31/2022 00:22:58 - INFO - __main__ - Step 360 Global step 360 Train loss 1.804200 on epoch=71
05/31/2022 00:23:03 - INFO - __main__ - Step 370 Global step 370 Train loss 1.623207 on epoch=73
05/31/2022 00:23:09 - INFO - __main__ - Step 380 Global step 380 Train loss 1.169525 on epoch=75
05/31/2022 00:23:14 - INFO - __main__ - Step 390 Global step 390 Train loss 1.359865 on epoch=77
05/31/2022 00:23:19 - INFO - __main__ - Step 400 Global step 400 Train loss 1.410153 on epoch=79
05/31/2022 00:23:20 - INFO - __main__ - Global step 400 Train loss 1.473390 ACC 0.546875 on epoch=79
05/31/2022 00:23:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.911331 on epoch=81
05/31/2022 00:23:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.856315 on epoch=83
05/31/2022 00:23:37 - INFO - __main__ - Step 430 Global step 430 Train loss 1.296142 on epoch=85
05/31/2022 00:23:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.908054 on epoch=87
05/31/2022 00:23:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.897114 on epoch=89
05/31/2022 00:23:48 - INFO - __main__ - Global step 450 Train loss 0.973791 ACC 0.546875 on epoch=89
05/31/2022 00:23:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.870025 on epoch=91
05/31/2022 00:23:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.766483 on epoch=93
05/31/2022 00:24:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.729535 on epoch=95
05/31/2022 00:24:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.943710 on epoch=97
05/31/2022 00:24:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.382829 on epoch=99
05/31/2022 00:24:16 - INFO - __main__ - Global step 500 Train loss 0.738516 ACC 0.8125 on epoch=99
05/31/2022 00:24:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.313065 on epoch=101
05/31/2022 00:24:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.226758 on epoch=103
05/31/2022 00:24:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.176934 on epoch=105
05/31/2022 00:24:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.129673 on epoch=107
05/31/2022 00:24:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.160976 on epoch=109
05/31/2022 00:24:44 - INFO - __main__ - Global step 550 Train loss 0.201481 ACC 0.84375 on epoch=109
05/31/2022 00:24:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.092894 on epoch=111
05/31/2022 00:24:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.072947 on epoch=113
05/31/2022 00:25:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.032736 on epoch=115
05/31/2022 00:25:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.028204 on epoch=117
05/31/2022 00:25:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.035222 on epoch=119
05/31/2022 00:25:12 - INFO - __main__ - Global step 600 Train loss 0.052401 ACC 0.859375 on epoch=119
05/31/2022 00:25:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.054240 on epoch=121
05/31/2022 00:25:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.025347 on epoch=123
05/31/2022 00:25:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.019312 on epoch=125
05/31/2022 00:25:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.007636 on epoch=127
05/31/2022 00:25:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.006765 on epoch=129
05/31/2022 00:25:41 - INFO - __main__ - Global step 650 Train loss 0.022660 ACC 0.828125 on epoch=129
05/31/2022 00:25:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.008960 on epoch=131
05/31/2022 00:25:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.024186 on epoch=133
05/31/2022 00:25:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.004316 on epoch=135
05/31/2022 00:26:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.002305 on epoch=137
05/31/2022 00:26:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001951 on epoch=139
05/31/2022 00:26:08 - INFO - __main__ - Global step 700 Train loss 0.008344 ACC 0.84375 on epoch=139
05/31/2022 00:26:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.005262 on epoch=141
05/31/2022 00:26:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.002532 on epoch=143
05/31/2022 00:26:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000786 on epoch=145
05/31/2022 00:26:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.001604 on epoch=147
05/31/2022 00:26:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001244 on epoch=149
05/31/2022 00:26:36 - INFO - __main__ - Global step 750 Train loss 0.002286 ACC 0.828125 on epoch=149
05/31/2022 00:26:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000527 on epoch=151
05/31/2022 00:26:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000949 on epoch=153
05/31/2022 00:26:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.021094 on epoch=155
05/31/2022 00:26:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.005177 on epoch=157
05/31/2022 00:27:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000794 on epoch=159
05/31/2022 00:27:03 - INFO - __main__ - Global step 800 Train loss 0.005708 ACC 0.828125 on epoch=159
05/31/2022 00:27:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.005352 on epoch=161
05/31/2022 00:27:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000766 on epoch=163
05/31/2022 00:27:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.004679 on epoch=165
05/31/2022 00:27:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000631 on epoch=167
05/31/2022 00:27:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000217 on epoch=169
05/31/2022 00:27:31 - INFO - __main__ - Global step 850 Train loss 0.002329 ACC 0.859375 on epoch=169
05/31/2022 00:27:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000241 on epoch=171
05/31/2022 00:27:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000150 on epoch=173
05/31/2022 00:27:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.010257 on epoch=175
05/31/2022 00:27:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.017262 on epoch=177
05/31/2022 00:27:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001845 on epoch=179
05/31/2022 00:27:58 - INFO - __main__ - Global step 900 Train loss 0.005951 ACC 0.859375 on epoch=179
05/31/2022 00:28:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.007334 on epoch=181
05/31/2022 00:28:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.001152 on epoch=183
05/31/2022 00:28:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000171 on epoch=185
05/31/2022 00:28:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000185 on epoch=187
05/31/2022 00:28:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000183 on epoch=189
05/31/2022 00:28:26 - INFO - __main__ - Global step 950 Train loss 0.001805 ACC 0.859375 on epoch=189
05/31/2022 00:28:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000947 on epoch=191
05/31/2022 00:28:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000062 on epoch=193
05/31/2022 00:28:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000324 on epoch=195
05/31/2022 00:28:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000524 on epoch=197
05/31/2022 00:28:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001083 on epoch=199
05/31/2022 00:28:53 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:28:53 - INFO - __main__ - Printing 3 examples
05/31/2022 00:28:53 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/31/2022 00:28:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:28:53 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/31/2022 00:28:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:28:53 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/31/2022 00:28:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:28:53 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:28:53 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:28:53 - INFO - __main__ - Global step 1000 Train loss 0.000588 ACC 0.859375 on epoch=199
05/31/2022 00:28:53 - INFO - __main__ - save last model!
05/31/2022 00:28:53 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:28:53 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:28:53 - INFO - __main__ - Printing 3 examples
05/31/2022 00:28:53 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
05/31/2022 00:28:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:28:53 - INFO - __main__ -  [superglue-cb] premise: A:  nanny, sort of? Uh-huh. Uh-huh. B: and you know, I could envision a society where that would happen and make an interesting, uh, uh, story or whatever. A: Yeah. B: I don't think I have a philosophical problem with that. [SEP] hypothesis: she has a philosophical problem with that
05/31/2022 00:28:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:28:53 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, you're probably right, two years might be a little too long. B: Yeah, and there will be a lot of rebellion in that and when you get people who have no desire to be there in the first place, I don't think that they're going to be serving anybody. [SEP] hypothesis: they're going to be serving somebody
05/31/2022 00:28:53 - INFO - __main__ - ['contradiction']
05/31/2022 00:28:53 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:28:53 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:28:53 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:29:00 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 00:29:00 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 00:29:00 - INFO - __main__ - Printing 3 examples
05/31/2022 00:29:00 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 00:29:00 - INFO - __main__ - ['contradiction']
05/31/2022 00:29:00 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 00:29:00 - INFO - __main__ - ['neutral']
05/31/2022 00:29:00 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 00:29:00 - INFO - __main__ - ['entailment']
05/31/2022 00:29:00 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:29:00 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:29:00 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 00:29:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_21_0.0002_8_predictions.txt
05/31/2022 00:29:02 - INFO - __main__ - ACC on test data: 0.8571
05/31/2022 00:29:02 - INFO - __main__ - prefix=superglue-cb_32_21, lr=0.0002, bsz=8, dev_performance=0.859375, test_performance=0.8571428571428571
05/31/2022 00:29:02 - INFO - __main__ - Running ... prefix=superglue-cb_32_21, lr=0.0001, bsz=8 ...
05/31/2022 00:29:03 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:29:03 - INFO - __main__ - Printing 3 examples
05/31/2022 00:29:03 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/31/2022 00:29:03 - INFO - __main__ - ['contradiction']
05/31/2022 00:29:03 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/31/2022 00:29:03 - INFO - __main__ - ['contradiction']
05/31/2022 00:29:03 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/31/2022 00:29:03 - INFO - __main__ - ['contradiction']
05/31/2022 00:29:03 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:29:03 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:29:03 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:29:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:29:03 - INFO - __main__ - Printing 3 examples
05/31/2022 00:29:03 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
05/31/2022 00:29:03 - INFO - __main__ - ['contradiction']
05/31/2022 00:29:03 - INFO - __main__ -  [superglue-cb] premise: A:  nanny, sort of? Uh-huh. Uh-huh. B: and you know, I could envision a society where that would happen and make an interesting, uh, uh, story or whatever. A: Yeah. B: I don't think I have a philosophical problem with that. [SEP] hypothesis: she has a philosophical problem with that
05/31/2022 00:29:03 - INFO - __main__ - ['contradiction']
05/31/2022 00:29:03 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, you're probably right, two years might be a little too long. B: Yeah, and there will be a lot of rebellion in that and when you get people who have no desire to be there in the first place, I don't think that they're going to be serving anybody. [SEP] hypothesis: they're going to be serving somebody
05/31/2022 00:29:03 - INFO - __main__ - ['contradiction']
05/31/2022 00:29:03 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:29:03 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:29:03 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:29:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:29:06 - INFO - __main__ - Starting training!
05/31/2022 00:29:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:29:16 - INFO - __main__ - Starting training!
05/31/2022 00:29:21 - INFO - __main__ - Step 10 Global step 10 Train loss 24.500719 on epoch=1
05/31/2022 00:29:25 - INFO - __main__ - Step 20 Global step 20 Train loss 22.017923 on epoch=3
05/31/2022 00:29:31 - INFO - __main__ - Step 30 Global step 30 Train loss 17.006084 on epoch=5
05/31/2022 00:29:36 - INFO - __main__ - Step 40 Global step 40 Train loss 14.869436 on epoch=7
05/31/2022 00:29:41 - INFO - __main__ - Step 50 Global step 50 Train loss 13.496069 on epoch=9
05/31/2022 00:29:52 - INFO - __main__ - Global step 50 Train loss 18.378046 ACC 0.0 on epoch=9
05/31/2022 00:29:57 - INFO - __main__ - Step 60 Global step 60 Train loss 12.758024 on epoch=11
05/31/2022 00:30:03 - INFO - __main__ - Step 70 Global step 70 Train loss 12.394320 on epoch=13
05/31/2022 00:30:08 - INFO - __main__ - Step 80 Global step 80 Train loss 11.809921 on epoch=15
05/31/2022 00:30:13 - INFO - __main__ - Step 90 Global step 90 Train loss 11.574501 on epoch=17
05/31/2022 00:30:18 - INFO - __main__ - Step 100 Global step 100 Train loss 11.446573 on epoch=19
05/31/2022 00:30:21 - INFO - __main__ - Global step 100 Train loss 11.996668 ACC 0.0 on epoch=19
05/31/2022 00:30:27 - INFO - __main__ - Step 110 Global step 110 Train loss 11.506100 on epoch=21
05/31/2022 00:30:32 - INFO - __main__ - Step 120 Global step 120 Train loss 10.664321 on epoch=23
05/31/2022 00:30:37 - INFO - __main__ - Step 130 Global step 130 Train loss 10.394773 on epoch=25
05/31/2022 00:30:42 - INFO - __main__ - Step 140 Global step 140 Train loss 10.251019 on epoch=27
05/31/2022 00:30:47 - INFO - __main__ - Step 150 Global step 150 Train loss 9.618366 on epoch=29
05/31/2022 00:30:50 - INFO - __main__ - Global step 150 Train loss 10.486916 ACC 0.078125 on epoch=29
05/31/2022 00:30:56 - INFO - __main__ - Step 160 Global step 160 Train loss 10.079140 on epoch=31
05/31/2022 00:31:01 - INFO - __main__ - Step 170 Global step 170 Train loss 9.428194 on epoch=33
05/31/2022 00:31:06 - INFO - __main__ - Step 180 Global step 180 Train loss 9.200157 on epoch=35
05/31/2022 00:31:11 - INFO - __main__ - Step 190 Global step 190 Train loss 8.897570 on epoch=37
05/31/2022 00:31:16 - INFO - __main__ - Step 200 Global step 200 Train loss 9.182165 on epoch=39
05/31/2022 00:31:18 - INFO - __main__ - Global step 200 Train loss 9.357446 ACC 0.0625 on epoch=39
05/31/2022 00:31:23 - INFO - __main__ - Step 210 Global step 210 Train loss 8.750129 on epoch=41
05/31/2022 00:31:29 - INFO - __main__ - Step 220 Global step 220 Train loss 8.053236 on epoch=43
05/31/2022 00:31:34 - INFO - __main__ - Step 230 Global step 230 Train loss 7.898238 on epoch=45
05/31/2022 00:31:39 - INFO - __main__ - Step 240 Global step 240 Train loss 7.800774 on epoch=47
05/31/2022 00:31:44 - INFO - __main__ - Step 250 Global step 250 Train loss 7.443494 on epoch=49
05/31/2022 00:31:46 - INFO - __main__ - Global step 250 Train loss 7.989173 ACC 0.015625 on epoch=49
05/31/2022 00:31:51 - INFO - __main__ - Step 260 Global step 260 Train loss 7.659917 on epoch=51
05/31/2022 00:31:56 - INFO - __main__ - Step 270 Global step 270 Train loss 7.319337 on epoch=53
05/31/2022 00:32:02 - INFO - __main__ - Step 280 Global step 280 Train loss 7.781584 on epoch=55
05/31/2022 00:32:07 - INFO - __main__ - Step 290 Global step 290 Train loss 7.087312 on epoch=57
05/31/2022 00:32:12 - INFO - __main__ - Step 300 Global step 300 Train loss 6.097167 on epoch=59
05/31/2022 00:32:14 - INFO - __main__ - Global step 300 Train loss 7.189063 ACC 0.046875 on epoch=59
05/31/2022 00:32:19 - INFO - __main__ - Step 310 Global step 310 Train loss 6.088425 on epoch=61
05/31/2022 00:32:25 - INFO - __main__ - Step 320 Global step 320 Train loss 6.184614 on epoch=63
05/31/2022 00:32:30 - INFO - __main__ - Step 330 Global step 330 Train loss 5.676678 on epoch=65
05/31/2022 00:32:35 - INFO - __main__ - Step 340 Global step 340 Train loss 5.425599 on epoch=67
05/31/2022 00:32:40 - INFO - __main__ - Step 350 Global step 350 Train loss 4.869436 on epoch=69
05/31/2022 00:32:42 - INFO - __main__ - Global step 350 Train loss 5.648950 ACC 0.0625 on epoch=69
05/31/2022 00:32:47 - INFO - __main__ - Step 360 Global step 360 Train loss 5.112078 on epoch=71
05/31/2022 00:32:52 - INFO - __main__ - Step 370 Global step 370 Train loss 5.160444 on epoch=73
05/31/2022 00:32:57 - INFO - __main__ - Step 380 Global step 380 Train loss 3.926343 on epoch=75
05/31/2022 00:33:03 - INFO - __main__ - Step 390 Global step 390 Train loss 3.634807 on epoch=77
05/31/2022 00:33:08 - INFO - __main__ - Step 400 Global step 400 Train loss 3.346336 on epoch=79
05/31/2022 00:33:09 - INFO - __main__ - Global step 400 Train loss 4.236001 ACC 0.03125 on epoch=79
05/31/2022 00:33:14 - INFO - __main__ - Step 410 Global step 410 Train loss 3.210258 on epoch=81
05/31/2022 00:33:20 - INFO - __main__ - Step 420 Global step 420 Train loss 3.049604 on epoch=83
05/31/2022 00:33:25 - INFO - __main__ - Step 430 Global step 430 Train loss 3.421425 on epoch=85
05/31/2022 00:33:30 - INFO - __main__ - Step 440 Global step 440 Train loss 2.808021 on epoch=87
05/31/2022 00:33:35 - INFO - __main__ - Step 450 Global step 450 Train loss 2.518217 on epoch=89
05/31/2022 00:33:37 - INFO - __main__ - Global step 450 Train loss 3.001505 ACC 0.421875 on epoch=89
05/31/2022 00:33:43 - INFO - __main__ - Step 460 Global step 460 Train loss 2.443690 on epoch=91
05/31/2022 00:33:48 - INFO - __main__ - Step 470 Global step 470 Train loss 2.442703 on epoch=93
05/31/2022 00:33:53 - INFO - __main__ - Step 480 Global step 480 Train loss 2.156738 on epoch=95
05/31/2022 00:33:58 - INFO - __main__ - Step 490 Global step 490 Train loss 2.418039 on epoch=97
05/31/2022 00:34:04 - INFO - __main__ - Step 500 Global step 500 Train loss 2.773106 on epoch=99
05/31/2022 00:34:05 - INFO - __main__ - Global step 500 Train loss 2.446855 ACC 0.5 on epoch=99
05/31/2022 00:34:11 - INFO - __main__ - Step 510 Global step 510 Train loss 2.428831 on epoch=101
05/31/2022 00:34:16 - INFO - __main__ - Step 520 Global step 520 Train loss 1.977806 on epoch=103
05/31/2022 00:34:21 - INFO - __main__ - Step 530 Global step 530 Train loss 2.622183 on epoch=105
05/31/2022 00:34:26 - INFO - __main__ - Step 540 Global step 540 Train loss 2.248345 on epoch=107
05/31/2022 00:34:32 - INFO - __main__ - Step 550 Global step 550 Train loss 2.041065 on epoch=109
05/31/2022 00:34:33 - INFO - __main__ - Global step 550 Train loss 2.263646 ACC 0.5 on epoch=109
05/31/2022 00:34:38 - INFO - __main__ - Step 560 Global step 560 Train loss 2.272444 on epoch=111
05/31/2022 00:34:43 - INFO - __main__ - Step 570 Global step 570 Train loss 1.900121 on epoch=113
05/31/2022 00:34:49 - INFO - __main__ - Step 580 Global step 580 Train loss 2.333894 on epoch=115
05/31/2022 00:34:54 - INFO - __main__ - Step 590 Global step 590 Train loss 2.719611 on epoch=117
05/31/2022 00:34:59 - INFO - __main__ - Step 600 Global step 600 Train loss 2.482423 on epoch=119
05/31/2022 00:35:00 - INFO - __main__ - Global step 600 Train loss 2.341698 ACC 0.5 on epoch=119
05/31/2022 00:35:05 - INFO - __main__ - Step 610 Global step 610 Train loss 2.205463 on epoch=121
05/31/2022 00:35:10 - INFO - __main__ - Step 620 Global step 620 Train loss 2.220019 on epoch=123
05/31/2022 00:35:16 - INFO - __main__ - Step 630 Global step 630 Train loss 2.767263 on epoch=125
05/31/2022 00:35:21 - INFO - __main__ - Step 640 Global step 640 Train loss 2.160194 on epoch=127
05/31/2022 00:35:26 - INFO - __main__ - Step 650 Global step 650 Train loss 2.233296 on epoch=129
05/31/2022 00:35:27 - INFO - __main__ - Global step 650 Train loss 2.317247 ACC 0.5 on epoch=129
05/31/2022 00:35:33 - INFO - __main__ - Step 660 Global step 660 Train loss 1.581639 on epoch=131
05/31/2022 00:35:38 - INFO - __main__ - Step 670 Global step 670 Train loss 2.160713 on epoch=133
05/31/2022 00:35:43 - INFO - __main__ - Step 680 Global step 680 Train loss 1.815850 on epoch=135
05/31/2022 00:35:48 - INFO - __main__ - Step 690 Global step 690 Train loss 2.042019 on epoch=137
05/31/2022 00:35:53 - INFO - __main__ - Step 700 Global step 700 Train loss 1.824041 on epoch=139
05/31/2022 00:35:55 - INFO - __main__ - Global step 700 Train loss 1.884852 ACC 0.5 on epoch=139
05/31/2022 00:36:00 - INFO - __main__ - Step 710 Global step 710 Train loss 2.135753 on epoch=141
05/31/2022 00:36:05 - INFO - __main__ - Step 720 Global step 720 Train loss 1.529704 on epoch=143
05/31/2022 00:36:10 - INFO - __main__ - Step 730 Global step 730 Train loss 1.721696 on epoch=145
05/31/2022 00:36:15 - INFO - __main__ - Step 740 Global step 740 Train loss 1.546372 on epoch=147
05/31/2022 00:36:21 - INFO - __main__ - Step 750 Global step 750 Train loss 1.720716 on epoch=149
05/31/2022 00:36:22 - INFO - __main__ - Global step 750 Train loss 1.730848 ACC 0.5 on epoch=149
05/31/2022 00:36:27 - INFO - __main__ - Step 760 Global step 760 Train loss 1.852956 on epoch=151
05/31/2022 00:36:32 - INFO - __main__ - Step 770 Global step 770 Train loss 1.910459 on epoch=153
05/31/2022 00:36:37 - INFO - __main__ - Step 780 Global step 780 Train loss 1.430060 on epoch=155
05/31/2022 00:36:43 - INFO - __main__ - Step 790 Global step 790 Train loss 1.467877 on epoch=157
05/31/2022 00:36:48 - INFO - __main__ - Step 800 Global step 800 Train loss 1.463524 on epoch=159
05/31/2022 00:36:49 - INFO - __main__ - Global step 800 Train loss 1.624975 ACC 0.5 on epoch=159
05/31/2022 00:36:54 - INFO - __main__ - Step 810 Global step 810 Train loss 1.524847 on epoch=161
05/31/2022 00:36:59 - INFO - __main__ - Step 820 Global step 820 Train loss 1.557334 on epoch=163
05/31/2022 00:37:05 - INFO - __main__ - Step 830 Global step 830 Train loss 1.651069 on epoch=165
05/31/2022 00:37:10 - INFO - __main__ - Step 840 Global step 840 Train loss 1.429660 on epoch=167
05/31/2022 00:37:15 - INFO - __main__ - Step 850 Global step 850 Train loss 1.424546 on epoch=169
05/31/2022 00:37:16 - INFO - __main__ - Global step 850 Train loss 1.517491 ACC 0.5 on epoch=169
05/31/2022 00:37:21 - INFO - __main__ - Step 860 Global step 860 Train loss 1.384803 on epoch=171
05/31/2022 00:37:27 - INFO - __main__ - Step 870 Global step 870 Train loss 1.159850 on epoch=173
05/31/2022 00:37:32 - INFO - __main__ - Step 880 Global step 880 Train loss 1.160102 on epoch=175
05/31/2022 00:37:37 - INFO - __main__ - Step 890 Global step 890 Train loss 1.207806 on epoch=177
05/31/2022 00:37:42 - INFO - __main__ - Step 900 Global step 900 Train loss 1.385728 on epoch=179
05/31/2022 00:37:44 - INFO - __main__ - Global step 900 Train loss 1.259658 ACC 0.5 on epoch=179
05/31/2022 00:37:49 - INFO - __main__ - Step 910 Global step 910 Train loss 1.304760 on epoch=181
05/31/2022 00:37:54 - INFO - __main__ - Step 920 Global step 920 Train loss 1.201900 on epoch=183
05/31/2022 00:37:59 - INFO - __main__ - Step 930 Global step 930 Train loss 1.681865 on epoch=185
05/31/2022 00:38:04 - INFO - __main__ - Step 940 Global step 940 Train loss 1.163381 on epoch=187
05/31/2022 00:38:10 - INFO - __main__ - Step 950 Global step 950 Train loss 1.283533 on epoch=189
05/31/2022 00:38:11 - INFO - __main__ - Global step 950 Train loss 1.327088 ACC 0.5 on epoch=189
05/31/2022 00:38:16 - INFO - __main__ - Step 960 Global step 960 Train loss 1.430554 on epoch=191
05/31/2022 00:38:21 - INFO - __main__ - Step 970 Global step 970 Train loss 1.314531 on epoch=193
05/31/2022 00:38:27 - INFO - __main__ - Step 980 Global step 980 Train loss 1.147335 on epoch=195
05/31/2022 00:38:32 - INFO - __main__ - Step 990 Global step 990 Train loss 1.351375 on epoch=197
05/31/2022 00:38:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.341628 on epoch=199
05/31/2022 00:38:38 - INFO - __main__ - Global step 1000 Train loss 1.317084 ACC 0.5 on epoch=199
05/31/2022 00:38:38 - INFO - __main__ - save last model!
05/31/2022 00:38:38 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:38:38 - INFO - __main__ - Printing 3 examples
05/31/2022 00:38:38 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/31/2022 00:38:38 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:38 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/31/2022 00:38:38 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:38 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/31/2022 00:38:38 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:38 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:38:38 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:38:38 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:38:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:38:38 - INFO - __main__ - Printing 3 examples
05/31/2022 00:38:38 - INFO - __main__ -  [superglue-cb] premise: B: I do not know. I wonder where he gets it? You know, you must, I think TV is bad. Because they, uh, show all sorts of violence on, A: That and I do not think a lot of parents, I mean, I do not know how it is in the Air Force base. But, uh, I just do not think a lot of people, because of the economy, both need to work, you know. I just do not think a lot of parents are that involved any more. [SEP] hypothesis: a lot of parents are that involved
05/31/2022 00:38:38 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:38 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
05/31/2022 00:38:38 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:38 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
05/31/2022 00:38:38 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:38 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:38:39 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:38:39 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:38:45 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 00:38:46 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 00:38:46 - INFO - __main__ - Printing 3 examples
05/31/2022 00:38:46 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 00:38:46 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:46 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 00:38:46 - INFO - __main__ - ['neutral']
05/31/2022 00:38:46 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 00:38:46 - INFO - __main__ - ['entailment']
05/31/2022 00:38:46 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:38:46 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:38:46 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 00:38:48 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_21_0.0001_8_predictions.txt
05/31/2022 00:38:48 - INFO - __main__ - ACC on test data: 0.5000
05/31/2022 00:38:48 - INFO - __main__ - prefix=superglue-cb_32_21, lr=0.0001, bsz=8, dev_performance=0.5, test_performance=0.5
05/31/2022 00:38:48 - INFO - __main__ - Running ... prefix=superglue-cb_32_42, lr=0.0005, bsz=8 ...
05/31/2022 00:38:49 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:38:49 - INFO - __main__ - Printing 3 examples
05/31/2022 00:38:49 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/31/2022 00:38:49 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:49 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/31/2022 00:38:49 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:49 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/31/2022 00:38:49 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:49 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:38:49 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:38:49 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:38:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:38:49 - INFO - __main__ - Printing 3 examples
05/31/2022 00:38:49 - INFO - __main__ -  [superglue-cb] premise: B: I do not know. I wonder where he gets it? You know, you must, I think TV is bad. Because they, uh, show all sorts of violence on, A: That and I do not think a lot of parents, I mean, I do not know how it is in the Air Force base. But, uh, I just do not think a lot of people, because of the economy, both need to work, you know. I just do not think a lot of parents are that involved any more. [SEP] hypothesis: a lot of parents are that involved
05/31/2022 00:38:49 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:49 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
05/31/2022 00:38:49 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:49 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
05/31/2022 00:38:49 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:49 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:38:49 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:38:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:38:49 - INFO - __main__ - Starting training!
05/31/2022 00:38:49 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:39:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:39:02 - INFO - __main__ - Starting training!
05/31/2022 00:39:07 - INFO - __main__ - Step 10 Global step 10 Train loss 23.754978 on epoch=1
05/31/2022 00:39:12 - INFO - __main__ - Step 20 Global step 20 Train loss 12.567850 on epoch=3
05/31/2022 00:39:17 - INFO - __main__ - Step 30 Global step 30 Train loss 10.642684 on epoch=5
05/31/2022 00:39:22 - INFO - __main__ - Step 40 Global step 40 Train loss 9.475576 on epoch=7
05/31/2022 00:39:27 - INFO - __main__ - Step 50 Global step 50 Train loss 7.912904 on epoch=9
05/31/2022 00:39:29 - INFO - __main__ - Global step 50 Train loss 12.870798 ACC 0.0 on epoch=9
05/31/2022 00:39:35 - INFO - __main__ - Step 60 Global step 60 Train loss 7.587758 on epoch=11
05/31/2022 00:39:40 - INFO - __main__ - Step 70 Global step 70 Train loss 5.618158 on epoch=13
05/31/2022 00:39:45 - INFO - __main__ - Step 80 Global step 80 Train loss 3.932697 on epoch=15
05/31/2022 00:39:51 - INFO - __main__ - Step 90 Global step 90 Train loss 2.797342 on epoch=17
05/31/2022 00:39:56 - INFO - __main__ - Step 100 Global step 100 Train loss 2.373125 on epoch=19
05/31/2022 00:39:57 - INFO - __main__ - Global step 100 Train loss 4.461816 ACC 0.5 on epoch=19
05/31/2022 00:40:03 - INFO - __main__ - Step 110 Global step 110 Train loss 2.273630 on epoch=21
05/31/2022 00:40:07 - INFO - __main__ - Step 120 Global step 120 Train loss 2.219482 on epoch=23
05/31/2022 00:40:13 - INFO - __main__ - Step 130 Global step 130 Train loss 1.983282 on epoch=25
05/31/2022 00:40:18 - INFO - __main__ - Step 140 Global step 140 Train loss 2.122350 on epoch=27
05/31/2022 00:40:23 - INFO - __main__ - Step 150 Global step 150 Train loss 1.704774 on epoch=29
05/31/2022 00:40:24 - INFO - __main__ - Global step 150 Train loss 2.060704 ACC 0.5 on epoch=29
05/31/2022 00:40:29 - INFO - __main__ - Step 160 Global step 160 Train loss 1.417677 on epoch=31
05/31/2022 00:40:34 - INFO - __main__ - Step 170 Global step 170 Train loss 1.362089 on epoch=33
05/31/2022 00:40:40 - INFO - __main__ - Step 180 Global step 180 Train loss 1.534814 on epoch=35
05/31/2022 00:40:45 - INFO - __main__ - Step 190 Global step 190 Train loss 1.498959 on epoch=37
05/31/2022 00:40:50 - INFO - __main__ - Step 200 Global step 200 Train loss 1.306342 on epoch=39
05/31/2022 00:40:51 - INFO - __main__ - Global step 200 Train loss 1.423976 ACC 0.5 on epoch=39
05/31/2022 00:40:57 - INFO - __main__ - Step 210 Global step 210 Train loss 1.352015 on epoch=41
05/31/2022 00:41:02 - INFO - __main__ - Step 220 Global step 220 Train loss 1.118666 on epoch=43
05/31/2022 00:41:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.768800 on epoch=45
05/31/2022 00:41:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.851325 on epoch=47
05/31/2022 00:41:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.719267 on epoch=49
05/31/2022 00:41:19 - INFO - __main__ - Global step 250 Train loss 0.962015 ACC 0.5 on epoch=49
05/31/2022 00:41:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.736011 on epoch=51
05/31/2022 00:41:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.719376 on epoch=53
05/31/2022 00:41:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.473516 on epoch=55
05/31/2022 00:41:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.683816 on epoch=57
05/31/2022 00:41:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.591511 on epoch=59
05/31/2022 00:41:46 - INFO - __main__ - Global step 300 Train loss 0.640846 ACC 0.484375 on epoch=59
05/31/2022 00:41:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.518578 on epoch=61
05/31/2022 00:41:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.531591 on epoch=63
05/31/2022 00:42:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.560824 on epoch=65
05/31/2022 00:42:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.492381 on epoch=67
05/31/2022 00:42:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.416247 on epoch=69
05/31/2022 00:42:13 - INFO - __main__ - Global step 350 Train loss 0.503924 ACC 0.5 on epoch=69
05/31/2022 00:42:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.508009 on epoch=71
05/31/2022 00:42:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.479271 on epoch=73
05/31/2022 00:42:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.410801 on epoch=75
05/31/2022 00:42:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.481080 on epoch=77
05/31/2022 00:42:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.492179 on epoch=79
05/31/2022 00:42:40 - INFO - __main__ - Global step 400 Train loss 0.474268 ACC 0.5 on epoch=79
05/31/2022 00:42:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.486786 on epoch=81
05/31/2022 00:42:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.459300 on epoch=83
05/31/2022 00:42:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.442242 on epoch=85
05/31/2022 00:43:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.407379 on epoch=87
05/31/2022 00:43:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.456816 on epoch=89
05/31/2022 00:43:08 - INFO - __main__ - Global step 450 Train loss 0.450505 ACC 0.59375 on epoch=89
05/31/2022 00:43:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.404103 on epoch=91
05/31/2022 00:43:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.473482 on epoch=93
05/31/2022 00:43:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.399466 on epoch=95
05/31/2022 00:43:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.459650 on epoch=97
05/31/2022 00:43:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.356580 on epoch=99
05/31/2022 00:43:35 - INFO - __main__ - Global step 500 Train loss 0.418656 ACC 0.609375 on epoch=99
05/31/2022 00:43:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.368655 on epoch=101
05/31/2022 00:43:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.394625 on epoch=103
05/31/2022 00:43:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.465673 on epoch=105
05/31/2022 00:43:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.370677 on epoch=107
05/31/2022 00:44:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.345930 on epoch=109
05/31/2022 00:44:03 - INFO - __main__ - Global step 550 Train loss 0.389112 ACC 0.421875 on epoch=109
05/31/2022 00:44:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.379203 on epoch=111
05/31/2022 00:44:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.396874 on epoch=113
05/31/2022 00:44:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.399273 on epoch=115
05/31/2022 00:44:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.424498 on epoch=117
05/31/2022 00:44:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.414987 on epoch=119
05/31/2022 00:44:31 - INFO - __main__ - Global step 600 Train loss 0.402967 ACC 0.515625 on epoch=119
05/31/2022 00:44:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.327755 on epoch=121
05/31/2022 00:44:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.304387 on epoch=123
05/31/2022 00:44:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.321340 on epoch=125
05/31/2022 00:44:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.367914 on epoch=127
05/31/2022 00:44:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.383663 on epoch=129
05/31/2022 00:44:58 - INFO - __main__ - Global step 650 Train loss 0.341012 ACC 0.4375 on epoch=129
05/31/2022 00:45:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.337035 on epoch=131
05/31/2022 00:45:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.318908 on epoch=133
05/31/2022 00:45:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.302086 on epoch=135
05/31/2022 00:45:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.352869 on epoch=137
05/31/2022 00:45:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.316794 on epoch=139
05/31/2022 00:45:25 - INFO - __main__ - Global step 700 Train loss 0.325538 ACC 0.359375 on epoch=139
05/31/2022 00:45:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.286680 on epoch=141
05/31/2022 00:45:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.308032 on epoch=143
05/31/2022 00:45:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.309540 on epoch=145
05/31/2022 00:45:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.253755 on epoch=147
05/31/2022 00:45:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.255255 on epoch=149
05/31/2022 00:45:53 - INFO - __main__ - Global step 750 Train loss 0.282652 ACC 0.65625 on epoch=149
05/31/2022 00:45:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.291156 on epoch=151
05/31/2022 00:46:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.273390 on epoch=153
05/31/2022 00:46:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.271628 on epoch=155
05/31/2022 00:46:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.322828 on epoch=157
05/31/2022 00:46:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.278658 on epoch=159
05/31/2022 00:46:21 - INFO - __main__ - Global step 800 Train loss 0.287532 ACC 0.484375 on epoch=159
05/31/2022 00:46:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.277170 on epoch=161
05/31/2022 00:46:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.200985 on epoch=163
05/31/2022 00:46:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.218614 on epoch=165
05/31/2022 00:46:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.155520 on epoch=167
05/31/2022 00:46:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.152186 on epoch=169
05/31/2022 00:46:48 - INFO - __main__ - Global step 850 Train loss 0.200895 ACC 0.421875 on epoch=169
05/31/2022 00:46:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.094150 on epoch=171
05/31/2022 00:46:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.157117 on epoch=173
05/31/2022 00:47:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.066111 on epoch=175
05/31/2022 00:47:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.083920 on epoch=177
05/31/2022 00:47:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.018214 on epoch=179
05/31/2022 00:47:16 - INFO - __main__ - Global step 900 Train loss 0.083903 ACC 0.3125 on epoch=179
05/31/2022 00:47:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.012113 on epoch=181
05/31/2022 00:47:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.080504 on epoch=183
05/31/2022 00:47:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.039911 on epoch=185
05/31/2022 00:47:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.060258 on epoch=187
05/31/2022 00:47:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.045027 on epoch=189
05/31/2022 00:47:43 - INFO - __main__ - Global step 950 Train loss 0.047562 ACC 0.5 on epoch=189
05/31/2022 00:47:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.012116 on epoch=191
05/31/2022 00:47:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.002106 on epoch=193
05/31/2022 00:47:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.071405 on epoch=195
05/31/2022 00:48:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.032543 on epoch=197
05/31/2022 00:48:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.023186 on epoch=199
05/31/2022 00:48:10 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:48:10 - INFO - __main__ - Printing 3 examples
05/31/2022 00:48:10 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/31/2022 00:48:10 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:10 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/31/2022 00:48:10 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:10 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/31/2022 00:48:10 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:10 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:48:10 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:48:11 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:48:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:48:11 - INFO - __main__ - Printing 3 examples
05/31/2022 00:48:11 - INFO - __main__ -  [superglue-cb] premise: B: I do not know. I wonder where he gets it? You know, you must, I think TV is bad. Because they, uh, show all sorts of violence on, A: That and I do not think a lot of parents, I mean, I do not know how it is in the Air Force base. But, uh, I just do not think a lot of people, because of the economy, both need to work, you know. I just do not think a lot of parents are that involved any more. [SEP] hypothesis: a lot of parents are that involved
05/31/2022 00:48:11 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:11 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
05/31/2022 00:48:11 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:11 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
05/31/2022 00:48:11 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:11 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:48:11 - INFO - __main__ - Global step 1000 Train loss 0.028271 ACC 0.453125 on epoch=199
05/31/2022 00:48:11 - INFO - __main__ - save last model!
05/31/2022 00:48:11 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:48:11 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:48:18 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 00:48:18 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 00:48:18 - INFO - __main__ - Printing 3 examples
05/31/2022 00:48:18 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 00:48:18 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:18 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 00:48:18 - INFO - __main__ - ['neutral']
05/31/2022 00:48:18 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 00:48:18 - INFO - __main__ - ['entailment']
05/31/2022 00:48:18 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:48:18 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:48:18 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 00:48:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_42_0.0005_8_predictions.txt
05/31/2022 00:48:20 - INFO - __main__ - ACC on test data: 0.5893
05/31/2022 00:48:20 - INFO - __main__ - prefix=superglue-cb_32_42, lr=0.0005, bsz=8, dev_performance=0.65625, test_performance=0.5892857142857143
05/31/2022 00:48:20 - INFO - __main__ - Running ... prefix=superglue-cb_32_42, lr=0.0003, bsz=8 ...
05/31/2022 00:48:21 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:48:21 - INFO - __main__ - Printing 3 examples
05/31/2022 00:48:21 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/31/2022 00:48:21 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:21 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/31/2022 00:48:21 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:21 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/31/2022 00:48:21 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:21 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:48:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:48:21 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:48:21 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:48:21 - INFO - __main__ - Printing 3 examples
05/31/2022 00:48:21 - INFO - __main__ -  [superglue-cb] premise: B: I do not know. I wonder where he gets it? You know, you must, I think TV is bad. Because they, uh, show all sorts of violence on, A: That and I do not think a lot of parents, I mean, I do not know how it is in the Air Force base. But, uh, I just do not think a lot of people, because of the economy, both need to work, you know. I just do not think a lot of parents are that involved any more. [SEP] hypothesis: a lot of parents are that involved
05/31/2022 00:48:21 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:21 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
05/31/2022 00:48:21 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:21 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
05/31/2022 00:48:21 - INFO - __main__ - ['contradiction']
05/31/2022 00:48:21 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:48:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:48:21 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:48:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:48:24 - INFO - __main__ - Starting training!
05/31/2022 00:48:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:48:34 - INFO - __main__ - Starting training!
05/31/2022 00:48:39 - INFO - __main__ - Step 10 Global step 10 Train loss 24.455856 on epoch=1
05/31/2022 00:48:44 - INFO - __main__ - Step 20 Global step 20 Train loss 17.673929 on epoch=3
05/31/2022 00:48:48 - INFO - __main__ - Step 30 Global step 30 Train loss 12.320065 on epoch=5
05/31/2022 00:48:54 - INFO - __main__ - Step 40 Global step 40 Train loss 10.537963 on epoch=7
05/31/2022 00:48:59 - INFO - __main__ - Step 50 Global step 50 Train loss 10.300520 on epoch=9
05/31/2022 00:49:00 - INFO - __main__ - Global step 50 Train loss 15.057669 ACC 0.0 on epoch=9
05/31/2022 00:49:06 - INFO - __main__ - Step 60 Global step 60 Train loss 9.792472 on epoch=11
05/31/2022 00:49:11 - INFO - __main__ - Step 70 Global step 70 Train loss 8.728289 on epoch=13
05/31/2022 00:49:16 - INFO - __main__ - Step 80 Global step 80 Train loss 8.286374 on epoch=15
05/31/2022 00:49:22 - INFO - __main__ - Step 90 Global step 90 Train loss 7.529178 on epoch=17
05/31/2022 00:49:27 - INFO - __main__ - Step 100 Global step 100 Train loss 6.459240 on epoch=19
05/31/2022 00:49:29 - INFO - __main__ - Global step 100 Train loss 8.159111 ACC 0.03125 on epoch=19
05/31/2022 00:49:34 - INFO - __main__ - Step 110 Global step 110 Train loss 6.701684 on epoch=21
05/31/2022 00:49:39 - INFO - __main__ - Step 120 Global step 120 Train loss 5.482467 on epoch=23
05/31/2022 00:49:45 - INFO - __main__ - Step 130 Global step 130 Train loss 3.816310 on epoch=25
05/31/2022 00:49:50 - INFO - __main__ - Step 140 Global step 140 Train loss 3.677058 on epoch=27
05/31/2022 00:49:55 - INFO - __main__ - Step 150 Global step 150 Train loss 2.509184 on epoch=29
05/31/2022 00:49:56 - INFO - __main__ - Global step 150 Train loss 4.437341 ACC 0.5 on epoch=29
05/31/2022 00:50:02 - INFO - __main__ - Step 160 Global step 160 Train loss 2.149700 on epoch=31
05/31/2022 00:50:07 - INFO - __main__ - Step 170 Global step 170 Train loss 2.516456 on epoch=33
05/31/2022 00:50:12 - INFO - __main__ - Step 180 Global step 180 Train loss 1.708375 on epoch=35
05/31/2022 00:50:18 - INFO - __main__ - Step 190 Global step 190 Train loss 2.313338 on epoch=37
05/31/2022 00:50:23 - INFO - __main__ - Step 200 Global step 200 Train loss 1.871923 on epoch=39
05/31/2022 00:50:24 - INFO - __main__ - Global step 200 Train loss 2.111958 ACC 0.5 on epoch=39
05/31/2022 00:50:29 - INFO - __main__ - Step 210 Global step 210 Train loss 2.030083 on epoch=41
05/31/2022 00:50:35 - INFO - __main__ - Step 220 Global step 220 Train loss 1.627336 on epoch=43
05/31/2022 00:50:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.918346 on epoch=45
05/31/2022 00:50:45 - INFO - __main__ - Step 240 Global step 240 Train loss 1.667623 on epoch=47
05/31/2022 00:50:50 - INFO - __main__ - Step 250 Global step 250 Train loss 1.604160 on epoch=49
05/31/2022 00:50:51 - INFO - __main__ - Global step 250 Train loss 1.769510 ACC 0.5 on epoch=49
05/31/2022 00:50:57 - INFO - __main__ - Step 260 Global step 260 Train loss 1.589057 on epoch=51
05/31/2022 00:51:02 - INFO - __main__ - Step 270 Global step 270 Train loss 1.780382 on epoch=53
05/31/2022 00:51:07 - INFO - __main__ - Step 280 Global step 280 Train loss 1.311324 on epoch=55
05/31/2022 00:51:12 - INFO - __main__ - Step 290 Global step 290 Train loss 1.169321 on epoch=57
05/31/2022 00:51:18 - INFO - __main__ - Step 300 Global step 300 Train loss 1.717064 on epoch=59
05/31/2022 00:51:19 - INFO - __main__ - Global step 300 Train loss 1.513429 ACC 0.5 on epoch=59
05/31/2022 00:51:24 - INFO - __main__ - Step 310 Global step 310 Train loss 1.341538 on epoch=61
05/31/2022 00:51:29 - INFO - __main__ - Step 320 Global step 320 Train loss 1.008888 on epoch=63
05/31/2022 00:51:34 - INFO - __main__ - Step 330 Global step 330 Train loss 1.208372 on epoch=65
05/31/2022 00:51:39 - INFO - __main__ - Step 340 Global step 340 Train loss 1.134664 on epoch=67
05/31/2022 00:51:45 - INFO - __main__ - Step 350 Global step 350 Train loss 1.141812 on epoch=69
05/31/2022 00:51:46 - INFO - __main__ - Global step 350 Train loss 1.167055 ACC 0.5 on epoch=69
05/31/2022 00:51:51 - INFO - __main__ - Step 360 Global step 360 Train loss 1.022688 on epoch=71
05/31/2022 00:51:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.982917 on epoch=73
05/31/2022 00:52:01 - INFO - __main__ - Step 380 Global step 380 Train loss 1.032733 on epoch=75
05/31/2022 00:52:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.793212 on epoch=77
05/31/2022 00:52:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.681547 on epoch=79
05/31/2022 00:52:14 - INFO - __main__ - Global step 400 Train loss 0.902619 ACC 0.40625 on epoch=79
05/31/2022 00:52:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.917517 on epoch=81
05/31/2022 00:52:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.496220 on epoch=83
05/31/2022 00:52:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.717123 on epoch=85
05/31/2022 00:52:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.526115 on epoch=87
05/31/2022 00:52:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.364335 on epoch=89
05/31/2022 00:52:41 - INFO - __main__ - Global step 450 Train loss 0.604262 ACC 0.828125 on epoch=89
05/31/2022 00:52:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.191960 on epoch=91
05/31/2022 00:52:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.135934 on epoch=93
05/31/2022 00:52:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.079421 on epoch=95
05/31/2022 00:53:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.040772 on epoch=97
05/31/2022 00:53:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.046617 on epoch=99
05/31/2022 00:53:09 - INFO - __main__ - Global step 500 Train loss 0.098941 ACC 0.78125 on epoch=99
05/31/2022 00:53:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.035792 on epoch=101
05/31/2022 00:53:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.022142 on epoch=103
05/31/2022 00:53:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.031368 on epoch=105
05/31/2022 00:53:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.075951 on epoch=107
05/31/2022 00:53:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004121 on epoch=109
05/31/2022 00:53:36 - INFO - __main__ - Global step 550 Train loss 0.033875 ACC 0.921875 on epoch=109
05/31/2022 00:53:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.005148 on epoch=111
05/31/2022 00:53:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.018521 on epoch=113
05/31/2022 00:53:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.004846 on epoch=115
05/31/2022 00:53:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.004155 on epoch=117
05/31/2022 00:54:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001814 on epoch=119
05/31/2022 00:54:04 - INFO - __main__ - Global step 600 Train loss 0.006897 ACC 0.953125 on epoch=119
05/31/2022 00:54:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.002481 on epoch=121
05/31/2022 00:54:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.016071 on epoch=123
05/31/2022 00:54:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.001209 on epoch=125
05/31/2022 00:54:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001178 on epoch=127
05/31/2022 00:54:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000778 on epoch=129
05/31/2022 00:54:33 - INFO - __main__ - Global step 650 Train loss 0.004343 ACC 0.9375 on epoch=129
05/31/2022 00:54:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.003418 on epoch=131
05/31/2022 00:54:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000368 on epoch=133
05/31/2022 00:54:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.001091 on epoch=135
05/31/2022 00:54:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001886 on epoch=137
05/31/2022 00:54:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000948 on epoch=139
05/31/2022 00:55:00 - INFO - __main__ - Global step 700 Train loss 0.001542 ACC 0.921875 on epoch=139
05/31/2022 00:55:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000710 on epoch=141
05/31/2022 00:55:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000502 on epoch=143
05/31/2022 00:55:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000977 on epoch=145
05/31/2022 00:55:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000211 on epoch=147
05/31/2022 00:55:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000805 on epoch=149
05/31/2022 00:55:28 - INFO - __main__ - Global step 750 Train loss 0.000641 ACC 0.890625 on epoch=149
05/31/2022 00:55:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001890 on epoch=151
05/31/2022 00:55:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000486 on epoch=153
05/31/2022 00:55:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001072 on epoch=155
05/31/2022 00:55:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000443 on epoch=157
05/31/2022 00:55:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.001138 on epoch=159
05/31/2022 00:55:55 - INFO - __main__ - Global step 800 Train loss 0.001006 ACC 0.890625 on epoch=159
05/31/2022 00:56:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.002466 on epoch=161
05/31/2022 00:56:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000093 on epoch=163
05/31/2022 00:56:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000111 on epoch=165
05/31/2022 00:56:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000523 on epoch=167
05/31/2022 00:56:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000119 on epoch=169
05/31/2022 00:56:23 - INFO - __main__ - Global step 850 Train loss 0.000662 ACC 0.921875 on epoch=169
05/31/2022 00:56:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.001459 on epoch=171
05/31/2022 00:56:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000910 on epoch=173
05/31/2022 00:56:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000114 on epoch=175
05/31/2022 00:56:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000281 on epoch=177
05/31/2022 00:56:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000353 on epoch=179
05/31/2022 00:56:50 - INFO - __main__ - Global step 900 Train loss 0.000623 ACC 0.875 on epoch=179
05/31/2022 00:56:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000603 on epoch=181
05/31/2022 00:57:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000149 on epoch=183
05/31/2022 00:57:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000292 on epoch=185
05/31/2022 00:57:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.002637 on epoch=187
05/31/2022 00:57:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.010475 on epoch=189
05/31/2022 00:57:18 - INFO - __main__ - Global step 950 Train loss 0.002831 ACC 0.90625 on epoch=189
05/31/2022 00:57:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000135 on epoch=191
05/31/2022 00:57:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000764 on epoch=193
05/31/2022 00:57:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001442 on epoch=195
05/31/2022 00:57:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.050938 on epoch=197
05/31/2022 00:57:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001429 on epoch=199
05/31/2022 00:57:45 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:57:45 - INFO - __main__ - Printing 3 examples
05/31/2022 00:57:45 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/31/2022 00:57:45 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:45 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/31/2022 00:57:45 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:45 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/31/2022 00:57:45 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:45 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:57:45 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:57:45 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:57:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:57:45 - INFO - __main__ - Printing 3 examples
05/31/2022 00:57:45 - INFO - __main__ -  [superglue-cb] premise: B: I do not know. I wonder where he gets it? You know, you must, I think TV is bad. Because they, uh, show all sorts of violence on, A: That and I do not think a lot of parents, I mean, I do not know how it is in the Air Force base. But, uh, I just do not think a lot of people, because of the economy, both need to work, you know. I just do not think a lot of parents are that involved any more. [SEP] hypothesis: a lot of parents are that involved
05/31/2022 00:57:45 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:45 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
05/31/2022 00:57:45 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:45 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
05/31/2022 00:57:45 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:45 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:57:45 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:57:46 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:57:46 - INFO - __main__ - Global step 1000 Train loss 0.010942 ACC 0.859375 on epoch=199
05/31/2022 00:57:46 - INFO - __main__ - save last model!
05/31/2022 00:57:53 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 00:57:54 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 00:57:54 - INFO - __main__ - Printing 3 examples
05/31/2022 00:57:54 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 00:57:54 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:54 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 00:57:54 - INFO - __main__ - ['neutral']
05/31/2022 00:57:54 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 00:57:54 - INFO - __main__ - ['entailment']
05/31/2022 00:57:54 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:57:54 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:57:54 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 00:57:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_42_0.0003_8_predictions.txt
05/31/2022 00:57:55 - INFO - __main__ - ACC on test data: 0.8750
05/31/2022 00:57:55 - INFO - __main__ - prefix=superglue-cb_32_42, lr=0.0003, bsz=8, dev_performance=0.953125, test_performance=0.875
05/31/2022 00:57:55 - INFO - __main__ - Running ... prefix=superglue-cb_32_42, lr=0.0002, bsz=8 ...
05/31/2022 00:57:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:57:56 - INFO - __main__ - Starting training!
05/31/2022 00:57:56 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 00:57:56 - INFO - __main__ - Printing 3 examples
05/31/2022 00:57:56 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/31/2022 00:57:56 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:56 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/31/2022 00:57:56 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:56 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/31/2022 00:57:56 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:57:56 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:57:56 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 00:57:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:57:56 - INFO - __main__ - Printing 3 examples
05/31/2022 00:57:56 - INFO - __main__ -  [superglue-cb] premise: B: I do not know. I wonder where he gets it? You know, you must, I think TV is bad. Because they, uh, show all sorts of violence on, A: That and I do not think a lot of parents, I mean, I do not know how it is in the Air Force base. But, uh, I just do not think a lot of people, because of the economy, both need to work, you know. I just do not think a lot of parents are that involved any more. [SEP] hypothesis: a lot of parents are that involved
05/31/2022 00:57:56 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:56 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
05/31/2022 00:57:56 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:56 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
05/31/2022 00:57:56 - INFO - __main__ - ['contradiction']
05/31/2022 00:57:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:57:57 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:57:57 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:58:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 00:58:10 - INFO - __main__ - Starting training!
05/31/2022 00:58:14 - INFO - __main__ - Step 10 Global step 10 Train loss 23.986258 on epoch=1
05/31/2022 00:58:19 - INFO - __main__ - Step 20 Global step 20 Train loss 18.983685 on epoch=3
05/31/2022 00:58:24 - INFO - __main__ - Step 30 Global step 30 Train loss 12.904406 on epoch=5
05/31/2022 00:58:29 - INFO - __main__ - Step 40 Global step 40 Train loss 11.654769 on epoch=7
05/31/2022 00:58:35 - INFO - __main__ - Step 50 Global step 50 Train loss 10.487042 on epoch=9
05/31/2022 00:58:36 - INFO - __main__ - Global step 50 Train loss 15.603232 ACC 0.0 on epoch=9
05/31/2022 00:58:42 - INFO - __main__ - Step 60 Global step 60 Train loss 10.808980 on epoch=11
05/31/2022 00:58:47 - INFO - __main__ - Step 70 Global step 70 Train loss 9.973113 on epoch=13
05/31/2022 00:58:52 - INFO - __main__ - Step 80 Global step 80 Train loss 9.407709 on epoch=15
05/31/2022 00:58:58 - INFO - __main__ - Step 90 Global step 90 Train loss 9.023432 on epoch=17
05/31/2022 00:59:03 - INFO - __main__ - Step 100 Global step 100 Train loss 8.359819 on epoch=19
05/31/2022 00:59:04 - INFO - __main__ - Global step 100 Train loss 9.514610 ACC 0.0 on epoch=19
05/31/2022 00:59:09 - INFO - __main__ - Step 110 Global step 110 Train loss 7.931066 on epoch=21
05/31/2022 00:59:14 - INFO - __main__ - Step 120 Global step 120 Train loss 7.518795 on epoch=23
05/31/2022 00:59:20 - INFO - __main__ - Step 130 Global step 130 Train loss 6.929444 on epoch=25
05/31/2022 00:59:25 - INFO - __main__ - Step 140 Global step 140 Train loss 6.382023 on epoch=27
05/31/2022 00:59:30 - INFO - __main__ - Step 150 Global step 150 Train loss 5.926574 on epoch=29
05/31/2022 00:59:32 - INFO - __main__ - Global step 150 Train loss 6.937580 ACC 0.0 on epoch=29
05/31/2022 00:59:37 - INFO - __main__ - Step 160 Global step 160 Train loss 5.573451 on epoch=31
05/31/2022 00:59:42 - INFO - __main__ - Step 170 Global step 170 Train loss 5.406631 on epoch=33
05/31/2022 00:59:47 - INFO - __main__ - Step 180 Global step 180 Train loss 4.955186 on epoch=35
05/31/2022 00:59:52 - INFO - __main__ - Step 190 Global step 190 Train loss 4.268459 on epoch=37
05/31/2022 00:59:57 - INFO - __main__ - Step 200 Global step 200 Train loss 3.193334 on epoch=39
05/31/2022 00:59:59 - INFO - __main__ - Global step 200 Train loss 4.679412 ACC 0.0 on epoch=39
05/31/2022 01:00:04 - INFO - __main__ - Step 210 Global step 210 Train loss 2.769231 on epoch=41
05/31/2022 01:00:09 - INFO - __main__ - Step 220 Global step 220 Train loss 2.592290 on epoch=43
05/31/2022 01:00:14 - INFO - __main__ - Step 230 Global step 230 Train loss 3.196511 on epoch=45
05/31/2022 01:00:19 - INFO - __main__ - Step 240 Global step 240 Train loss 1.909718 on epoch=47
05/31/2022 01:00:25 - INFO - __main__ - Step 250 Global step 250 Train loss 2.271472 on epoch=49
05/31/2022 01:00:26 - INFO - __main__ - Global step 250 Train loss 2.547845 ACC 0.5 on epoch=49
05/31/2022 01:00:32 - INFO - __main__ - Step 260 Global step 260 Train loss 2.471456 on epoch=51
05/31/2022 01:00:37 - INFO - __main__ - Step 270 Global step 270 Train loss 1.923476 on epoch=53
05/31/2022 01:00:42 - INFO - __main__ - Step 280 Global step 280 Train loss 2.393143 on epoch=55
05/31/2022 01:00:47 - INFO - __main__ - Step 290 Global step 290 Train loss 1.851646 on epoch=57
05/31/2022 01:00:53 - INFO - __main__ - Step 300 Global step 300 Train loss 1.806409 on epoch=59
05/31/2022 01:00:54 - INFO - __main__ - Global step 300 Train loss 2.089226 ACC 0.5 on epoch=59
05/31/2022 01:00:59 - INFO - __main__ - Step 310 Global step 310 Train loss 2.055247 on epoch=61
05/31/2022 01:01:04 - INFO - __main__ - Step 320 Global step 320 Train loss 1.476856 on epoch=63
05/31/2022 01:01:10 - INFO - __main__ - Step 330 Global step 330 Train loss 1.671821 on epoch=65
05/31/2022 01:01:15 - INFO - __main__ - Step 340 Global step 340 Train loss 1.824305 on epoch=67
05/31/2022 01:01:20 - INFO - __main__ - Step 350 Global step 350 Train loss 1.776857 on epoch=69
05/31/2022 01:01:21 - INFO - __main__ - Global step 350 Train loss 1.761017 ACC 0.5 on epoch=69
05/31/2022 01:01:27 - INFO - __main__ - Step 360 Global step 360 Train loss 1.623442 on epoch=71
05/31/2022 01:01:32 - INFO - __main__ - Step 370 Global step 370 Train loss 1.811426 on epoch=73
05/31/2022 01:01:37 - INFO - __main__ - Step 380 Global step 380 Train loss 1.778123 on epoch=75
05/31/2022 01:01:42 - INFO - __main__ - Step 390 Global step 390 Train loss 1.314232 on epoch=77
05/31/2022 01:01:48 - INFO - __main__ - Step 400 Global step 400 Train loss 1.223174 on epoch=79
05/31/2022 01:01:49 - INFO - __main__ - Global step 400 Train loss 1.550079 ACC 0.5 on epoch=79
05/31/2022 01:01:54 - INFO - __main__ - Step 410 Global step 410 Train loss 1.359727 on epoch=81
05/31/2022 01:01:59 - INFO - __main__ - Step 420 Global step 420 Train loss 1.386701 on epoch=83
05/31/2022 01:02:05 - INFO - __main__ - Step 430 Global step 430 Train loss 1.058195 on epoch=85
05/31/2022 01:02:10 - INFO - __main__ - Step 440 Global step 440 Train loss 1.093413 on epoch=87
05/31/2022 01:02:15 - INFO - __main__ - Step 450 Global step 450 Train loss 1.023783 on epoch=89
05/31/2022 01:02:17 - INFO - __main__ - Global step 450 Train loss 1.184364 ACC 0.5 on epoch=89
05/31/2022 01:02:22 - INFO - __main__ - Step 460 Global step 460 Train loss 1.070615 on epoch=91
05/31/2022 01:02:27 - INFO - __main__ - Step 470 Global step 470 Train loss 1.046292 on epoch=93
05/31/2022 01:02:32 - INFO - __main__ - Step 480 Global step 480 Train loss 1.118139 on epoch=95
05/31/2022 01:02:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.908984 on epoch=97
05/31/2022 01:02:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.962880 on epoch=99
05/31/2022 01:02:44 - INFO - __main__ - Global step 500 Train loss 1.021382 ACC 0.5 on epoch=99
05/31/2022 01:02:49 - INFO - __main__ - Step 510 Global step 510 Train loss 1.083964 on epoch=101
05/31/2022 01:02:55 - INFO - __main__ - Step 520 Global step 520 Train loss 1.033400 on epoch=103
05/31/2022 01:03:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.992991 on epoch=105
05/31/2022 01:03:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.637698 on epoch=107
05/31/2022 01:03:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.734133 on epoch=109
05/31/2022 01:03:12 - INFO - __main__ - Global step 550 Train loss 0.896437 ACC 0.5 on epoch=109
05/31/2022 01:03:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.906601 on epoch=111
05/31/2022 01:03:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.800808 on epoch=113
05/31/2022 01:03:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.703438 on epoch=115
05/31/2022 01:03:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.772551 on epoch=117
05/31/2022 01:03:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.726240 on epoch=119
05/31/2022 01:03:39 - INFO - __main__ - Global step 600 Train loss 0.781928 ACC 0.5625 on epoch=119
05/31/2022 01:03:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.745719 on epoch=121
05/31/2022 01:03:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.597628 on epoch=123
05/31/2022 01:03:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.553856 on epoch=125
05/31/2022 01:04:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.594736 on epoch=127
05/31/2022 01:04:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.437878 on epoch=129
05/31/2022 01:04:07 - INFO - __main__ - Global step 650 Train loss 0.585963 ACC 0.5 on epoch=129
05/31/2022 01:04:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.408470 on epoch=131
05/31/2022 01:04:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.462079 on epoch=133
05/31/2022 01:04:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.427160 on epoch=135
05/31/2022 01:04:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.472533 on epoch=137
05/31/2022 01:04:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.515494 on epoch=139
05/31/2022 01:04:34 - INFO - __main__ - Global step 700 Train loss 0.457147 ACC 0.671875 on epoch=139
05/31/2022 01:04:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.368028 on epoch=141
05/31/2022 01:04:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.406791 on epoch=143
05/31/2022 01:04:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.323420 on epoch=145
05/31/2022 01:04:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.425689 on epoch=147
05/31/2022 01:05:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.347365 on epoch=149
05/31/2022 01:05:02 - INFO - __main__ - Global step 750 Train loss 0.374258 ACC 0.703125 on epoch=149
05/31/2022 01:05:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.320172 on epoch=151
05/31/2022 01:05:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.306171 on epoch=153
05/31/2022 01:05:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.302531 on epoch=155
05/31/2022 01:05:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.291557 on epoch=157
05/31/2022 01:05:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.331372 on epoch=159
05/31/2022 01:05:31 - INFO - __main__ - Global step 800 Train loss 0.310361 ACC 0.296875 on epoch=159
05/31/2022 01:05:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.245581 on epoch=161
05/31/2022 01:05:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.246983 on epoch=163
05/31/2022 01:05:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.158562 on epoch=165
05/31/2022 01:05:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.133098 on epoch=167
05/31/2022 01:05:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.084918 on epoch=169
05/31/2022 01:05:58 - INFO - __main__ - Global step 850 Train loss 0.173828 ACC 0.609375 on epoch=169
05/31/2022 01:06:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.063956 on epoch=171
05/31/2022 01:06:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.024297 on epoch=173
05/31/2022 01:06:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.014780 on epoch=175
05/31/2022 01:06:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.023050 on epoch=177
05/31/2022 01:06:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.040395 on epoch=179
05/31/2022 01:06:25 - INFO - __main__ - Global step 900 Train loss 0.033296 ACC 0.640625 on epoch=179
05/31/2022 01:06:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.011066 on epoch=181
05/31/2022 01:06:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.013607 on epoch=183
05/31/2022 01:06:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.006678 on epoch=185
05/31/2022 01:06:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001535 on epoch=187
05/31/2022 01:06:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.005244 on epoch=189
05/31/2022 01:06:52 - INFO - __main__ - Global step 950 Train loss 0.007626 ACC 0.53125 on epoch=189
05/31/2022 01:06:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001254 on epoch=191
05/31/2022 01:07:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001518 on epoch=193
05/31/2022 01:07:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.004946 on epoch=195
05/31/2022 01:07:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000984 on epoch=197
05/31/2022 01:07:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000859 on epoch=199
05/31/2022 01:07:19 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 01:07:19 - INFO - __main__ - Printing 3 examples
05/31/2022 01:07:19 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/31/2022 01:07:19 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:19 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/31/2022 01:07:19 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:19 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/31/2022 01:07:19 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:07:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:07:19 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 01:07:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 01:07:19 - INFO - __main__ - Printing 3 examples
05/31/2022 01:07:19 - INFO - __main__ -  [superglue-cb] premise: B: I do not know. I wonder where he gets it? You know, you must, I think TV is bad. Because they, uh, show all sorts of violence on, A: That and I do not think a lot of parents, I mean, I do not know how it is in the Air Force base. But, uh, I just do not think a lot of people, because of the economy, both need to work, you know. I just do not think a lot of parents are that involved any more. [SEP] hypothesis: a lot of parents are that involved
05/31/2022 01:07:19 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:19 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
05/31/2022 01:07:19 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:19 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
05/31/2022 01:07:19 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:07:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:07:19 - INFO - __main__ - Global step 1000 Train loss 0.001912 ACC 0.625 on epoch=199
05/31/2022 01:07:19 - INFO - __main__ - save last model!
05/31/2022 01:07:19 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 01:07:27 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 01:07:27 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 01:07:27 - INFO - __main__ - Printing 3 examples
05/31/2022 01:07:27 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 01:07:27 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:27 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 01:07:27 - INFO - __main__ - ['neutral']
05/31/2022 01:07:27 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 01:07:27 - INFO - __main__ - ['entailment']
05/31/2022 01:07:27 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:07:27 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:07:27 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 01:07:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_42_0.0002_8_predictions.txt
05/31/2022 01:07:29 - INFO - __main__ - ACC on test data: 0.5357
05/31/2022 01:07:29 - INFO - __main__ - prefix=superglue-cb_32_42, lr=0.0002, bsz=8, dev_performance=0.703125, test_performance=0.5357142857142857
05/31/2022 01:07:29 - INFO - __main__ - Running ... prefix=superglue-cb_32_42, lr=0.0001, bsz=8 ...
05/31/2022 01:07:30 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 01:07:30 - INFO - __main__ - Printing 3 examples
05/31/2022 01:07:30 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/31/2022 01:07:30 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:30 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/31/2022 01:07:30 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:30 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/31/2022 01:07:30 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:30 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:07:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:07:30 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 01:07:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 01:07:30 - INFO - __main__ - Printing 3 examples
05/31/2022 01:07:30 - INFO - __main__ -  [superglue-cb] premise: B: I do not know. I wonder where he gets it? You know, you must, I think TV is bad. Because they, uh, show all sorts of violence on, A: That and I do not think a lot of parents, I mean, I do not know how it is in the Air Force base. But, uh, I just do not think a lot of people, because of the economy, both need to work, you know. I just do not think a lot of parents are that involved any more. [SEP] hypothesis: a lot of parents are that involved
05/31/2022 01:07:30 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:30 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
05/31/2022 01:07:30 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:30 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
05/31/2022 01:07:30 - INFO - __main__ - ['contradiction']
05/31/2022 01:07:30 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:07:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:07:30 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 01:07:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 01:07:33 - INFO - __main__ - Starting training!
05/31/2022 01:07:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 01:07:43 - INFO - __main__ - Starting training!
05/31/2022 01:07:48 - INFO - __main__ - Step 10 Global step 10 Train loss 24.597984 on epoch=1
05/31/2022 01:07:53 - INFO - __main__ - Step 20 Global step 20 Train loss 21.765667 on epoch=3
05/31/2022 01:07:58 - INFO - __main__ - Step 30 Global step 30 Train loss 17.774481 on epoch=5
05/31/2022 01:08:03 - INFO - __main__ - Step 40 Global step 40 Train loss 14.396083 on epoch=7
05/31/2022 01:08:08 - INFO - __main__ - Step 50 Global step 50 Train loss 13.308642 on epoch=9
05/31/2022 01:08:19 - INFO - __main__ - Global step 50 Train loss 18.368572 ACC 0.0 on epoch=9
05/31/2022 01:08:25 - INFO - __main__ - Step 60 Global step 60 Train loss 12.899994 on epoch=11
05/31/2022 01:08:30 - INFO - __main__ - Step 70 Global step 70 Train loss 11.637747 on epoch=13
05/31/2022 01:08:35 - INFO - __main__ - Step 80 Global step 80 Train loss 11.268281 on epoch=15
05/31/2022 01:08:40 - INFO - __main__ - Step 90 Global step 90 Train loss 11.370686 on epoch=17
05/31/2022 01:08:46 - INFO - __main__ - Step 100 Global step 100 Train loss 10.623922 on epoch=19
05/31/2022 01:08:47 - INFO - __main__ - Global step 100 Train loss 11.560125 ACC 0.015625 on epoch=19
05/31/2022 01:08:53 - INFO - __main__ - Step 110 Global step 110 Train loss 10.916109 on epoch=21
05/31/2022 01:08:58 - INFO - __main__ - Step 120 Global step 120 Train loss 10.092911 on epoch=23
05/31/2022 01:09:04 - INFO - __main__ - Step 130 Global step 130 Train loss 10.387602 on epoch=25
05/31/2022 01:09:09 - INFO - __main__ - Step 140 Global step 140 Train loss 9.499194 on epoch=27
05/31/2022 01:09:14 - INFO - __main__ - Step 150 Global step 150 Train loss 10.225397 on epoch=29
05/31/2022 01:09:16 - INFO - __main__ - Global step 150 Train loss 10.224242 ACC 0.0 on epoch=29
05/31/2022 01:09:21 - INFO - __main__ - Step 160 Global step 160 Train loss 9.259054 on epoch=31
05/31/2022 01:09:26 - INFO - __main__ - Step 170 Global step 170 Train loss 9.489972 on epoch=33
05/31/2022 01:09:31 - INFO - __main__ - Step 180 Global step 180 Train loss 9.017983 on epoch=35
05/31/2022 01:09:37 - INFO - __main__ - Step 190 Global step 190 Train loss 8.700922 on epoch=37
05/31/2022 01:09:42 - INFO - __main__ - Step 200 Global step 200 Train loss 8.524780 on epoch=39
05/31/2022 01:09:43 - INFO - __main__ - Global step 200 Train loss 8.998543 ACC 0.0 on epoch=39
05/31/2022 01:09:48 - INFO - __main__ - Step 210 Global step 210 Train loss 8.254183 on epoch=41
05/31/2022 01:09:54 - INFO - __main__ - Step 220 Global step 220 Train loss 7.987849 on epoch=43
05/31/2022 01:09:59 - INFO - __main__ - Step 230 Global step 230 Train loss 7.924570 on epoch=45
05/31/2022 01:10:04 - INFO - __main__ - Step 240 Global step 240 Train loss 7.600384 on epoch=47
05/31/2022 01:10:09 - INFO - __main__ - Step 250 Global step 250 Train loss 7.652209 on epoch=49
05/31/2022 01:10:11 - INFO - __main__ - Global step 250 Train loss 7.883839 ACC 0.0 on epoch=49
05/31/2022 01:10:16 - INFO - __main__ - Step 260 Global step 260 Train loss 7.576163 on epoch=51
05/31/2022 01:10:21 - INFO - __main__ - Step 270 Global step 270 Train loss 7.002015 on epoch=53
05/31/2022 01:10:26 - INFO - __main__ - Step 280 Global step 280 Train loss 6.321454 on epoch=55
05/31/2022 01:10:32 - INFO - __main__ - Step 290 Global step 290 Train loss 6.581834 on epoch=57
05/31/2022 01:10:37 - INFO - __main__ - Step 300 Global step 300 Train loss 6.232009 on epoch=59
05/31/2022 01:10:38 - INFO - __main__ - Global step 300 Train loss 6.742695 ACC 0.0 on epoch=59
05/31/2022 01:10:43 - INFO - __main__ - Step 310 Global step 310 Train loss 5.798926 on epoch=61
05/31/2022 01:10:49 - INFO - __main__ - Step 320 Global step 320 Train loss 5.782452 on epoch=63
05/31/2022 01:10:54 - INFO - __main__ - Step 330 Global step 330 Train loss 5.699031 on epoch=65
05/31/2022 01:10:59 - INFO - __main__ - Step 340 Global step 340 Train loss 5.107732 on epoch=67
05/31/2022 01:11:04 - INFO - __main__ - Step 350 Global step 350 Train loss 4.525780 on epoch=69
05/31/2022 01:11:06 - INFO - __main__ - Global step 350 Train loss 5.382784 ACC 0.0 on epoch=69
05/31/2022 01:11:11 - INFO - __main__ - Step 360 Global step 360 Train loss 4.440865 on epoch=71
05/31/2022 01:11:16 - INFO - __main__ - Step 370 Global step 370 Train loss 3.830992 on epoch=73
05/31/2022 01:11:22 - INFO - __main__ - Step 380 Global step 380 Train loss 3.539133 on epoch=75
05/31/2022 01:11:27 - INFO - __main__ - Step 390 Global step 390 Train loss 3.431118 on epoch=77
05/31/2022 01:11:32 - INFO - __main__ - Step 400 Global step 400 Train loss 2.793936 on epoch=79
05/31/2022 01:11:34 - INFO - __main__ - Global step 400 Train loss 3.607209 ACC 0.0 on epoch=79
05/31/2022 01:11:39 - INFO - __main__ - Step 410 Global step 410 Train loss 3.101900 on epoch=81
05/31/2022 01:11:44 - INFO - __main__ - Step 420 Global step 420 Train loss 2.938360 on epoch=83
05/31/2022 01:11:49 - INFO - __main__ - Step 430 Global step 430 Train loss 2.322645 on epoch=85
05/31/2022 01:11:54 - INFO - __main__ - Step 440 Global step 440 Train loss 1.970317 on epoch=87
05/31/2022 01:12:00 - INFO - __main__ - Step 450 Global step 450 Train loss 2.330989 on epoch=89
05/31/2022 01:12:01 - INFO - __main__ - Global step 450 Train loss 2.532842 ACC 0.5 on epoch=89
05/31/2022 01:12:06 - INFO - __main__ - Step 460 Global step 460 Train loss 2.366273 on epoch=91
05/31/2022 01:12:12 - INFO - __main__ - Step 470 Global step 470 Train loss 2.351784 on epoch=93
05/31/2022 01:12:17 - INFO - __main__ - Step 480 Global step 480 Train loss 1.983999 on epoch=95
05/31/2022 01:12:22 - INFO - __main__ - Step 490 Global step 490 Train loss 2.257240 on epoch=97
05/31/2022 01:12:27 - INFO - __main__ - Step 500 Global step 500 Train loss 2.013196 on epoch=99
05/31/2022 01:12:29 - INFO - __main__ - Global step 500 Train loss 2.194499 ACC 0.5 on epoch=99
05/31/2022 01:12:34 - INFO - __main__ - Step 510 Global step 510 Train loss 2.717229 on epoch=101
05/31/2022 01:12:39 - INFO - __main__ - Step 520 Global step 520 Train loss 1.951460 on epoch=103
05/31/2022 01:12:44 - INFO - __main__ - Step 530 Global step 530 Train loss 1.872326 on epoch=105
05/31/2022 01:12:50 - INFO - __main__ - Step 540 Global step 540 Train loss 1.960861 on epoch=107
05/31/2022 01:12:55 - INFO - __main__ - Step 550 Global step 550 Train loss 1.798728 on epoch=109
05/31/2022 01:12:56 - INFO - __main__ - Global step 550 Train loss 2.060121 ACC 0.5 on epoch=109
05/31/2022 01:13:01 - INFO - __main__ - Step 560 Global step 560 Train loss 2.382114 on epoch=111
05/31/2022 01:13:06 - INFO - __main__ - Step 570 Global step 570 Train loss 2.281035 on epoch=113
05/31/2022 01:13:12 - INFO - __main__ - Step 580 Global step 580 Train loss 1.658136 on epoch=115
05/31/2022 01:13:17 - INFO - __main__ - Step 590 Global step 590 Train loss 2.256529 on epoch=117
05/31/2022 01:13:22 - INFO - __main__ - Step 600 Global step 600 Train loss 2.188752 on epoch=119
05/31/2022 01:13:23 - INFO - __main__ - Global step 600 Train loss 2.153313 ACC 0.5 on epoch=119
05/31/2022 01:13:29 - INFO - __main__ - Step 610 Global step 610 Train loss 1.818816 on epoch=121
05/31/2022 01:13:34 - INFO - __main__ - Step 620 Global step 620 Train loss 1.852863 on epoch=123
05/31/2022 01:13:39 - INFO - __main__ - Step 630 Global step 630 Train loss 1.596097 on epoch=125
05/31/2022 01:13:44 - INFO - __main__ - Step 640 Global step 640 Train loss 1.869883 on epoch=127
05/31/2022 01:13:50 - INFO - __main__ - Step 650 Global step 650 Train loss 1.793889 on epoch=129
05/31/2022 01:13:51 - INFO - __main__ - Global step 650 Train loss 1.786310 ACC 0.5 on epoch=129
05/31/2022 01:13:56 - INFO - __main__ - Step 660 Global step 660 Train loss 2.038408 on epoch=131
05/31/2022 01:14:01 - INFO - __main__ - Step 670 Global step 670 Train loss 1.643133 on epoch=133
05/31/2022 01:14:06 - INFO - __main__ - Step 680 Global step 680 Train loss 1.197189 on epoch=135
05/31/2022 01:14:12 - INFO - __main__ - Step 690 Global step 690 Train loss 1.385031 on epoch=137
05/31/2022 01:14:17 - INFO - __main__ - Step 700 Global step 700 Train loss 1.763708 on epoch=139
05/31/2022 01:14:18 - INFO - __main__ - Global step 700 Train loss 1.605493 ACC 0.5 on epoch=139
05/31/2022 01:14:23 - INFO - __main__ - Step 710 Global step 710 Train loss 1.520184 on epoch=141
05/31/2022 01:14:28 - INFO - __main__ - Step 720 Global step 720 Train loss 1.112297 on epoch=143
05/31/2022 01:14:34 - INFO - __main__ - Step 730 Global step 730 Train loss 1.781701 on epoch=145
05/31/2022 01:14:39 - INFO - __main__ - Step 740 Global step 740 Train loss 1.397900 on epoch=147
05/31/2022 01:14:44 - INFO - __main__ - Step 750 Global step 750 Train loss 1.332469 on epoch=149
05/31/2022 01:14:45 - INFO - __main__ - Global step 750 Train loss 1.428910 ACC 0.5 on epoch=149
05/31/2022 01:14:51 - INFO - __main__ - Step 760 Global step 760 Train loss 1.322010 on epoch=151
05/31/2022 01:14:56 - INFO - __main__ - Step 770 Global step 770 Train loss 1.237099 on epoch=153
05/31/2022 01:15:01 - INFO - __main__ - Step 780 Global step 780 Train loss 1.160272 on epoch=155
05/31/2022 01:15:06 - INFO - __main__ - Step 790 Global step 790 Train loss 1.403021 on epoch=157
05/31/2022 01:15:12 - INFO - __main__ - Step 800 Global step 800 Train loss 1.034617 on epoch=159
05/31/2022 01:15:13 - INFO - __main__ - Global step 800 Train loss 1.231404 ACC 0.5 on epoch=159
05/31/2022 01:15:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.858532 on epoch=161
05/31/2022 01:15:23 - INFO - __main__ - Step 820 Global step 820 Train loss 1.234622 on epoch=163
05/31/2022 01:15:29 - INFO - __main__ - Step 830 Global step 830 Train loss 1.148743 on epoch=165
05/31/2022 01:15:34 - INFO - __main__ - Step 840 Global step 840 Train loss 1.078423 on epoch=167
05/31/2022 01:15:39 - INFO - __main__ - Step 850 Global step 850 Train loss 1.128440 on epoch=169
05/31/2022 01:15:40 - INFO - __main__ - Global step 850 Train loss 1.089752 ACC 0.671875 on epoch=169
05/31/2022 01:15:46 - INFO - __main__ - Step 860 Global step 860 Train loss 1.109757 on epoch=171
05/31/2022 01:15:51 - INFO - __main__ - Step 870 Global step 870 Train loss 1.091528 on epoch=173
05/31/2022 01:15:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.861037 on epoch=175
05/31/2022 01:16:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.890742 on epoch=177
05/31/2022 01:16:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.975002 on epoch=179
05/31/2022 01:16:08 - INFO - __main__ - Global step 900 Train loss 0.985613 ACC 0.6875 on epoch=179
05/31/2022 01:16:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.956110 on epoch=181
05/31/2022 01:16:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.901366 on epoch=183
05/31/2022 01:16:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.677728 on epoch=185
05/31/2022 01:16:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.819016 on epoch=187
05/31/2022 01:16:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.989094 on epoch=189
05/31/2022 01:16:36 - INFO - __main__ - Global step 950 Train loss 0.868663 ACC 0.6875 on epoch=189
05/31/2022 01:16:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.524579 on epoch=191
05/31/2022 01:16:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.455762 on epoch=193
05/31/2022 01:16:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.371357 on epoch=195
05/31/2022 01:16:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.255953 on epoch=197
05/31/2022 01:17:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.249099 on epoch=199
05/31/2022 01:17:04 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 01:17:04 - INFO - __main__ - Printing 3 examples
05/31/2022 01:17:04 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/31/2022 01:17:04 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:04 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/31/2022 01:17:04 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:04 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/31/2022 01:17:04 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:04 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:17:04 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:17:04 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 01:17:04 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 01:17:04 - INFO - __main__ - Printing 3 examples
05/31/2022 01:17:04 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/31/2022 01:17:04 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:04 - INFO - __main__ -  [superglue-cb] premise: She didn't know if they had given themselves sufficient time to think things over before they married - that was the kind of question her sister Louise asked. Edward stayed in the Engineers for a bit, then came out and was not very successful in finding a job to suit him. That wasn't his fault and if anyone said that it was Nenna would still feel like poking a hole in them. [SEP] hypothesis: it was Edward's fault
05/31/2022 01:17:04 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:04 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/31/2022 01:17:04 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:04 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:17:04 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:17:04 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 01:17:04 - INFO - __main__ - Global step 1000 Train loss 0.371350 ACC 0.796875 on epoch=199
05/31/2022 01:17:05 - INFO - __main__ - save last model!
05/31/2022 01:17:11 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 01:17:12 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 01:17:12 - INFO - __main__ - Printing 3 examples
05/31/2022 01:17:12 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 01:17:12 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:12 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 01:17:12 - INFO - __main__ - ['neutral']
05/31/2022 01:17:12 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 01:17:12 - INFO - __main__ - ['entailment']
05/31/2022 01:17:12 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:17:12 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:17:12 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 01:17:14 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_42_0.0001_8_predictions.txt
05/31/2022 01:17:14 - INFO - __main__ - ACC on test data: 0.8214
05/31/2022 01:17:14 - INFO - __main__ - prefix=superglue-cb_32_42, lr=0.0001, bsz=8, dev_performance=0.796875, test_performance=0.8214285714285714
05/31/2022 01:17:14 - INFO - __main__ - Running ... prefix=superglue-cb_32_87, lr=0.0005, bsz=8 ...
05/31/2022 01:17:15 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 01:17:15 - INFO - __main__ - Printing 3 examples
05/31/2022 01:17:15 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/31/2022 01:17:15 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:15 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/31/2022 01:17:15 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:15 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/31/2022 01:17:15 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:15 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:17:15 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:17:15 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 01:17:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 01:17:15 - INFO - __main__ - Printing 3 examples
05/31/2022 01:17:15 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/31/2022 01:17:15 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:15 - INFO - __main__ -  [superglue-cb] premise: She didn't know if they had given themselves sufficient time to think things over before they married - that was the kind of question her sister Louise asked. Edward stayed in the Engineers for a bit, then came out and was not very successful in finding a job to suit him. That wasn't his fault and if anyone said that it was Nenna would still feel like poking a hole in them. [SEP] hypothesis: it was Edward's fault
05/31/2022 01:17:15 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:15 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/31/2022 01:17:15 - INFO - __main__ - ['contradiction']
05/31/2022 01:17:15 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:17:15 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:17:15 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 01:17:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 01:17:17 - INFO - __main__ - Starting training!
05/31/2022 01:17:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 01:17:28 - INFO - __main__ - Starting training!
05/31/2022 01:17:32 - INFO - __main__ - Step 10 Global step 10 Train loss 24.952864 on epoch=1
05/31/2022 01:17:37 - INFO - __main__ - Step 20 Global step 20 Train loss 18.760580 on epoch=3
05/31/2022 01:17:42 - INFO - __main__ - Step 30 Global step 30 Train loss 13.669134 on epoch=5
05/31/2022 01:17:47 - INFO - __main__ - Step 40 Global step 40 Train loss 11.574293 on epoch=7
05/31/2022 01:17:52 - INFO - __main__ - Step 50 Global step 50 Train loss 10.462106 on epoch=9
05/31/2022 01:17:54 - INFO - __main__ - Global step 50 Train loss 15.883797 ACC 0.03125 on epoch=9
05/31/2022 01:18:00 - INFO - __main__ - Step 60 Global step 60 Train loss 8.170189 on epoch=11
05/31/2022 01:18:05 - INFO - __main__ - Step 70 Global step 70 Train loss 7.293645 on epoch=13
05/31/2022 01:18:10 - INFO - __main__ - Step 80 Global step 80 Train loss 6.645328 on epoch=15
05/31/2022 01:18:16 - INFO - __main__ - Step 90 Global step 90 Train loss 5.047685 on epoch=17
05/31/2022 01:18:21 - INFO - __main__ - Step 100 Global step 100 Train loss 3.747654 on epoch=19
05/31/2022 01:18:22 - INFO - __main__ - Global step 100 Train loss 6.180900 ACC 0.421875 on epoch=19
05/31/2022 01:18:28 - INFO - __main__ - Step 110 Global step 110 Train loss 2.492697 on epoch=21
05/31/2022 01:18:33 - INFO - __main__ - Step 120 Global step 120 Train loss 2.076952 on epoch=23
05/31/2022 01:18:38 - INFO - __main__ - Step 130 Global step 130 Train loss 2.239401 on epoch=25
05/31/2022 01:18:43 - INFO - __main__ - Step 140 Global step 140 Train loss 2.008904 on epoch=27
05/31/2022 01:18:49 - INFO - __main__ - Step 150 Global step 150 Train loss 1.754876 on epoch=29
05/31/2022 01:18:50 - INFO - __main__ - Global step 150 Train loss 2.114566 ACC 0.5 on epoch=29
05/31/2022 01:18:55 - INFO - __main__ - Step 160 Global step 160 Train loss 1.769536 on epoch=31
05/31/2022 01:19:01 - INFO - __main__ - Step 170 Global step 170 Train loss 2.320030 on epoch=33
05/31/2022 01:19:06 - INFO - __main__ - Step 180 Global step 180 Train loss 1.616471 on epoch=35
05/31/2022 01:19:11 - INFO - __main__ - Step 190 Global step 190 Train loss 1.714569 on epoch=37
05/31/2022 01:19:16 - INFO - __main__ - Step 200 Global step 200 Train loss 1.329780 on epoch=39
05/31/2022 01:19:17 - INFO - __main__ - Global step 200 Train loss 1.750077 ACC 0.5 on epoch=39
05/31/2022 01:19:23 - INFO - __main__ - Step 210 Global step 210 Train loss 1.477654 on epoch=41
05/31/2022 01:19:28 - INFO - __main__ - Step 220 Global step 220 Train loss 1.038231 on epoch=43
05/31/2022 01:19:33 - INFO - __main__ - Step 230 Global step 230 Train loss 1.186916 on epoch=45
05/31/2022 01:19:38 - INFO - __main__ - Step 240 Global step 240 Train loss 1.142964 on epoch=47
05/31/2022 01:19:43 - INFO - __main__ - Step 250 Global step 250 Train loss 1.004896 on epoch=49
05/31/2022 01:19:45 - INFO - __main__ - Global step 250 Train loss 1.170132 ACC 0.5 on epoch=49
05/31/2022 01:19:50 - INFO - __main__ - Step 260 Global step 260 Train loss 1.011749 on epoch=51
05/31/2022 01:19:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.786284 on epoch=53
05/31/2022 01:20:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.784599 on epoch=55
05/31/2022 01:20:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.674981 on epoch=57
05/31/2022 01:20:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.715261 on epoch=59
05/31/2022 01:20:12 - INFO - __main__ - Global step 300 Train loss 0.794575 ACC 0.5 on epoch=59
05/31/2022 01:20:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.671067 on epoch=61
05/31/2022 01:20:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.912124 on epoch=63
05/31/2022 01:20:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.742602 on epoch=65
05/31/2022 01:20:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.791321 on epoch=67
05/31/2022 01:20:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.550725 on epoch=69
05/31/2022 01:20:39 - INFO - __main__ - Global step 350 Train loss 0.733568 ACC 0.703125 on epoch=69
05/31/2022 01:20:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.630196 on epoch=71
05/31/2022 01:20:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.543356 on epoch=73
05/31/2022 01:20:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.628504 on epoch=75
05/31/2022 01:21:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.447402 on epoch=77
05/31/2022 01:21:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.398779 on epoch=79
05/31/2022 01:21:07 - INFO - __main__ - Global step 400 Train loss 0.529647 ACC 0.828125 on epoch=79
05/31/2022 01:21:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.499039 on epoch=81
05/31/2022 01:21:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.493623 on epoch=83
05/31/2022 01:21:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.518586 on epoch=85
05/31/2022 01:21:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.374014 on epoch=87
05/31/2022 01:21:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.464861 on epoch=89
05/31/2022 01:21:35 - INFO - __main__ - Global step 450 Train loss 0.470025 ACC 0.859375 on epoch=89
05/31/2022 01:21:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.458876 on epoch=91
05/31/2022 01:21:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.692269 on epoch=93
05/31/2022 01:21:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.315089 on epoch=95
05/31/2022 01:21:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.317433 on epoch=97
05/31/2022 01:22:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.244275 on epoch=99
05/31/2022 01:22:03 - INFO - __main__ - Global step 500 Train loss 0.405588 ACC 0.8125 on epoch=99
05/31/2022 01:22:08 - INFO - __main__ - Step 510 Global step 510 Train loss 1.069363 on epoch=101
05/31/2022 01:22:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.240714 on epoch=103
05/31/2022 01:22:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.189712 on epoch=105
05/31/2022 01:22:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.166646 on epoch=107
05/31/2022 01:22:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.095399 on epoch=109
05/31/2022 01:22:30 - INFO - __main__ - Global step 550 Train loss 0.352367 ACC 0.96875 on epoch=109
05/31/2022 01:22:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.044196 on epoch=111
05/31/2022 01:22:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.031255 on epoch=113
05/31/2022 01:22:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.036227 on epoch=115
05/31/2022 01:22:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.004351 on epoch=117
05/31/2022 01:22:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.012676 on epoch=119
05/31/2022 01:22:58 - INFO - __main__ - Global step 600 Train loss 0.025741 ACC 0.890625 on epoch=119
05/31/2022 01:23:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.034195 on epoch=121
05/31/2022 01:23:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.065963 on epoch=123
05/31/2022 01:23:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.027696 on epoch=125
05/31/2022 01:23:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.136926 on epoch=127
05/31/2022 01:23:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.172434 on epoch=129
05/31/2022 01:23:25 - INFO - __main__ - Global step 650 Train loss 0.087443 ACC 0.921875 on epoch=129
05/31/2022 01:23:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.244383 on epoch=131
05/31/2022 01:23:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.848254 on epoch=133
05/31/2022 01:23:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.486355 on epoch=135
05/31/2022 01:23:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.449798 on epoch=137
05/31/2022 01:23:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.465734 on epoch=139
05/31/2022 01:23:52 - INFO - __main__ - Global step 700 Train loss 0.498905 ACC 0.703125 on epoch=139
05/31/2022 01:23:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.459595 on epoch=141
05/31/2022 01:24:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.433971 on epoch=143
05/31/2022 01:24:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.365007 on epoch=145
05/31/2022 01:24:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.348180 on epoch=147
05/31/2022 01:24:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.371662 on epoch=149
05/31/2022 01:24:20 - INFO - __main__ - Global step 750 Train loss 0.395683 ACC 0.5625 on epoch=149
05/31/2022 01:24:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.307436 on epoch=151
05/31/2022 01:24:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.347011 on epoch=153
05/31/2022 01:24:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.361974 on epoch=155
05/31/2022 01:24:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.353504 on epoch=157
05/31/2022 01:24:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.303462 on epoch=159
05/31/2022 01:24:47 - INFO - __main__ - Global step 800 Train loss 0.334678 ACC 0.515625 on epoch=159
05/31/2022 01:24:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.363674 on epoch=161
05/31/2022 01:24:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.322675 on epoch=163
05/31/2022 01:25:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.355531 on epoch=165
05/31/2022 01:25:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.338703 on epoch=167
05/31/2022 01:25:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.304808 on epoch=169
05/31/2022 01:25:14 - INFO - __main__ - Global step 850 Train loss 0.337078 ACC 0.5625 on epoch=169
05/31/2022 01:25:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.299632 on epoch=171
05/31/2022 01:25:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.296501 on epoch=173
05/31/2022 01:25:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.272163 on epoch=175
05/31/2022 01:25:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.286277 on epoch=177
05/31/2022 01:25:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.282037 on epoch=179
05/31/2022 01:25:42 - INFO - __main__ - Global step 900 Train loss 0.287322 ACC 0.5 on epoch=179
05/31/2022 01:25:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.238464 on epoch=181
05/31/2022 01:25:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.241107 on epoch=183
05/31/2022 01:25:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.326358 on epoch=185
05/31/2022 01:26:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.298581 on epoch=187
05/31/2022 01:26:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.302311 on epoch=189
05/31/2022 01:26:09 - INFO - __main__ - Global step 950 Train loss 0.281364 ACC 0.625 on epoch=189
05/31/2022 01:26:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.279948 on epoch=191
05/31/2022 01:26:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.288259 on epoch=193
05/31/2022 01:26:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.261913 on epoch=195
05/31/2022 01:26:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.246548 on epoch=197
05/31/2022 01:26:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.236894 on epoch=199
05/31/2022 01:26:37 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 01:26:37 - INFO - __main__ - Printing 3 examples
05/31/2022 01:26:37 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/31/2022 01:26:37 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:37 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/31/2022 01:26:37 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:37 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/31/2022 01:26:37 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:37 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:26:37 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:26:37 - INFO - __main__ - Global step 1000 Train loss 0.262712 ACC 0.734375 on epoch=199
05/31/2022 01:26:37 - INFO - __main__ - save last model!
05/31/2022 01:26:37 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 01:26:37 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 01:26:37 - INFO - __main__ - Printing 3 examples
05/31/2022 01:26:37 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/31/2022 01:26:37 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:37 - INFO - __main__ -  [superglue-cb] premise: She didn't know if they had given themselves sufficient time to think things over before they married - that was the kind of question her sister Louise asked. Edward stayed in the Engineers for a bit, then came out and was not very successful in finding a job to suit him. That wasn't his fault and if anyone said that it was Nenna would still feel like poking a hole in them. [SEP] hypothesis: it was Edward's fault
05/31/2022 01:26:37 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:37 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/31/2022 01:26:37 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:37 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:26:37 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:26:37 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 01:26:44 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 01:26:45 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 01:26:45 - INFO - __main__ - Printing 3 examples
05/31/2022 01:26:45 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 01:26:45 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:45 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 01:26:45 - INFO - __main__ - ['neutral']
05/31/2022 01:26:45 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 01:26:45 - INFO - __main__ - ['entailment']
05/31/2022 01:26:45 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:26:45 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:26:45 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 01:26:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_87_0.0005_8_predictions.txt
05/31/2022 01:26:46 - INFO - __main__ - ACC on test data: 0.8750
05/31/2022 01:26:47 - INFO - __main__ - prefix=superglue-cb_32_87, lr=0.0005, bsz=8, dev_performance=0.96875, test_performance=0.875
05/31/2022 01:26:47 - INFO - __main__ - Running ... prefix=superglue-cb_32_87, lr=0.0003, bsz=8 ...
05/31/2022 01:26:47 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 01:26:47 - INFO - __main__ - Printing 3 examples
05/31/2022 01:26:47 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/31/2022 01:26:47 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:47 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/31/2022 01:26:47 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:47 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/31/2022 01:26:47 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:47 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:26:47 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:26:48 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 01:26:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 01:26:48 - INFO - __main__ - Printing 3 examples
05/31/2022 01:26:48 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/31/2022 01:26:48 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:48 - INFO - __main__ -  [superglue-cb] premise: She didn't know if they had given themselves sufficient time to think things over before they married - that was the kind of question her sister Louise asked. Edward stayed in the Engineers for a bit, then came out and was not very successful in finding a job to suit him. That wasn't his fault and if anyone said that it was Nenna would still feel like poking a hole in them. [SEP] hypothesis: it was Edward's fault
05/31/2022 01:26:48 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:48 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/31/2022 01:26:48 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:48 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:26:48 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:26:48 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 01:26:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 01:26:48 - INFO - __main__ - Starting training!
05/31/2022 01:26:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 01:26:59 - INFO - __main__ - Starting training!
05/31/2022 01:27:03 - INFO - __main__ - Step 10 Global step 10 Train loss 24.974375 on epoch=1
05/31/2022 01:27:08 - INFO - __main__ - Step 20 Global step 20 Train loss 17.773785 on epoch=3
05/31/2022 01:27:13 - INFO - __main__ - Step 30 Global step 30 Train loss 14.608345 on epoch=5
05/31/2022 01:27:17 - INFO - __main__ - Step 40 Global step 40 Train loss 12.413198 on epoch=7
05/31/2022 01:27:22 - INFO - __main__ - Step 50 Global step 50 Train loss 11.757248 on epoch=9
05/31/2022 01:27:24 - INFO - __main__ - Global step 50 Train loss 16.305389 ACC 0.078125 on epoch=9
05/31/2022 01:27:30 - INFO - __main__ - Step 60 Global step 60 Train loss 12.614028 on epoch=11
05/31/2022 01:27:35 - INFO - __main__ - Step 70 Global step 70 Train loss 11.024168 on epoch=13
05/31/2022 01:27:41 - INFO - __main__ - Step 80 Global step 80 Train loss 9.223733 on epoch=15
05/31/2022 01:27:46 - INFO - __main__ - Step 90 Global step 90 Train loss 8.887010 on epoch=17
05/31/2022 01:27:51 - INFO - __main__ - Step 100 Global step 100 Train loss 7.756930 on epoch=19
05/31/2022 01:27:52 - INFO - __main__ - Global step 100 Train loss 9.901174 ACC 0.03125 on epoch=19
05/31/2022 01:27:58 - INFO - __main__ - Step 110 Global step 110 Train loss 7.198968 on epoch=21
05/31/2022 01:28:03 - INFO - __main__ - Step 120 Global step 120 Train loss 6.661382 on epoch=23
05/31/2022 01:28:08 - INFO - __main__ - Step 130 Global step 130 Train loss 6.030280 on epoch=25
05/31/2022 01:28:13 - INFO - __main__ - Step 140 Global step 140 Train loss 4.814740 on epoch=27
05/31/2022 01:28:18 - INFO - __main__ - Step 150 Global step 150 Train loss 3.763767 on epoch=29
05/31/2022 01:28:20 - INFO - __main__ - Global step 150 Train loss 5.693828 ACC 0.25 on epoch=29
05/31/2022 01:28:26 - INFO - __main__ - Step 160 Global step 160 Train loss 3.044158 on epoch=31
05/31/2022 01:28:31 - INFO - __main__ - Step 170 Global step 170 Train loss 2.583010 on epoch=33
05/31/2022 01:28:36 - INFO - __main__ - Step 180 Global step 180 Train loss 2.795203 on epoch=35
05/31/2022 01:28:41 - INFO - __main__ - Step 190 Global step 190 Train loss 2.027538 on epoch=37
05/31/2022 01:28:46 - INFO - __main__ - Step 200 Global step 200 Train loss 2.475775 on epoch=39
05/31/2022 01:28:48 - INFO - __main__ - Global step 200 Train loss 2.585137 ACC 0.5 on epoch=39
05/31/2022 01:28:53 - INFO - __main__ - Step 210 Global step 210 Train loss 2.535427 on epoch=41
05/31/2022 01:28:58 - INFO - __main__ - Step 220 Global step 220 Train loss 2.131073 on epoch=43
05/31/2022 01:29:04 - INFO - __main__ - Step 230 Global step 230 Train loss 1.790890 on epoch=45
05/31/2022 01:29:09 - INFO - __main__ - Step 240 Global step 240 Train loss 1.766726 on epoch=47
05/31/2022 01:29:14 - INFO - __main__ - Step 250 Global step 250 Train loss 2.269960 on epoch=49
05/31/2022 01:29:15 - INFO - __main__ - Global step 250 Train loss 2.098815 ACC 0.5 on epoch=49
05/31/2022 01:29:20 - INFO - __main__ - Step 260 Global step 260 Train loss 1.593722 on epoch=51
05/31/2022 01:29:26 - INFO - __main__ - Step 270 Global step 270 Train loss 1.171605 on epoch=53
05/31/2022 01:29:31 - INFO - __main__ - Step 280 Global step 280 Train loss 1.579968 on epoch=55
05/31/2022 01:29:36 - INFO - __main__ - Step 290 Global step 290 Train loss 1.859602 on epoch=57
05/31/2022 01:29:41 - INFO - __main__ - Step 300 Global step 300 Train loss 1.602447 on epoch=59
05/31/2022 01:29:42 - INFO - __main__ - Global step 300 Train loss 1.561469 ACC 0.5 on epoch=59
05/31/2022 01:29:48 - INFO - __main__ - Step 310 Global step 310 Train loss 1.658952 on epoch=61
05/31/2022 01:29:53 - INFO - __main__ - Step 320 Global step 320 Train loss 1.513528 on epoch=63
05/31/2022 01:29:58 - INFO - __main__ - Step 330 Global step 330 Train loss 1.315057 on epoch=65
05/31/2022 01:30:03 - INFO - __main__ - Step 340 Global step 340 Train loss 1.606167 on epoch=67
05/31/2022 01:30:09 - INFO - __main__ - Step 350 Global step 350 Train loss 1.259452 on epoch=69
05/31/2022 01:30:10 - INFO - __main__ - Global step 350 Train loss 1.470631 ACC 0.5 on epoch=69
05/31/2022 01:30:15 - INFO - __main__ - Step 360 Global step 360 Train loss 1.268094 on epoch=71
05/31/2022 01:30:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.935731 on epoch=73
05/31/2022 01:30:26 - INFO - __main__ - Step 380 Global step 380 Train loss 1.028779 on epoch=75
05/31/2022 01:30:31 - INFO - __main__ - Step 390 Global step 390 Train loss 1.166176 on epoch=77
05/31/2022 01:30:36 - INFO - __main__ - Step 400 Global step 400 Train loss 1.306383 on epoch=79
05/31/2022 01:30:37 - INFO - __main__ - Global step 400 Train loss 1.141033 ACC 0.5 on epoch=79
05/31/2022 01:30:42 - INFO - __main__ - Step 410 Global step 410 Train loss 1.193862 on epoch=81
05/31/2022 01:30:48 - INFO - __main__ - Step 420 Global step 420 Train loss 1.192948 on epoch=83
05/31/2022 01:30:53 - INFO - __main__ - Step 430 Global step 430 Train loss 1.244575 on epoch=85
05/31/2022 01:30:58 - INFO - __main__ - Step 440 Global step 440 Train loss 1.097026 on epoch=87
05/31/2022 01:31:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.951740 on epoch=89
05/31/2022 01:31:05 - INFO - __main__ - Global step 450 Train loss 1.136030 ACC 0.5 on epoch=89
05/31/2022 01:31:10 - INFO - __main__ - Step 460 Global step 460 Train loss 1.020928 on epoch=91
05/31/2022 01:31:15 - INFO - __main__ - Step 470 Global step 470 Train loss 1.141721 on epoch=93
05/31/2022 01:31:21 - INFO - __main__ - Step 480 Global step 480 Train loss 1.020129 on epoch=95
05/31/2022 01:31:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.938972 on epoch=97
05/31/2022 01:31:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.712740 on epoch=99
05/31/2022 01:31:32 - INFO - __main__ - Global step 500 Train loss 0.966898 ACC 0.5 on epoch=99
05/31/2022 01:31:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.951279 on epoch=101
05/31/2022 01:31:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.720528 on epoch=103
05/31/2022 01:31:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.729228 on epoch=105
05/31/2022 01:31:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.647645 on epoch=107
05/31/2022 01:31:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.878002 on epoch=109
05/31/2022 01:32:00 - INFO - __main__ - Global step 550 Train loss 0.785337 ACC 0.5 on epoch=109
05/31/2022 01:32:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.787169 on epoch=111
05/31/2022 01:32:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.720451 on epoch=113
05/31/2022 01:32:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.736752 on epoch=115
05/31/2022 01:32:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.698172 on epoch=117
05/31/2022 01:32:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.713660 on epoch=119
05/31/2022 01:32:27 - INFO - __main__ - Global step 600 Train loss 0.731241 ACC 0.5 on epoch=119
05/31/2022 01:32:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.564502 on epoch=121
05/31/2022 01:32:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.548107 on epoch=123
05/31/2022 01:32:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.722250 on epoch=125
05/31/2022 01:32:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.467159 on epoch=127
05/31/2022 01:32:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.830280 on epoch=129
05/31/2022 01:32:55 - INFO - __main__ - Global step 650 Train loss 0.626460 ACC 0.5 on epoch=129
05/31/2022 01:33:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.489171 on epoch=131
05/31/2022 01:33:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.602228 on epoch=133
05/31/2022 01:33:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.518259 on epoch=135
05/31/2022 01:33:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.539346 on epoch=137
05/31/2022 01:33:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.510739 on epoch=139
05/31/2022 01:33:22 - INFO - __main__ - Global step 700 Train loss 0.531949 ACC 0.5 on epoch=139
05/31/2022 01:33:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.540224 on epoch=141
05/31/2022 01:33:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.515133 on epoch=143
05/31/2022 01:33:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.545628 on epoch=145
05/31/2022 01:33:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.563687 on epoch=147
05/31/2022 01:33:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.433958 on epoch=149
05/31/2022 01:33:50 - INFO - __main__ - Global step 750 Train loss 0.519726 ACC 0.5 on epoch=149
05/31/2022 01:33:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.525844 on epoch=151
05/31/2022 01:34:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.532625 on epoch=153
05/31/2022 01:34:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.532403 on epoch=155
05/31/2022 01:34:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.394263 on epoch=157
05/31/2022 01:34:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.433850 on epoch=159
05/31/2022 01:34:17 - INFO - __main__ - Global step 800 Train loss 0.483797 ACC 0.5 on epoch=159
05/31/2022 01:34:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.490052 on epoch=161
05/31/2022 01:34:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.444673 on epoch=163
05/31/2022 01:34:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.429981 on epoch=165
05/31/2022 01:34:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.453185 on epoch=167
05/31/2022 01:34:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.494081 on epoch=169
05/31/2022 01:34:45 - INFO - __main__ - Global step 850 Train loss 0.462394 ACC 0.5 on epoch=169
05/31/2022 01:34:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.465881 on epoch=171
05/31/2022 01:34:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.381927 on epoch=173
05/31/2022 01:35:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.456898 on epoch=175
05/31/2022 01:35:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.420998 on epoch=177
05/31/2022 01:35:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.463305 on epoch=179
05/31/2022 01:35:12 - INFO - __main__ - Global step 900 Train loss 0.437802 ACC 0.5 on epoch=179
05/31/2022 01:35:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.446177 on epoch=181
05/31/2022 01:35:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.463259 on epoch=183
05/31/2022 01:35:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.437541 on epoch=185
05/31/2022 01:35:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.420090 on epoch=187
05/31/2022 01:35:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.416434 on epoch=189
05/31/2022 01:35:39 - INFO - __main__ - Global step 950 Train loss 0.436700 ACC 0.5 on epoch=189
05/31/2022 01:35:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.433705 on epoch=191
05/31/2022 01:35:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.435640 on epoch=193
05/31/2022 01:35:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.394164 on epoch=195
05/31/2022 01:36:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.459361 on epoch=197
05/31/2022 01:36:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.464225 on epoch=199
05/31/2022 01:36:07 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 01:36:07 - INFO - __main__ - Printing 3 examples
05/31/2022 01:36:07 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/31/2022 01:36:07 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:07 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/31/2022 01:36:07 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:07 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/31/2022 01:36:07 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:07 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:36:07 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:36:07 - INFO - __main__ - Global step 1000 Train loss 0.437419 ACC 0.5 on epoch=199
05/31/2022 01:36:07 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 01:36:07 - INFO - __main__ - save last model!
05/31/2022 01:36:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 01:36:07 - INFO - __main__ - Printing 3 examples
05/31/2022 01:36:07 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/31/2022 01:36:07 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:07 - INFO - __main__ -  [superglue-cb] premise: She didn't know if they had given themselves sufficient time to think things over before they married - that was the kind of question her sister Louise asked. Edward stayed in the Engineers for a bit, then came out and was not very successful in finding a job to suit him. That wasn't his fault and if anyone said that it was Nenna would still feel like poking a hole in them. [SEP] hypothesis: it was Edward's fault
05/31/2022 01:36:07 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:07 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/31/2022 01:36:07 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:07 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:36:07 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:36:07 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 01:36:14 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 01:36:14 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 01:36:14 - INFO - __main__ - Printing 3 examples
05/31/2022 01:36:14 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 01:36:14 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:14 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 01:36:14 - INFO - __main__ - ['neutral']
05/31/2022 01:36:14 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 01:36:14 - INFO - __main__ - ['entailment']
05/31/2022 01:36:14 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:36:14 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:36:14 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 01:36:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_87_0.0003_8_predictions.txt
05/31/2022 01:36:15 - INFO - __main__ - ACC on test data: 0.5000
05/31/2022 01:36:16 - INFO - __main__ - prefix=superglue-cb_32_87, lr=0.0003, bsz=8, dev_performance=0.5, test_performance=0.5
05/31/2022 01:36:16 - INFO - __main__ - Running ... prefix=superglue-cb_32_87, lr=0.0002, bsz=8 ...
05/31/2022 01:36:17 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 01:36:17 - INFO - __main__ - Printing 3 examples
05/31/2022 01:36:17 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/31/2022 01:36:17 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:17 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/31/2022 01:36:17 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:17 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/31/2022 01:36:17 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:17 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:36:17 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:36:17 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 01:36:17 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 01:36:17 - INFO - __main__ - Printing 3 examples
05/31/2022 01:36:17 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/31/2022 01:36:17 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:17 - INFO - __main__ -  [superglue-cb] premise: She didn't know if they had given themselves sufficient time to think things over before they married - that was the kind of question her sister Louise asked. Edward stayed in the Engineers for a bit, then came out and was not very successful in finding a job to suit him. That wasn't his fault and if anyone said that it was Nenna would still feel like poking a hole in them. [SEP] hypothesis: it was Edward's fault
05/31/2022 01:36:17 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:17 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/31/2022 01:36:17 - INFO - __main__ - ['contradiction']
05/31/2022 01:36:17 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:36:17 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:36:17 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 01:36:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 01:36:18 - INFO - __main__ - Starting training!
05/31/2022 01:36:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 01:36:30 - INFO - __main__ - Starting training!
05/31/2022 01:36:34 - INFO - __main__ - Step 10 Global step 10 Train loss 23.617687 on epoch=1
05/31/2022 01:36:39 - INFO - __main__ - Step 20 Global step 20 Train loss 19.527304 on epoch=3
05/31/2022 01:36:44 - INFO - __main__ - Step 30 Global step 30 Train loss 14.095421 on epoch=5
05/31/2022 01:36:49 - INFO - __main__ - Step 40 Global step 40 Train loss 12.870115 on epoch=7
05/31/2022 01:36:55 - INFO - __main__ - Step 50 Global step 50 Train loss 11.974388 on epoch=9
05/31/2022 01:36:59 - INFO - __main__ - Global step 50 Train loss 16.416985 ACC 0.03125 on epoch=9
05/31/2022 01:37:05 - INFO - __main__ - Step 60 Global step 60 Train loss 11.213543 on epoch=11
05/31/2022 01:37:10 - INFO - __main__ - Step 70 Global step 70 Train loss 10.067856 on epoch=13
05/31/2022 01:37:15 - INFO - __main__ - Step 80 Global step 80 Train loss 9.486177 on epoch=15
05/31/2022 01:37:20 - INFO - __main__ - Step 90 Global step 90 Train loss 9.018946 on epoch=17
05/31/2022 01:37:25 - INFO - __main__ - Step 100 Global step 100 Train loss 8.574043 on epoch=19
05/31/2022 01:37:27 - INFO - __main__ - Global step 100 Train loss 9.672112 ACC 0.0 on epoch=19
05/31/2022 01:37:32 - INFO - __main__ - Step 110 Global step 110 Train loss 8.102473 on epoch=21
05/31/2022 01:37:37 - INFO - __main__ - Step 120 Global step 120 Train loss 7.728414 on epoch=23
05/31/2022 01:37:42 - INFO - __main__ - Step 130 Global step 130 Train loss 7.592534 on epoch=25
05/31/2022 01:37:48 - INFO - __main__ - Step 140 Global step 140 Train loss 6.968629 on epoch=27
05/31/2022 01:37:53 - INFO - __main__ - Step 150 Global step 150 Train loss 6.424595 on epoch=29
05/31/2022 01:37:54 - INFO - __main__ - Global step 150 Train loss 7.363329 ACC 0.0 on epoch=29
05/31/2022 01:38:00 - INFO - __main__ - Step 160 Global step 160 Train loss 5.842003 on epoch=31
05/31/2022 01:38:05 - INFO - __main__ - Step 170 Global step 170 Train loss 5.379336 on epoch=33
05/31/2022 01:38:10 - INFO - __main__ - Step 180 Global step 180 Train loss 4.418680 on epoch=35
05/31/2022 01:38:15 - INFO - __main__ - Step 190 Global step 190 Train loss 4.112940 on epoch=37
05/31/2022 01:38:20 - INFO - __main__ - Step 200 Global step 200 Train loss 3.642554 on epoch=39
05/31/2022 01:38:22 - INFO - __main__ - Global step 200 Train loss 4.679103 ACC 0.0 on epoch=39
05/31/2022 01:38:27 - INFO - __main__ - Step 210 Global step 210 Train loss 2.500910 on epoch=41
05/31/2022 01:38:32 - INFO - __main__ - Step 220 Global step 220 Train loss 2.539871 on epoch=43
05/31/2022 01:38:37 - INFO - __main__ - Step 230 Global step 230 Train loss 2.841712 on epoch=45
05/31/2022 01:38:43 - INFO - __main__ - Step 240 Global step 240 Train loss 2.183510 on epoch=47
05/31/2022 01:38:48 - INFO - __main__ - Step 250 Global step 250 Train loss 1.937099 on epoch=49
05/31/2022 01:38:49 - INFO - __main__ - Global step 250 Train loss 2.400620 ACC 0.5 on epoch=49
05/31/2022 01:38:55 - INFO - __main__ - Step 260 Global step 260 Train loss 2.422559 on epoch=51
05/31/2022 01:39:00 - INFO - __main__ - Step 270 Global step 270 Train loss 2.363396 on epoch=53
05/31/2022 01:39:05 - INFO - __main__ - Step 280 Global step 280 Train loss 1.755470 on epoch=55
05/31/2022 01:39:10 - INFO - __main__ - Step 290 Global step 290 Train loss 1.789026 on epoch=57
05/31/2022 01:39:15 - INFO - __main__ - Step 300 Global step 300 Train loss 1.805674 on epoch=59
05/31/2022 01:39:16 - INFO - __main__ - Global step 300 Train loss 2.027225 ACC 0.5 on epoch=59
05/31/2022 01:39:22 - INFO - __main__ - Step 310 Global step 310 Train loss 1.710893 on epoch=61
05/31/2022 01:39:27 - INFO - __main__ - Step 320 Global step 320 Train loss 1.912394 on epoch=63
05/31/2022 01:39:32 - INFO - __main__ - Step 330 Global step 330 Train loss 2.072041 on epoch=65
05/31/2022 01:39:37 - INFO - __main__ - Step 340 Global step 340 Train loss 1.570202 on epoch=67
05/31/2022 01:39:42 - INFO - __main__ - Step 350 Global step 350 Train loss 1.736525 on epoch=69
05/31/2022 01:39:43 - INFO - __main__ - Global step 350 Train loss 1.800411 ACC 0.5 on epoch=69
05/31/2022 01:39:49 - INFO - __main__ - Step 360 Global step 360 Train loss 1.332188 on epoch=71
05/31/2022 01:39:54 - INFO - __main__ - Step 370 Global step 370 Train loss 1.440286 on epoch=73
05/31/2022 01:39:59 - INFO - __main__ - Step 380 Global step 380 Train loss 1.571149 on epoch=75
05/31/2022 01:40:04 - INFO - __main__ - Step 390 Global step 390 Train loss 1.617205 on epoch=77
05/31/2022 01:40:09 - INFO - __main__ - Step 400 Global step 400 Train loss 1.433000 on epoch=79
05/31/2022 01:40:11 - INFO - __main__ - Global step 400 Train loss 1.478766 ACC 0.5 on epoch=79
05/31/2022 01:40:16 - INFO - __main__ - Step 410 Global step 410 Train loss 1.050583 on epoch=81
05/31/2022 01:40:21 - INFO - __main__ - Step 420 Global step 420 Train loss 1.148603 on epoch=83
05/31/2022 01:40:26 - INFO - __main__ - Step 430 Global step 430 Train loss 1.451366 on epoch=85
05/31/2022 01:40:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.766480 on epoch=87
05/31/2022 01:40:37 - INFO - __main__ - Step 450 Global step 450 Train loss 1.156220 on epoch=89
05/31/2022 01:40:38 - INFO - __main__ - Global step 450 Train loss 1.114650 ACC 0.734375 on epoch=89
05/31/2022 01:40:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.976888 on epoch=91
05/31/2022 01:40:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.830367 on epoch=93
05/31/2022 01:40:54 - INFO - __main__ - Step 480 Global step 480 Train loss 1.062715 on epoch=95
05/31/2022 01:40:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.691740 on epoch=97
05/31/2022 01:41:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.787738 on epoch=99
05/31/2022 01:41:06 - INFO - __main__ - Global step 500 Train loss 0.869890 ACC 0.71875 on epoch=99
05/31/2022 01:41:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.859032 on epoch=101
05/31/2022 01:41:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.556700 on epoch=103
05/31/2022 01:41:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.551789 on epoch=105
05/31/2022 01:41:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.520627 on epoch=107
05/31/2022 01:41:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.375987 on epoch=109
05/31/2022 01:41:32 - INFO - __main__ - Global step 550 Train loss 0.572827 ACC 0.234375 on epoch=109
05/31/2022 01:41:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.310980 on epoch=111
05/31/2022 01:41:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.234579 on epoch=113
05/31/2022 01:41:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.146719 on epoch=115
05/31/2022 01:41:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.115100 on epoch=117
05/31/2022 01:41:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.174237 on epoch=119
05/31/2022 01:42:00 - INFO - __main__ - Global step 600 Train loss 0.196323 ACC 0.90625 on epoch=119
05/31/2022 01:42:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.061034 on epoch=121
05/31/2022 01:42:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.122894 on epoch=123
05/31/2022 01:42:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.028317 on epoch=125
05/31/2022 01:42:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.033877 on epoch=127
05/31/2022 01:42:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.027317 on epoch=129
05/31/2022 01:42:28 - INFO - __main__ - Global step 650 Train loss 0.054688 ACC 0.71875 on epoch=129
05/31/2022 01:42:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.011007 on epoch=131
05/31/2022 01:42:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.005352 on epoch=133
05/31/2022 01:42:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.007281 on epoch=135
05/31/2022 01:42:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.026529 on epoch=137
05/31/2022 01:42:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.007764 on epoch=139
05/31/2022 01:42:55 - INFO - __main__ - Global step 700 Train loss 0.011587 ACC 0.9375 on epoch=139
05/31/2022 01:43:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.011489 on epoch=141
05/31/2022 01:43:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.005210 on epoch=143
05/31/2022 01:43:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.003561 on epoch=145
05/31/2022 01:43:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.002174 on epoch=147
05/31/2022 01:43:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.016852 on epoch=149
05/31/2022 01:43:23 - INFO - __main__ - Global step 750 Train loss 0.007857 ACC 0.796875 on epoch=149
05/31/2022 01:43:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.003027 on epoch=151
05/31/2022 01:43:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.031608 on epoch=153
05/31/2022 01:43:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.004148 on epoch=155
05/31/2022 01:43:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.071993 on epoch=157
05/31/2022 01:43:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002281 on epoch=159
05/31/2022 01:43:50 - INFO - __main__ - Global step 800 Train loss 0.022611 ACC 0.90625 on epoch=159
05/31/2022 01:43:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001165 on epoch=161
05/31/2022 01:44:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.009847 on epoch=163
05/31/2022 01:44:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001375 on epoch=165
05/31/2022 01:44:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000412 on epoch=167
05/31/2022 01:44:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000284 on epoch=169
05/31/2022 01:44:18 - INFO - __main__ - Global step 850 Train loss 0.002617 ACC 0.9375 on epoch=169
05/31/2022 01:44:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002571 on epoch=171
05/31/2022 01:44:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.002066 on epoch=173
05/31/2022 01:44:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000356 on epoch=175
05/31/2022 01:44:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.007161 on epoch=177
05/31/2022 01:44:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000572 on epoch=179
05/31/2022 01:44:45 - INFO - __main__ - Global step 900 Train loss 0.002545 ACC 0.84375 on epoch=179
05/31/2022 01:44:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.002765 on epoch=181
05/31/2022 01:44:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000323 on epoch=183
05/31/2022 01:45:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000389 on epoch=185
05/31/2022 01:45:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000086 on epoch=187
05/31/2022 01:45:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000145 on epoch=189
05/31/2022 01:45:12 - INFO - __main__ - Global step 950 Train loss 0.000742 ACC 0.875 on epoch=189
05/31/2022 01:45:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000135 on epoch=191
05/31/2022 01:45:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000149 on epoch=193
05/31/2022 01:45:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000324 on epoch=195
05/31/2022 01:45:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000322 on epoch=197
05/31/2022 01:45:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.003217 on epoch=199
05/31/2022 01:45:40 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 01:45:40 - INFO - __main__ - Printing 3 examples
05/31/2022 01:45:40 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/31/2022 01:45:40 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:40 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/31/2022 01:45:40 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:40 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/31/2022 01:45:40 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:40 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:45:40 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:45:40 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 01:45:40 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 01:45:40 - INFO - __main__ - Printing 3 examples
05/31/2022 01:45:40 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/31/2022 01:45:40 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:40 - INFO - __main__ -  [superglue-cb] premise: She didn't know if they had given themselves sufficient time to think things over before they married - that was the kind of question her sister Louise asked. Edward stayed in the Engineers for a bit, then came out and was not very successful in finding a job to suit him. That wasn't his fault and if anyone said that it was Nenna would still feel like poking a hole in them. [SEP] hypothesis: it was Edward's fault
05/31/2022 01:45:40 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:40 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/31/2022 01:45:40 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:40 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:45:40 - INFO - __main__ - Global step 1000 Train loss 0.000829 ACC 0.9375 on epoch=199
05/31/2022 01:45:40 - INFO - __main__ - save last model!
05/31/2022 01:45:40 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:45:40 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 01:45:47 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 01:45:47 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 01:45:47 - INFO - __main__ - Printing 3 examples
05/31/2022 01:45:47 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 01:45:47 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:47 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 01:45:47 - INFO - __main__ - ['neutral']
05/31/2022 01:45:47 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 01:45:47 - INFO - __main__ - ['entailment']
05/31/2022 01:45:47 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:45:47 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:45:47 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 01:45:49 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_87_0.0002_8_predictions.txt
05/31/2022 01:45:49 - INFO - __main__ - ACC on test data: 0.8750
05/31/2022 01:45:49 - INFO - __main__ - prefix=superglue-cb_32_87, lr=0.0002, bsz=8, dev_performance=0.9375, test_performance=0.875
05/31/2022 01:45:49 - INFO - __main__ - Running ... prefix=superglue-cb_32_87, lr=0.0001, bsz=8 ...
05/31/2022 01:45:50 - INFO - __main__ - Start tokenizing ... 80 instances
05/31/2022 01:45:50 - INFO - __main__ - Printing 3 examples
05/31/2022 01:45:50 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/31/2022 01:45:50 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:50 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/31/2022 01:45:50 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:50 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/31/2022 01:45:50 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:50 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:45:50 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:45:50 - INFO - __main__ - Loaded 80 examples from train data
05/31/2022 01:45:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 01:45:50 - INFO - __main__ - Printing 3 examples
05/31/2022 01:45:50 - INFO - __main__ -  [superglue-cb] premise: He had seen something I should have, which was a car turning in from Soho Square and coming up behind me. My right foot hovered over the accelerator pedal and I balanced Armstrong on the clutch. I wasn't as convinced as Malpass that Nevil was out of harm's way. [SEP] hypothesis: Nevil was out of harm's way
05/31/2022 01:45:50 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:50 - INFO - __main__ -  [superglue-cb] premise: She didn't know if they had given themselves sufficient time to think things over before they married - that was the kind of question her sister Louise asked. Edward stayed in the Engineers for a bit, then came out and was not very successful in finding a job to suit him. That wasn't his fault and if anyone said that it was Nenna would still feel like poking a hole in them. [SEP] hypothesis: it was Edward's fault
05/31/2022 01:45:50 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:50 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/31/2022 01:45:50 - INFO - __main__ - ['contradiction']
05/31/2022 01:45:50 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:45:50 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:45:50 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 01:45:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 01:45:53 - INFO - __main__ - Starting training!
05/31/2022 01:46:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 01:46:03 - INFO - __main__ - Starting training!
05/31/2022 01:46:08 - INFO - __main__ - Step 10 Global step 10 Train loss 24.791672 on epoch=1
05/31/2022 01:46:12 - INFO - __main__ - Step 20 Global step 20 Train loss 24.000427 on epoch=3
05/31/2022 01:46:17 - INFO - __main__ - Step 30 Global step 30 Train loss 18.471497 on epoch=5
05/31/2022 01:46:22 - INFO - __main__ - Step 40 Global step 40 Train loss 15.363360 on epoch=7
05/31/2022 01:46:27 - INFO - __main__ - Step 50 Global step 50 Train loss 14.000730 on epoch=9
05/31/2022 01:46:48 - INFO - __main__ - Global step 50 Train loss 19.325537 ACC 0.0 on epoch=9
05/31/2022 01:46:54 - INFO - __main__ - Step 60 Global step 60 Train loss 12.749941 on epoch=11
05/31/2022 01:46:59 - INFO - __main__ - Step 70 Global step 70 Train loss 12.116163 on epoch=13
05/31/2022 01:47:04 - INFO - __main__ - Step 80 Global step 80 Train loss 11.760572 on epoch=15
05/31/2022 01:47:09 - INFO - __main__ - Step 90 Global step 90 Train loss 11.512962 on epoch=17
05/31/2022 01:47:15 - INFO - __main__ - Step 100 Global step 100 Train loss 10.825811 on epoch=19
05/31/2022 01:47:27 - INFO - __main__ - Global step 100 Train loss 11.793090 ACC 0.0 on epoch=19
05/31/2022 01:47:32 - INFO - __main__ - Step 110 Global step 110 Train loss 10.827039 on epoch=21
05/31/2022 01:47:37 - INFO - __main__ - Step 120 Global step 120 Train loss 10.059145 on epoch=23
05/31/2022 01:47:42 - INFO - __main__ - Step 130 Global step 130 Train loss 9.978713 on epoch=25
05/31/2022 01:47:48 - INFO - __main__ - Step 140 Global step 140 Train loss 9.989283 on epoch=27
05/31/2022 01:47:53 - INFO - __main__ - Step 150 Global step 150 Train loss 9.922475 on epoch=29
05/31/2022 01:48:02 - INFO - __main__ - Global step 150 Train loss 10.155332 ACC 0.0 on epoch=29
05/31/2022 01:48:07 - INFO - __main__ - Step 160 Global step 160 Train loss 9.538572 on epoch=31
05/31/2022 01:48:12 - INFO - __main__ - Step 170 Global step 170 Train loss 9.753691 on epoch=33
05/31/2022 01:48:17 - INFO - __main__ - Step 180 Global step 180 Train loss 9.007235 on epoch=35
05/31/2022 01:48:22 - INFO - __main__ - Step 190 Global step 190 Train loss 8.721214 on epoch=37
05/31/2022 01:48:28 - INFO - __main__ - Step 200 Global step 200 Train loss 8.503065 on epoch=39
05/31/2022 01:48:31 - INFO - __main__ - Global step 200 Train loss 9.104755 ACC 0.0 on epoch=39
05/31/2022 01:48:36 - INFO - __main__ - Step 210 Global step 210 Train loss 7.990744 on epoch=41
05/31/2022 01:48:42 - INFO - __main__ - Step 220 Global step 220 Train loss 7.962424 on epoch=43
05/31/2022 01:48:47 - INFO - __main__ - Step 230 Global step 230 Train loss 8.020295 on epoch=45
05/31/2022 01:48:52 - INFO - __main__ - Step 240 Global step 240 Train loss 8.003999 on epoch=47
05/31/2022 01:48:57 - INFO - __main__ - Step 250 Global step 250 Train loss 7.386913 on epoch=49
05/31/2022 01:49:00 - INFO - __main__ - Global step 250 Train loss 7.872874 ACC 0.0 on epoch=49
05/31/2022 01:49:05 - INFO - __main__ - Step 260 Global step 260 Train loss 7.061059 on epoch=51
05/31/2022 01:49:11 - INFO - __main__ - Step 270 Global step 270 Train loss 6.941390 on epoch=53
05/31/2022 01:49:16 - INFO - __main__ - Step 280 Global step 280 Train loss 7.239766 on epoch=55
05/31/2022 01:49:21 - INFO - __main__ - Step 290 Global step 290 Train loss 6.803953 on epoch=57
05/31/2022 01:49:26 - INFO - __main__ - Step 300 Global step 300 Train loss 6.746962 on epoch=59
05/31/2022 01:49:28 - INFO - __main__ - Global step 300 Train loss 6.958626 ACC 0.0 on epoch=59
05/31/2022 01:49:34 - INFO - __main__ - Step 310 Global step 310 Train loss 6.364359 on epoch=61
05/31/2022 01:49:39 - INFO - __main__ - Step 320 Global step 320 Train loss 5.987466 on epoch=63
05/31/2022 01:49:44 - INFO - __main__ - Step 330 Global step 330 Train loss 5.625626 on epoch=65
05/31/2022 01:49:49 - INFO - __main__ - Step 340 Global step 340 Train loss 5.126042 on epoch=67
05/31/2022 01:49:54 - INFO - __main__ - Step 350 Global step 350 Train loss 5.095060 on epoch=69
05/31/2022 01:49:56 - INFO - __main__ - Global step 350 Train loss 5.639711 ACC 0.015625 on epoch=69
05/31/2022 01:50:02 - INFO - __main__ - Step 360 Global step 360 Train loss 4.591173 on epoch=71
05/31/2022 01:50:07 - INFO - __main__ - Step 370 Global step 370 Train loss 4.586972 on epoch=73
05/31/2022 01:50:13 - INFO - __main__ - Step 380 Global step 380 Train loss 3.239336 on epoch=75
05/31/2022 01:50:18 - INFO - __main__ - Step 390 Global step 390 Train loss 3.347208 on epoch=77
05/31/2022 01:50:23 - INFO - __main__ - Step 400 Global step 400 Train loss 3.499843 on epoch=79
05/31/2022 01:50:25 - INFO - __main__ - Global step 400 Train loss 3.852907 ACC 0.03125 on epoch=79
05/31/2022 01:50:31 - INFO - __main__ - Step 410 Global step 410 Train loss 3.282198 on epoch=81
05/31/2022 01:50:36 - INFO - __main__ - Step 420 Global step 420 Train loss 3.108529 on epoch=83
05/31/2022 01:50:41 - INFO - __main__ - Step 430 Global step 430 Train loss 2.384280 on epoch=85
05/31/2022 01:50:46 - INFO - __main__ - Step 440 Global step 440 Train loss 2.459135 on epoch=87
05/31/2022 01:50:52 - INFO - __main__ - Step 450 Global step 450 Train loss 2.561444 on epoch=89
05/31/2022 01:50:53 - INFO - __main__ - Global step 450 Train loss 2.759117 ACC 0.5 on epoch=89
05/31/2022 01:50:58 - INFO - __main__ - Step 460 Global step 460 Train loss 2.193217 on epoch=91
05/31/2022 01:51:03 - INFO - __main__ - Step 470 Global step 470 Train loss 2.587798 on epoch=93
05/31/2022 01:51:09 - INFO - __main__ - Step 480 Global step 480 Train loss 2.639907 on epoch=95
05/31/2022 01:51:14 - INFO - __main__ - Step 490 Global step 490 Train loss 2.104685 on epoch=97
05/31/2022 01:51:19 - INFO - __main__ - Step 500 Global step 500 Train loss 1.839842 on epoch=99
05/31/2022 01:51:20 - INFO - __main__ - Global step 500 Train loss 2.273090 ACC 0.5 on epoch=99
05/31/2022 01:51:25 - INFO - __main__ - Step 510 Global step 510 Train loss 2.057674 on epoch=101
05/31/2022 01:51:30 - INFO - __main__ - Step 520 Global step 520 Train loss 2.647105 on epoch=103
05/31/2022 01:51:36 - INFO - __main__ - Step 530 Global step 530 Train loss 1.822162 on epoch=105
05/31/2022 01:51:41 - INFO - __main__ - Step 540 Global step 540 Train loss 1.979589 on epoch=107
05/31/2022 01:51:46 - INFO - __main__ - Step 550 Global step 550 Train loss 1.796336 on epoch=109
05/31/2022 01:51:47 - INFO - __main__ - Global step 550 Train loss 2.060573 ACC 0.5 on epoch=109
05/31/2022 01:51:52 - INFO - __main__ - Step 560 Global step 560 Train loss 1.821321 on epoch=111
05/31/2022 01:51:57 - INFO - __main__ - Step 570 Global step 570 Train loss 1.797402 on epoch=113
05/31/2022 01:52:03 - INFO - __main__ - Step 580 Global step 580 Train loss 1.656782 on epoch=115
05/31/2022 01:52:08 - INFO - __main__ - Step 590 Global step 590 Train loss 2.288898 on epoch=117
05/31/2022 01:52:13 - INFO - __main__ - Step 600 Global step 600 Train loss 2.482217 on epoch=119
05/31/2022 01:52:14 - INFO - __main__ - Global step 600 Train loss 2.009324 ACC 0.5 on epoch=119
05/31/2022 01:52:19 - INFO - __main__ - Step 610 Global step 610 Train loss 1.741940 on epoch=121
05/31/2022 01:52:24 - INFO - __main__ - Step 620 Global step 620 Train loss 2.201969 on epoch=123
05/31/2022 01:52:30 - INFO - __main__ - Step 630 Global step 630 Train loss 1.779579 on epoch=125
05/31/2022 01:52:35 - INFO - __main__ - Step 640 Global step 640 Train loss 2.087622 on epoch=127
05/31/2022 01:52:40 - INFO - __main__ - Step 650 Global step 650 Train loss 1.732650 on epoch=129
05/31/2022 01:52:41 - INFO - __main__ - Global step 650 Train loss 1.908752 ACC 0.5 on epoch=129
05/31/2022 01:52:46 - INFO - __main__ - Step 660 Global step 660 Train loss 1.766491 on epoch=131
05/31/2022 01:52:52 - INFO - __main__ - Step 670 Global step 670 Train loss 1.753527 on epoch=133
05/31/2022 01:52:57 - INFO - __main__ - Step 680 Global step 680 Train loss 1.716786 on epoch=135
05/31/2022 01:53:02 - INFO - __main__ - Step 690 Global step 690 Train loss 1.543056 on epoch=137
05/31/2022 01:53:07 - INFO - __main__ - Step 700 Global step 700 Train loss 1.170059 on epoch=139
05/31/2022 01:53:08 - INFO - __main__ - Global step 700 Train loss 1.589984 ACC 0.5 on epoch=139
05/31/2022 01:53:14 - INFO - __main__ - Step 710 Global step 710 Train loss 1.813964 on epoch=141
05/31/2022 01:53:19 - INFO - __main__ - Step 720 Global step 720 Train loss 1.598396 on epoch=143
05/31/2022 01:53:24 - INFO - __main__ - Step 730 Global step 730 Train loss 1.628763 on epoch=145
05/31/2022 01:53:29 - INFO - __main__ - Step 740 Global step 740 Train loss 1.503999 on epoch=147
05/31/2022 01:53:34 - INFO - __main__ - Step 750 Global step 750 Train loss 1.522958 on epoch=149
05/31/2022 01:53:35 - INFO - __main__ - Global step 750 Train loss 1.613616 ACC 0.5 on epoch=149
05/31/2022 01:53:40 - INFO - __main__ - Step 760 Global step 760 Train loss 1.811293 on epoch=151
05/31/2022 01:53:46 - INFO - __main__ - Step 770 Global step 770 Train loss 1.474591 on epoch=153
05/31/2022 01:53:51 - INFO - __main__ - Step 780 Global step 780 Train loss 1.351329 on epoch=155
05/31/2022 01:53:56 - INFO - __main__ - Step 790 Global step 790 Train loss 1.203225 on epoch=157
05/31/2022 01:54:01 - INFO - __main__ - Step 800 Global step 800 Train loss 1.460749 on epoch=159
05/31/2022 01:54:02 - INFO - __main__ - Global step 800 Train loss 1.460237 ACC 0.5 on epoch=159
05/31/2022 01:54:07 - INFO - __main__ - Step 810 Global step 810 Train loss 1.562097 on epoch=161
05/31/2022 01:54:12 - INFO - __main__ - Step 820 Global step 820 Train loss 1.394789 on epoch=163
05/31/2022 01:54:18 - INFO - __main__ - Step 830 Global step 830 Train loss 1.093953 on epoch=165
05/31/2022 01:54:23 - INFO - __main__ - Step 840 Global step 840 Train loss 1.707684 on epoch=167
05/31/2022 01:54:28 - INFO - __main__ - Step 850 Global step 850 Train loss 1.271227 on epoch=169
05/31/2022 01:54:29 - INFO - __main__ - Global step 850 Train loss 1.405950 ACC 0.5 on epoch=169
05/31/2022 01:54:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.831870 on epoch=171
05/31/2022 01:54:39 - INFO - __main__ - Step 870 Global step 870 Train loss 1.384440 on epoch=173
05/31/2022 01:54:44 - INFO - __main__ - Step 880 Global step 880 Train loss 1.064528 on epoch=175
05/31/2022 01:54:49 - INFO - __main__ - Step 890 Global step 890 Train loss 1.220737 on epoch=177
05/31/2022 01:54:55 - INFO - __main__ - Step 900 Global step 900 Train loss 1.085677 on epoch=179
05/31/2022 01:54:56 - INFO - __main__ - Global step 900 Train loss 1.117450 ACC 0.5 on epoch=179
05/31/2022 01:55:01 - INFO - __main__ - Step 910 Global step 910 Train loss 1.081548 on epoch=181
05/31/2022 01:55:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.948270 on epoch=183
05/31/2022 01:55:11 - INFO - __main__ - Step 930 Global step 930 Train loss 1.222996 on epoch=185
05/31/2022 01:55:16 - INFO - __main__ - Step 940 Global step 940 Train loss 1.274936 on epoch=187
05/31/2022 01:55:22 - INFO - __main__ - Step 950 Global step 950 Train loss 1.102153 on epoch=189
05/31/2022 01:55:23 - INFO - __main__ - Global step 950 Train loss 1.125980 ACC 0.5 on epoch=189
05/31/2022 01:55:28 - INFO - __main__ - Step 960 Global step 960 Train loss 1.069845 on epoch=191
05/31/2022 01:55:33 - INFO - __main__ - Step 970 Global step 970 Train loss 1.123767 on epoch=193
05/31/2022 01:55:38 - INFO - __main__ - Step 980 Global step 980 Train loss 1.157179 on epoch=195
05/31/2022 01:55:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.913718 on epoch=197
05/31/2022 01:55:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.954741 on epoch=199
05/31/2022 01:55:50 - INFO - __main__ - Global step 1000 Train loss 1.043850 ACC 0.5 on epoch=199
05/31/2022 01:55:50 - INFO - __main__ - save last model!
05/31/2022 01:55:57 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 01:55:58 - INFO - __main__ - Start tokenizing ... 56 instances
05/31/2022 01:55:58 - INFO - __main__ - Printing 3 examples
05/31/2022 01:55:58 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/31/2022 01:55:58 - INFO - __main__ - ['contradiction']
05/31/2022 01:55:58 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/31/2022 01:55:58 - INFO - __main__ - ['neutral']
05/31/2022 01:55:58 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/31/2022 01:55:58 - INFO - __main__ - ['entailment']
05/31/2022 01:55:58 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:55:58 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:55:58 - INFO - __main__ - Loaded 56 examples from test data
05/31/2022 01:55:59 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-superglue-cb/superglue-cb_32_87_0.0001_8_predictions.txt
05/31/2022 01:55:59 - INFO - __main__ - ACC on test data: 0.5000
05/31/2022 01:55:59 - INFO - __main__ - prefix=superglue-cb_32_87, lr=0.0001, bsz=8, dev_performance=0.5, test_performance=0.5
