05/21/2022 21:23:52 - INFO - __main__ - Namespace(task_dir='data_32/ethos-race/', task_name='ethos-race', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:23:52 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race
05/21/2022 21:23:52 - INFO - __main__ - Namespace(task_dir='data_32/ethos-race/', task_name='ethos-race', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:23:52 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race
05/21/2022 21:23:53 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:23:53 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:23:53 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:23:53 - INFO - __main__ - Using 2 gpus
05/21/2022 21:23:53 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:23:53 - INFO - __main__ - Using 2 gpus
05/21/2022 21:23:53 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_32_100', 'ethos-race_32_13', 'ethos-race_32_21', 'ethos-race_32_42', 'ethos-race_32_87']
05/21/2022 21:23:53 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_32_100', 'ethos-race_32_13', 'ethos-race_32_21', 'ethos-race_32_42', 'ethos-race_32_87']
05/21/2022 21:23:58 - INFO - __main__ - Running ... prefix=ethos-race_32_100, lr=0.0005, bsz=8 ...
06/01/2022 10:17:59 - INFO - __main__ - Namespace(task_dir='data_32/ethos-race/', task_name='ethos-race', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/01/2022 10:17:59 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race
06/01/2022 10:17:59 - INFO - __main__ - Namespace(task_dir='data_32/ethos-race/', task_name='ethos-race', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/01/2022 10:17:59 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race
06/01/2022 10:18:00 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/01/2022 10:18:00 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/01/2022 10:18:00 - INFO - __main__ - args.device: cuda:0
06/01/2022 10:18:00 - INFO - __main__ - Using 2 gpus
06/01/2022 10:18:00 - INFO - __main__ - args.device: cuda:1
06/01/2022 10:18:00 - INFO - __main__ - Using 2 gpus
06/01/2022 10:18:00 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_32_100', 'ethos-race_32_13', 'ethos-race_32_21', 'ethos-race_32_42', 'ethos-race_32_87']
06/01/2022 10:18:00 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_32_100', 'ethos-race_32_13', 'ethos-race_32_21', 'ethos-race_32_42', 'ethos-race_32_87']
06/01/2022 10:18:04 - INFO - __main__ - Running ... prefix=ethos-race_32_100, lr=0.0005, bsz=8 ...
06/01/2022 10:18:05 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 10:18:05 - INFO - __main__ - Printing 3 examples
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:18:05 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:18:05 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 10:18:05 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 10:18:05 - INFO - __main__ - Printing 3 examples
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] or when the man walked up to the little girl with the gay pride flag i wanted to jump into the video and punch him but ya know thats assault
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] Being a lesbian is fine if you really love being an animal
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] this was shit the guy should of boxed his head off and thrown him through the top window,fuck peoado muhamed and islam
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:18:05 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 10:18:05 - INFO - __main__ - Printing 3 examples
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:18:05 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:18:05 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:18:05 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 10:18:05 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 10:18:05 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 10:18:05 - INFO - __main__ - Printing 3 examples
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] or when the man walked up to the little girl with the gay pride flag i wanted to jump into the video and punch him but ya know thats assault
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] Being a lesbian is fine if you really love being an animal
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ -  [ethos-race] this was shit the guy should of boxed his head off and thrown him through the top window,fuck peoado muhamed and islam
06/01/2022 10:18:05 - INFO - __main__ - ['false']
06/01/2022 10:18:05 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:18:05 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:18:05 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 10:18:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 10:18:18 - INFO - __main__ - Starting training!
06/01/2022 10:18:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 10:18:19 - INFO - __main__ - Starting training!
06/01/2022 10:18:24 - INFO - __main__ - Step 10 Global step 10 Train loss 22.950815 on epoch=2
06/01/2022 10:18:29 - INFO - __main__ - Step 20 Global step 20 Train loss 18.095182 on epoch=4
06/01/2022 10:18:34 - INFO - __main__ - Step 30 Global step 30 Train loss 16.148754 on epoch=7
06/01/2022 10:18:40 - INFO - __main__ - Step 40 Global step 40 Train loss 14.121214 on epoch=9
06/01/2022 10:18:45 - INFO - __main__ - Step 50 Global step 50 Train loss 12.905556 on epoch=12
06/01/2022 10:18:47 - INFO - __main__ - Global step 50 Train loss 16.844305 Classification-F1 0.0 on epoch=12
06/01/2022 10:18:54 - INFO - __main__ - Step 60 Global step 60 Train loss 6.064849 on epoch=14
06/01/2022 10:19:00 - INFO - __main__ - Step 70 Global step 70 Train loss 2.181655 on epoch=17
06/01/2022 10:19:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.772004 on epoch=19
06/01/2022 10:19:10 - INFO - __main__ - Step 90 Global step 90 Train loss 1.196100 on epoch=22
06/01/2022 10:19:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.461388 on epoch=24
06/01/2022 10:19:16 - INFO - __main__ - Global step 100 Train loss 2.135200 Classification-F1 0.3478260869565218 on epoch=24
06/01/2022 10:19:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.800522 on epoch=27
06/01/2022 10:19:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.409860 on epoch=29
06/01/2022 10:19:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.540704 on epoch=32
06/01/2022 10:19:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.324684 on epoch=34
06/01/2022 10:19:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.479253 on epoch=37
06/01/2022 10:19:45 - INFO - __main__ - Global step 150 Train loss 0.511005 Classification-F1 0.35214211076280044 on epoch=37
06/01/2022 10:19:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.389448 on epoch=39
06/01/2022 10:19:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.426347 on epoch=42
06/01/2022 10:20:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.388947 on epoch=44
06/01/2022 10:20:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.454589 on epoch=47
06/01/2022 10:20:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.413773 on epoch=49
06/01/2022 10:20:13 - INFO - __main__ - Global step 200 Train loss 0.414621 Classification-F1 0.3478260869565218 on epoch=49
06/01/2022 10:20:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.401700 on epoch=52
06/01/2022 10:20:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.282037 on epoch=54
06/01/2022 10:20:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.293638 on epoch=57
06/01/2022 10:20:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.303007 on epoch=59
06/01/2022 10:20:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.294840 on epoch=62
06/01/2022 10:20:40 - INFO - __main__ - Global step 250 Train loss 0.315045 Classification-F1 0.7916666666666667 on epoch=62
06/01/2022 10:20:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.301362 on epoch=64
06/01/2022 10:20:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.253244 on epoch=67
06/01/2022 10:20:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.183101 on epoch=69
06/01/2022 10:21:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.200335 on epoch=72
06/01/2022 10:21:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.209733 on epoch=74
06/01/2022 10:21:08 - INFO - __main__ - Global step 300 Train loss 0.229555 Classification-F1 0.7330367074527252 on epoch=74
06/01/2022 10:21:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.156530 on epoch=77
06/01/2022 10:21:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.139125 on epoch=79
06/01/2022 10:21:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.144695 on epoch=82
06/01/2022 10:21:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.077909 on epoch=84
06/01/2022 10:21:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.078080 on epoch=87
06/01/2022 10:21:35 - INFO - __main__ - Global step 350 Train loss 0.119268 Classification-F1 0.7942857142857143 on epoch=87
06/01/2022 10:21:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.041304 on epoch=89
06/01/2022 10:21:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.110306 on epoch=92
06/01/2022 10:21:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.045155 on epoch=94
06/01/2022 10:21:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.131103 on epoch=97
06/01/2022 10:22:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.042928 on epoch=99
06/01/2022 10:22:03 - INFO - __main__ - Global step 400 Train loss 0.074159 Classification-F1 0.8076362576508307 on epoch=99
06/01/2022 10:22:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.022184 on epoch=102
06/01/2022 10:22:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.058060 on epoch=104
06/01/2022 10:22:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.008385 on epoch=107
06/01/2022 10:22:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.014591 on epoch=109
06/01/2022 10:22:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.008753 on epoch=112
06/01/2022 10:22:30 - INFO - __main__ - Global step 450 Train loss 0.022395 Classification-F1 0.7664071190211346 on epoch=112
06/01/2022 10:22:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.039164 on epoch=114
06/01/2022 10:22:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.008321 on epoch=117
06/01/2022 10:22:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.017442 on epoch=119
06/01/2022 10:22:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.009141 on epoch=122
06/01/2022 10:22:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001829 on epoch=124
06/01/2022 10:22:57 - INFO - __main__ - Global step 500 Train loss 0.015179 Classification-F1 0.7916666666666667 on epoch=124
06/01/2022 10:23:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002964 on epoch=127
06/01/2022 10:23:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000833 on epoch=129
06/01/2022 10:23:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.035658 on epoch=132
06/01/2022 10:23:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.031343 on epoch=134
06/01/2022 10:23:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.005928 on epoch=137
06/01/2022 10:23:24 - INFO - __main__ - Global step 550 Train loss 0.015345 Classification-F1 0.7285067873303168 on epoch=137
06/01/2022 10:23:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000484 on epoch=139
06/01/2022 10:23:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.005391 on epoch=142
06/01/2022 10:23:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.002244 on epoch=144
06/01/2022 10:23:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.019425 on epoch=147
06/01/2022 10:23:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000523 on epoch=149
06/01/2022 10:23:51 - INFO - __main__ - Global step 600 Train loss 0.005613 Classification-F1 0.7569444444444444 on epoch=149
06/01/2022 10:23:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.028148 on epoch=152
06/01/2022 10:24:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.011536 on epoch=154
06/01/2022 10:24:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.024629 on epoch=157
06/01/2022 10:24:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.002888 on epoch=159
06/01/2022 10:24:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001217 on epoch=162
06/01/2022 10:24:18 - INFO - __main__ - Global step 650 Train loss 0.013684 Classification-F1 0.7942857142857143 on epoch=162
06/01/2022 10:24:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001853 on epoch=164
06/01/2022 10:24:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000858 on epoch=167
06/01/2022 10:24:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.004929 on epoch=169
06/01/2022 10:24:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.004492 on epoch=172
06/01/2022 10:24:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.002679 on epoch=174
06/01/2022 10:24:45 - INFO - __main__ - Global step 700 Train loss 0.002962 Classification-F1 0.7413049726933028 on epoch=174
06/01/2022 10:24:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000468 on epoch=177
06/01/2022 10:24:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.005896 on epoch=179
06/01/2022 10:25:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.002021 on epoch=182
06/01/2022 10:25:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.002482 on epoch=184
06/01/2022 10:25:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000265 on epoch=187
06/01/2022 10:25:11 - INFO - __main__ - Global step 750 Train loss 0.002226 Classification-F1 0.7465502675302733 on epoch=187
06/01/2022 10:25:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000865 on epoch=189
06/01/2022 10:25:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000202 on epoch=192
06/01/2022 10:25:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000039 on epoch=194
06/01/2022 10:25:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000149 on epoch=197
06/01/2022 10:25:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000085 on epoch=199
06/01/2022 10:25:38 - INFO - __main__ - Global step 800 Train loss 0.000268 Classification-F1 0.7783461210571185 on epoch=199
06/01/2022 10:25:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.018841 on epoch=202
06/01/2022 10:25:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.075501 on epoch=204
06/01/2022 10:25:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000865 on epoch=207
06/01/2022 10:25:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000266 on epoch=209
06/01/2022 10:26:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000448 on epoch=212
06/01/2022 10:26:05 - INFO - __main__ - Global step 850 Train loss 0.019184 Classification-F1 0.7783461210571185 on epoch=212
06/01/2022 10:26:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.029189 on epoch=214
06/01/2022 10:26:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.041360 on epoch=217
06/01/2022 10:26:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.008229 on epoch=219
06/01/2022 10:26:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.010418 on epoch=222
06/01/2022 10:26:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000875 on epoch=224
06/01/2022 10:26:32 - INFO - __main__ - Global step 900 Train loss 0.018014 Classification-F1 0.7916666666666667 on epoch=224
06/01/2022 10:26:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000356 on epoch=227
06/01/2022 10:26:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000666 on epoch=229
06/01/2022 10:26:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000346 on epoch=232
06/01/2022 10:26:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000148 on epoch=234
06/01/2022 10:26:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000072 on epoch=237
06/01/2022 10:26:59 - INFO - __main__ - Global step 950 Train loss 0.000318 Classification-F1 0.7916666666666667 on epoch=237
06/01/2022 10:27:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.002072 on epoch=239
06/01/2022 10:27:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.011294 on epoch=242
06/01/2022 10:27:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.046273 on epoch=244
06/01/2022 10:27:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000774 on epoch=247
06/01/2022 10:27:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.003013 on epoch=249
06/01/2022 10:27:26 - INFO - __main__ - Global step 1000 Train loss 0.012685 Classification-F1 0.7884841363102233 on epoch=249
06/01/2022 10:27:26 - INFO - __main__ - save last model!
06/01/2022 10:27:26 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 10:27:26 - INFO - __main__ - Printing 3 examples
06/01/2022 10:27:26 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/01/2022 10:27:26 - INFO - __main__ - ['false']
06/01/2022 10:27:26 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/01/2022 10:27:26 - INFO - __main__ - ['false']
06/01/2022 10:27:26 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
06/01/2022 10:27:26 - INFO - __main__ - ['false']
06/01/2022 10:27:26 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:27:26 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:27:26 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 10:27:26 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 10:27:26 - INFO - __main__ - Printing 3 examples
06/01/2022 10:27:26 - INFO - __main__ -  [ethos-race] or when the man walked up to the little girl with the gay pride flag i wanted to jump into the video and punch him but ya know thats assault
06/01/2022 10:27:26 - INFO - __main__ - ['false']
06/01/2022 10:27:26 - INFO - __main__ -  [ethos-race] Being a lesbian is fine if you really love being an animal
06/01/2022 10:27:26 - INFO - __main__ - ['false']
06/01/2022 10:27:26 - INFO - __main__ -  [ethos-race] this was shit the guy should of boxed his head off and thrown him through the top window,fuck peoado muhamed and islam
06/01/2022 10:27:26 - INFO - __main__ - ['false']
06/01/2022 10:27:26 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:27:26 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:27:26 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 10:27:32 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 10:27:33 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 10:27:33 - INFO - __main__ - Printing 3 examples
06/01/2022 10:27:33 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 10:27:33 - INFO - __main__ - ['true']
06/01/2022 10:27:33 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 10:27:33 - INFO - __main__ - ['false']
06/01/2022 10:27:33 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 10:27:33 - INFO - __main__ - ['false']
06/01/2022 10:27:33 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:27:33 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:27:33 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 10:27:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_100_0.0005_8_predictions.txt
06/01/2022 10:27:35 - INFO - __main__ - Classification-F1 on test data: 0.9019
06/01/2022 10:27:35 - INFO - __main__ - prefix=ethos-race_32_100, lr=0.0005, bsz=8, dev_performance=0.8076362576508307, test_performance=0.90187232122716
06/01/2022 10:27:35 - INFO - __main__ - Running ... prefix=ethos-race_32_100, lr=0.0003, bsz=8 ...
06/01/2022 10:27:36 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 10:27:36 - INFO - __main__ - Printing 3 examples
06/01/2022 10:27:36 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/01/2022 10:27:36 - INFO - __main__ - ['false']
06/01/2022 10:27:36 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/01/2022 10:27:36 - INFO - __main__ - ['false']
06/01/2022 10:27:36 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
06/01/2022 10:27:36 - INFO - __main__ - ['false']
06/01/2022 10:27:36 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:27:36 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:27:36 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 10:27:36 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 10:27:36 - INFO - __main__ - Printing 3 examples
06/01/2022 10:27:36 - INFO - __main__ -  [ethos-race] or when the man walked up to the little girl with the gay pride flag i wanted to jump into the video and punch him but ya know thats assault
06/01/2022 10:27:36 - INFO - __main__ - ['false']
06/01/2022 10:27:36 - INFO - __main__ -  [ethos-race] Being a lesbian is fine if you really love being an animal
06/01/2022 10:27:36 - INFO - __main__ - ['false']
06/01/2022 10:27:36 - INFO - __main__ -  [ethos-race] this was shit the guy should of boxed his head off and thrown him through the top window,fuck peoado muhamed and islam
06/01/2022 10:27:36 - INFO - __main__ - ['false']
06/01/2022 10:27:36 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:27:36 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:27:36 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 10:27:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 10:27:37 - INFO - __main__ - Starting training!
06/01/2022 10:27:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 10:27:49 - INFO - __main__ - Starting training!
06/01/2022 10:27:54 - INFO - __main__ - Step 10 Global step 10 Train loss 23.410357 on epoch=2
06/01/2022 10:27:59 - INFO - __main__ - Step 20 Global step 20 Train loss 18.240259 on epoch=4
06/01/2022 10:28:04 - INFO - __main__ - Step 30 Global step 30 Train loss 17.178745 on epoch=7
06/01/2022 10:28:10 - INFO - __main__ - Step 40 Global step 40 Train loss 16.170446 on epoch=9
06/01/2022 10:28:15 - INFO - __main__ - Step 50 Global step 50 Train loss 15.196817 on epoch=12
06/01/2022 10:28:16 - INFO - __main__ - Global step 50 Train loss 18.039326 Classification-F1 0.0 on epoch=12
06/01/2022 10:28:21 - INFO - __main__ - Step 60 Global step 60 Train loss 13.953146 on epoch=14
06/01/2022 10:28:27 - INFO - __main__ - Step 70 Global step 70 Train loss 13.277819 on epoch=17
06/01/2022 10:28:32 - INFO - __main__ - Step 80 Global step 80 Train loss 11.553065 on epoch=19
06/01/2022 10:28:37 - INFO - __main__ - Step 90 Global step 90 Train loss 9.471419 on epoch=22
06/01/2022 10:28:43 - INFO - __main__ - Step 100 Global step 100 Train loss 3.571830 on epoch=24
06/01/2022 10:28:43 - INFO - __main__ - Global step 100 Train loss 10.365456 Classification-F1 0.33578570081116776 on epoch=24
06/01/2022 10:28:49 - INFO - __main__ - Step 110 Global step 110 Train loss 2.933242 on epoch=27
06/01/2022 10:28:54 - INFO - __main__ - Step 120 Global step 120 Train loss 3.062250 on epoch=29
06/01/2022 10:28:59 - INFO - __main__ - Step 130 Global step 130 Train loss 1.828262 on epoch=32
06/01/2022 10:29:05 - INFO - __main__ - Step 140 Global step 140 Train loss 1.375763 on epoch=34
06/01/2022 10:29:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.771401 on epoch=37
06/01/2022 10:29:11 - INFO - __main__ - Global step 150 Train loss 1.994183 Classification-F1 0.3181818181818182 on epoch=37
06/01/2022 10:29:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.461156 on epoch=39
06/01/2022 10:29:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.481373 on epoch=42
06/01/2022 10:29:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.477621 on epoch=44
06/01/2022 10:29:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.460964 on epoch=47
06/01/2022 10:29:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.444732 on epoch=49
06/01/2022 10:29:38 - INFO - __main__ - Global step 200 Train loss 0.465169 Classification-F1 0.4222222222222222 on epoch=49
06/01/2022 10:29:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.389327 on epoch=52
06/01/2022 10:29:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.339098 on epoch=54
06/01/2022 10:29:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.332326 on epoch=57
06/01/2022 10:30:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.322415 on epoch=59
06/01/2022 10:30:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.280144 on epoch=62
06/01/2022 10:30:06 - INFO - __main__ - Global step 250 Train loss 0.332662 Classification-F1 0.7532314923619272 on epoch=62
06/01/2022 10:30:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.241099 on epoch=64
06/01/2022 10:30:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.291340 on epoch=67
06/01/2022 10:30:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.451085 on epoch=69
06/01/2022 10:30:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.385833 on epoch=72
06/01/2022 10:30:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.296961 on epoch=74
06/01/2022 10:30:33 - INFO - __main__ - Global step 300 Train loss 0.333264 Classification-F1 0.38613111026904134 on epoch=74
06/01/2022 10:30:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.249428 on epoch=77
06/01/2022 10:30:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.217736 on epoch=79
06/01/2022 10:30:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.165465 on epoch=82
06/01/2022 10:30:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.210487 on epoch=84
06/01/2022 10:31:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.212509 on epoch=87
06/01/2022 10:31:00 - INFO - __main__ - Global step 350 Train loss 0.211125 Classification-F1 0.6474735605170389 on epoch=87
06/01/2022 10:31:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.202802 on epoch=89
06/01/2022 10:31:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.264527 on epoch=92
06/01/2022 10:31:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.213073 on epoch=94
06/01/2022 10:31:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.109214 on epoch=97
06/01/2022 10:31:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.084783 on epoch=99
06/01/2022 10:31:28 - INFO - __main__ - Global step 400 Train loss 0.174880 Classification-F1 0.7997775305895439 on epoch=99
06/01/2022 10:31:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.103776 on epoch=102
06/01/2022 10:31:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.116675 on epoch=104
06/01/2022 10:31:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.160714 on epoch=107
06/01/2022 10:31:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.072355 on epoch=109
06/01/2022 10:31:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.072045 on epoch=112
06/01/2022 10:31:55 - INFO - __main__ - Global step 450 Train loss 0.105113 Classification-F1 0.765625 on epoch=112
06/01/2022 10:32:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.082404 on epoch=114
06/01/2022 10:32:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.046388 on epoch=117
06/01/2022 10:32:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.053720 on epoch=119
06/01/2022 10:32:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.078697 on epoch=122
06/01/2022 10:32:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.078337 on epoch=124
06/01/2022 10:32:22 - INFO - __main__ - Global step 500 Train loss 0.067909 Classification-F1 0.7482517482517483 on epoch=124
06/01/2022 10:32:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.101024 on epoch=127
06/01/2022 10:32:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.028155 on epoch=129
06/01/2022 10:32:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.055598 on epoch=132
06/01/2022 10:32:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.025339 on epoch=134
06/01/2022 10:32:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.028753 on epoch=137
06/01/2022 10:32:50 - INFO - __main__ - Global step 550 Train loss 0.047774 Classification-F1 0.8076362576508307 on epoch=137
06/01/2022 10:32:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.031775 on epoch=139
06/01/2022 10:33:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.141183 on epoch=142
06/01/2022 10:33:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.095501 on epoch=144
06/01/2022 10:33:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.016651 on epoch=147
06/01/2022 10:33:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.012724 on epoch=149
06/01/2022 10:33:17 - INFO - __main__ - Global step 600 Train loss 0.059567 Classification-F1 0.8653198653198653 on epoch=149
06/01/2022 10:33:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.022632 on epoch=152
06/01/2022 10:33:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.009029 on epoch=154
06/01/2022 10:33:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.012227 on epoch=157
06/01/2022 10:33:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.073443 on epoch=159
06/01/2022 10:33:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.071644 on epoch=162
06/01/2022 10:33:45 - INFO - __main__ - Global step 650 Train loss 0.037795 Classification-F1 0.8076362576508307 on epoch=162
06/01/2022 10:33:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.048308 on epoch=164
06/01/2022 10:33:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.035400 on epoch=167
06/01/2022 10:34:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.015468 on epoch=169
06/01/2022 10:34:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.042526 on epoch=172
06/01/2022 10:34:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.008449 on epoch=174
06/01/2022 10:34:12 - INFO - __main__ - Global step 700 Train loss 0.030030 Classification-F1 0.8263888888888891 on epoch=174
06/01/2022 10:34:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.013843 on epoch=177
06/01/2022 10:34:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.008763 on epoch=179
06/01/2022 10:34:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.002094 on epoch=182
06/01/2022 10:34:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.037795 on epoch=184
06/01/2022 10:34:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.009176 on epoch=187
06/01/2022 10:34:40 - INFO - __main__ - Global step 750 Train loss 0.014334 Classification-F1 0.8447829836159816 on epoch=187
06/01/2022 10:34:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.016872 on epoch=189
06/01/2022 10:34:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.014522 on epoch=192
06/01/2022 10:34:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.056550 on epoch=194
06/01/2022 10:35:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.008966 on epoch=197
06/01/2022 10:35:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002577 on epoch=199
06/01/2022 10:35:07 - INFO - __main__ - Global step 800 Train loss 0.019897 Classification-F1 0.8316498316498316 on epoch=199
06/01/2022 10:35:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.032812 on epoch=202
06/01/2022 10:35:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001117 on epoch=204
06/01/2022 10:35:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.002925 on epoch=207
06/01/2022 10:35:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.013648 on epoch=209
06/01/2022 10:35:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.008522 on epoch=212
06/01/2022 10:35:34 - INFO - __main__ - Global step 850 Train loss 0.011805 Classification-F1 0.8479301605181638 on epoch=212
06/01/2022 10:35:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.003193 on epoch=214
06/01/2022 10:35:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.001113 on epoch=217
06/01/2022 10:35:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001252 on epoch=219
06/01/2022 10:35:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001337 on epoch=222
06/01/2022 10:36:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000987 on epoch=224
06/01/2022 10:36:01 - INFO - __main__ - Global step 900 Train loss 0.001576 Classification-F1 0.8263888888888891 on epoch=224
06/01/2022 10:36:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000743 on epoch=227
06/01/2022 10:36:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.002503 on epoch=229
06/01/2022 10:36:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.002051 on epoch=232
06/01/2022 10:36:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.003473 on epoch=234
06/01/2022 10:36:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001112 on epoch=237
06/01/2022 10:36:28 - INFO - __main__ - Global step 950 Train loss 0.001976 Classification-F1 0.849624060150376 on epoch=237
06/01/2022 10:36:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.005743 on epoch=239
06/01/2022 10:36:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.012685 on epoch=242
06/01/2022 10:36:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000368 on epoch=244
06/01/2022 10:36:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000386 on epoch=247
06/01/2022 10:36:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.007187 on epoch=249
06/01/2022 10:36:55 - INFO - __main__ - Global step 1000 Train loss 0.005274 Classification-F1 0.8653198653198653 on epoch=249
06/01/2022 10:36:55 - INFO - __main__ - save last model!
06/01/2022 10:36:56 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 10:36:56 - INFO - __main__ - Printing 3 examples
06/01/2022 10:36:56 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/01/2022 10:36:56 - INFO - __main__ - ['false']
06/01/2022 10:36:56 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/01/2022 10:36:56 - INFO - __main__ - ['false']
06/01/2022 10:36:56 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
06/01/2022 10:36:56 - INFO - __main__ - ['false']
06/01/2022 10:36:56 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:36:56 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:36:56 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 10:36:56 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 10:36:56 - INFO - __main__ - Printing 3 examples
06/01/2022 10:36:56 - INFO - __main__ -  [ethos-race] or when the man walked up to the little girl with the gay pride flag i wanted to jump into the video and punch him but ya know thats assault
06/01/2022 10:36:56 - INFO - __main__ - ['false']
06/01/2022 10:36:56 - INFO - __main__ -  [ethos-race] Being a lesbian is fine if you really love being an animal
06/01/2022 10:36:56 - INFO - __main__ - ['false']
06/01/2022 10:36:56 - INFO - __main__ -  [ethos-race] this was shit the guy should of boxed his head off and thrown him through the top window,fuck peoado muhamed and islam
06/01/2022 10:36:56 - INFO - __main__ - ['false']
06/01/2022 10:36:56 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:36:56 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:36:56 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 10:37:02 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 10:37:03 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 10:37:03 - INFO - __main__ - Printing 3 examples
06/01/2022 10:37:03 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 10:37:03 - INFO - __main__ - ['true']
06/01/2022 10:37:03 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 10:37:03 - INFO - __main__ - ['false']
06/01/2022 10:37:03 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 10:37:03 - INFO - __main__ - ['false']
06/01/2022 10:37:03 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:37:03 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:37:03 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 10:37:04 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_100_0.0003_8_predictions.txt
06/01/2022 10:37:04 - INFO - __main__ - Classification-F1 on test data: 0.8317
06/01/2022 10:37:05 - INFO - __main__ - prefix=ethos-race_32_100, lr=0.0003, bsz=8, dev_performance=0.8653198653198653, test_performance=0.831656346749226
06/01/2022 10:37:05 - INFO - __main__ - Running ... prefix=ethos-race_32_100, lr=0.0002, bsz=8 ...
06/01/2022 10:37:06 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 10:37:06 - INFO - __main__ - Printing 3 examples
06/01/2022 10:37:06 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/01/2022 10:37:06 - INFO - __main__ - ['false']
06/01/2022 10:37:06 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/01/2022 10:37:06 - INFO - __main__ - ['false']
06/01/2022 10:37:06 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
06/01/2022 10:37:06 - INFO - __main__ - ['false']
06/01/2022 10:37:06 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:37:06 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:37:06 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 10:37:06 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 10:37:06 - INFO - __main__ - Printing 3 examples
06/01/2022 10:37:06 - INFO - __main__ -  [ethos-race] or when the man walked up to the little girl with the gay pride flag i wanted to jump into the video and punch him but ya know thats assault
06/01/2022 10:37:06 - INFO - __main__ - ['false']
06/01/2022 10:37:06 - INFO - __main__ -  [ethos-race] Being a lesbian is fine if you really love being an animal
06/01/2022 10:37:06 - INFO - __main__ - ['false']
06/01/2022 10:37:06 - INFO - __main__ -  [ethos-race] this was shit the guy should of boxed his head off and thrown him through the top window,fuck peoado muhamed and islam
06/01/2022 10:37:06 - INFO - __main__ - ['false']
06/01/2022 10:37:06 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:37:06 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:37:06 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 10:37:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 10:37:09 - INFO - __main__ - Starting training!
06/01/2022 10:37:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 10:37:19 - INFO - __main__ - Starting training!
06/01/2022 10:37:24 - INFO - __main__ - Step 10 Global step 10 Train loss 24.412235 on epoch=2
06/01/2022 10:37:29 - INFO - __main__ - Step 20 Global step 20 Train loss 19.897940 on epoch=4
06/01/2022 10:37:34 - INFO - __main__ - Step 30 Global step 30 Train loss 19.061525 on epoch=7
06/01/2022 10:37:39 - INFO - __main__ - Step 40 Global step 40 Train loss 18.136311 on epoch=9
06/01/2022 10:37:45 - INFO - __main__ - Step 50 Global step 50 Train loss 16.569355 on epoch=12
06/01/2022 10:37:46 - INFO - __main__ - Global step 50 Train loss 19.615473 Classification-F1 0.0 on epoch=12
06/01/2022 10:37:52 - INFO - __main__ - Step 60 Global step 60 Train loss 16.247446 on epoch=14
06/01/2022 10:37:58 - INFO - __main__ - Step 70 Global step 70 Train loss 15.861429 on epoch=17
06/01/2022 10:38:03 - INFO - __main__ - Step 80 Global step 80 Train loss 14.054158 on epoch=19
06/01/2022 10:38:08 - INFO - __main__ - Step 90 Global step 90 Train loss 13.826365 on epoch=22
06/01/2022 10:38:14 - INFO - __main__ - Step 100 Global step 100 Train loss 13.123055 on epoch=24
06/01/2022 10:38:14 - INFO - __main__ - Global step 100 Train loss 14.622492 Classification-F1 0.0 on epoch=24
06/01/2022 10:38:20 - INFO - __main__ - Step 110 Global step 110 Train loss 12.223169 on epoch=27
06/01/2022 10:38:25 - INFO - __main__ - Step 120 Global step 120 Train loss 11.456549 on epoch=29
06/01/2022 10:38:30 - INFO - __main__ - Step 130 Global step 130 Train loss 8.287184 on epoch=32
06/01/2022 10:38:36 - INFO - __main__ - Step 140 Global step 140 Train loss 4.902664 on epoch=34
06/01/2022 10:38:41 - INFO - __main__ - Step 150 Global step 150 Train loss 1.943935 on epoch=37
06/01/2022 10:38:41 - INFO - __main__ - Global step 150 Train loss 7.762700 Classification-F1 0.3344537815126051 on epoch=37
06/01/2022 10:38:48 - INFO - __main__ - Step 160 Global step 160 Train loss 1.171973 on epoch=39
06/01/2022 10:38:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.416195 on epoch=42
06/01/2022 10:38:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.673487 on epoch=44
06/01/2022 10:39:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.437179 on epoch=47
06/01/2022 10:39:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.379252 on epoch=49
06/01/2022 10:39:10 - INFO - __main__ - Global step 200 Train loss 0.615617 Classification-F1 0.7222222222222221 on epoch=49
06/01/2022 10:39:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.357965 on epoch=52
06/01/2022 10:39:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.364164 on epoch=54
06/01/2022 10:39:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.305205 on epoch=57
06/01/2022 10:39:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.207671 on epoch=59
06/01/2022 10:39:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.199883 on epoch=62
06/01/2022 10:39:38 - INFO - __main__ - Global step 250 Train loss 0.286978 Classification-F1 0.8124467178175618 on epoch=62
06/01/2022 10:39:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.145435 on epoch=64
06/01/2022 10:39:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.158945 on epoch=67
06/01/2022 10:39:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.188740 on epoch=69
06/01/2022 10:40:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.155299 on epoch=72
06/01/2022 10:40:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.115163 on epoch=74
06/01/2022 10:40:06 - INFO - __main__ - Global step 300 Train loss 0.152717 Classification-F1 0.8447829836159816 on epoch=74
06/01/2022 10:40:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.136705 on epoch=77
06/01/2022 10:40:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.160844 on epoch=79
06/01/2022 10:40:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.222102 on epoch=82
06/01/2022 10:40:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.117766 on epoch=84
06/01/2022 10:40:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.193531 on epoch=87
06/01/2022 10:40:34 - INFO - __main__ - Global step 350 Train loss 0.166190 Classification-F1 0.7997775305895439 on epoch=87
06/01/2022 10:40:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.120310 on epoch=89
06/01/2022 10:40:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.056220 on epoch=92
06/01/2022 10:40:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.136132 on epoch=94
06/01/2022 10:40:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.118008 on epoch=97
06/01/2022 10:41:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.131944 on epoch=99
06/01/2022 10:41:02 - INFO - __main__ - Global step 400 Train loss 0.112523 Classification-F1 0.8830409356725146 on epoch=99
06/01/2022 10:41:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.093185 on epoch=102
06/01/2022 10:41:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.131534 on epoch=104
06/01/2022 10:41:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.049659 on epoch=107
06/01/2022 10:41:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.112066 on epoch=109
06/01/2022 10:41:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.131430 on epoch=112
06/01/2022 10:41:30 - INFO - __main__ - Global step 450 Train loss 0.103575 Classification-F1 0.8499583217560432 on epoch=112
06/01/2022 10:41:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.125470 on epoch=114
06/01/2022 10:41:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.096722 on epoch=117
06/01/2022 10:41:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.300938 on epoch=119
06/01/2022 10:41:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.093401 on epoch=122
06/01/2022 10:41:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.100407 on epoch=124
06/01/2022 10:41:57 - INFO - __main__ - Global step 500 Train loss 0.143387 Classification-F1 0.898989898989899 on epoch=124
06/01/2022 10:42:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.111865 on epoch=127
06/01/2022 10:42:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.103536 on epoch=129
06/01/2022 10:42:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.134682 on epoch=132
06/01/2022 10:42:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.071536 on epoch=134
06/01/2022 10:42:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.104463 on epoch=137
06/01/2022 10:42:25 - INFO - __main__ - Global step 550 Train loss 0.105217 Classification-F1 0.8141368628555337 on epoch=137
06/01/2022 10:42:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.111770 on epoch=139
06/01/2022 10:42:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.046508 on epoch=142
06/01/2022 10:42:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.153948 on epoch=144
06/01/2022 10:42:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.040298 on epoch=147
06/01/2022 10:42:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.078832 on epoch=149
06/01/2022 10:42:53 - INFO - __main__ - Global step 600 Train loss 0.086271 Classification-F1 0.8447829836159816 on epoch=149
06/01/2022 10:42:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.018102 on epoch=152
06/01/2022 10:43:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.032488 on epoch=154
06/01/2022 10:43:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.036354 on epoch=157
06/01/2022 10:43:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.044963 on epoch=159
06/01/2022 10:43:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.034783 on epoch=162
06/01/2022 10:43:21 - INFO - __main__ - Global step 650 Train loss 0.033338 Classification-F1 0.7997775305895439 on epoch=162
06/01/2022 10:43:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.041605 on epoch=164
06/01/2022 10:43:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.059308 on epoch=167
06/01/2022 10:43:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.015250 on epoch=169
06/01/2022 10:43:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.023476 on epoch=172
06/01/2022 10:43:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.043493 on epoch=174
06/01/2022 10:43:48 - INFO - __main__ - Global step 700 Train loss 0.036626 Classification-F1 0.8316498316498316 on epoch=174
06/01/2022 10:43:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.015882 on epoch=177
06/01/2022 10:43:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.014603 on epoch=179
06/01/2022 10:44:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.012619 on epoch=182
06/01/2022 10:44:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.012560 on epoch=184
06/01/2022 10:44:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.012614 on epoch=187
06/01/2022 10:44:16 - INFO - __main__ - Global step 750 Train loss 0.013656 Classification-F1 0.8479301605181638 on epoch=187
06/01/2022 10:44:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.004462 on epoch=189
06/01/2022 10:44:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.010085 on epoch=192
06/01/2022 10:44:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.026982 on epoch=194
06/01/2022 10:44:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002914 on epoch=197
06/01/2022 10:44:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.010305 on epoch=199
06/01/2022 10:44:43 - INFO - __main__ - Global step 800 Train loss 0.010950 Classification-F1 0.8642533936651584 on epoch=199
06/01/2022 10:44:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.007881 on epoch=202
06/01/2022 10:44:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.002064 on epoch=204
06/01/2022 10:45:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001577 on epoch=207
06/01/2022 10:45:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.004672 on epoch=209
06/01/2022 10:45:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001738 on epoch=212
06/01/2022 10:45:11 - INFO - __main__ - Global step 850 Train loss 0.003586 Classification-F1 0.848951048951049 on epoch=212
06/01/2022 10:45:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.009824 on epoch=214
06/01/2022 10:45:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.005178 on epoch=217
06/01/2022 10:45:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000662 on epoch=219
06/01/2022 10:45:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.037242 on epoch=222
06/01/2022 10:45:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.015355 on epoch=224
06/01/2022 10:45:39 - INFO - __main__ - Global step 900 Train loss 0.013652 Classification-F1 0.8285714285714286 on epoch=224
06/01/2022 10:45:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.005905 on epoch=227
06/01/2022 10:45:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.015539 on epoch=229
06/01/2022 10:45:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.003574 on epoch=232
06/01/2022 10:46:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000744 on epoch=234
06/01/2022 10:46:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001625 on epoch=237
06/01/2022 10:46:06 - INFO - __main__ - Global step 950 Train loss 0.005477 Classification-F1 0.846547314578005 on epoch=237
06/01/2022 10:46:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001710 on epoch=239
06/01/2022 10:46:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.014665 on epoch=242
06/01/2022 10:46:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.049244 on epoch=244
06/01/2022 10:46:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.009530 on epoch=247
06/01/2022 10:46:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.006808 on epoch=249
06/01/2022 10:46:34 - INFO - __main__ - Global step 1000 Train loss 0.016392 Classification-F1 0.8316498316498316 on epoch=249
06/01/2022 10:46:34 - INFO - __main__ - save last model!
06/01/2022 10:46:34 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 10:46:34 - INFO - __main__ - Printing 3 examples
06/01/2022 10:46:34 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/01/2022 10:46:34 - INFO - __main__ - ['false']
06/01/2022 10:46:34 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/01/2022 10:46:34 - INFO - __main__ - ['false']
06/01/2022 10:46:34 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
06/01/2022 10:46:34 - INFO - __main__ - ['false']
06/01/2022 10:46:34 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:46:34 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:46:34 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 10:46:34 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 10:46:34 - INFO - __main__ - Printing 3 examples
06/01/2022 10:46:34 - INFO - __main__ -  [ethos-race] or when the man walked up to the little girl with the gay pride flag i wanted to jump into the video and punch him but ya know thats assault
06/01/2022 10:46:34 - INFO - __main__ - ['false']
06/01/2022 10:46:34 - INFO - __main__ -  [ethos-race] Being a lesbian is fine if you really love being an animal
06/01/2022 10:46:34 - INFO - __main__ - ['false']
06/01/2022 10:46:34 - INFO - __main__ -  [ethos-race] this was shit the guy should of boxed his head off and thrown him through the top window,fuck peoado muhamed and islam
06/01/2022 10:46:34 - INFO - __main__ - ['false']
06/01/2022 10:46:34 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:46:34 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:46:34 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 10:46:41 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 10:46:41 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 10:46:41 - INFO - __main__ - Printing 3 examples
06/01/2022 10:46:41 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 10:46:41 - INFO - __main__ - ['true']
06/01/2022 10:46:41 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 10:46:41 - INFO - __main__ - ['false']
06/01/2022 10:46:41 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 10:46:41 - INFO - __main__ - ['false']
06/01/2022 10:46:41 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:46:41 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:46:41 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 10:46:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_100_0.0002_8_predictions.txt
06/01/2022 10:46:43 - INFO - __main__ - Classification-F1 on test data: 0.7752
06/01/2022 10:46:43 - INFO - __main__ - prefix=ethos-race_32_100, lr=0.0002, bsz=8, dev_performance=0.898989898989899, test_performance=0.7751937984496124
06/01/2022 10:46:43 - INFO - __main__ - Running ... prefix=ethos-race_32_100, lr=0.0001, bsz=8 ...
06/01/2022 10:46:44 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 10:46:44 - INFO - __main__ - Printing 3 examples
06/01/2022 10:46:44 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/01/2022 10:46:44 - INFO - __main__ - ['false']
06/01/2022 10:46:44 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/01/2022 10:46:44 - INFO - __main__ - ['false']
06/01/2022 10:46:44 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
06/01/2022 10:46:44 - INFO - __main__ - ['false']
06/01/2022 10:46:44 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:46:44 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:46:44 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 10:46:44 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 10:46:44 - INFO - __main__ - Printing 3 examples
06/01/2022 10:46:44 - INFO - __main__ -  [ethos-race] or when the man walked up to the little girl with the gay pride flag i wanted to jump into the video and punch him but ya know thats assault
06/01/2022 10:46:44 - INFO - __main__ - ['false']
06/01/2022 10:46:44 - INFO - __main__ -  [ethos-race] Being a lesbian is fine if you really love being an animal
06/01/2022 10:46:44 - INFO - __main__ - ['false']
06/01/2022 10:46:44 - INFO - __main__ -  [ethos-race] this was shit the guy should of boxed his head off and thrown him through the top window,fuck peoado muhamed and islam
06/01/2022 10:46:44 - INFO - __main__ - ['false']
06/01/2022 10:46:44 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:46:44 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:46:44 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 10:46:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 10:46:45 - INFO - __main__ - Starting training!
06/01/2022 10:46:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 10:46:57 - INFO - __main__ - Starting training!
06/01/2022 10:47:02 - INFO - __main__ - Step 10 Global step 10 Train loss 25.207380 on epoch=2
06/01/2022 10:47:07 - INFO - __main__ - Step 20 Global step 20 Train loss 21.799255 on epoch=4
06/01/2022 10:47:12 - INFO - __main__ - Step 30 Global step 30 Train loss 19.550198 on epoch=7
06/01/2022 10:47:18 - INFO - __main__ - Step 40 Global step 40 Train loss 19.106110 on epoch=9
06/01/2022 10:47:23 - INFO - __main__ - Step 50 Global step 50 Train loss 17.980131 on epoch=12
06/01/2022 10:47:43 - INFO - __main__ - Global step 50 Train loss 20.728613 Classification-F1 0.0 on epoch=12
06/01/2022 10:47:49 - INFO - __main__ - Step 60 Global step 60 Train loss 17.957043 on epoch=14
06/01/2022 10:47:54 - INFO - __main__ - Step 70 Global step 70 Train loss 16.863590 on epoch=17
06/01/2022 10:47:59 - INFO - __main__ - Step 80 Global step 80 Train loss 17.335241 on epoch=19
06/01/2022 10:48:05 - INFO - __main__ - Step 90 Global step 90 Train loss 15.860306 on epoch=22
06/01/2022 10:48:10 - INFO - __main__ - Step 100 Global step 100 Train loss 15.441675 on epoch=24
06/01/2022 10:48:15 - INFO - __main__ - Global step 100 Train loss 16.691570 Classification-F1 0.0 on epoch=24
06/01/2022 10:48:21 - INFO - __main__ - Step 110 Global step 110 Train loss 16.118876 on epoch=27
06/01/2022 10:48:26 - INFO - __main__ - Step 120 Global step 120 Train loss 15.370773 on epoch=29
06/01/2022 10:48:31 - INFO - __main__ - Step 130 Global step 130 Train loss 15.231822 on epoch=32
06/01/2022 10:48:36 - INFO - __main__ - Step 140 Global step 140 Train loss 15.259338 on epoch=34
06/01/2022 10:48:42 - INFO - __main__ - Step 150 Global step 150 Train loss 14.413013 on epoch=37
06/01/2022 10:48:46 - INFO - __main__ - Global step 150 Train loss 15.278765 Classification-F1 0.0 on epoch=37
06/01/2022 10:48:52 - INFO - __main__ - Step 160 Global step 160 Train loss 14.868428 on epoch=39
06/01/2022 10:48:57 - INFO - __main__ - Step 170 Global step 170 Train loss 13.741155 on epoch=42
06/01/2022 10:49:02 - INFO - __main__ - Step 180 Global step 180 Train loss 13.570767 on epoch=44
06/01/2022 10:49:07 - INFO - __main__ - Step 190 Global step 190 Train loss 12.515841 on epoch=47
06/01/2022 10:49:13 - INFO - __main__ - Step 200 Global step 200 Train loss 11.919341 on epoch=49
06/01/2022 10:49:17 - INFO - __main__ - Global step 200 Train loss 13.323106 Classification-F1 0.0 on epoch=49
06/01/2022 10:49:22 - INFO - __main__ - Step 210 Global step 210 Train loss 12.069407 on epoch=52
06/01/2022 10:49:27 - INFO - __main__ - Step 220 Global step 220 Train loss 10.753763 on epoch=54
06/01/2022 10:49:33 - INFO - __main__ - Step 230 Global step 230 Train loss 9.302416 on epoch=57
06/01/2022 10:49:38 - INFO - __main__ - Step 240 Global step 240 Train loss 7.255882 on epoch=59
06/01/2022 10:49:43 - INFO - __main__ - Step 250 Global step 250 Train loss 4.978801 on epoch=62
06/01/2022 10:49:44 - INFO - __main__ - Global step 250 Train loss 8.872054 Classification-F1 0.23443223443223446 on epoch=62
06/01/2022 10:49:50 - INFO - __main__ - Step 260 Global step 260 Train loss 5.448642 on epoch=64
06/01/2022 10:49:55 - INFO - __main__ - Step 270 Global step 270 Train loss 1.989949 on epoch=67
06/01/2022 10:50:00 - INFO - __main__ - Step 280 Global step 280 Train loss 1.101583 on epoch=69
06/01/2022 10:50:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.677013 on epoch=72
06/01/2022 10:50:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.887874 on epoch=74
06/01/2022 10:50:12 - INFO - __main__ - Global step 300 Train loss 2.021012 Classification-F1 0.44919278252611583 on epoch=74
06/01/2022 10:50:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.722144 on epoch=77
06/01/2022 10:50:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.432135 on epoch=79
06/01/2022 10:50:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.438184 on epoch=82
06/01/2022 10:50:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.453017 on epoch=84
06/01/2022 10:50:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.488754 on epoch=87
06/01/2022 10:50:39 - INFO - __main__ - Global step 350 Train loss 0.506847 Classification-F1 0.7321428571428572 on epoch=87
06/01/2022 10:50:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.413130 on epoch=89
06/01/2022 10:50:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.462617 on epoch=92
06/01/2022 10:50:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.299160 on epoch=94
06/01/2022 10:51:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.270109 on epoch=97
06/01/2022 10:51:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.257616 on epoch=99
06/01/2022 10:51:07 - INFO - __main__ - Global step 400 Train loss 0.340526 Classification-F1 0.6760443307757886 on epoch=99
06/01/2022 10:51:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.277939 on epoch=102
06/01/2022 10:51:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.255651 on epoch=104
06/01/2022 10:51:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.263545 on epoch=107
06/01/2022 10:51:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.203939 on epoch=109
06/01/2022 10:51:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.226143 on epoch=112
06/01/2022 10:51:33 - INFO - __main__ - Global step 450 Train loss 0.245443 Classification-F1 0.7499305362600723 on epoch=112
06/01/2022 10:51:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.193204 on epoch=114
06/01/2022 10:51:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.231657 on epoch=117
06/01/2022 10:51:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.183709 on epoch=119
06/01/2022 10:51:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.190167 on epoch=122
06/01/2022 10:52:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.173045 on epoch=124
06/01/2022 10:52:01 - INFO - __main__ - Global step 500 Train loss 0.194356 Classification-F1 0.7 on epoch=124
06/01/2022 10:52:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.156942 on epoch=127
06/01/2022 10:52:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.153469 on epoch=129
06/01/2022 10:52:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.139066 on epoch=132
06/01/2022 10:52:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.092702 on epoch=134
06/01/2022 10:52:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.098867 on epoch=137
06/01/2022 10:52:28 - INFO - __main__ - Global step 550 Train loss 0.128209 Classification-F1 0.7321428571428572 on epoch=137
06/01/2022 10:52:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.077946 on epoch=139
06/01/2022 10:52:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.148650 on epoch=142
06/01/2022 10:52:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.104666 on epoch=144
06/01/2022 10:52:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.087931 on epoch=147
06/01/2022 10:52:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.096619 on epoch=149
06/01/2022 10:52:55 - INFO - __main__ - Global step 600 Train loss 0.103162 Classification-F1 0.765625 on epoch=149
06/01/2022 10:53:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.071164 on epoch=152
06/01/2022 10:53:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.068380 on epoch=154
06/01/2022 10:53:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.078933 on epoch=157
06/01/2022 10:53:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.070007 on epoch=159
06/01/2022 10:53:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.243053 on epoch=162
06/01/2022 10:53:22 - INFO - __main__ - Global step 650 Train loss 0.106307 Classification-F1 0.765625 on epoch=162
06/01/2022 10:53:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.106536 on epoch=164
06/01/2022 10:53:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.059434 on epoch=167
06/01/2022 10:53:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.059603 on epoch=169
06/01/2022 10:53:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.034930 on epoch=172
06/01/2022 10:53:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.047842 on epoch=174
06/01/2022 10:53:49 - INFO - __main__ - Global step 700 Train loss 0.061669 Classification-F1 0.8285714285714286 on epoch=174
06/01/2022 10:53:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.032214 on epoch=177
06/01/2022 10:54:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.022737 on epoch=179
06/01/2022 10:54:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.028493 on epoch=182
06/01/2022 10:54:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.032682 on epoch=184
06/01/2022 10:54:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.022246 on epoch=187
06/01/2022 10:54:17 - INFO - __main__ - Global step 750 Train loss 0.027674 Classification-F1 0.8285714285714286 on epoch=187
06/01/2022 10:54:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.034113 on epoch=189
06/01/2022 10:54:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.016732 on epoch=192
06/01/2022 10:54:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.037811 on epoch=194
06/01/2022 10:54:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.026174 on epoch=197
06/01/2022 10:54:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.039895 on epoch=199
06/01/2022 10:54:44 - INFO - __main__ - Global step 800 Train loss 0.030945 Classification-F1 0.846547314578005 on epoch=199
06/01/2022 10:54:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.101344 on epoch=202
06/01/2022 10:54:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.026823 on epoch=204
06/01/2022 10:55:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.027244 on epoch=207
06/01/2022 10:55:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.010210 on epoch=209
06/01/2022 10:55:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.003528 on epoch=212
06/01/2022 10:55:11 - INFO - __main__ - Global step 850 Train loss 0.033830 Classification-F1 0.8642533936651584 on epoch=212
06/01/2022 10:55:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.013149 on epoch=214
06/01/2022 10:55:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.018283 on epoch=217
06/01/2022 10:55:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.022461 on epoch=219
06/01/2022 10:55:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.010813 on epoch=222
06/01/2022 10:55:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.019062 on epoch=224
06/01/2022 10:55:39 - INFO - __main__ - Global step 900 Train loss 0.016754 Classification-F1 0.8479301605181638 on epoch=224
06/01/2022 10:55:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.027863 on epoch=227
06/01/2022 10:55:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.021641 on epoch=229
06/01/2022 10:55:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.005622 on epoch=232
06/01/2022 10:56:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.014119 on epoch=234
06/01/2022 10:56:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.021243 on epoch=237
06/01/2022 10:56:06 - INFO - __main__ - Global step 950 Train loss 0.018097 Classification-F1 0.8316498316498316 on epoch=237
06/01/2022 10:56:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.038457 on epoch=239
06/01/2022 10:56:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.014894 on epoch=242
06/01/2022 10:56:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.007564 on epoch=244
06/01/2022 10:56:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.008478 on epoch=247
06/01/2022 10:56:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.008618 on epoch=249
06/01/2022 10:56:33 - INFO - __main__ - Global step 1000 Train loss 0.015602 Classification-F1 0.846547314578005 on epoch=249
06/01/2022 10:56:33 - INFO - __main__ - save last model!
06/01/2022 10:56:34 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 10:56:34 - INFO - __main__ - Printing 3 examples
06/01/2022 10:56:34 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/01/2022 10:56:34 - INFO - __main__ - ['false']
06/01/2022 10:56:34 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
06/01/2022 10:56:34 - INFO - __main__ - ['false']
06/01/2022 10:56:34 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/01/2022 10:56:34 - INFO - __main__ - ['false']
06/01/2022 10:56:34 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:56:34 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:56:34 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 10:56:34 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 10:56:34 - INFO - __main__ - Printing 3 examples
06/01/2022 10:56:34 - INFO - __main__ -  [ethos-race] The men are turtles...idiots even in uk… get lost
06/01/2022 10:56:34 - INFO - __main__ - ['false']
06/01/2022 10:56:34 - INFO - __main__ -  [ethos-race] Christians can get crazy religious and I get so f u c k i n g annoyed
06/01/2022 10:56:34 - INFO - __main__ - ['false']
06/01/2022 10:56:34 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
06/01/2022 10:56:34 - INFO - __main__ - ['false']
06/01/2022 10:56:34 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:56:34 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:56:34 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 10:56:41 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 10:56:42 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 10:56:42 - INFO - __main__ - Printing 3 examples
06/01/2022 10:56:42 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 10:56:42 - INFO - __main__ - ['true']
06/01/2022 10:56:42 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 10:56:42 - INFO - __main__ - ['false']
06/01/2022 10:56:42 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 10:56:42 - INFO - __main__ - ['false']
06/01/2022 10:56:42 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:56:42 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:56:42 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 10:56:43 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_100_0.0001_8_predictions.txt
06/01/2022 10:56:43 - INFO - __main__ - Classification-F1 on test data: 0.7992
06/01/2022 10:56:44 - INFO - __main__ - prefix=ethos-race_32_100, lr=0.0001, bsz=8, dev_performance=0.8642533936651584, test_performance=0.7992188886916386
06/01/2022 10:56:44 - INFO - __main__ - Running ... prefix=ethos-race_32_13, lr=0.0005, bsz=8 ...
06/01/2022 10:56:45 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 10:56:45 - INFO - __main__ - Printing 3 examples
06/01/2022 10:56:45 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/01/2022 10:56:45 - INFO - __main__ - ['false']
06/01/2022 10:56:45 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
06/01/2022 10:56:45 - INFO - __main__ - ['false']
06/01/2022 10:56:45 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/01/2022 10:56:45 - INFO - __main__ - ['false']
06/01/2022 10:56:45 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:56:45 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:56:45 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 10:56:45 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 10:56:45 - INFO - __main__ - Printing 3 examples
06/01/2022 10:56:45 - INFO - __main__ -  [ethos-race] The men are turtles...idiots even in uk… get lost
06/01/2022 10:56:45 - INFO - __main__ - ['false']
06/01/2022 10:56:45 - INFO - __main__ -  [ethos-race] Christians can get crazy religious and I get so f u c k i n g annoyed
06/01/2022 10:56:45 - INFO - __main__ - ['false']
06/01/2022 10:56:45 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
06/01/2022 10:56:45 - INFO - __main__ - ['false']
06/01/2022 10:56:45 - INFO - __main__ - Tokenizing Input ...
06/01/2022 10:56:45 - INFO - __main__ - Tokenizing Output ...
06/01/2022 10:56:45 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 10:56:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 10:56:47 - INFO - __main__ - Starting training!
06/01/2022 10:56:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 10:56:56 - INFO - __main__ - Starting training!
06/01/2022 10:57:00 - INFO - __main__ - Step 10 Global step 10 Train loss 23.633432 on epoch=2
06/01/2022 10:57:06 - INFO - __main__ - Step 20 Global step 20 Train loss 18.256514 on epoch=4
06/01/2022 10:57:11 - INFO - __main__ - Step 30 Global step 30 Train loss 15.504644 on epoch=7
06/01/2022 10:57:16 - INFO - __main__ - Step 40 Global step 40 Train loss 13.690855 on epoch=9
06/01/2022 10:57:21 - INFO - __main__ - Step 50 Global step 50 Train loss 9.520655 on epoch=12
06/01/2022 10:57:22 - INFO - __main__ - Global step 50 Train loss 16.121222 Classification-F1 0.12295081967213115 on epoch=12
06/01/2022 10:57:27 - INFO - __main__ - Step 60 Global step 60 Train loss 4.419836 on epoch=14
06/01/2022 10:57:33 - INFO - __main__ - Step 70 Global step 70 Train loss 2.563896 on epoch=17
06/01/2022 10:57:38 - INFO - __main__ - Step 80 Global step 80 Train loss 1.444621 on epoch=19
06/01/2022 10:57:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.689491 on epoch=22
06/01/2022 10:57:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.417563 on epoch=24
06/01/2022 10:57:49 - INFO - __main__ - Global step 100 Train loss 1.907081 Classification-F1 0.45632475534613987 on epoch=24
06/01/2022 10:57:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.561429 on epoch=27
06/01/2022 10:58:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.423831 on epoch=29
06/01/2022 10:58:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.527673 on epoch=32
06/01/2022 10:58:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.394632 on epoch=34
06/01/2022 10:58:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.507422 on epoch=37
06/01/2022 10:58:16 - INFO - __main__ - Global step 150 Train loss 0.482997 Classification-F1 0.37777777777777777 on epoch=37
06/01/2022 10:58:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.537342 on epoch=39
06/01/2022 10:58:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.396114 on epoch=42
06/01/2022 10:58:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.400520 on epoch=44
06/01/2022 10:58:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.378273 on epoch=47
06/01/2022 10:58:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.396819 on epoch=49
06/01/2022 10:58:43 - INFO - __main__ - Global step 200 Train loss 0.421814 Classification-F1 0.49760765550239233 on epoch=49
06/01/2022 10:58:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.337451 on epoch=52
06/01/2022 10:58:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.344174 on epoch=54
06/01/2022 10:59:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.325834 on epoch=57
06/01/2022 10:59:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.349054 on epoch=59
06/01/2022 10:59:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.411136 on epoch=62
06/01/2022 10:59:11 - INFO - __main__ - Global step 250 Train loss 0.353530 Classification-F1 0.7827903091060985 on epoch=62
06/01/2022 10:59:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.284299 on epoch=64
06/01/2022 10:59:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.275275 on epoch=67
06/01/2022 10:59:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.307394 on epoch=69
06/01/2022 10:59:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.265522 on epoch=72
06/01/2022 10:59:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.329281 on epoch=74
06/01/2022 10:59:39 - INFO - __main__ - Global step 300 Train loss 0.292354 Classification-F1 0.8447829836159816 on epoch=74
06/01/2022 10:59:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.192690 on epoch=77
06/01/2022 10:59:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.160561 on epoch=79
06/01/2022 10:59:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.186033 on epoch=82
06/01/2022 11:00:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.218463 on epoch=84
06/01/2022 11:00:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.172449 on epoch=87
06/01/2022 11:00:07 - INFO - __main__ - Global step 350 Train loss 0.186039 Classification-F1 0.797979797979798 on epoch=87
06/01/2022 11:00:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.197714 on epoch=89
06/01/2022 11:00:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.206304 on epoch=92
06/01/2022 11:00:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.136827 on epoch=94
06/01/2022 11:00:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.144208 on epoch=97
06/01/2022 11:00:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.082423 on epoch=99
06/01/2022 11:00:34 - INFO - __main__ - Global step 400 Train loss 0.153495 Classification-F1 0.8333333333333334 on epoch=99
06/01/2022 11:00:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.087619 on epoch=102
06/01/2022 11:00:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.088669 on epoch=104
06/01/2022 11:00:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.140523 on epoch=107
06/01/2022 11:00:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.166644 on epoch=109
06/01/2022 11:01:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.126632 on epoch=112
06/01/2022 11:01:02 - INFO - __main__ - Global step 450 Train loss 0.122017 Classification-F1 0.765625 on epoch=112
06/01/2022 11:01:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.124148 on epoch=114
06/01/2022 11:01:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.082931 on epoch=117
06/01/2022 11:01:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.072340 on epoch=119
06/01/2022 11:01:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.047628 on epoch=122
06/01/2022 11:01:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.046501 on epoch=124
06/01/2022 11:01:29 - INFO - __main__ - Global step 500 Train loss 0.074710 Classification-F1 0.7499305362600723 on epoch=124
06/01/2022 11:01:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.071178 on epoch=127
06/01/2022 11:01:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.142368 on epoch=129
06/01/2022 11:01:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.084466 on epoch=132
06/01/2022 11:01:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.128432 on epoch=134
06/01/2022 11:01:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.161049 on epoch=137
06/01/2022 11:01:56 - INFO - __main__ - Global step 550 Train loss 0.117499 Classification-F1 0.8792756539235412 on epoch=137
06/01/2022 11:02:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.187126 on epoch=139
06/01/2022 11:02:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.035654 on epoch=142
06/01/2022 11:02:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.041796 on epoch=144
06/01/2022 11:02:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.027966 on epoch=147
06/01/2022 11:02:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.081053 on epoch=149
06/01/2022 11:02:24 - INFO - __main__ - Global step 600 Train loss 0.074719 Classification-F1 0.7499305362600723 on epoch=149
06/01/2022 11:02:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.090756 on epoch=152
06/01/2022 11:02:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.038107 on epoch=154
06/01/2022 11:02:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.023736 on epoch=157
06/01/2022 11:02:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.034766 on epoch=159
06/01/2022 11:02:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.042325 on epoch=162
06/01/2022 11:02:51 - INFO - __main__ - Global step 650 Train loss 0.045938 Classification-F1 0.8628571428571428 on epoch=162
06/01/2022 11:02:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.031115 on epoch=164
06/01/2022 11:03:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.016887 on epoch=167
06/01/2022 11:03:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.005637 on epoch=169
06/01/2022 11:03:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.036670 on epoch=172
06/01/2022 11:03:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.005189 on epoch=174
06/01/2022 11:03:18 - INFO - __main__ - Global step 700 Train loss 0.019099 Classification-F1 0.8792756539235412 on epoch=174
06/01/2022 11:03:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.009065 on epoch=177
06/01/2022 11:03:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.015461 on epoch=179
06/01/2022 11:03:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.011851 on epoch=182
06/01/2022 11:03:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.008330 on epoch=184
06/01/2022 11:03:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.014136 on epoch=187
06/01/2022 11:03:45 - INFO - __main__ - Global step 750 Train loss 0.011769 Classification-F1 0.8792756539235412 on epoch=187
06/01/2022 11:03:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.003830 on epoch=189
06/01/2022 11:03:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.018500 on epoch=192
06/01/2022 11:04:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.016359 on epoch=194
06/01/2022 11:04:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.101843 on epoch=197
06/01/2022 11:04:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.028269 on epoch=199
06/01/2022 11:04:12 - INFO - __main__ - Global step 800 Train loss 0.033760 Classification-F1 0.8303167420814479 on epoch=199
06/01/2022 11:04:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.007126 on epoch=202
06/01/2022 11:04:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.036408 on epoch=204
06/01/2022 11:04:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.005085 on epoch=207
06/01/2022 11:04:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.012492 on epoch=209
06/01/2022 11:04:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.020475 on epoch=212
06/01/2022 11:04:40 - INFO - __main__ - Global step 850 Train loss 0.016317 Classification-F1 0.8792756539235412 on epoch=212
06/01/2022 11:04:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.016752 on epoch=214
06/01/2022 11:04:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.080243 on epoch=217
06/01/2022 11:04:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.006191 on epoch=219
06/01/2022 11:05:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.099543 on epoch=222
06/01/2022 11:05:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.008258 on epoch=224
06/01/2022 11:05:07 - INFO - __main__ - Global step 900 Train loss 0.042197 Classification-F1 0.8628571428571428 on epoch=224
06/01/2022 11:05:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.051028 on epoch=227
06/01/2022 11:05:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.064035 on epoch=229
06/01/2022 11:05:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.003410 on epoch=232
06/01/2022 11:05:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.007232 on epoch=234
06/01/2022 11:05:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.014328 on epoch=237
06/01/2022 11:05:34 - INFO - __main__ - Global step 950 Train loss 0.028006 Classification-F1 0.765625 on epoch=237
06/01/2022 11:05:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.002554 on epoch=239
06/01/2022 11:05:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001055 on epoch=242
06/01/2022 11:05:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.054232 on epoch=244
06/01/2022 11:05:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.011184 on epoch=247
06/01/2022 11:06:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001200 on epoch=249
06/01/2022 11:06:02 - INFO - __main__ - Global step 1000 Train loss 0.014045 Classification-F1 0.797979797979798 on epoch=249
06/01/2022 11:06:02 - INFO - __main__ - save last model!
06/01/2022 11:06:02 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:06:02 - INFO - __main__ - Printing 3 examples
06/01/2022 11:06:02 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/01/2022 11:06:02 - INFO - __main__ - ['false']
06/01/2022 11:06:02 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
06/01/2022 11:06:02 - INFO - __main__ - ['false']
06/01/2022 11:06:02 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/01/2022 11:06:02 - INFO - __main__ - ['false']
06/01/2022 11:06:02 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:06:02 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:06:02 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:06:02 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:06:02 - INFO - __main__ - Printing 3 examples
06/01/2022 11:06:02 - INFO - __main__ -  [ethos-race] The men are turtles...idiots even in uk… get lost
06/01/2022 11:06:02 - INFO - __main__ - ['false']
06/01/2022 11:06:02 - INFO - __main__ -  [ethos-race] Christians can get crazy religious and I get so f u c k i n g annoyed
06/01/2022 11:06:02 - INFO - __main__ - ['false']
06/01/2022 11:06:02 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
06/01/2022 11:06:02 - INFO - __main__ - ['false']
06/01/2022 11:06:02 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:06:02 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:06:02 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:06:09 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 11:06:10 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 11:06:10 - INFO - __main__ - Printing 3 examples
06/01/2022 11:06:10 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 11:06:10 - INFO - __main__ - ['true']
06/01/2022 11:06:10 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 11:06:10 - INFO - __main__ - ['false']
06/01/2022 11:06:10 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 11:06:10 - INFO - __main__ - ['false']
06/01/2022 11:06:10 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:06:10 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:06:10 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 11:06:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_13_0.0005_8_predictions.txt
06/01/2022 11:06:11 - INFO - __main__ - Classification-F1 on test data: 0.8249
06/01/2022 11:06:12 - INFO - __main__ - prefix=ethos-race_32_13, lr=0.0005, bsz=8, dev_performance=0.8792756539235412, test_performance=0.8248792270531402
06/01/2022 11:06:12 - INFO - __main__ - Running ... prefix=ethos-race_32_13, lr=0.0003, bsz=8 ...
06/01/2022 11:06:12 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:06:12 - INFO - __main__ - Printing 3 examples
06/01/2022 11:06:12 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/01/2022 11:06:12 - INFO - __main__ - ['false']
06/01/2022 11:06:12 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
06/01/2022 11:06:12 - INFO - __main__ - ['false']
06/01/2022 11:06:12 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/01/2022 11:06:12 - INFO - __main__ - ['false']
06/01/2022 11:06:12 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:06:12 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:06:13 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:06:13 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:06:13 - INFO - __main__ - Printing 3 examples
06/01/2022 11:06:13 - INFO - __main__ -  [ethos-race] The men are turtles...idiots even in uk… get lost
06/01/2022 11:06:13 - INFO - __main__ - ['false']
06/01/2022 11:06:13 - INFO - __main__ -  [ethos-race] Christians can get crazy religious and I get so f u c k i n g annoyed
06/01/2022 11:06:13 - INFO - __main__ - ['false']
06/01/2022 11:06:13 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
06/01/2022 11:06:13 - INFO - __main__ - ['false']
06/01/2022 11:06:13 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:06:13 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:06:13 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:06:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:06:14 - INFO - __main__ - Starting training!
06/01/2022 11:06:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:06:25 - INFO - __main__ - Starting training!
06/01/2022 11:06:30 - INFO - __main__ - Step 10 Global step 10 Train loss 24.764156 on epoch=2
06/01/2022 11:06:35 - INFO - __main__ - Step 20 Global step 20 Train loss 20.395576 on epoch=4
06/01/2022 11:06:41 - INFO - __main__ - Step 30 Global step 30 Train loss 17.473501 on epoch=7
06/01/2022 11:06:46 - INFO - __main__ - Step 40 Global step 40 Train loss 17.964218 on epoch=9
06/01/2022 11:06:51 - INFO - __main__ - Step 50 Global step 50 Train loss 15.843475 on epoch=12
06/01/2022 11:06:55 - INFO - __main__ - Global step 50 Train loss 19.288185 Classification-F1 0.0 on epoch=12
06/01/2022 11:07:01 - INFO - __main__ - Step 60 Global step 60 Train loss 14.905943 on epoch=14
06/01/2022 11:07:06 - INFO - __main__ - Step 70 Global step 70 Train loss 13.763151 on epoch=17
06/01/2022 11:07:12 - INFO - __main__ - Step 80 Global step 80 Train loss 11.986775 on epoch=19
06/01/2022 11:07:17 - INFO - __main__ - Step 90 Global step 90 Train loss 11.128449 on epoch=22
06/01/2022 11:07:22 - INFO - __main__ - Step 100 Global step 100 Train loss 8.447653 on epoch=24
06/01/2022 11:07:24 - INFO - __main__ - Global step 100 Train loss 12.046392 Classification-F1 0.0 on epoch=24
06/01/2022 11:07:29 - INFO - __main__ - Step 110 Global step 110 Train loss 4.961769 on epoch=27
06/01/2022 11:07:35 - INFO - __main__ - Step 120 Global step 120 Train loss 3.004033 on epoch=29
06/01/2022 11:07:40 - INFO - __main__ - Step 130 Global step 130 Train loss 3.109342 on epoch=32
06/01/2022 11:07:45 - INFO - __main__ - Step 140 Global step 140 Train loss 2.597935 on epoch=34
06/01/2022 11:07:51 - INFO - __main__ - Step 150 Global step 150 Train loss 1.693427 on epoch=37
06/01/2022 11:07:51 - INFO - __main__ - Global step 150 Train loss 3.073301 Classification-F1 0.38613111026904134 on epoch=37
06/01/2022 11:07:57 - INFO - __main__ - Step 160 Global step 160 Train loss 2.131490 on epoch=39
06/01/2022 11:08:03 - INFO - __main__ - Step 170 Global step 170 Train loss 1.566130 on epoch=42
06/01/2022 11:08:08 - INFO - __main__ - Step 180 Global step 180 Train loss 1.594246 on epoch=44
06/01/2022 11:08:13 - INFO - __main__ - Step 190 Global step 190 Train loss 1.899476 on epoch=47
06/01/2022 11:08:19 - INFO - __main__ - Step 200 Global step 200 Train loss 1.354355 on epoch=49
06/01/2022 11:08:20 - INFO - __main__ - Global step 200 Train loss 1.709139 Classification-F1 0.42233632862644416 on epoch=49
06/01/2022 11:08:26 - INFO - __main__ - Step 210 Global step 210 Train loss 1.588631 on epoch=52
06/01/2022 11:08:31 - INFO - __main__ - Step 220 Global step 220 Train loss 1.679151 on epoch=54
06/01/2022 11:08:36 - INFO - __main__ - Step 230 Global step 230 Train loss 1.109204 on epoch=57
06/01/2022 11:08:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.531389 on epoch=59
06/01/2022 11:08:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.472846 on epoch=62
06/01/2022 11:08:48 - INFO - __main__ - Global step 250 Train loss 1.076244 Classification-F1 0.40476190476190477 on epoch=62
06/01/2022 11:08:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.376663 on epoch=64
06/01/2022 11:08:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.301621 on epoch=67
06/01/2022 11:09:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.303276 on epoch=69
06/01/2022 11:09:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.202374 on epoch=72
06/01/2022 11:09:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.182185 on epoch=74
06/01/2022 11:09:15 - INFO - __main__ - Global step 300 Train loss 0.273224 Classification-F1 0.49935815147625157 on epoch=74
06/01/2022 11:09:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.113302 on epoch=77
06/01/2022 11:09:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.138544 on epoch=79
06/01/2022 11:09:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.153718 on epoch=82
06/01/2022 11:09:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.124254 on epoch=84
06/01/2022 11:09:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.081881 on epoch=87
06/01/2022 11:09:43 - INFO - __main__ - Global step 350 Train loss 0.122340 Classification-F1 0.848951048951049 on epoch=87
06/01/2022 11:09:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.092552 on epoch=89
06/01/2022 11:09:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.046775 on epoch=92
06/01/2022 11:09:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.051662 on epoch=94
06/01/2022 11:10:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.088064 on epoch=97
06/01/2022 11:10:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.411314 on epoch=99
06/01/2022 11:10:10 - INFO - __main__ - Global step 400 Train loss 0.138073 Classification-F1 0.8044444444444443 on epoch=99
06/01/2022 11:10:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.388919 on epoch=102
06/01/2022 11:10:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.135469 on epoch=104
06/01/2022 11:10:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.023955 on epoch=107
06/01/2022 11:10:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.066030 on epoch=109
06/01/2022 11:10:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.012988 on epoch=112
06/01/2022 11:10:37 - INFO - __main__ - Global step 450 Train loss 0.125472 Classification-F1 0.8642533936651584 on epoch=112
06/01/2022 11:10:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.016949 on epoch=114
06/01/2022 11:10:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.016525 on epoch=117
06/01/2022 11:10:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.233831 on epoch=119
06/01/2022 11:10:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.429339 on epoch=122
06/01/2022 11:11:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.366929 on epoch=124
06/01/2022 11:11:05 - INFO - __main__ - Global step 500 Train loss 0.212715 Classification-F1 0.8660714285714286 on epoch=124
06/01/2022 11:11:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.055462 on epoch=127
06/01/2022 11:11:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.026579 on epoch=129
06/01/2022 11:11:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.011850 on epoch=132
06/01/2022 11:11:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.018851 on epoch=134
06/01/2022 11:11:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.052244 on epoch=137
06/01/2022 11:11:33 - INFO - __main__ - Global step 550 Train loss 0.032997 Classification-F1 0.8660714285714286 on epoch=137
06/01/2022 11:11:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.012628 on epoch=139
06/01/2022 11:11:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.014386 on epoch=142
06/01/2022 11:11:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.010304 on epoch=144
06/01/2022 11:11:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.006610 on epoch=147
06/01/2022 11:12:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.026775 on epoch=149
06/01/2022 11:12:00 - INFO - __main__ - Global step 600 Train loss 0.014141 Classification-F1 0.667735354124162 on epoch=149
06/01/2022 11:12:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.009989 on epoch=152
06/01/2022 11:12:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.075229 on epoch=154
06/01/2022 11:12:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.024227 on epoch=157
06/01/2022 11:12:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.025544 on epoch=159
06/01/2022 11:12:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.003274 on epoch=162
06/01/2022 11:12:28 - INFO - __main__ - Global step 650 Train loss 0.027652 Classification-F1 0.8833009169213671 on epoch=162
06/01/2022 11:12:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.007330 on epoch=164
06/01/2022 11:12:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001030 on epoch=167
06/01/2022 11:12:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.056569 on epoch=169
06/01/2022 11:12:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000311 on epoch=172
06/01/2022 11:12:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000232 on epoch=174
06/01/2022 11:12:55 - INFO - __main__ - Global step 700 Train loss 0.013094 Classification-F1 0.849624060150376 on epoch=174
06/01/2022 11:13:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000747 on epoch=177
06/01/2022 11:13:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000648 on epoch=179
06/01/2022 11:13:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.017102 on epoch=182
06/01/2022 11:13:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.028991 on epoch=184
06/01/2022 11:13:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.003180 on epoch=187
06/01/2022 11:13:23 - INFO - __main__ - Global step 750 Train loss 0.010133 Classification-F1 0.8660714285714286 on epoch=187
06/01/2022 11:13:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.002873 on epoch=189
06/01/2022 11:13:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.002006 on epoch=192
06/01/2022 11:13:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000341 on epoch=194
06/01/2022 11:13:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001098 on epoch=197
06/01/2022 11:13:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002543 on epoch=199
06/01/2022 11:13:50 - INFO - __main__ - Global step 800 Train loss 0.001772 Classification-F1 0.848951048951049 on epoch=199
06/01/2022 11:13:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001234 on epoch=202
06/01/2022 11:14:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.003259 on epoch=204
06/01/2022 11:14:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000208 on epoch=207
06/01/2022 11:14:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.010030 on epoch=209
06/01/2022 11:14:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.106524 on epoch=212
06/01/2022 11:14:17 - INFO - __main__ - Global step 850 Train loss 0.024251 Classification-F1 0.8806479113384484 on epoch=212
06/01/2022 11:14:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.025426 on epoch=214
06/01/2022 11:14:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000899 on epoch=217
06/01/2022 11:14:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000353 on epoch=219
06/01/2022 11:14:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000147 on epoch=222
06/01/2022 11:14:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.062813 on epoch=224
06/01/2022 11:14:44 - INFO - __main__ - Global step 900 Train loss 0.017928 Classification-F1 0.881723458180794 on epoch=224
06/01/2022 11:14:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001438 on epoch=227
06/01/2022 11:14:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000962 on epoch=229
06/01/2022 11:15:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000144 on epoch=232
06/01/2022 11:15:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.082373 on epoch=234
06/01/2022 11:15:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.003581 on epoch=237
06/01/2022 11:15:11 - INFO - __main__ - Global step 950 Train loss 0.017700 Classification-F1 0.898989898989899 on epoch=237
06/01/2022 11:15:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000163 on epoch=239
06/01/2022 11:15:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000281 on epoch=242
06/01/2022 11:15:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.007454 on epoch=244
06/01/2022 11:15:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.004044 on epoch=247
06/01/2022 11:15:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.007404 on epoch=249
06/01/2022 11:15:39 - INFO - __main__ - Global step 1000 Train loss 0.003869 Classification-F1 0.8628571428571428 on epoch=249
06/01/2022 11:15:39 - INFO - __main__ - save last model!
06/01/2022 11:15:39 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:15:39 - INFO - __main__ - Printing 3 examples
06/01/2022 11:15:39 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/01/2022 11:15:39 - INFO - __main__ - ['false']
06/01/2022 11:15:39 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
06/01/2022 11:15:39 - INFO - __main__ - ['false']
06/01/2022 11:15:39 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/01/2022 11:15:39 - INFO - __main__ - ['false']
06/01/2022 11:15:39 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:15:39 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:15:40 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:15:40 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:15:40 - INFO - __main__ - Printing 3 examples
06/01/2022 11:15:40 - INFO - __main__ -  [ethos-race] The men are turtles...idiots even in uk… get lost
06/01/2022 11:15:40 - INFO - __main__ - ['false']
06/01/2022 11:15:40 - INFO - __main__ -  [ethos-race] Christians can get crazy religious and I get so f u c k i n g annoyed
06/01/2022 11:15:40 - INFO - __main__ - ['false']
06/01/2022 11:15:40 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
06/01/2022 11:15:40 - INFO - __main__ - ['false']
06/01/2022 11:15:40 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:15:40 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:15:40 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:15:46 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 11:15:47 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 11:15:47 - INFO - __main__ - Printing 3 examples
06/01/2022 11:15:47 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 11:15:47 - INFO - __main__ - ['true']
06/01/2022 11:15:47 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 11:15:47 - INFO - __main__ - ['false']
06/01/2022 11:15:47 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 11:15:47 - INFO - __main__ - ['false']
06/01/2022 11:15:47 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:15:47 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:15:47 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 11:15:48 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_13_0.0003_8_predictions.txt
06/01/2022 11:15:48 - INFO - __main__ - Classification-F1 on test data: 0.8301
06/01/2022 11:15:49 - INFO - __main__ - prefix=ethos-race_32_13, lr=0.0003, bsz=8, dev_performance=0.898989898989899, test_performance=0.8301082904313865
06/01/2022 11:15:49 - INFO - __main__ - Running ... prefix=ethos-race_32_13, lr=0.0002, bsz=8 ...
06/01/2022 11:15:50 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:15:50 - INFO - __main__ - Printing 3 examples
06/01/2022 11:15:50 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/01/2022 11:15:50 - INFO - __main__ - ['false']
06/01/2022 11:15:50 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
06/01/2022 11:15:50 - INFO - __main__ - ['false']
06/01/2022 11:15:50 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/01/2022 11:15:50 - INFO - __main__ - ['false']
06/01/2022 11:15:50 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:15:50 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:15:50 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:15:50 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:15:50 - INFO - __main__ - Printing 3 examples
06/01/2022 11:15:50 - INFO - __main__ -  [ethos-race] The men are turtles...idiots even in uk… get lost
06/01/2022 11:15:50 - INFO - __main__ - ['false']
06/01/2022 11:15:50 - INFO - __main__ -  [ethos-race] Christians can get crazy religious and I get so f u c k i n g annoyed
06/01/2022 11:15:50 - INFO - __main__ - ['false']
06/01/2022 11:15:50 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
06/01/2022 11:15:50 - INFO - __main__ - ['false']
06/01/2022 11:15:50 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:15:50 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:15:50 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:15:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:15:50 - INFO - __main__ - Starting training!
06/01/2022 11:16:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:16:01 - INFO - __main__ - Starting training!
06/01/2022 11:16:06 - INFO - __main__ - Step 10 Global step 10 Train loss 23.598703 on epoch=2
06/01/2022 11:16:11 - INFO - __main__ - Step 20 Global step 20 Train loss 18.785717 on epoch=4
06/01/2022 11:16:16 - INFO - __main__ - Step 30 Global step 30 Train loss 18.451870 on epoch=7
06/01/2022 11:16:21 - INFO - __main__ - Step 40 Global step 40 Train loss 17.000969 on epoch=9
06/01/2022 11:16:27 - INFO - __main__ - Step 50 Global step 50 Train loss 16.106087 on epoch=12
06/01/2022 11:16:45 - INFO - __main__ - Global step 50 Train loss 18.788670 Classification-F1 0.0 on epoch=12
06/01/2022 11:16:51 - INFO - __main__ - Step 60 Global step 60 Train loss 15.468984 on epoch=14
06/01/2022 11:16:56 - INFO - __main__ - Step 70 Global step 70 Train loss 14.577600 on epoch=17
06/01/2022 11:17:02 - INFO - __main__ - Step 80 Global step 80 Train loss 13.837832 on epoch=19
06/01/2022 11:17:07 - INFO - __main__ - Step 90 Global step 90 Train loss 13.463727 on epoch=22
06/01/2022 11:17:12 - INFO - __main__ - Step 100 Global step 100 Train loss 12.838358 on epoch=24
06/01/2022 11:17:30 - INFO - __main__ - Global step 100 Train loss 14.037300 Classification-F1 0.0 on epoch=24
06/01/2022 11:17:36 - INFO - __main__ - Step 110 Global step 110 Train loss 11.324290 on epoch=27
06/01/2022 11:17:41 - INFO - __main__ - Step 120 Global step 120 Train loss 8.900799 on epoch=29
06/01/2022 11:17:47 - INFO - __main__ - Step 130 Global step 130 Train loss 5.956611 on epoch=32
06/01/2022 11:17:52 - INFO - __main__ - Step 140 Global step 140 Train loss 2.695163 on epoch=34
06/01/2022 11:17:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.734976 on epoch=37
06/01/2022 11:17:58 - INFO - __main__ - Global step 150 Train loss 5.922368 Classification-F1 0.5739425748687866 on epoch=37
06/01/2022 11:18:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.683670 on epoch=39
06/01/2022 11:18:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.405772 on epoch=42
06/01/2022 11:18:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.368646 on epoch=44
06/01/2022 11:18:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.532300 on epoch=47
06/01/2022 11:18:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.285640 on epoch=49
06/01/2022 11:18:26 - INFO - __main__ - Global step 200 Train loss 0.455205 Classification-F1 0.6632996632996633 on epoch=49
06/01/2022 11:18:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.258731 on epoch=52
06/01/2022 11:18:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.236805 on epoch=54
06/01/2022 11:18:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.370147 on epoch=57
06/01/2022 11:18:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.267145 on epoch=59
06/01/2022 11:18:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.194655 on epoch=62
06/01/2022 11:18:54 - INFO - __main__ - Global step 250 Train loss 0.265496 Classification-F1 0.6157059314954052 on epoch=62
06/01/2022 11:19:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.205409 on epoch=64
06/01/2022 11:19:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.167040 on epoch=67
06/01/2022 11:19:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.219939 on epoch=69
06/01/2022 11:19:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.161751 on epoch=72
06/01/2022 11:19:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.139943 on epoch=74
06/01/2022 11:19:22 - INFO - __main__ - Global step 300 Train loss 0.178816 Classification-F1 0.667735354124162 on epoch=74
06/01/2022 11:19:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.152033 on epoch=77
06/01/2022 11:19:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.115598 on epoch=79
06/01/2022 11:19:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.112841 on epoch=82
06/01/2022 11:19:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.169123 on epoch=84
06/01/2022 11:19:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.122874 on epoch=87
06/01/2022 11:19:50 - INFO - __main__ - Global step 350 Train loss 0.134494 Classification-F1 0.8447829836159816 on epoch=87
06/01/2022 11:19:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.134778 on epoch=89
06/01/2022 11:20:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.115565 on epoch=92
06/01/2022 11:20:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.054348 on epoch=94
06/01/2022 11:20:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.118620 on epoch=97
06/01/2022 11:20:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.104269 on epoch=99
06/01/2022 11:20:18 - INFO - __main__ - Global step 400 Train loss 0.105516 Classification-F1 0.7376858058874963 on epoch=99
06/01/2022 11:20:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.118376 on epoch=102
06/01/2022 11:20:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.088304 on epoch=104
06/01/2022 11:20:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.052138 on epoch=107
06/01/2022 11:20:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.090116 on epoch=109
06/01/2022 11:20:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.050496 on epoch=112
06/01/2022 11:20:46 - INFO - __main__ - Global step 450 Train loss 0.079886 Classification-F1 0.8447829836159816 on epoch=112
06/01/2022 11:20:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.058663 on epoch=114
06/01/2022 11:20:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.090733 on epoch=117
06/01/2022 11:21:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.028203 on epoch=119
06/01/2022 11:21:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.047816 on epoch=122
06/01/2022 11:21:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.118733 on epoch=124
06/01/2022 11:21:13 - INFO - __main__ - Global step 500 Train loss 0.068830 Classification-F1 0.7465502675302732 on epoch=124
06/01/2022 11:21:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.044229 on epoch=127
06/01/2022 11:21:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.080788 on epoch=129
06/01/2022 11:21:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.062392 on epoch=132
06/01/2022 11:21:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.038538 on epoch=134
06/01/2022 11:21:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.016615 on epoch=137
06/01/2022 11:21:41 - INFO - __main__ - Global step 550 Train loss 0.048512 Classification-F1 0.8825174825174824 on epoch=137
06/01/2022 11:21:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.050432 on epoch=139
06/01/2022 11:21:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.015085 on epoch=142
06/01/2022 11:21:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.070543 on epoch=144
06/01/2022 11:22:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.019954 on epoch=147
06/01/2022 11:22:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.033426 on epoch=149
06/01/2022 11:22:09 - INFO - __main__ - Global step 600 Train loss 0.037888 Classification-F1 0.8642533936651584 on epoch=149
06/01/2022 11:22:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.021439 on epoch=152
06/01/2022 11:22:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.047728 on epoch=154
06/01/2022 11:22:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.020573 on epoch=157
06/01/2022 11:22:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.006907 on epoch=159
06/01/2022 11:22:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.026057 on epoch=162
06/01/2022 11:22:36 - INFO - __main__ - Global step 650 Train loss 0.024541 Classification-F1 0.8124467178175618 on epoch=162
06/01/2022 11:22:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.004818 on epoch=164
06/01/2022 11:22:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.011115 on epoch=167
06/01/2022 11:22:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.015612 on epoch=169
06/01/2022 11:22:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.116181 on epoch=172
06/01/2022 11:23:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.063803 on epoch=174
06/01/2022 11:23:03 - INFO - __main__ - Global step 700 Train loss 0.042306 Classification-F1 0.8237367802585194 on epoch=174
06/01/2022 11:23:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.165592 on epoch=177
06/01/2022 11:23:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.184968 on epoch=179
06/01/2022 11:23:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.194024 on epoch=182
06/01/2022 11:23:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.152186 on epoch=184
06/01/2022 11:23:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.205184 on epoch=187
06/01/2022 11:23:31 - INFO - __main__ - Global step 750 Train loss 0.180391 Classification-F1 0.848951048951049 on epoch=187
06/01/2022 11:23:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.141896 on epoch=189
06/01/2022 11:23:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.121642 on epoch=192
06/01/2022 11:23:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.101474 on epoch=194
06/01/2022 11:23:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.017729 on epoch=197
06/01/2022 11:23:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.005251 on epoch=199
06/01/2022 11:23:58 - INFO - __main__ - Global step 800 Train loss 0.077599 Classification-F1 0.8479301605181638 on epoch=199
06/01/2022 11:24:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.002134 on epoch=202
06/01/2022 11:24:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001362 on epoch=204
06/01/2022 11:24:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001692 on epoch=207
06/01/2022 11:24:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001977 on epoch=209
06/01/2022 11:24:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001139 on epoch=212
06/01/2022 11:24:26 - INFO - __main__ - Global step 850 Train loss 0.001661 Classification-F1 0.8628571428571428 on epoch=212
06/01/2022 11:24:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.003126 on epoch=214
06/01/2022 11:24:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.001994 on epoch=217
06/01/2022 11:24:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.002314 on epoch=219
06/01/2022 11:24:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002533 on epoch=222
06/01/2022 11:24:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000643 on epoch=224
06/01/2022 11:24:53 - INFO - __main__ - Global step 900 Train loss 0.002122 Classification-F1 0.846547314578005 on epoch=224
06/01/2022 11:24:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001007 on epoch=227
06/01/2022 11:25:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.013692 on epoch=229
06/01/2022 11:25:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000975 on epoch=232
06/01/2022 11:25:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000419 on epoch=234
06/01/2022 11:25:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.002213 on epoch=237
06/01/2022 11:25:20 - INFO - __main__ - Global step 950 Train loss 0.003661 Classification-F1 0.846547314578005 on epoch=237
06/01/2022 11:25:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.003657 on epoch=239
06/01/2022 11:25:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.006159 on epoch=242
06/01/2022 11:25:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000318 on epoch=244
06/01/2022 11:25:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.003941 on epoch=247
06/01/2022 11:25:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.004082 on epoch=249
06/01/2022 11:25:48 - INFO - __main__ - Global step 1000 Train loss 0.003631 Classification-F1 0.848951048951049 on epoch=249
06/01/2022 11:25:48 - INFO - __main__ - save last model!
06/01/2022 11:25:48 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:25:48 - INFO - __main__ - Printing 3 examples
06/01/2022 11:25:48 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/01/2022 11:25:48 - INFO - __main__ - ['false']
06/01/2022 11:25:48 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
06/01/2022 11:25:48 - INFO - __main__ - ['false']
06/01/2022 11:25:48 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/01/2022 11:25:48 - INFO - __main__ - ['false']
06/01/2022 11:25:48 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:25:48 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:25:48 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:25:48 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:25:48 - INFO - __main__ - Printing 3 examples
06/01/2022 11:25:48 - INFO - __main__ -  [ethos-race] The men are turtles...idiots even in uk… get lost
06/01/2022 11:25:48 - INFO - __main__ - ['false']
06/01/2022 11:25:48 - INFO - __main__ -  [ethos-race] Christians can get crazy religious and I get so f u c k i n g annoyed
06/01/2022 11:25:48 - INFO - __main__ - ['false']
06/01/2022 11:25:48 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
06/01/2022 11:25:48 - INFO - __main__ - ['false']
06/01/2022 11:25:48 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:25:48 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:25:48 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:25:55 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 11:25:55 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 11:25:55 - INFO - __main__ - Printing 3 examples
06/01/2022 11:25:55 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 11:25:55 - INFO - __main__ - ['true']
06/01/2022 11:25:55 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 11:25:55 - INFO - __main__ - ['false']
06/01/2022 11:25:55 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 11:25:55 - INFO - __main__ - ['false']
06/01/2022 11:25:55 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:25:55 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:25:56 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 11:25:57 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_13_0.0002_8_predictions.txt
06/01/2022 11:25:57 - INFO - __main__ - Classification-F1 on test data: 0.7992
06/01/2022 11:25:57 - INFO - __main__ - prefix=ethos-race_32_13, lr=0.0002, bsz=8, dev_performance=0.8825174825174824, test_performance=0.7992188886916386
06/01/2022 11:25:57 - INFO - __main__ - Running ... prefix=ethos-race_32_13, lr=0.0001, bsz=8 ...
06/01/2022 11:25:58 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:25:58 - INFO - __main__ - Printing 3 examples
06/01/2022 11:25:58 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/01/2022 11:25:58 - INFO - __main__ - ['false']
06/01/2022 11:25:58 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
06/01/2022 11:25:58 - INFO - __main__ - ['false']
06/01/2022 11:25:58 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/01/2022 11:25:58 - INFO - __main__ - ['false']
06/01/2022 11:25:58 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:25:58 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:25:58 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:25:58 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:25:58 - INFO - __main__ - Printing 3 examples
06/01/2022 11:25:58 - INFO - __main__ -  [ethos-race] The men are turtles...idiots even in uk… get lost
06/01/2022 11:25:58 - INFO - __main__ - ['false']
06/01/2022 11:25:58 - INFO - __main__ -  [ethos-race] Christians can get crazy religious and I get so f u c k i n g annoyed
06/01/2022 11:25:58 - INFO - __main__ - ['false']
06/01/2022 11:25:58 - INFO - __main__ -  [ethos-race] Homosexuality and transgenderism is a mental illness and a sexual perversion!
06/01/2022 11:25:58 - INFO - __main__ - ['false']
06/01/2022 11:25:58 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:25:58 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:25:58 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:26:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:26:01 - INFO - __main__ - Starting training!
06/01/2022 11:26:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:26:09 - INFO - __main__ - Starting training!
06/01/2022 11:26:14 - INFO - __main__ - Step 10 Global step 10 Train loss 24.212076 on epoch=2
06/01/2022 11:26:19 - INFO - __main__ - Step 20 Global step 20 Train loss 21.193447 on epoch=4
06/01/2022 11:26:24 - INFO - __main__ - Step 30 Global step 30 Train loss 19.604721 on epoch=7
06/01/2022 11:26:30 - INFO - __main__ - Step 40 Global step 40 Train loss 18.511625 on epoch=9
06/01/2022 11:26:35 - INFO - __main__ - Step 50 Global step 50 Train loss 18.179470 on epoch=12
06/01/2022 11:26:46 - INFO - __main__ - Global step 50 Train loss 20.340267 Classification-F1 0.0 on epoch=12
06/01/2022 11:26:52 - INFO - __main__ - Step 60 Global step 60 Train loss 17.852041 on epoch=14
06/01/2022 11:26:57 - INFO - __main__ - Step 70 Global step 70 Train loss 17.244041 on epoch=17
06/01/2022 11:27:02 - INFO - __main__ - Step 80 Global step 80 Train loss 16.926744 on epoch=19
06/01/2022 11:27:08 - INFO - __main__ - Step 90 Global step 90 Train loss 16.225195 on epoch=22
06/01/2022 11:27:13 - INFO - __main__ - Step 100 Global step 100 Train loss 15.877684 on epoch=24
06/01/2022 11:27:14 - INFO - __main__ - Global step 100 Train loss 16.825142 Classification-F1 0.0 on epoch=24
06/01/2022 11:27:20 - INFO - __main__ - Step 110 Global step 110 Train loss 15.313662 on epoch=27
06/01/2022 11:27:25 - INFO - __main__ - Step 120 Global step 120 Train loss 15.632895 on epoch=29
06/01/2022 11:27:30 - INFO - __main__ - Step 130 Global step 130 Train loss 15.314885 on epoch=32
06/01/2022 11:27:36 - INFO - __main__ - Step 140 Global step 140 Train loss 14.992289 on epoch=34
06/01/2022 11:27:41 - INFO - __main__ - Step 150 Global step 150 Train loss 14.419237 on epoch=37
06/01/2022 11:27:42 - INFO - __main__ - Global step 150 Train loss 15.134594 Classification-F1 0.0 on epoch=37
06/01/2022 11:27:47 - INFO - __main__ - Step 160 Global step 160 Train loss 14.173140 on epoch=39
06/01/2022 11:27:53 - INFO - __main__ - Step 170 Global step 170 Train loss 13.111023 on epoch=42
06/01/2022 11:27:58 - INFO - __main__ - Step 180 Global step 180 Train loss 12.715990 on epoch=44
06/01/2022 11:28:03 - INFO - __main__ - Step 190 Global step 190 Train loss 12.311982 on epoch=47
06/01/2022 11:28:08 - INFO - __main__ - Step 200 Global step 200 Train loss 12.002366 on epoch=49
06/01/2022 11:28:09 - INFO - __main__ - Global step 200 Train loss 12.862899 Classification-F1 0.0 on epoch=49
06/01/2022 11:28:15 - INFO - __main__ - Step 210 Global step 210 Train loss 11.425097 on epoch=52
06/01/2022 11:28:20 - INFO - __main__ - Step 220 Global step 220 Train loss 10.191648 on epoch=54
06/01/2022 11:28:26 - INFO - __main__ - Step 230 Global step 230 Train loss 6.810039 on epoch=57
06/01/2022 11:28:31 - INFO - __main__ - Step 240 Global step 240 Train loss 5.042952 on epoch=59
06/01/2022 11:28:36 - INFO - __main__ - Step 250 Global step 250 Train loss 4.123526 on epoch=62
06/01/2022 11:28:41 - INFO - __main__ - Global step 250 Train loss 7.518652 Classification-F1 0.23529411764705885 on epoch=62
06/01/2022 11:28:47 - INFO - __main__ - Step 260 Global step 260 Train loss 4.171709 on epoch=64
06/01/2022 11:28:53 - INFO - __main__ - Step 270 Global step 270 Train loss 3.564114 on epoch=67
06/01/2022 11:28:58 - INFO - __main__ - Step 280 Global step 280 Train loss 2.514925 on epoch=69
06/01/2022 11:29:03 - INFO - __main__ - Step 290 Global step 290 Train loss 1.773145 on epoch=72
06/01/2022 11:29:09 - INFO - __main__ - Step 300 Global step 300 Train loss 1.276044 on epoch=74
06/01/2022 11:29:09 - INFO - __main__ - Global step 300 Train loss 2.659987 Classification-F1 0.43823618242222895 on epoch=74
06/01/2022 11:29:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.723981 on epoch=77
06/01/2022 11:29:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.484449 on epoch=79
06/01/2022 11:29:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.494481 on epoch=82
06/01/2022 11:29:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.418923 on epoch=84
06/01/2022 11:29:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.389956 on epoch=87
06/01/2022 11:29:37 - INFO - __main__ - Global step 350 Train loss 0.502358 Classification-F1 0.5437904815544916 on epoch=87
06/01/2022 11:29:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.329448 on epoch=89
06/01/2022 11:29:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.251523 on epoch=92
06/01/2022 11:29:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.189080 on epoch=94
06/01/2022 11:29:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.253758 on epoch=97
06/01/2022 11:30:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.232436 on epoch=99
06/01/2022 11:30:05 - INFO - __main__ - Global step 400 Train loss 0.251249 Classification-F1 0.5125 on epoch=99
06/01/2022 11:30:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.203333 on epoch=102
06/01/2022 11:30:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.149731 on epoch=104
06/01/2022 11:30:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.230026 on epoch=107
06/01/2022 11:30:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.202596 on epoch=109
06/01/2022 11:30:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.163560 on epoch=112
06/01/2022 11:30:32 - INFO - __main__ - Global step 450 Train loss 0.189849 Classification-F1 0.7159565580618211 on epoch=112
06/01/2022 11:30:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.180181 on epoch=114
06/01/2022 11:30:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.139102 on epoch=117
06/01/2022 11:30:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.207969 on epoch=119
06/01/2022 11:30:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.231885 on epoch=122
06/01/2022 11:30:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.156812 on epoch=124
06/01/2022 11:31:00 - INFO - __main__ - Global step 500 Train loss 0.183190 Classification-F1 0.6914285714285715 on epoch=124
06/01/2022 11:31:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.091486 on epoch=127
06/01/2022 11:31:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.105225 on epoch=129
06/01/2022 11:31:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.106319 on epoch=132
06/01/2022 11:31:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.078935 on epoch=134
06/01/2022 11:31:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.097893 on epoch=137
06/01/2022 11:31:27 - INFO - __main__ - Global step 550 Train loss 0.095972 Classification-F1 0.696969696969697 on epoch=137
06/01/2022 11:31:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.113853 on epoch=139
06/01/2022 11:31:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.134641 on epoch=142
06/01/2022 11:31:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.076967 on epoch=144
06/01/2022 11:31:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.072826 on epoch=147
06/01/2022 11:31:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.053028 on epoch=149
06/01/2022 11:31:54 - INFO - __main__ - Global step 600 Train loss 0.090263 Classification-F1 0.8331479421579533 on epoch=149
06/01/2022 11:32:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.037978 on epoch=152
06/01/2022 11:32:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.051999 on epoch=154
06/01/2022 11:32:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.064087 on epoch=157
06/01/2022 11:32:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.040559 on epoch=159
06/01/2022 11:32:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.045090 on epoch=162
06/01/2022 11:32:22 - INFO - __main__ - Global step 650 Train loss 0.047943 Classification-F1 0.783273131425396 on epoch=162
06/01/2022 11:32:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.034829 on epoch=164
06/01/2022 11:32:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.044493 on epoch=167
06/01/2022 11:32:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.058445 on epoch=169
06/01/2022 11:32:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.072652 on epoch=172
06/01/2022 11:32:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.056735 on epoch=174
06/01/2022 11:32:49 - INFO - __main__ - Global step 700 Train loss 0.053431 Classification-F1 0.799777530589544 on epoch=174
06/01/2022 11:32:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.040364 on epoch=177
06/01/2022 11:33:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.043692 on epoch=179
06/01/2022 11:33:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.031728 on epoch=182
06/01/2022 11:33:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.122579 on epoch=184
06/01/2022 11:33:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.058271 on epoch=187
06/01/2022 11:33:17 - INFO - __main__ - Global step 750 Train loss 0.059327 Classification-F1 0.8166157265907197 on epoch=187
06/01/2022 11:33:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.040581 on epoch=189
06/01/2022 11:33:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.036314 on epoch=192
06/01/2022 11:33:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.016566 on epoch=194
06/01/2022 11:33:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.040495 on epoch=197
06/01/2022 11:33:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.031393 on epoch=199
06/01/2022 11:33:44 - INFO - __main__ - Global step 800 Train loss 0.033070 Classification-F1 0.8331479421579533 on epoch=199
06/01/2022 11:33:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.034450 on epoch=202
06/01/2022 11:33:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.045130 on epoch=204
06/01/2022 11:34:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.018027 on epoch=207
06/01/2022 11:34:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.036544 on epoch=209
06/01/2022 11:34:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.017758 on epoch=212
06/01/2022 11:34:12 - INFO - __main__ - Global step 850 Train loss 0.030382 Classification-F1 0.8499583217560432 on epoch=212
06/01/2022 11:34:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.017489 on epoch=214
06/01/2022 11:34:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.047815 on epoch=217
06/01/2022 11:34:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.008732 on epoch=219
06/01/2022 11:34:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.043267 on epoch=222
06/01/2022 11:34:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.037479 on epoch=224
06/01/2022 11:34:40 - INFO - __main__ - Global step 900 Train loss 0.030956 Classification-F1 0.8665183537263627 on epoch=224
06/01/2022 11:34:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.017320 on epoch=227
06/01/2022 11:34:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.054520 on epoch=229
06/01/2022 11:34:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.004293 on epoch=232
06/01/2022 11:35:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.017678 on epoch=234
06/01/2022 11:35:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.006766 on epoch=237
06/01/2022 11:35:08 - INFO - __main__ - Global step 950 Train loss 0.020115 Classification-F1 0.8166157265907196 on epoch=237
06/01/2022 11:35:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.004412 on epoch=239
06/01/2022 11:35:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.008552 on epoch=242
06/01/2022 11:35:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.002792 on epoch=244
06/01/2022 11:35:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.019692 on epoch=247
06/01/2022 11:35:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.008046 on epoch=249
06/01/2022 11:35:35 - INFO - __main__ - Global step 1000 Train loss 0.008698 Classification-F1 0.8325892857142857 on epoch=249
06/01/2022 11:35:35 - INFO - __main__ - save last model!
06/01/2022 11:35:35 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:35:35 - INFO - __main__ - Printing 3 examples
06/01/2022 11:35:35 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/01/2022 11:35:35 - INFO - __main__ - ['false']
06/01/2022 11:35:35 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/01/2022 11:35:35 - INFO - __main__ - ['false']
06/01/2022 11:35:35 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/01/2022 11:35:35 - INFO - __main__ - ['false']
06/01/2022 11:35:35 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:35:35 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:35:35 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:35:35 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:35:35 - INFO - __main__ - Printing 3 examples
06/01/2022 11:35:35 - INFO - __main__ -  [ethos-race] Bulgars are Indo Europeans, Turks are not, you ignorant pakisven. Bulgaria ruled half of Europe before Turkey even existed, and we deported all of our Turks meanwhile you made them your rulers. Sven is a fake german country ruled by arabs.
06/01/2022 11:35:35 - INFO - __main__ - ['false']
06/01/2022 11:35:35 - INFO - __main__ -  [ethos-race] The Problem with Islam is that they think they have the right to tell other Human beings what to do or how to be a good Person. What is right and what is wrong. If for example 2 Men love each other, they hurt nobody but you think your God told you to stop them. Who are you to interfere in other ppls Life? Non of your Business! If you think they will Burn for that, then let it be at that. But no, you think you have to stop them bc your God told you. Thats the thing!
06/01/2022 11:35:35 - INFO - __main__ - ['false']
06/01/2022 11:35:35 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 11:35:35 - INFO - __main__ - ['false']
06/01/2022 11:35:35 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:35:35 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:35:36 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:35:42 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 11:35:43 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 11:35:43 - INFO - __main__ - Printing 3 examples
06/01/2022 11:35:43 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 11:35:43 - INFO - __main__ - ['true']
06/01/2022 11:35:43 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 11:35:43 - INFO - __main__ - ['false']
06/01/2022 11:35:43 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 11:35:43 - INFO - __main__ - ['false']
06/01/2022 11:35:43 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:35:43 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:35:43 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 11:35:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_13_0.0001_8_predictions.txt
06/01/2022 11:35:44 - INFO - __main__ - Classification-F1 on test data: 0.7088
06/01/2022 11:35:45 - INFO - __main__ - prefix=ethos-race_32_13, lr=0.0001, bsz=8, dev_performance=0.8665183537263627, test_performance=0.7087517934002869
06/01/2022 11:35:45 - INFO - __main__ - Running ... prefix=ethos-race_32_21, lr=0.0005, bsz=8 ...
06/01/2022 11:35:46 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:35:46 - INFO - __main__ - Printing 3 examples
06/01/2022 11:35:46 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/01/2022 11:35:46 - INFO - __main__ - ['false']
06/01/2022 11:35:46 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/01/2022 11:35:46 - INFO - __main__ - ['false']
06/01/2022 11:35:46 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/01/2022 11:35:46 - INFO - __main__ - ['false']
06/01/2022 11:35:46 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:35:46 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:35:46 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:35:46 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:35:46 - INFO - __main__ - Printing 3 examples
06/01/2022 11:35:46 - INFO - __main__ -  [ethos-race] Bulgars are Indo Europeans, Turks are not, you ignorant pakisven. Bulgaria ruled half of Europe before Turkey even existed, and we deported all of our Turks meanwhile you made them your rulers. Sven is a fake german country ruled by arabs.
06/01/2022 11:35:46 - INFO - __main__ - ['false']
06/01/2022 11:35:46 - INFO - __main__ -  [ethos-race] The Problem with Islam is that they think they have the right to tell other Human beings what to do or how to be a good Person. What is right and what is wrong. If for example 2 Men love each other, they hurt nobody but you think your God told you to stop them. Who are you to interfere in other ppls Life? Non of your Business! If you think they will Burn for that, then let it be at that. But no, you think you have to stop them bc your God told you. Thats the thing!
06/01/2022 11:35:46 - INFO - __main__ - ['false']
06/01/2022 11:35:46 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 11:35:46 - INFO - __main__ - ['false']
06/01/2022 11:35:46 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:35:46 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:35:46 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:35:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:35:48 - INFO - __main__ - Starting training!
06/01/2022 11:35:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:35:59 - INFO - __main__ - Starting training!
06/01/2022 11:36:03 - INFO - __main__ - Step 10 Global step 10 Train loss 23.405148 on epoch=2
06/01/2022 11:36:09 - INFO - __main__ - Step 20 Global step 20 Train loss 18.935961 on epoch=4
06/01/2022 11:36:15 - INFO - __main__ - Step 30 Global step 30 Train loss 15.938383 on epoch=7
06/01/2022 11:36:20 - INFO - __main__ - Step 40 Global step 40 Train loss 14.027113 on epoch=9
06/01/2022 11:36:25 - INFO - __main__ - Step 50 Global step 50 Train loss 11.537465 on epoch=12
06/01/2022 11:36:38 - INFO - __main__ - Global step 50 Train loss 16.768814 Classification-F1 0.0 on epoch=12
06/01/2022 11:36:44 - INFO - __main__ - Step 60 Global step 60 Train loss 7.386417 on epoch=14
06/01/2022 11:36:49 - INFO - __main__ - Step 70 Global step 70 Train loss 4.120239 on epoch=17
06/01/2022 11:36:54 - INFO - __main__ - Step 80 Global step 80 Train loss 1.389472 on epoch=19
06/01/2022 11:37:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.610504 on epoch=22
06/01/2022 11:37:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.576368 on epoch=24
06/01/2022 11:37:06 - INFO - __main__ - Global step 100 Train loss 2.816600 Classification-F1 0.3478260869565218 on epoch=24
06/01/2022 11:37:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.397310 on epoch=27
06/01/2022 11:37:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.440772 on epoch=29
06/01/2022 11:37:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.441712 on epoch=32
06/01/2022 11:37:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.394300 on epoch=34
06/01/2022 11:37:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.441768 on epoch=37
06/01/2022 11:37:34 - INFO - __main__ - Global step 150 Train loss 0.423172 Classification-F1 0.3181818181818182 on epoch=37
06/01/2022 11:37:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.373577 on epoch=39
06/01/2022 11:37:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.430727 on epoch=42
06/01/2022 11:37:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.441760 on epoch=44
06/01/2022 11:37:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.417711 on epoch=47
06/01/2022 11:38:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.343172 on epoch=49
06/01/2022 11:38:01 - INFO - __main__ - Global step 200 Train loss 0.401389 Classification-F1 0.52 on epoch=49
06/01/2022 11:38:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.443100 on epoch=52
06/01/2022 11:38:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.332695 on epoch=54
06/01/2022 11:38:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.361180 on epoch=57
06/01/2022 11:38:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.404046 on epoch=59
06/01/2022 11:38:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.345760 on epoch=62
06/01/2022 11:38:29 - INFO - __main__ - Global step 250 Train loss 0.377356 Classification-F1 0.35214211076280044 on epoch=62
06/01/2022 11:38:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.369772 on epoch=64
06/01/2022 11:38:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.303468 on epoch=67
06/01/2022 11:38:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.284136 on epoch=69
06/01/2022 11:38:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.284712 on epoch=72
06/01/2022 11:38:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.182567 on epoch=74
06/01/2022 11:38:56 - INFO - __main__ - Global step 300 Train loss 0.284931 Classification-F1 0.5193312434691745 on epoch=74
06/01/2022 11:39:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.522727 on epoch=77
06/01/2022 11:39:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.190884 on epoch=79
06/01/2022 11:39:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.243701 on epoch=82
06/01/2022 11:39:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.210227 on epoch=84
06/01/2022 11:39:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.121434 on epoch=87
06/01/2022 11:39:24 - INFO - __main__ - Global step 350 Train loss 0.257795 Classification-F1 0.4871794871794871 on epoch=87
06/01/2022 11:39:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.178229 on epoch=89
06/01/2022 11:39:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.107905 on epoch=92
06/01/2022 11:39:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.070031 on epoch=94
06/01/2022 11:39:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.158148 on epoch=97
06/01/2022 11:39:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.030126 on epoch=99
06/01/2022 11:39:51 - INFO - __main__ - Global step 400 Train loss 0.108888 Classification-F1 0.7330367074527252 on epoch=99
06/01/2022 11:39:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.035254 on epoch=102
06/01/2022 11:40:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.019960 on epoch=104
06/01/2022 11:40:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.011118 on epoch=107
06/01/2022 11:40:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.058390 on epoch=109
06/01/2022 11:40:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.013014 on epoch=112
06/01/2022 11:40:19 - INFO - __main__ - Global step 450 Train loss 0.027547 Classification-F1 0.7827903091060985 on epoch=112
06/01/2022 11:40:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.003784 on epoch=114
06/01/2022 11:40:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.041784 on epoch=117
06/01/2022 11:40:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001333 on epoch=119
06/01/2022 11:40:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002559 on epoch=122
06/01/2022 11:40:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002913 on epoch=124
06/01/2022 11:40:47 - INFO - __main__ - Global step 500 Train loss 0.010475 Classification-F1 0.7664071190211346 on epoch=124
06/01/2022 11:40:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.002973 on epoch=127
06/01/2022 11:40:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.005922 on epoch=129
06/01/2022 11:41:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003944 on epoch=132
06/01/2022 11:41:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000683 on epoch=134
06/01/2022 11:41:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001131 on epoch=137
06/01/2022 11:41:14 - INFO - __main__ - Global step 550 Train loss 0.002930 Classification-F1 0.8124467178175618 on epoch=137
06/01/2022 11:41:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000567 on epoch=139
06/01/2022 11:41:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000107 on epoch=142
06/01/2022 11:41:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000319 on epoch=144
06/01/2022 11:41:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000149 on epoch=147
06/01/2022 11:41:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000933 on epoch=149
06/01/2022 11:41:42 - INFO - __main__ - Global step 600 Train loss 0.000415 Classification-F1 0.8141368628555337 on epoch=149
06/01/2022 11:41:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000276 on epoch=152
06/01/2022 11:41:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.002071 on epoch=154
06/01/2022 11:41:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000164 on epoch=157
06/01/2022 11:42:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000256 on epoch=159
06/01/2022 11:42:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000843 on epoch=162
06/01/2022 11:42:10 - INFO - __main__ - Global step 650 Train loss 0.000722 Classification-F1 0.8153846153846154 on epoch=162
06/01/2022 11:42:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000687 on epoch=164
06/01/2022 11:42:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000053 on epoch=167
06/01/2022 11:42:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000092 on epoch=169
06/01/2022 11:42:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000205 on epoch=172
06/01/2022 11:42:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000223 on epoch=174
06/01/2022 11:42:38 - INFO - __main__ - Global step 700 Train loss 0.000252 Classification-F1 0.8153846153846154 on epoch=174
06/01/2022 11:42:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000640 on epoch=177
06/01/2022 11:42:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.007508 on epoch=179
06/01/2022 11:42:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.032220 on epoch=182
06/01/2022 11:42:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.001786 on epoch=184
06/01/2022 11:43:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001933 on epoch=187
06/01/2022 11:43:06 - INFO - __main__ - Global step 750 Train loss 0.008817 Classification-F1 0.8660714285714286 on epoch=187
06/01/2022 11:43:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.011189 on epoch=189
06/01/2022 11:43:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000483 on epoch=192
06/01/2022 11:43:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003334 on epoch=194
06/01/2022 11:43:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001287 on epoch=197
06/01/2022 11:43:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.004450 on epoch=199
06/01/2022 11:43:34 - INFO - __main__ - Global step 800 Train loss 0.004149 Classification-F1 0.8447829836159816 on epoch=199
06/01/2022 11:43:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000440 on epoch=202
06/01/2022 11:43:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001133 on epoch=204
06/01/2022 11:43:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000144 on epoch=207
06/01/2022 11:43:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000071 on epoch=209
06/01/2022 11:44:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000435 on epoch=212
06/01/2022 11:44:01 - INFO - __main__ - Global step 850 Train loss 0.000445 Classification-F1 0.8642533936651584 on epoch=212
06/01/2022 11:44:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000622 on epoch=214
06/01/2022 11:44:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000059 on epoch=217
06/01/2022 11:44:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000051 on epoch=219
06/01/2022 11:44:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000010 on epoch=222
06/01/2022 11:44:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000235 on epoch=224
06/01/2022 11:44:29 - INFO - __main__ - Global step 900 Train loss 0.000195 Classification-F1 0.8479301605181638 on epoch=224
06/01/2022 11:44:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000025 on epoch=227
06/01/2022 11:44:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000082 on epoch=229
06/01/2022 11:44:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000027 on epoch=232
06/01/2022 11:44:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000050 on epoch=234
06/01/2022 11:44:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000025 on epoch=237
06/01/2022 11:44:56 - INFO - __main__ - Global step 950 Train loss 0.000042 Classification-F1 0.8303167420814479 on epoch=237
06/01/2022 11:45:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000031 on epoch=239
06/01/2022 11:45:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000096 on epoch=242
06/01/2022 11:45:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000043 on epoch=244
06/01/2022 11:45:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000201 on epoch=247
06/01/2022 11:45:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000009 on epoch=249
06/01/2022 11:45:23 - INFO - __main__ - Global step 1000 Train loss 0.000076 Classification-F1 0.8653198653198653 on epoch=249
06/01/2022 11:45:23 - INFO - __main__ - save last model!
06/01/2022 11:45:24 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:45:24 - INFO - __main__ - Printing 3 examples
06/01/2022 11:45:24 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/01/2022 11:45:24 - INFO - __main__ - ['false']
06/01/2022 11:45:24 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/01/2022 11:45:24 - INFO - __main__ - ['false']
06/01/2022 11:45:24 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/01/2022 11:45:24 - INFO - __main__ - ['false']
06/01/2022 11:45:24 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:45:24 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:45:24 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:45:24 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:45:24 - INFO - __main__ - Printing 3 examples
06/01/2022 11:45:24 - INFO - __main__ -  [ethos-race] Bulgars are Indo Europeans, Turks are not, you ignorant pakisven. Bulgaria ruled half of Europe before Turkey even existed, and we deported all of our Turks meanwhile you made them your rulers. Sven is a fake german country ruled by arabs.
06/01/2022 11:45:24 - INFO - __main__ - ['false']
06/01/2022 11:45:24 - INFO - __main__ -  [ethos-race] The Problem with Islam is that they think they have the right to tell other Human beings what to do or how to be a good Person. What is right and what is wrong. If for example 2 Men love each other, they hurt nobody but you think your God told you to stop them. Who are you to interfere in other ppls Life? Non of your Business! If you think they will Burn for that, then let it be at that. But no, you think you have to stop them bc your God told you. Thats the thing!
06/01/2022 11:45:24 - INFO - __main__ - ['false']
06/01/2022 11:45:24 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 11:45:24 - INFO - __main__ - ['false']
06/01/2022 11:45:24 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:45:24 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:45:24 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:45:31 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 11:45:31 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 11:45:31 - INFO - __main__ - Printing 3 examples
06/01/2022 11:45:31 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 11:45:31 - INFO - __main__ - ['true']
06/01/2022 11:45:31 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 11:45:31 - INFO - __main__ - ['false']
06/01/2022 11:45:31 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 11:45:31 - INFO - __main__ - ['false']
06/01/2022 11:45:31 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:45:31 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:45:31 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 11:45:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_21_0.0005_8_predictions.txt
06/01/2022 11:45:33 - INFO - __main__ - Classification-F1 on test data: 0.7078
06/01/2022 11:45:33 - INFO - __main__ - prefix=ethos-race_32_21, lr=0.0005, bsz=8, dev_performance=0.8660714285714286, test_performance=0.7078358208955223
06/01/2022 11:45:33 - INFO - __main__ - Running ... prefix=ethos-race_32_21, lr=0.0003, bsz=8 ...
06/01/2022 11:45:34 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:45:34 - INFO - __main__ - Printing 3 examples
06/01/2022 11:45:34 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/01/2022 11:45:34 - INFO - __main__ - ['false']
06/01/2022 11:45:34 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/01/2022 11:45:34 - INFO - __main__ - ['false']
06/01/2022 11:45:34 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/01/2022 11:45:34 - INFO - __main__ - ['false']
06/01/2022 11:45:34 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:45:34 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:45:34 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:45:34 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:45:34 - INFO - __main__ - Printing 3 examples
06/01/2022 11:45:34 - INFO - __main__ -  [ethos-race] Bulgars are Indo Europeans, Turks are not, you ignorant pakisven. Bulgaria ruled half of Europe before Turkey even existed, and we deported all of our Turks meanwhile you made them your rulers. Sven is a fake german country ruled by arabs.
06/01/2022 11:45:34 - INFO - __main__ - ['false']
06/01/2022 11:45:34 - INFO - __main__ -  [ethos-race] The Problem with Islam is that they think they have the right to tell other Human beings what to do or how to be a good Person. What is right and what is wrong. If for example 2 Men love each other, they hurt nobody but you think your God told you to stop them. Who are you to interfere in other ppls Life? Non of your Business! If you think they will Burn for that, then let it be at that. But no, you think you have to stop them bc your God told you. Thats the thing!
06/01/2022 11:45:34 - INFO - __main__ - ['false']
06/01/2022 11:45:34 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 11:45:34 - INFO - __main__ - ['false']
06/01/2022 11:45:34 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:45:34 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:45:34 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:45:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:45:37 - INFO - __main__ - Starting training!
06/01/2022 11:45:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:45:47 - INFO - __main__ - Starting training!
06/01/2022 11:45:52 - INFO - __main__ - Step 10 Global step 10 Train loss 23.707325 on epoch=2
06/01/2022 11:45:57 - INFO - __main__ - Step 20 Global step 20 Train loss 19.951649 on epoch=4
06/01/2022 11:46:03 - INFO - __main__ - Step 30 Global step 30 Train loss 17.003700 on epoch=7
06/01/2022 11:46:08 - INFO - __main__ - Step 40 Global step 40 Train loss 16.701323 on epoch=9
06/01/2022 11:46:13 - INFO - __main__ - Step 50 Global step 50 Train loss 15.173444 on epoch=12
06/01/2022 11:46:31 - INFO - __main__ - Global step 50 Train loss 18.507488 Classification-F1 0.0 on epoch=12
06/01/2022 11:46:37 - INFO - __main__ - Step 60 Global step 60 Train loss 15.065207 on epoch=14
06/01/2022 11:46:42 - INFO - __main__ - Step 70 Global step 70 Train loss 13.870157 on epoch=17
06/01/2022 11:46:47 - INFO - __main__ - Step 80 Global step 80 Train loss 12.177081 on epoch=19
06/01/2022 11:46:53 - INFO - __main__ - Step 90 Global step 90 Train loss 9.993437 on epoch=22
06/01/2022 11:46:58 - INFO - __main__ - Step 100 Global step 100 Train loss 6.844286 on epoch=24
06/01/2022 11:46:59 - INFO - __main__ - Global step 100 Train loss 11.590035 Classification-F1 0.3478260869565218 on epoch=24
06/01/2022 11:47:05 - INFO - __main__ - Step 110 Global step 110 Train loss 2.773646 on epoch=27
06/01/2022 11:47:10 - INFO - __main__ - Step 120 Global step 120 Train loss 3.456868 on epoch=29
06/01/2022 11:47:15 - INFO - __main__ - Step 130 Global step 130 Train loss 2.830887 on epoch=32
06/01/2022 11:47:20 - INFO - __main__ - Step 140 Global step 140 Train loss 2.146391 on epoch=34
06/01/2022 11:47:25 - INFO - __main__ - Step 150 Global step 150 Train loss 1.164858 on epoch=37
06/01/2022 11:47:26 - INFO - __main__ - Global step 150 Train loss 2.474530 Classification-F1 0.52 on epoch=37
06/01/2022 11:47:32 - INFO - __main__ - Step 160 Global step 160 Train loss 1.532594 on epoch=39
06/01/2022 11:47:38 - INFO - __main__ - Step 170 Global step 170 Train loss 2.246128 on epoch=42
06/01/2022 11:47:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.872403 on epoch=44
06/01/2022 11:47:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.596552 on epoch=47
06/01/2022 11:47:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.461119 on epoch=49
06/01/2022 11:47:54 - INFO - __main__ - Global step 200 Train loss 1.141759 Classification-F1 0.5688416211555044 on epoch=49
06/01/2022 11:48:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.675423 on epoch=52
06/01/2022 11:48:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.531542 on epoch=54
06/01/2022 11:48:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.384082 on epoch=57
06/01/2022 11:48:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.778217 on epoch=59
06/01/2022 11:48:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.682843 on epoch=62
06/01/2022 11:48:22 - INFO - __main__ - Global step 250 Train loss 0.610421 Classification-F1 0.5628096764791606 on epoch=62
06/01/2022 11:48:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.424496 on epoch=64
06/01/2022 11:48:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.445808 on epoch=67
06/01/2022 11:48:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.395662 on epoch=69
06/01/2022 11:48:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.336324 on epoch=72
06/01/2022 11:48:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.396680 on epoch=74
06/01/2022 11:48:49 - INFO - __main__ - Global step 300 Train loss 0.399794 Classification-F1 0.7991071428571428 on epoch=74
06/01/2022 11:48:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.350337 on epoch=77
06/01/2022 11:49:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.275006 on epoch=79
06/01/2022 11:49:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.305131 on epoch=82
06/01/2022 11:49:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.375377 on epoch=84
06/01/2022 11:49:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.283019 on epoch=87
06/01/2022 11:49:17 - INFO - __main__ - Global step 350 Train loss 0.317774 Classification-F1 0.6850879901204074 on epoch=87
06/01/2022 11:49:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.259359 on epoch=89
06/01/2022 11:49:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.268432 on epoch=92
06/01/2022 11:49:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.340877 on epoch=94
06/01/2022 11:49:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.182969 on epoch=97
06/01/2022 11:49:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.264107 on epoch=99
06/01/2022 11:49:44 - INFO - __main__ - Global step 400 Train loss 0.263149 Classification-F1 0.8499583217560432 on epoch=99
06/01/2022 11:49:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.166839 on epoch=102
06/01/2022 11:49:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.148662 on epoch=104
06/01/2022 11:50:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.164908 on epoch=107
06/01/2022 11:50:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.176460 on epoch=109
06/01/2022 11:50:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.162896 on epoch=112
06/01/2022 11:50:12 - INFO - __main__ - Global step 450 Train loss 0.163953 Classification-F1 0.6052631578947368 on epoch=112
06/01/2022 11:50:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.153420 on epoch=114
06/01/2022 11:50:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.116670 on epoch=117
06/01/2022 11:50:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.125518 on epoch=119
06/01/2022 11:50:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.067605 on epoch=122
06/01/2022 11:50:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.120724 on epoch=124
06/01/2022 11:50:40 - INFO - __main__ - Global step 500 Train loss 0.116787 Classification-F1 0.846547314578005 on epoch=124
06/01/2022 11:50:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.057763 on epoch=127
06/01/2022 11:50:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.083505 on epoch=129
06/01/2022 11:50:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.072820 on epoch=132
06/01/2022 11:51:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.056801 on epoch=134
06/01/2022 11:51:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.048480 on epoch=137
06/01/2022 11:51:07 - INFO - __main__ - Global step 550 Train loss 0.063874 Classification-F1 0.8660714285714286 on epoch=137
06/01/2022 11:51:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.040900 on epoch=139
06/01/2022 11:51:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.067169 on epoch=142
06/01/2022 11:51:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.058152 on epoch=144
06/01/2022 11:51:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.051371 on epoch=147
06/01/2022 11:51:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.028459 on epoch=149
06/01/2022 11:51:34 - INFO - __main__ - Global step 600 Train loss 0.049210 Classification-F1 0.8153846153846154 on epoch=149
06/01/2022 11:51:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.041927 on epoch=152
06/01/2022 11:51:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.023359 on epoch=154
06/01/2022 11:51:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.030041 on epoch=157
06/01/2022 11:51:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.020025 on epoch=159
06/01/2022 11:52:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.007008 on epoch=162
06/01/2022 11:52:02 - INFO - __main__ - Global step 650 Train loss 0.024472 Classification-F1 0.8124467178175618 on epoch=162
06/01/2022 11:52:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.062055 on epoch=164
06/01/2022 11:52:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.030079 on epoch=167
06/01/2022 11:52:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.033431 on epoch=169
06/01/2022 11:52:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.100994 on epoch=172
06/01/2022 11:52:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.031414 on epoch=174
06/01/2022 11:52:29 - INFO - __main__ - Global step 700 Train loss 0.051595 Classification-F1 0.8325892857142857 on epoch=174
06/01/2022 11:52:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.015984 on epoch=177
06/01/2022 11:52:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.062114 on epoch=179
06/01/2022 11:52:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.061772 on epoch=182
06/01/2022 11:52:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.020464 on epoch=184
06/01/2022 11:52:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.033235 on epoch=187
06/01/2022 11:52:57 - INFO - __main__ - Global step 750 Train loss 0.038714 Classification-F1 0.797979797979798 on epoch=187
06/01/2022 11:53:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.051560 on epoch=189
06/01/2022 11:53:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.026610 on epoch=192
06/01/2022 11:53:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.042145 on epoch=194
06/01/2022 11:53:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.041207 on epoch=197
06/01/2022 11:53:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.029986 on epoch=199
06/01/2022 11:53:24 - INFO - __main__ - Global step 800 Train loss 0.038302 Classification-F1 0.7997775305895439 on epoch=199
06/01/2022 11:53:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.015226 on epoch=202
06/01/2022 11:53:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.012954 on epoch=204
06/01/2022 11:53:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.010637 on epoch=207
06/01/2022 11:53:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.067931 on epoch=209
06/01/2022 11:53:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.006396 on epoch=212
06/01/2022 11:53:52 - INFO - __main__ - Global step 850 Train loss 0.022629 Classification-F1 0.8325892857142857 on epoch=212
06/01/2022 11:53:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.011815 on epoch=214
06/01/2022 11:54:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.006917 on epoch=217
06/01/2022 11:54:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.031250 on epoch=219
06/01/2022 11:54:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002629 on epoch=222
06/01/2022 11:54:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.013042 on epoch=224
06/01/2022 11:54:19 - INFO - __main__ - Global step 900 Train loss 0.013131 Classification-F1 0.7666666666666666 on epoch=224
06/01/2022 11:54:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.013646 on epoch=227
06/01/2022 11:54:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.002519 on epoch=229
06/01/2022 11:54:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.002954 on epoch=232
06/01/2022 11:54:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.031441 on epoch=234
06/01/2022 11:54:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.002902 on epoch=237
06/01/2022 11:54:47 - INFO - __main__ - Global step 950 Train loss 0.010692 Classification-F1 0.7333333333333334 on epoch=237
06/01/2022 11:54:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.009579 on epoch=239
06/01/2022 11:54:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.017961 on epoch=242
06/01/2022 11:55:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.080794 on epoch=244
06/01/2022 11:55:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.018253 on epoch=247
06/01/2022 11:55:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001539 on epoch=249
06/01/2022 11:55:14 - INFO - __main__ - Global step 1000 Train loss 0.025625 Classification-F1 0.7827903091060985 on epoch=249
06/01/2022 11:55:14 - INFO - __main__ - save last model!
06/01/2022 11:55:15 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:55:15 - INFO - __main__ - Printing 3 examples
06/01/2022 11:55:15 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/01/2022 11:55:15 - INFO - __main__ - ['false']
06/01/2022 11:55:15 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/01/2022 11:55:15 - INFO - __main__ - ['false']
06/01/2022 11:55:15 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/01/2022 11:55:15 - INFO - __main__ - ['false']
06/01/2022 11:55:15 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:55:15 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:55:15 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:55:15 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:55:15 - INFO - __main__ - Printing 3 examples
06/01/2022 11:55:15 - INFO - __main__ -  [ethos-race] Bulgars are Indo Europeans, Turks are not, you ignorant pakisven. Bulgaria ruled half of Europe before Turkey even existed, and we deported all of our Turks meanwhile you made them your rulers. Sven is a fake german country ruled by arabs.
06/01/2022 11:55:15 - INFO - __main__ - ['false']
06/01/2022 11:55:15 - INFO - __main__ -  [ethos-race] The Problem with Islam is that they think they have the right to tell other Human beings what to do or how to be a good Person. What is right and what is wrong. If for example 2 Men love each other, they hurt nobody but you think your God told you to stop them. Who are you to interfere in other ppls Life? Non of your Business! If you think they will Burn for that, then let it be at that. But no, you think you have to stop them bc your God told you. Thats the thing!
06/01/2022 11:55:15 - INFO - __main__ - ['false']
06/01/2022 11:55:15 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 11:55:15 - INFO - __main__ - ['false']
06/01/2022 11:55:15 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:55:15 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:55:15 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:55:21 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 11:55:22 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 11:55:22 - INFO - __main__ - Printing 3 examples
06/01/2022 11:55:22 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 11:55:22 - INFO - __main__ - ['true']
06/01/2022 11:55:22 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 11:55:22 - INFO - __main__ - ['false']
06/01/2022 11:55:22 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 11:55:22 - INFO - __main__ - ['false']
06/01/2022 11:55:22 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:55:22 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:55:22 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 11:55:24 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_21_0.0003_8_predictions.txt
06/01/2022 11:55:24 - INFO - __main__ - Classification-F1 on test data: 0.7231
06/01/2022 11:55:24 - INFO - __main__ - prefix=ethos-race_32_21, lr=0.0003, bsz=8, dev_performance=0.8660714285714286, test_performance=0.7230691908192326
06/01/2022 11:55:24 - INFO - __main__ - Running ... prefix=ethos-race_32_21, lr=0.0002, bsz=8 ...
06/01/2022 11:55:25 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 11:55:25 - INFO - __main__ - Printing 3 examples
06/01/2022 11:55:25 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/01/2022 11:55:25 - INFO - __main__ - ['false']
06/01/2022 11:55:25 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/01/2022 11:55:25 - INFO - __main__ - ['false']
06/01/2022 11:55:25 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/01/2022 11:55:25 - INFO - __main__ - ['false']
06/01/2022 11:55:25 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:55:25 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:55:25 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 11:55:25 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 11:55:25 - INFO - __main__ - Printing 3 examples
06/01/2022 11:55:25 - INFO - __main__ -  [ethos-race] Bulgars are Indo Europeans, Turks are not, you ignorant pakisven. Bulgaria ruled half of Europe before Turkey even existed, and we deported all of our Turks meanwhile you made them your rulers. Sven is a fake german country ruled by arabs.
06/01/2022 11:55:25 - INFO - __main__ - ['false']
06/01/2022 11:55:25 - INFO - __main__ -  [ethos-race] The Problem with Islam is that they think they have the right to tell other Human beings what to do or how to be a good Person. What is right and what is wrong. If for example 2 Men love each other, they hurt nobody but you think your God told you to stop them. Who are you to interfere in other ppls Life? Non of your Business! If you think they will Burn for that, then let it be at that. But no, you think you have to stop them bc your God told you. Thats the thing!
06/01/2022 11:55:25 - INFO - __main__ - ['false']
06/01/2022 11:55:25 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 11:55:25 - INFO - __main__ - ['false']
06/01/2022 11:55:25 - INFO - __main__ - Tokenizing Input ...
06/01/2022 11:55:25 - INFO - __main__ - Tokenizing Output ...
06/01/2022 11:55:25 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 11:55:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:55:26 - INFO - __main__ - Starting training!
06/01/2022 11:55:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 11:55:38 - INFO - __main__ - Starting training!
06/01/2022 11:55:43 - INFO - __main__ - Step 10 Global step 10 Train loss 24.552128 on epoch=2
06/01/2022 11:55:48 - INFO - __main__ - Step 20 Global step 20 Train loss 21.443029 on epoch=4
06/01/2022 11:55:54 - INFO - __main__ - Step 30 Global step 30 Train loss 17.888462 on epoch=7
06/01/2022 11:55:59 - INFO - __main__ - Step 40 Global step 40 Train loss 17.842632 on epoch=9
06/01/2022 11:56:04 - INFO - __main__ - Step 50 Global step 50 Train loss 16.981007 on epoch=12
06/01/2022 11:56:21 - INFO - __main__ - Global step 50 Train loss 19.741451 Classification-F1 0.0 on epoch=12
06/01/2022 11:56:27 - INFO - __main__ - Step 60 Global step 60 Train loss 15.191025 on epoch=14
06/01/2022 11:56:33 - INFO - __main__ - Step 70 Global step 70 Train loss 14.419675 on epoch=17
06/01/2022 11:56:38 - INFO - __main__ - Step 80 Global step 80 Train loss 13.566040 on epoch=19
06/01/2022 11:56:43 - INFO - __main__ - Step 90 Global step 90 Train loss 12.941477 on epoch=22
06/01/2022 11:56:49 - INFO - __main__ - Step 100 Global step 100 Train loss 12.105820 on epoch=24
06/01/2022 11:57:02 - INFO - __main__ - Global step 100 Train loss 13.644807 Classification-F1 0.0 on epoch=24
06/01/2022 11:57:08 - INFO - __main__ - Step 110 Global step 110 Train loss 10.242793 on epoch=27
06/01/2022 11:57:13 - INFO - __main__ - Step 120 Global step 120 Train loss 5.084375 on epoch=29
06/01/2022 11:57:18 - INFO - __main__ - Step 130 Global step 130 Train loss 1.051204 on epoch=32
06/01/2022 11:57:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.579299 on epoch=34
06/01/2022 11:57:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.447680 on epoch=37
06/01/2022 11:57:30 - INFO - __main__ - Global step 150 Train loss 3.481071 Classification-F1 0.5693779904306221 on epoch=37
06/01/2022 11:57:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.426850 on epoch=39
06/01/2022 11:57:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.357675 on epoch=42
06/01/2022 11:57:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.726640 on epoch=44
06/01/2022 11:57:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.346453 on epoch=47
06/01/2022 11:57:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.274280 on epoch=49
06/01/2022 11:57:58 - INFO - __main__ - Global step 200 Train loss 0.426379 Classification-F1 0.6419437340153453 on epoch=49
06/01/2022 11:58:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.245643 on epoch=52
06/01/2022 11:58:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.221307 on epoch=54
06/01/2022 11:58:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.284120 on epoch=57
06/01/2022 11:58:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.181277 on epoch=59
06/01/2022 11:58:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.127727 on epoch=62
06/01/2022 11:58:26 - INFO - __main__ - Global step 250 Train loss 0.212015 Classification-F1 0.6789636722050127 on epoch=62
06/01/2022 11:58:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.197916 on epoch=64
06/01/2022 11:58:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.215046 on epoch=67
06/01/2022 11:58:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.137515 on epoch=69
06/01/2022 11:58:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.171930 on epoch=72
06/01/2022 11:58:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.089001 on epoch=74
06/01/2022 11:58:54 - INFO - __main__ - Global step 300 Train loss 0.162282 Classification-F1 0.7493734335839599 on epoch=74
06/01/2022 11:59:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.067069 on epoch=77
06/01/2022 11:59:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.103793 on epoch=79
06/01/2022 11:59:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.065143 on epoch=82
06/01/2022 11:59:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.072786 on epoch=84
06/01/2022 11:59:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.095937 on epoch=87
06/01/2022 11:59:23 - INFO - __main__ - Global step 350 Train loss 0.080946 Classification-F1 0.7818181818181817 on epoch=87
06/01/2022 11:59:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.041011 on epoch=89
06/01/2022 11:59:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.065024 on epoch=92
06/01/2022 11:59:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.060722 on epoch=94
06/01/2022 11:59:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.056374 on epoch=97
06/01/2022 11:59:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.037953 on epoch=99
06/01/2022 11:59:51 - INFO - __main__ - Global step 400 Train loss 0.052217 Classification-F1 0.8333333333333334 on epoch=99
06/01/2022 11:59:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.048546 on epoch=102
06/01/2022 12:00:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.031338 on epoch=104
06/01/2022 12:00:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.022863 on epoch=107
06/01/2022 12:00:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.021522 on epoch=109
06/01/2022 12:00:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.029391 on epoch=112
06/01/2022 12:00:19 - INFO - __main__ - Global step 450 Train loss 0.030732 Classification-F1 0.8325892857142857 on epoch=112
06/01/2022 12:00:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.011686 on epoch=114
06/01/2022 12:00:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.032218 on epoch=117
06/01/2022 12:00:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.011298 on epoch=119
06/01/2022 12:00:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.007004 on epoch=122
06/01/2022 12:00:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.027962 on epoch=124
06/01/2022 12:00:47 - INFO - __main__ - Global step 500 Train loss 0.018034 Classification-F1 0.8316498316498316 on epoch=124
06/01/2022 12:00:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.018491 on epoch=127
06/01/2022 12:00:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.014768 on epoch=129
06/01/2022 12:01:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.004570 on epoch=132
06/01/2022 12:01:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.008449 on epoch=134
06/01/2022 12:01:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.015370 on epoch=137
06/01/2022 12:01:14 - INFO - __main__ - Global step 550 Train loss 0.012330 Classification-F1 0.849624060150376 on epoch=137
06/01/2022 12:01:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.014290 on epoch=139
06/01/2022 12:01:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002035 on epoch=142
06/01/2022 12:01:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.016092 on epoch=144
06/01/2022 12:01:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.016627 on epoch=147
06/01/2022 12:01:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.023540 on epoch=149
06/01/2022 12:01:43 - INFO - __main__ - Global step 600 Train loss 0.014517 Classification-F1 0.848951048951049 on epoch=149
06/01/2022 12:01:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.028796 on epoch=152
06/01/2022 12:01:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001334 on epoch=154
06/01/2022 12:01:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.003544 on epoch=157
06/01/2022 12:02:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.019218 on epoch=159
06/01/2022 12:02:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.016186 on epoch=162
06/01/2022 12:02:11 - INFO - __main__ - Global step 650 Train loss 0.013815 Classification-F1 0.8166157265907196 on epoch=162
06/01/2022 12:02:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.004396 on epoch=164
06/01/2022 12:02:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.011326 on epoch=167
06/01/2022 12:02:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.010601 on epoch=169
06/01/2022 12:02:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001374 on epoch=172
06/01/2022 12:02:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.006196 on epoch=174
06/01/2022 12:02:38 - INFO - __main__ - Global step 700 Train loss 0.006779 Classification-F1 0.8325892857142857 on epoch=174
06/01/2022 12:02:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.006491 on epoch=177
06/01/2022 12:02:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000513 on epoch=179
06/01/2022 12:02:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.007772 on epoch=182
06/01/2022 12:03:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.011512 on epoch=184
06/01/2022 12:03:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001316 on epoch=187
06/01/2022 12:03:06 - INFO - __main__ - Global step 750 Train loss 0.005521 Classification-F1 0.848951048951049 on epoch=187
06/01/2022 12:03:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.002715 on epoch=189
06/01/2022 12:03:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000457 on epoch=192
06/01/2022 12:03:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.002257 on epoch=194
06/01/2022 12:03:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001320 on epoch=197
06/01/2022 12:03:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.005960 on epoch=199
06/01/2022 12:03:33 - INFO - __main__ - Global step 800 Train loss 0.002542 Classification-F1 0.8141368628555337 on epoch=199
06/01/2022 12:03:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.006899 on epoch=202
06/01/2022 12:03:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001604 on epoch=204
06/01/2022 12:03:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.004962 on epoch=207
06/01/2022 12:03:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000049 on epoch=209
06/01/2022 12:04:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000551 on epoch=212
06/01/2022 12:04:01 - INFO - __main__ - Global step 850 Train loss 0.002813 Classification-F1 0.848951048951049 on epoch=212
06/01/2022 12:04:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002147 on epoch=214
06/01/2022 12:04:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000289 on epoch=217
06/01/2022 12:04:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000239 on epoch=219
06/01/2022 12:04:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000439 on epoch=222
06/01/2022 12:04:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000126 on epoch=224
06/01/2022 12:04:29 - INFO - __main__ - Global step 900 Train loss 0.000648 Classification-F1 0.848951048951049 on epoch=224
06/01/2022 12:04:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000850 on epoch=227
06/01/2022 12:04:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000305 on epoch=229
06/01/2022 12:04:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000106 on epoch=232
06/01/2022 12:04:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000152 on epoch=234
06/01/2022 12:04:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000510 on epoch=237
06/01/2022 12:04:56 - INFO - __main__ - Global step 950 Train loss 0.000385 Classification-F1 0.8665183537263627 on epoch=237
06/01/2022 12:05:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000303 on epoch=239
06/01/2022 12:05:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000169 on epoch=242
06/01/2022 12:05:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000069 on epoch=244
06/01/2022 12:05:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.004473 on epoch=247
06/01/2022 12:05:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.004193 on epoch=249
06/01/2022 12:05:24 - INFO - __main__ - Global step 1000 Train loss 0.001841 Classification-F1 0.8141368628555337 on epoch=249
06/01/2022 12:05:24 - INFO - __main__ - save last model!
06/01/2022 12:05:25 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:05:25 - INFO - __main__ - Printing 3 examples
06/01/2022 12:05:25 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/01/2022 12:05:25 - INFO - __main__ - ['false']
06/01/2022 12:05:25 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/01/2022 12:05:25 - INFO - __main__ - ['false']
06/01/2022 12:05:25 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/01/2022 12:05:25 - INFO - __main__ - ['false']
06/01/2022 12:05:25 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:05:25 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:05:25 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:05:25 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:05:25 - INFO - __main__ - Printing 3 examples
06/01/2022 12:05:25 - INFO - __main__ -  [ethos-race] Bulgars are Indo Europeans, Turks are not, you ignorant pakisven. Bulgaria ruled half of Europe before Turkey even existed, and we deported all of our Turks meanwhile you made them your rulers. Sven is a fake german country ruled by arabs.
06/01/2022 12:05:25 - INFO - __main__ - ['false']
06/01/2022 12:05:25 - INFO - __main__ -  [ethos-race] The Problem with Islam is that they think they have the right to tell other Human beings what to do or how to be a good Person. What is right and what is wrong. If for example 2 Men love each other, they hurt nobody but you think your God told you to stop them. Who are you to interfere in other ppls Life? Non of your Business! If you think they will Burn for that, then let it be at that. But no, you think you have to stop them bc your God told you. Thats the thing!
06/01/2022 12:05:25 - INFO - __main__ - ['false']
06/01/2022 12:05:25 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 12:05:25 - INFO - __main__ - ['false']
06/01/2022 12:05:25 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:05:25 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:05:25 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:05:31 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 12:05:32 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 12:05:32 - INFO - __main__ - Printing 3 examples
06/01/2022 12:05:32 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 12:05:32 - INFO - __main__ - ['true']
06/01/2022 12:05:32 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 12:05:32 - INFO - __main__ - ['false']
06/01/2022 12:05:32 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 12:05:32 - INFO - __main__ - ['false']
06/01/2022 12:05:32 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:05:32 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:05:32 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 12:05:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_21_0.0002_8_predictions.txt
06/01/2022 12:05:34 - INFO - __main__ - Classification-F1 on test data: 0.7980
06/01/2022 12:05:34 - INFO - __main__ - prefix=ethos-race_32_21, lr=0.0002, bsz=8, dev_performance=0.8665183537263627, test_performance=0.7979876160990713
06/01/2022 12:05:34 - INFO - __main__ - Running ... prefix=ethos-race_32_21, lr=0.0001, bsz=8 ...
06/01/2022 12:05:35 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:05:35 - INFO - __main__ - Printing 3 examples
06/01/2022 12:05:35 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/01/2022 12:05:35 - INFO - __main__ - ['false']
06/01/2022 12:05:35 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/01/2022 12:05:35 - INFO - __main__ - ['false']
06/01/2022 12:05:35 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/01/2022 12:05:35 - INFO - __main__ - ['false']
06/01/2022 12:05:35 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:05:35 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:05:35 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:05:35 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:05:35 - INFO - __main__ - Printing 3 examples
06/01/2022 12:05:35 - INFO - __main__ -  [ethos-race] Bulgars are Indo Europeans, Turks are not, you ignorant pakisven. Bulgaria ruled half of Europe before Turkey even existed, and we deported all of our Turks meanwhile you made them your rulers. Sven is a fake german country ruled by arabs.
06/01/2022 12:05:35 - INFO - __main__ - ['false']
06/01/2022 12:05:35 - INFO - __main__ -  [ethos-race] The Problem with Islam is that they think they have the right to tell other Human beings what to do or how to be a good Person. What is right and what is wrong. If for example 2 Men love each other, they hurt nobody but you think your God told you to stop them. Who are you to interfere in other ppls Life? Non of your Business! If you think they will Burn for that, then let it be at that. But no, you think you have to stop them bc your God told you. Thats the thing!
06/01/2022 12:05:35 - INFO - __main__ - ['false']
06/01/2022 12:05:35 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 12:05:35 - INFO - __main__ - ['false']
06/01/2022 12:05:35 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:05:35 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:05:35 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:05:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:05:36 - INFO - __main__ - Starting training!
06/01/2022 12:05:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:05:46 - INFO - __main__ - Starting training!
06/01/2022 12:05:51 - INFO - __main__ - Step 10 Global step 10 Train loss 24.176464 on epoch=2
06/01/2022 12:05:56 - INFO - __main__ - Step 20 Global step 20 Train loss 21.292715 on epoch=4
06/01/2022 12:06:01 - INFO - __main__ - Step 30 Global step 30 Train loss 18.934605 on epoch=7
06/01/2022 12:06:06 - INFO - __main__ - Step 40 Global step 40 Train loss 18.646656 on epoch=9
06/01/2022 12:06:12 - INFO - __main__ - Step 50 Global step 50 Train loss 17.638029 on epoch=12
06/01/2022 12:06:31 - INFO - __main__ - Global step 50 Train loss 20.137693 Classification-F1 0.0 on epoch=12
06/01/2022 12:06:37 - INFO - __main__ - Step 60 Global step 60 Train loss 17.346827 on epoch=14
06/01/2022 12:06:42 - INFO - __main__ - Step 70 Global step 70 Train loss 16.953981 on epoch=17
06/01/2022 12:06:48 - INFO - __main__ - Step 80 Global step 80 Train loss 16.391527 on epoch=19
06/01/2022 12:06:53 - INFO - __main__ - Step 90 Global step 90 Train loss 16.200159 on epoch=22
06/01/2022 12:06:58 - INFO - __main__ - Step 100 Global step 100 Train loss 16.120314 on epoch=24
06/01/2022 12:07:17 - INFO - __main__ - Global step 100 Train loss 16.602562 Classification-F1 0.0 on epoch=24
06/01/2022 12:07:22 - INFO - __main__ - Step 110 Global step 110 Train loss 15.194132 on epoch=27
06/01/2022 12:07:28 - INFO - __main__ - Step 120 Global step 120 Train loss 15.033358 on epoch=29
06/01/2022 12:07:33 - INFO - __main__ - Step 130 Global step 130 Train loss 14.818311 on epoch=32
06/01/2022 12:07:38 - INFO - __main__ - Step 140 Global step 140 Train loss 14.411716 on epoch=34
06/01/2022 12:07:44 - INFO - __main__ - Step 150 Global step 150 Train loss 13.866353 on epoch=37
06/01/2022 12:08:02 - INFO - __main__ - Global step 150 Train loss 14.664773 Classification-F1 0.0 on epoch=37
06/01/2022 12:08:08 - INFO - __main__ - Step 160 Global step 160 Train loss 14.120875 on epoch=39
06/01/2022 12:08:13 - INFO - __main__ - Step 170 Global step 170 Train loss 13.173566 on epoch=42
06/01/2022 12:08:18 - INFO - __main__ - Step 180 Global step 180 Train loss 12.488090 on epoch=44
06/01/2022 12:08:23 - INFO - __main__ - Step 190 Global step 190 Train loss 11.998438 on epoch=47
06/01/2022 12:08:29 - INFO - __main__ - Step 200 Global step 200 Train loss 10.980894 on epoch=49
06/01/2022 12:08:47 - INFO - __main__ - Global step 200 Train loss 12.552373 Classification-F1 0.0 on epoch=49
06/01/2022 12:08:52 - INFO - __main__ - Step 210 Global step 210 Train loss 9.580146 on epoch=52
06/01/2022 12:08:58 - INFO - __main__ - Step 220 Global step 220 Train loss 8.461283 on epoch=54
06/01/2022 12:09:03 - INFO - __main__ - Step 230 Global step 230 Train loss 5.545454 on epoch=57
06/01/2022 12:09:08 - INFO - __main__ - Step 240 Global step 240 Train loss 5.196177 on epoch=59
06/01/2022 12:09:14 - INFO - __main__ - Step 250 Global step 250 Train loss 2.580329 on epoch=62
06/01/2022 12:09:14 - INFO - __main__ - Global step 250 Train loss 6.272677 Classification-F1 0.52 on epoch=62
06/01/2022 12:09:20 - INFO - __main__ - Step 260 Global step 260 Train loss 1.394425 on epoch=64
06/01/2022 12:09:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.758393 on epoch=67
06/01/2022 12:09:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.827164 on epoch=69
06/01/2022 12:09:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.668782 on epoch=72
06/01/2022 12:09:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.464522 on epoch=74
06/01/2022 12:09:42 - INFO - __main__ - Global step 300 Train loss 0.822657 Classification-F1 0.43340931292738527 on epoch=74
06/01/2022 12:09:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.526789 on epoch=77
06/01/2022 12:09:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.485867 on epoch=79
06/01/2022 12:09:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.451593 on epoch=82
06/01/2022 12:10:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.396481 on epoch=84
06/01/2022 12:10:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.472675 on epoch=87
06/01/2022 12:10:09 - INFO - __main__ - Global step 350 Train loss 0.466681 Classification-F1 0.4505494505494505 on epoch=87
06/01/2022 12:10:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.376178 on epoch=89
06/01/2022 12:10:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.364967 on epoch=92
06/01/2022 12:10:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.397759 on epoch=94
06/01/2022 12:10:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.408658 on epoch=97
06/01/2022 12:10:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.527444 on epoch=99
06/01/2022 12:10:36 - INFO - __main__ - Global step 400 Train loss 0.415001 Classification-F1 0.5970695970695971 on epoch=99
06/01/2022 12:10:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.347726 on epoch=102
06/01/2022 12:10:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.361451 on epoch=104
06/01/2022 12:10:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.318201 on epoch=107
06/01/2022 12:10:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.340853 on epoch=109
06/01/2022 12:11:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.285102 on epoch=112
06/01/2022 12:11:03 - INFO - __main__ - Global step 450 Train loss 0.330667 Classification-F1 0.5278344505974935 on epoch=112
06/01/2022 12:11:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.332184 on epoch=114
06/01/2022 12:11:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.280342 on epoch=117
06/01/2022 12:11:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.375185 on epoch=119
06/01/2022 12:11:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.316792 on epoch=122
06/01/2022 12:11:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.279150 on epoch=124
06/01/2022 12:11:31 - INFO - __main__ - Global step 500 Train loss 0.316731 Classification-F1 0.6986607142857143 on epoch=124
06/01/2022 12:11:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.242864 on epoch=127
06/01/2022 12:11:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.225688 on epoch=129
06/01/2022 12:11:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.220312 on epoch=132
06/01/2022 12:11:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.193234 on epoch=134
06/01/2022 12:11:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.303399 on epoch=137
06/01/2022 12:11:59 - INFO - __main__ - Global step 550 Train loss 0.237099 Classification-F1 0.7127569698676428 on epoch=137
06/01/2022 12:12:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.257702 on epoch=139
06/01/2022 12:12:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.205255 on epoch=142
06/01/2022 12:12:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.246363 on epoch=144
06/01/2022 12:12:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.210694 on epoch=147
06/01/2022 12:12:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.176534 on epoch=149
06/01/2022 12:12:27 - INFO - __main__ - Global step 600 Train loss 0.219310 Classification-F1 0.7257142857142858 on epoch=149
06/01/2022 12:12:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.212847 on epoch=152
06/01/2022 12:12:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.218378 on epoch=154
06/01/2022 12:12:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.181734 on epoch=157
06/01/2022 12:12:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.156659 on epoch=159
06/01/2022 12:12:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.194481 on epoch=162
06/01/2022 12:12:55 - INFO - __main__ - Global step 650 Train loss 0.192820 Classification-F1 0.783273131425396 on epoch=162
06/01/2022 12:13:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.177974 on epoch=164
06/01/2022 12:13:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.226824 on epoch=167
06/01/2022 12:13:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.181661 on epoch=169
06/01/2022 12:13:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.244232 on epoch=172
06/01/2022 12:13:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.157060 on epoch=174
06/01/2022 12:13:23 - INFO - __main__ - Global step 700 Train loss 0.197550 Classification-F1 0.7757976430008624 on epoch=174
06/01/2022 12:13:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.171519 on epoch=177
06/01/2022 12:13:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.155648 on epoch=179
06/01/2022 12:13:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.175190 on epoch=182
06/01/2022 12:13:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.175510 on epoch=184
06/01/2022 12:13:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.142549 on epoch=187
06/01/2022 12:13:50 - INFO - __main__ - Global step 750 Train loss 0.164083 Classification-F1 0.5832074901842343 on epoch=187
06/01/2022 12:13:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.248299 on epoch=189
06/01/2022 12:14:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.344858 on epoch=192
06/01/2022 12:14:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.366498 on epoch=194
06/01/2022 12:14:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.141706 on epoch=197
06/01/2022 12:14:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.166631 on epoch=199
06/01/2022 12:14:17 - INFO - __main__ - Global step 800 Train loss 0.253598 Classification-F1 0.7179788484136309 on epoch=199
06/01/2022 12:14:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.200280 on epoch=202
06/01/2022 12:14:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.119493 on epoch=204
06/01/2022 12:14:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.104002 on epoch=207
06/01/2022 12:14:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.120160 on epoch=209
06/01/2022 12:14:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.146489 on epoch=212
06/01/2022 12:14:44 - INFO - __main__ - Global step 850 Train loss 0.138085 Classification-F1 0.783273131425396 on epoch=212
06/01/2022 12:14:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.099510 on epoch=214
06/01/2022 12:14:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.122496 on epoch=217
06/01/2022 12:15:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.094314 on epoch=219
06/01/2022 12:15:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.144218 on epoch=222
06/01/2022 12:15:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.172995 on epoch=224
06/01/2022 12:15:11 - INFO - __main__ - Global step 900 Train loss 0.126707 Classification-F1 0.7179788484136309 on epoch=224
06/01/2022 12:15:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.096674 on epoch=227
06/01/2022 12:15:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.068340 on epoch=229
06/01/2022 12:15:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.079402 on epoch=232
06/01/2022 12:15:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.083199 on epoch=234
06/01/2022 12:15:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.108825 on epoch=237
06/01/2022 12:15:38 - INFO - __main__ - Global step 950 Train loss 0.087288 Classification-F1 0.7997775305895439 on epoch=237
06/01/2022 12:15:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.037367 on epoch=239
06/01/2022 12:15:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.100054 on epoch=242
06/01/2022 12:15:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.085949 on epoch=244
06/01/2022 12:16:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.122208 on epoch=247
06/01/2022 12:16:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.069996 on epoch=249
06/01/2022 12:16:06 - INFO - __main__ - Global step 1000 Train loss 0.083115 Classification-F1 0.610991046619327 on epoch=249
06/01/2022 12:16:06 - INFO - __main__ - save last model!
06/01/2022 12:16:07 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:16:07 - INFO - __main__ - Printing 3 examples
06/01/2022 12:16:07 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
06/01/2022 12:16:07 - INFO - __main__ - ['false']
06/01/2022 12:16:07 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
06/01/2022 12:16:07 - INFO - __main__ - ['false']
06/01/2022 12:16:07 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
06/01/2022 12:16:07 - INFO - __main__ - ['false']
06/01/2022 12:16:07 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:16:07 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:16:07 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:16:07 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:16:07 - INFO - __main__ - Printing 3 examples
06/01/2022 12:16:07 - INFO - __main__ -  [ethos-race] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/01/2022 12:16:07 - INFO - __main__ - ['false']
06/01/2022 12:16:07 - INFO - __main__ -  [ethos-race] How is it to have a kid acting like a sloth or like a pug dog? Isn’t it hard? In your place I’d put it in a dedicated institute for problematics and pay a visit each year at max
06/01/2022 12:16:07 - INFO - __main__ - ['false']
06/01/2022 12:16:07 - INFO - __main__ -  [ethos-race] Watching this kid trying to step up and walk with that crutches is the best thing in the world. So idiot
06/01/2022 12:16:07 - INFO - __main__ - ['false']
06/01/2022 12:16:07 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:16:07 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:16:07 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:16:14 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 12:16:14 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 12:16:14 - INFO - __main__ - Printing 3 examples
06/01/2022 12:16:14 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 12:16:14 - INFO - __main__ - ['true']
06/01/2022 12:16:14 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 12:16:14 - INFO - __main__ - ['false']
06/01/2022 12:16:14 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 12:16:14 - INFO - __main__ - ['false']
06/01/2022 12:16:14 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:16:14 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:16:14 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 12:16:16 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_21_0.0001_8_predictions.txt
06/01/2022 12:16:16 - INFO - __main__ - Classification-F1 on test data: 0.6648
06/01/2022 12:16:16 - INFO - __main__ - prefix=ethos-race_32_21, lr=0.0001, bsz=8, dev_performance=0.7997775305895439, test_performance=0.6647679678338079
06/01/2022 12:16:16 - INFO - __main__ - Running ... prefix=ethos-race_32_42, lr=0.0005, bsz=8 ...
06/01/2022 12:16:17 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:16:17 - INFO - __main__ - Printing 3 examples
06/01/2022 12:16:17 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
06/01/2022 12:16:17 - INFO - __main__ - ['false']
06/01/2022 12:16:17 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
06/01/2022 12:16:17 - INFO - __main__ - ['false']
06/01/2022 12:16:17 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
06/01/2022 12:16:17 - INFO - __main__ - ['false']
06/01/2022 12:16:17 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:16:17 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:16:17 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:16:17 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:16:17 - INFO - __main__ - Printing 3 examples
06/01/2022 12:16:17 - INFO - __main__ -  [ethos-race] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/01/2022 12:16:17 - INFO - __main__ - ['false']
06/01/2022 12:16:17 - INFO - __main__ -  [ethos-race] How is it to have a kid acting like a sloth or like a pug dog? Isn’t it hard? In your place I’d put it in a dedicated institute for problematics and pay a visit each year at max
06/01/2022 12:16:17 - INFO - __main__ - ['false']
06/01/2022 12:16:17 - INFO - __main__ -  [ethos-race] Watching this kid trying to step up and walk with that crutches is the best thing in the world. So idiot
06/01/2022 12:16:17 - INFO - __main__ - ['false']
06/01/2022 12:16:17 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:16:17 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:16:18 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:16:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:16:20 - INFO - __main__ - Starting training!
06/01/2022 12:16:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:16:31 - INFO - __main__ - Starting training!
06/01/2022 12:16:36 - INFO - __main__ - Step 10 Global step 10 Train loss 23.938009 on epoch=2
06/01/2022 12:16:41 - INFO - __main__ - Step 20 Global step 20 Train loss 19.258909 on epoch=4
06/01/2022 12:16:46 - INFO - __main__ - Step 30 Global step 30 Train loss 15.590919 on epoch=7
06/01/2022 12:16:52 - INFO - __main__ - Step 40 Global step 40 Train loss 14.123647 on epoch=9
06/01/2022 12:16:57 - INFO - __main__ - Step 50 Global step 50 Train loss 11.029534 on epoch=12
06/01/2022 12:16:58 - INFO - __main__ - Global step 50 Train loss 16.788204 Classification-F1 0.0 on epoch=12
06/01/2022 12:17:03 - INFO - __main__ - Step 60 Global step 60 Train loss 4.915534 on epoch=14
06/01/2022 12:17:09 - INFO - __main__ - Step 70 Global step 70 Train loss 3.052511 on epoch=17
06/01/2022 12:17:14 - INFO - __main__ - Step 80 Global step 80 Train loss 3.166884 on epoch=19
06/01/2022 12:17:20 - INFO - __main__ - Step 90 Global step 90 Train loss 1.773262 on epoch=22
06/01/2022 12:17:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.895192 on epoch=24
06/01/2022 12:17:26 - INFO - __main__ - Global step 100 Train loss 2.760677 Classification-F1 0.5132867132867133 on epoch=24
06/01/2022 12:17:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.433175 on epoch=27
06/01/2022 12:17:37 - INFO - __main__ - Step 120 Global step 120 Train loss 1.065100 on epoch=29
06/01/2022 12:17:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.749786 on epoch=32
06/01/2022 12:17:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.705141 on epoch=34
06/01/2022 12:17:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.846317 on epoch=37
06/01/2022 12:17:53 - INFO - __main__ - Global step 150 Train loss 0.759904 Classification-F1 0.3478260869565218 on epoch=37
06/01/2022 12:17:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.435782 on epoch=39
06/01/2022 12:18:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.512330 on epoch=42
06/01/2022 12:18:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.749678 on epoch=44
06/01/2022 12:18:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.431134 on epoch=47
06/01/2022 12:18:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.393886 on epoch=49
06/01/2022 12:18:21 - INFO - __main__ - Global step 200 Train loss 0.504562 Classification-F1 0.4097222222222222 on epoch=49
06/01/2022 12:18:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.356538 on epoch=52
06/01/2022 12:18:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.429676 on epoch=54
06/01/2022 12:18:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.403414 on epoch=57
06/01/2022 12:18:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.333500 on epoch=59
06/01/2022 12:18:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.384435 on epoch=62
06/01/2022 12:18:48 - INFO - __main__ - Global step 250 Train loss 0.381513 Classification-F1 0.4608472400513479 on epoch=62
06/01/2022 12:18:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.423642 on epoch=64
06/01/2022 12:18:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.398534 on epoch=67
06/01/2022 12:19:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.363967 on epoch=69
06/01/2022 12:19:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.386139 on epoch=72
06/01/2022 12:19:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.376291 on epoch=74
06/01/2022 12:19:15 - INFO - __main__ - Global step 300 Train loss 0.389715 Classification-F1 0.41333333333333333 on epoch=74
06/01/2022 12:19:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.328008 on epoch=77
06/01/2022 12:19:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.316447 on epoch=79
06/01/2022 12:19:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.310557 on epoch=82
06/01/2022 12:19:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.307306 on epoch=84
06/01/2022 12:19:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.375484 on epoch=87
06/01/2022 12:19:43 - INFO - __main__ - Global step 350 Train loss 0.327560 Classification-F1 0.4505494505494505 on epoch=87
06/01/2022 12:19:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.338409 on epoch=89
06/01/2022 12:19:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.321692 on epoch=92
06/01/2022 12:19:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.336390 on epoch=94
06/01/2022 12:20:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.348962 on epoch=97
06/01/2022 12:20:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.333127 on epoch=99
06/01/2022 12:20:10 - INFO - __main__ - Global step 400 Train loss 0.335716 Classification-F1 0.4994438264738599 on epoch=99
06/01/2022 12:20:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.300694 on epoch=102
06/01/2022 12:20:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.443280 on epoch=104
06/01/2022 12:20:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.418360 on epoch=107
06/01/2022 12:20:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.370451 on epoch=109
06/01/2022 12:20:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.288891 on epoch=112
06/01/2022 12:20:38 - INFO - __main__ - Global step 450 Train loss 0.364335 Classification-F1 0.4871794871794871 on epoch=112
06/01/2022 12:20:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.282110 on epoch=114
06/01/2022 12:20:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.260086 on epoch=117
06/01/2022 12:20:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.341483 on epoch=119
06/01/2022 12:20:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.260687 on epoch=122
06/01/2022 12:21:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.194090 on epoch=124
06/01/2022 12:21:05 - INFO - __main__ - Global step 500 Train loss 0.267691 Classification-F1 0.6534017971758664 on epoch=124
06/01/2022 12:21:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.204556 on epoch=127
06/01/2022 12:21:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.207622 on epoch=129
06/01/2022 12:21:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.219309 on epoch=132
06/01/2022 12:21:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.193723 on epoch=134
06/01/2022 12:21:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.146688 on epoch=137
06/01/2022 12:21:34 - INFO - __main__ - Global step 550 Train loss 0.194380 Classification-F1 0.6770334928229664 on epoch=137
06/01/2022 12:21:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.215076 on epoch=139
06/01/2022 12:21:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.133670 on epoch=142
06/01/2022 12:21:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.132749 on epoch=144
06/01/2022 12:21:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.148040 on epoch=147
06/01/2022 12:22:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.112180 on epoch=149
06/01/2022 12:22:02 - INFO - __main__ - Global step 600 Train loss 0.148343 Classification-F1 0.6723196320781835 on epoch=149
06/01/2022 12:22:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.131868 on epoch=152
06/01/2022 12:22:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.104756 on epoch=154
06/01/2022 12:22:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.130070 on epoch=157
06/01/2022 12:22:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.139731 on epoch=159
06/01/2022 12:22:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.134009 on epoch=162
06/01/2022 12:22:30 - INFO - __main__ - Global step 650 Train loss 0.128087 Classification-F1 0.7330367074527252 on epoch=162
06/01/2022 12:22:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.099955 on epoch=164
06/01/2022 12:22:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.090802 on epoch=167
06/01/2022 12:22:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.126843 on epoch=169
06/01/2022 12:22:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.101178 on epoch=172
06/01/2022 12:22:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.072886 on epoch=174
06/01/2022 12:22:58 - INFO - __main__ - Global step 700 Train loss 0.098333 Classification-F1 0.7664071190211346 on epoch=174
06/01/2022 12:23:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.061328 on epoch=177
06/01/2022 12:23:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.083011 on epoch=179
06/01/2022 12:23:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.083313 on epoch=182
06/01/2022 12:23:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.062248 on epoch=184
06/01/2022 12:23:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.067419 on epoch=187
06/01/2022 12:23:26 - INFO - __main__ - Global step 750 Train loss 0.071464 Classification-F1 0.6122209165687424 on epoch=187
06/01/2022 12:23:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.041173 on epoch=189
06/01/2022 12:23:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.071932 on epoch=192
06/01/2022 12:23:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.065189 on epoch=194
06/01/2022 12:23:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.088485 on epoch=197
06/01/2022 12:23:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.075312 on epoch=199
06/01/2022 12:23:53 - INFO - __main__ - Global step 800 Train loss 0.068418 Classification-F1 0.797979797979798 on epoch=199
06/01/2022 12:23:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.095885 on epoch=202
06/01/2022 12:24:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.030501 on epoch=204
06/01/2022 12:24:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.070724 on epoch=207
06/01/2022 12:24:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.041514 on epoch=209
06/01/2022 12:24:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.016671 on epoch=212
06/01/2022 12:24:22 - INFO - __main__ - Global step 850 Train loss 0.051059 Classification-F1 0.8141368628555337 on epoch=212
06/01/2022 12:24:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.021035 on epoch=214
06/01/2022 12:24:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.027802 on epoch=217
06/01/2022 12:24:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.019956 on epoch=219
06/01/2022 12:24:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.020143 on epoch=222
06/01/2022 12:24:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.019179 on epoch=224
06/01/2022 12:24:50 - INFO - __main__ - Global step 900 Train loss 0.021623 Classification-F1 0.8303167420814479 on epoch=224
06/01/2022 12:24:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.079851 on epoch=227
06/01/2022 12:25:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.006512 on epoch=229
06/01/2022 12:25:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.042215 on epoch=232
06/01/2022 12:25:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.091603 on epoch=234
06/01/2022 12:25:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.008462 on epoch=237
06/01/2022 12:25:18 - INFO - __main__ - Global step 950 Train loss 0.045729 Classification-F1 0.846547314578005 on epoch=237
06/01/2022 12:25:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001400 on epoch=239
06/01/2022 12:25:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.002218 on epoch=242
06/01/2022 12:25:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001191 on epoch=244
06/01/2022 12:25:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.014284 on epoch=247
06/01/2022 12:25:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.012595 on epoch=249
06/01/2022 12:25:46 - INFO - __main__ - Global step 1000 Train loss 0.006338 Classification-F1 0.7493734335839599 on epoch=249
06/01/2022 12:25:46 - INFO - __main__ - save last model!
06/01/2022 12:25:47 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:25:47 - INFO - __main__ - Printing 3 examples
06/01/2022 12:25:47 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
06/01/2022 12:25:47 - INFO - __main__ - ['false']
06/01/2022 12:25:47 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
06/01/2022 12:25:47 - INFO - __main__ - ['false']
06/01/2022 12:25:47 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
06/01/2022 12:25:47 - INFO - __main__ - ['false']
06/01/2022 12:25:47 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:25:47 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:25:47 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:25:47 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:25:47 - INFO - __main__ - Printing 3 examples
06/01/2022 12:25:47 - INFO - __main__ -  [ethos-race] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/01/2022 12:25:47 - INFO - __main__ - ['false']
06/01/2022 12:25:47 - INFO - __main__ -  [ethos-race] How is it to have a kid acting like a sloth or like a pug dog? Isn’t it hard? In your place I’d put it in a dedicated institute for problematics and pay a visit each year at max
06/01/2022 12:25:47 - INFO - __main__ - ['false']
06/01/2022 12:25:47 - INFO - __main__ -  [ethos-race] Watching this kid trying to step up and walk with that crutches is the best thing in the world. So idiot
06/01/2022 12:25:47 - INFO - __main__ - ['false']
06/01/2022 12:25:47 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:25:47 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:25:47 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:25:52 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 12:25:53 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 12:25:53 - INFO - __main__ - Printing 3 examples
06/01/2022 12:25:53 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 12:25:53 - INFO - __main__ - ['true']
06/01/2022 12:25:53 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 12:25:53 - INFO - __main__ - ['false']
06/01/2022 12:25:53 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 12:25:53 - INFO - __main__ - ['false']
06/01/2022 12:25:53 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:25:53 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:25:53 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 12:25:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_42_0.0005_8_predictions.txt
06/01/2022 12:25:55 - INFO - __main__ - Classification-F1 on test data: 0.8851
06/01/2022 12:25:55 - INFO - __main__ - prefix=ethos-race_32_42, lr=0.0005, bsz=8, dev_performance=0.846547314578005, test_performance=0.8851232394366197
06/01/2022 12:25:55 - INFO - __main__ - Running ... prefix=ethos-race_32_42, lr=0.0003, bsz=8 ...
06/01/2022 12:25:56 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:25:56 - INFO - __main__ - Printing 3 examples
06/01/2022 12:25:56 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
06/01/2022 12:25:56 - INFO - __main__ - ['false']
06/01/2022 12:25:56 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
06/01/2022 12:25:56 - INFO - __main__ - ['false']
06/01/2022 12:25:56 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
06/01/2022 12:25:56 - INFO - __main__ - ['false']
06/01/2022 12:25:56 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:25:56 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:25:56 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:25:56 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:25:56 - INFO - __main__ - Printing 3 examples
06/01/2022 12:25:56 - INFO - __main__ -  [ethos-race] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/01/2022 12:25:56 - INFO - __main__ - ['false']
06/01/2022 12:25:56 - INFO - __main__ -  [ethos-race] How is it to have a kid acting like a sloth or like a pug dog? Isn’t it hard? In your place I’d put it in a dedicated institute for problematics and pay a visit each year at max
06/01/2022 12:25:56 - INFO - __main__ - ['false']
06/01/2022 12:25:56 - INFO - __main__ -  [ethos-race] Watching this kid trying to step up and walk with that crutches is the best thing in the world. So idiot
06/01/2022 12:25:56 - INFO - __main__ - ['false']
06/01/2022 12:25:56 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:25:56 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:25:56 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:25:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:25:58 - INFO - __main__ - Starting training!
06/01/2022 12:26:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:26:09 - INFO - __main__ - Starting training!
06/01/2022 12:26:14 - INFO - __main__ - Step 10 Global step 10 Train loss 24.761467 on epoch=2
06/01/2022 12:26:19 - INFO - __main__ - Step 20 Global step 20 Train loss 20.522175 on epoch=4
06/01/2022 12:26:24 - INFO - __main__ - Step 30 Global step 30 Train loss 18.134928 on epoch=7
06/01/2022 12:26:30 - INFO - __main__ - Step 40 Global step 40 Train loss 16.418552 on epoch=9
06/01/2022 12:26:35 - INFO - __main__ - Step 50 Global step 50 Train loss 14.421141 on epoch=12
06/01/2022 12:26:47 - INFO - __main__ - Global step 50 Train loss 18.851652 Classification-F1 0.0 on epoch=12
06/01/2022 12:26:53 - INFO - __main__ - Step 60 Global step 60 Train loss 13.585991 on epoch=14
06/01/2022 12:26:58 - INFO - __main__ - Step 70 Global step 70 Train loss 12.551434 on epoch=17
06/01/2022 12:27:03 - INFO - __main__ - Step 80 Global step 80 Train loss 10.325124 on epoch=19
06/01/2022 12:27:08 - INFO - __main__ - Step 90 Global step 90 Train loss 5.028031 on epoch=22
06/01/2022 12:27:14 - INFO - __main__ - Step 100 Global step 100 Train loss 2.967502 on epoch=24
06/01/2022 12:27:14 - INFO - __main__ - Global step 100 Train loss 8.891617 Classification-F1 0.38613111026904134 on epoch=24
06/01/2022 12:27:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.600025 on epoch=27
06/01/2022 12:27:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.440183 on epoch=29
06/01/2022 12:27:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.517355 on epoch=32
06/01/2022 12:27:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.360822 on epoch=34
06/01/2022 12:27:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.250302 on epoch=37
06/01/2022 12:27:42 - INFO - __main__ - Global step 150 Train loss 0.433737 Classification-F1 0.49285922471582627 on epoch=37
06/01/2022 12:27:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.291446 on epoch=39
06/01/2022 12:27:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.250482 on epoch=42
06/01/2022 12:27:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.261592 on epoch=44
06/01/2022 12:28:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.205223 on epoch=47
06/01/2022 12:28:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.101591 on epoch=49
06/01/2022 12:28:10 - INFO - __main__ - Global step 200 Train loss 0.222067 Classification-F1 0.7482517482517482 on epoch=49
06/01/2022 12:28:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.110930 on epoch=52
06/01/2022 12:28:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.177941 on epoch=54
06/01/2022 12:28:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.102865 on epoch=57
06/01/2022 12:28:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.101096 on epoch=59
06/01/2022 12:28:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.076437 on epoch=62
06/01/2022 12:28:38 - INFO - __main__ - Global step 250 Train loss 0.113854 Classification-F1 0.5911111111111111 on epoch=62
06/01/2022 12:28:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.077718 on epoch=64
06/01/2022 12:28:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.112599 on epoch=67
06/01/2022 12:28:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.100325 on epoch=69
06/01/2022 12:28:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.288117 on epoch=72
06/01/2022 12:29:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.443875 on epoch=74
06/01/2022 12:29:05 - INFO - __main__ - Global step 300 Train loss 0.204527 Classification-F1 0.7726610317691635 on epoch=74
06/01/2022 12:29:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.222604 on epoch=77
06/01/2022 12:29:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.077526 on epoch=79
06/01/2022 12:29:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.164875 on epoch=82
06/01/2022 12:29:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.102044 on epoch=84
06/01/2022 12:29:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.290939 on epoch=87
06/01/2022 12:29:32 - INFO - __main__ - Global step 350 Train loss 0.171598 Classification-F1 0.5604395604395604 on epoch=87
06/01/2022 12:29:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.231191 on epoch=89
06/01/2022 12:29:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.148100 on epoch=92
06/01/2022 12:29:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.168078 on epoch=94
06/01/2022 12:29:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.094221 on epoch=97
06/01/2022 12:29:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.076343 on epoch=99
06/01/2022 12:30:00 - INFO - __main__ - Global step 400 Train loss 0.143587 Classification-F1 0.765625 on epoch=99
06/01/2022 12:30:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.102526 on epoch=102
06/01/2022 12:30:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.078653 on epoch=104
06/01/2022 12:30:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.059985 on epoch=107
06/01/2022 12:30:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.077563 on epoch=109
06/01/2022 12:30:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.111804 on epoch=112
06/01/2022 12:30:27 - INFO - __main__ - Global step 450 Train loss 0.086106 Classification-F1 0.7146853146853146 on epoch=112
06/01/2022 12:30:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.086339 on epoch=114
06/01/2022 12:30:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.094076 on epoch=117
06/01/2022 12:30:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.008355 on epoch=119
06/01/2022 12:30:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.053917 on epoch=122
06/01/2022 12:30:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013714 on epoch=124
06/01/2022 12:30:54 - INFO - __main__ - Global step 500 Train loss 0.051280 Classification-F1 0.7306397306397308 on epoch=124
06/01/2022 12:30:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.006767 on epoch=127
06/01/2022 12:31:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.009471 on epoch=129
06/01/2022 12:31:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.025497 on epoch=132
06/01/2022 12:31:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.006767 on epoch=134
06/01/2022 12:31:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.023847 on epoch=137
06/01/2022 12:31:21 - INFO - __main__ - Global step 550 Train loss 0.014470 Classification-F1 0.7997775305895439 on epoch=137
06/01/2022 12:31:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.006658 on epoch=139
06/01/2022 12:31:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.028405 on epoch=142
06/01/2022 12:31:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.037541 on epoch=144
06/01/2022 12:31:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.026265 on epoch=147
06/01/2022 12:31:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002133 on epoch=149
06/01/2022 12:31:49 - INFO - __main__ - Global step 600 Train loss 0.020201 Classification-F1 0.783273131425396 on epoch=149
06/01/2022 12:31:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001765 on epoch=152
06/01/2022 12:31:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.005383 on epoch=154
06/01/2022 12:32:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.005287 on epoch=157
06/01/2022 12:32:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.005649 on epoch=159
06/01/2022 12:32:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001986 on epoch=162
06/01/2022 12:32:16 - INFO - __main__ - Global step 650 Train loss 0.004014 Classification-F1 0.7997775305895439 on epoch=162
06/01/2022 12:32:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.006319 on epoch=164
06/01/2022 12:32:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000405 on epoch=167
06/01/2022 12:32:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.004816 on epoch=169
06/01/2022 12:32:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000437 on epoch=172
06/01/2022 12:32:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001308 on epoch=174
06/01/2022 12:32:43 - INFO - __main__ - Global step 700 Train loss 0.002657 Classification-F1 0.8325892857142857 on epoch=174
06/01/2022 12:32:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.012134 on epoch=177
06/01/2022 12:32:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000131 on epoch=179
06/01/2022 12:32:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.003063 on epoch=182
06/01/2022 12:33:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.039003 on epoch=184
06/01/2022 12:33:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000423 on epoch=187
06/01/2022 12:33:11 - INFO - __main__ - Global step 750 Train loss 0.010951 Classification-F1 0.7997775305895439 on epoch=187
06/01/2022 12:33:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.002783 on epoch=189
06/01/2022 12:33:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.012590 on epoch=192
06/01/2022 12:33:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.005371 on epoch=194
06/01/2022 12:33:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000178 on epoch=197
06/01/2022 12:33:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.004221 on epoch=199
06/01/2022 12:33:38 - INFO - __main__ - Global step 800 Train loss 0.005029 Classification-F1 0.765625 on epoch=199
06/01/2022 12:33:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000847 on epoch=202
06/01/2022 12:33:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001369 on epoch=204
06/01/2022 12:33:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000400 on epoch=207
06/01/2022 12:33:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000595 on epoch=209
06/01/2022 12:34:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000184 on epoch=212
06/01/2022 12:34:05 - INFO - __main__ - Global step 850 Train loss 0.000679 Classification-F1 0.7827903091060985 on epoch=212
06/01/2022 12:34:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000127 on epoch=214
06/01/2022 12:34:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000124 on epoch=217
06/01/2022 12:34:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000044 on epoch=219
06/01/2022 12:34:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000530 on epoch=222
06/01/2022 12:34:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000246 on epoch=224
06/01/2022 12:34:32 - INFO - __main__ - Global step 900 Train loss 0.000214 Classification-F1 0.7333333333333334 on epoch=224
06/01/2022 12:34:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.002556 on epoch=227
06/01/2022 12:34:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.001380 on epoch=229
06/01/2022 12:34:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000613 on epoch=232
06/01/2022 12:34:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.003282 on epoch=234
06/01/2022 12:34:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.055033 on epoch=237
06/01/2022 12:35:00 - INFO - __main__ - Global step 950 Train loss 0.012573 Classification-F1 0.37211981566820274 on epoch=237
06/01/2022 12:35:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000428 on epoch=239
06/01/2022 12:35:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.023356 on epoch=242
06/01/2022 12:35:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.003585 on epoch=244
06/01/2022 12:35:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.004276 on epoch=247
06/01/2022 12:35:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000249 on epoch=249
06/01/2022 12:35:27 - INFO - __main__ - Global step 1000 Train loss 0.006379 Classification-F1 0.5042396834369701 on epoch=249
06/01/2022 12:35:27 - INFO - __main__ - save last model!
06/01/2022 12:35:28 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:35:28 - INFO - __main__ - Printing 3 examples
06/01/2022 12:35:28 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
06/01/2022 12:35:28 - INFO - __main__ - ['false']
06/01/2022 12:35:28 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
06/01/2022 12:35:28 - INFO - __main__ - ['false']
06/01/2022 12:35:28 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
06/01/2022 12:35:28 - INFO - __main__ - ['false']
06/01/2022 12:35:28 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:35:28 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:35:28 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:35:28 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:35:28 - INFO - __main__ - Printing 3 examples
06/01/2022 12:35:28 - INFO - __main__ -  [ethos-race] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/01/2022 12:35:28 - INFO - __main__ - ['false']
06/01/2022 12:35:28 - INFO - __main__ -  [ethos-race] How is it to have a kid acting like a sloth or like a pug dog? Isn’t it hard? In your place I’d put it in a dedicated institute for problematics and pay a visit each year at max
06/01/2022 12:35:28 - INFO - __main__ - ['false']
06/01/2022 12:35:28 - INFO - __main__ -  [ethos-race] Watching this kid trying to step up and walk with that crutches is the best thing in the world. So idiot
06/01/2022 12:35:28 - INFO - __main__ - ['false']
06/01/2022 12:35:28 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:35:28 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:35:28 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:35:35 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 12:35:35 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 12:35:35 - INFO - __main__ - Printing 3 examples
06/01/2022 12:35:35 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 12:35:35 - INFO - __main__ - ['true']
06/01/2022 12:35:35 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 12:35:35 - INFO - __main__ - ['false']
06/01/2022 12:35:35 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 12:35:35 - INFO - __main__ - ['false']
06/01/2022 12:35:35 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:35:35 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:35:35 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 12:35:37 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_42_0.0003_8_predictions.txt
06/01/2022 12:35:37 - INFO - __main__ - Classification-F1 on test data: 0.7683
06/01/2022 12:35:37 - INFO - __main__ - prefix=ethos-race_32_42, lr=0.0003, bsz=8, dev_performance=0.8325892857142857, test_performance=0.7683294869518906
06/01/2022 12:35:37 - INFO - __main__ - Running ... prefix=ethos-race_32_42, lr=0.0002, bsz=8 ...
06/01/2022 12:35:38 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:35:38 - INFO - __main__ - Printing 3 examples
06/01/2022 12:35:38 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
06/01/2022 12:35:38 - INFO - __main__ - ['false']
06/01/2022 12:35:38 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
06/01/2022 12:35:38 - INFO - __main__ - ['false']
06/01/2022 12:35:38 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
06/01/2022 12:35:38 - INFO - __main__ - ['false']
06/01/2022 12:35:38 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:35:38 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:35:38 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:35:38 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:35:38 - INFO - __main__ - Printing 3 examples
06/01/2022 12:35:38 - INFO - __main__ -  [ethos-race] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/01/2022 12:35:38 - INFO - __main__ - ['false']
06/01/2022 12:35:38 - INFO - __main__ -  [ethos-race] How is it to have a kid acting like a sloth or like a pug dog? Isn’t it hard? In your place I’d put it in a dedicated institute for problematics and pay a visit each year at max
06/01/2022 12:35:38 - INFO - __main__ - ['false']
06/01/2022 12:35:38 - INFO - __main__ -  [ethos-race] Watching this kid trying to step up and walk with that crutches is the best thing in the world. So idiot
06/01/2022 12:35:38 - INFO - __main__ - ['false']
06/01/2022 12:35:38 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:35:38 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:35:38 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:35:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:35:39 - INFO - __main__ - Starting training!
06/01/2022 12:35:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:35:51 - INFO - __main__ - Starting training!
06/01/2022 12:35:56 - INFO - __main__ - Step 10 Global step 10 Train loss 25.365755 on epoch=2
06/01/2022 12:36:01 - INFO - __main__ - Step 20 Global step 20 Train loss 20.823883 on epoch=4
06/01/2022 12:36:06 - INFO - __main__ - Step 30 Global step 30 Train loss 18.516623 on epoch=7
06/01/2022 12:36:11 - INFO - __main__ - Step 40 Global step 40 Train loss 17.448523 on epoch=9
06/01/2022 12:36:16 - INFO - __main__ - Step 50 Global step 50 Train loss 16.567135 on epoch=12
06/01/2022 12:36:17 - INFO - __main__ - Global step 50 Train loss 19.744383 Classification-F1 0.0 on epoch=12
06/01/2022 12:36:23 - INFO - __main__ - Step 60 Global step 60 Train loss 15.605087 on epoch=14
06/01/2022 12:36:28 - INFO - __main__ - Step 70 Global step 70 Train loss 15.258821 on epoch=17
06/01/2022 12:36:33 - INFO - __main__ - Step 80 Global step 80 Train loss 13.848738 on epoch=19
06/01/2022 12:36:39 - INFO - __main__ - Step 90 Global step 90 Train loss 13.548884 on epoch=22
06/01/2022 12:36:44 - INFO - __main__ - Step 100 Global step 100 Train loss 12.932837 on epoch=24
06/01/2022 12:36:45 - INFO - __main__ - Global step 100 Train loss 14.238874 Classification-F1 0.0 on epoch=24
06/01/2022 12:36:50 - INFO - __main__ - Step 110 Global step 110 Train loss 11.038355 on epoch=27
06/01/2022 12:36:55 - INFO - __main__ - Step 120 Global step 120 Train loss 8.143515 on epoch=29
06/01/2022 12:37:00 - INFO - __main__ - Step 130 Global step 130 Train loss 5.460843 on epoch=32
06/01/2022 12:37:05 - INFO - __main__ - Step 140 Global step 140 Train loss 2.322901 on epoch=34
06/01/2022 12:37:10 - INFO - __main__ - Step 150 Global step 150 Train loss 3.075244 on epoch=37
06/01/2022 12:37:11 - INFO - __main__ - Global step 150 Train loss 6.008172 Classification-F1 0.4034090909090909 on epoch=37
06/01/2022 12:37:17 - INFO - __main__ - Step 160 Global step 160 Train loss 2.344586 on epoch=39
06/01/2022 12:37:22 - INFO - __main__ - Step 170 Global step 170 Train loss 1.012047 on epoch=42
06/01/2022 12:37:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.583006 on epoch=44
06/01/2022 12:37:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.616970 on epoch=47
06/01/2022 12:37:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.569915 on epoch=49
06/01/2022 12:37:38 - INFO - __main__ - Global step 200 Train loss 1.025305 Classification-F1 0.5378690629011553 on epoch=49
06/01/2022 12:37:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.499914 on epoch=52
06/01/2022 12:37:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.377647 on epoch=54
06/01/2022 12:37:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.353551 on epoch=57
06/01/2022 12:38:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.417697 on epoch=59
06/01/2022 12:38:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.408458 on epoch=62
06/01/2022 12:38:06 - INFO - __main__ - Global step 250 Train loss 0.411453 Classification-F1 0.47447900936273024 on epoch=62
06/01/2022 12:38:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.473319 on epoch=64
06/01/2022 12:38:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.355445 on epoch=67
06/01/2022 12:38:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.394105 on epoch=69
06/01/2022 12:38:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.365876 on epoch=72
06/01/2022 12:38:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.343885 on epoch=74
06/01/2022 12:38:32 - INFO - __main__ - Global step 300 Train loss 0.386526 Classification-F1 0.45632475534613987 on epoch=74
06/01/2022 12:38:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.410940 on epoch=77
06/01/2022 12:38:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.430174 on epoch=79
06/01/2022 12:38:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.409586 on epoch=82
06/01/2022 12:38:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.383428 on epoch=84
06/01/2022 12:38:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.380964 on epoch=87
06/01/2022 12:38:59 - INFO - __main__ - Global step 350 Train loss 0.403018 Classification-F1 0.3838254172015404 on epoch=87
06/01/2022 12:39:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.308420 on epoch=89
06/01/2022 12:39:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.365442 on epoch=92
06/01/2022 12:39:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.307810 on epoch=94
06/01/2022 12:39:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.300009 on epoch=97
06/01/2022 12:39:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.292789 on epoch=99
06/01/2022 12:39:26 - INFO - __main__ - Global step 400 Train loss 0.314894 Classification-F1 0.6475524475524476 on epoch=99
06/01/2022 12:39:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.300543 on epoch=102
06/01/2022 12:39:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.240772 on epoch=104
06/01/2022 12:39:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.253174 on epoch=107
06/01/2022 12:39:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.270646 on epoch=109
06/01/2022 12:39:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.261314 on epoch=112
06/01/2022 12:39:54 - INFO - __main__ - Global step 450 Train loss 0.265290 Classification-F1 0.37478991596638656 on epoch=112
06/01/2022 12:39:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.287784 on epoch=114
06/01/2022 12:40:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.216508 on epoch=117
06/01/2022 12:40:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.208588 on epoch=119
06/01/2022 12:40:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.176755 on epoch=122
06/01/2022 12:40:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.156032 on epoch=124
06/01/2022 12:40:21 - INFO - __main__ - Global step 500 Train loss 0.209133 Classification-F1 0.6875 on epoch=124
06/01/2022 12:40:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.215880 on epoch=127
06/01/2022 12:40:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.153588 on epoch=129
06/01/2022 12:40:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.195007 on epoch=132
06/01/2022 12:40:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.130943 on epoch=134
06/01/2022 12:40:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.165194 on epoch=137
06/01/2022 12:40:49 - INFO - __main__ - Global step 550 Train loss 0.172123 Classification-F1 0.6419437340153453 on epoch=137
06/01/2022 12:40:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.137681 on epoch=139
06/01/2022 12:40:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.174252 on epoch=142
06/01/2022 12:41:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.117675 on epoch=144
06/01/2022 12:41:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.105027 on epoch=147
06/01/2022 12:41:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.056340 on epoch=149
06/01/2022 12:41:16 - INFO - __main__ - Global step 600 Train loss 0.118195 Classification-F1 0.7330367074527252 on epoch=149
06/01/2022 12:41:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.113192 on epoch=152
06/01/2022 12:41:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.108943 on epoch=154
06/01/2022 12:41:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.096774 on epoch=157
06/01/2022 12:41:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.088001 on epoch=159
06/01/2022 12:41:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.113574 on epoch=162
06/01/2022 12:41:43 - INFO - __main__ - Global step 650 Train loss 0.104097 Classification-F1 0.7165879410947487 on epoch=162
06/01/2022 12:41:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.115999 on epoch=164
06/01/2022 12:41:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.074537 on epoch=167
06/01/2022 12:41:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.046785 on epoch=169
06/01/2022 12:42:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.084602 on epoch=172
06/01/2022 12:42:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.058631 on epoch=174
06/01/2022 12:42:10 - INFO - __main__ - Global step 700 Train loss 0.076111 Classification-F1 0.6606334841628959 on epoch=174
06/01/2022 12:42:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.081093 on epoch=177
06/01/2022 12:42:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.078564 on epoch=179
06/01/2022 12:42:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.068009 on epoch=182
06/01/2022 12:42:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.052829 on epoch=184
06/01/2022 12:42:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.051189 on epoch=187
06/01/2022 12:42:37 - INFO - __main__ - Global step 750 Train loss 0.066337 Classification-F1 0.7165879410947484 on epoch=187
06/01/2022 12:42:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.049097 on epoch=189
06/01/2022 12:42:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.035373 on epoch=192
06/01/2022 12:42:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.018225 on epoch=194
06/01/2022 12:42:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.044635 on epoch=197
06/01/2022 12:43:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.046231 on epoch=199
06/01/2022 12:43:04 - INFO - __main__ - Global step 800 Train loss 0.038712 Classification-F1 0.7333333333333334 on epoch=199
06/01/2022 12:43:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.099177 on epoch=202
06/01/2022 12:43:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.031869 on epoch=204
06/01/2022 12:43:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.062064 on epoch=207
06/01/2022 12:43:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.057260 on epoch=209
06/01/2022 12:43:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.051138 on epoch=212
06/01/2022 12:43:32 - INFO - __main__ - Global step 850 Train loss 0.060301 Classification-F1 0.765625 on epoch=212
06/01/2022 12:43:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.101892 on epoch=214
06/01/2022 12:43:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.043148 on epoch=217
06/01/2022 12:43:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.038552 on epoch=219
06/01/2022 12:43:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.018022 on epoch=222
06/01/2022 12:43:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.007051 on epoch=224
06/01/2022 12:43:59 - INFO - __main__ - Global step 900 Train loss 0.041733 Classification-F1 0.7493734335839599 on epoch=224
06/01/2022 12:44:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.010091 on epoch=227
06/01/2022 12:44:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.007253 on epoch=229
06/01/2022 12:44:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.016260 on epoch=232
06/01/2022 12:44:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.014349 on epoch=234
06/01/2022 12:44:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.046863 on epoch=237
06/01/2022 12:44:27 - INFO - __main__ - Global step 950 Train loss 0.018963 Classification-F1 0.5977849023608277 on epoch=237
06/01/2022 12:44:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.015641 on epoch=239
06/01/2022 12:44:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.063902 on epoch=242
06/01/2022 12:44:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.017924 on epoch=244
06/01/2022 12:44:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.008813 on epoch=247
06/01/2022 12:44:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.009251 on epoch=249
06/01/2022 12:44:54 - INFO - __main__ - Global step 1000 Train loss 0.023106 Classification-F1 0.7499305362600723 on epoch=249
06/01/2022 12:44:54 - INFO - __main__ - save last model!
06/01/2022 12:44:54 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:44:54 - INFO - __main__ - Printing 3 examples
06/01/2022 12:44:54 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
06/01/2022 12:44:54 - INFO - __main__ - ['false']
06/01/2022 12:44:54 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
06/01/2022 12:44:54 - INFO - __main__ - ['false']
06/01/2022 12:44:54 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
06/01/2022 12:44:54 - INFO - __main__ - ['false']
06/01/2022 12:44:54 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:44:54 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:44:54 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:44:54 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:44:54 - INFO - __main__ - Printing 3 examples
06/01/2022 12:44:54 - INFO - __main__ -  [ethos-race] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/01/2022 12:44:54 - INFO - __main__ - ['false']
06/01/2022 12:44:54 - INFO - __main__ -  [ethos-race] How is it to have a kid acting like a sloth or like a pug dog? Isn’t it hard? In your place I’d put it in a dedicated institute for problematics and pay a visit each year at max
06/01/2022 12:44:54 - INFO - __main__ - ['false']
06/01/2022 12:44:54 - INFO - __main__ -  [ethos-race] Watching this kid trying to step up and walk with that crutches is the best thing in the world. So idiot
06/01/2022 12:44:54 - INFO - __main__ - ['false']
06/01/2022 12:44:54 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:44:54 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:44:55 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:45:01 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 12:45:02 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 12:45:02 - INFO - __main__ - Printing 3 examples
06/01/2022 12:45:02 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 12:45:02 - INFO - __main__ - ['true']
06/01/2022 12:45:02 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 12:45:02 - INFO - __main__ - ['false']
06/01/2022 12:45:02 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 12:45:02 - INFO - __main__ - ['false']
06/01/2022 12:45:02 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:45:02 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:45:02 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 12:45:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_42_0.0002_8_predictions.txt
06/01/2022 12:45:03 - INFO - __main__ - Classification-F1 on test data: 0.8249
06/01/2022 12:45:04 - INFO - __main__ - prefix=ethos-race_32_42, lr=0.0002, bsz=8, dev_performance=0.765625, test_performance=0.8248792270531402
06/01/2022 12:45:04 - INFO - __main__ - Running ... prefix=ethos-race_32_42, lr=0.0001, bsz=8 ...
06/01/2022 12:45:05 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:45:05 - INFO - __main__ - Printing 3 examples
06/01/2022 12:45:05 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
06/01/2022 12:45:05 - INFO - __main__ - ['false']
06/01/2022 12:45:05 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
06/01/2022 12:45:05 - INFO - __main__ - ['false']
06/01/2022 12:45:05 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
06/01/2022 12:45:05 - INFO - __main__ - ['false']
06/01/2022 12:45:05 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:45:05 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:45:05 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:45:05 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:45:05 - INFO - __main__ - Printing 3 examples
06/01/2022 12:45:05 - INFO - __main__ -  [ethos-race] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/01/2022 12:45:05 - INFO - __main__ - ['false']
06/01/2022 12:45:05 - INFO - __main__ -  [ethos-race] How is it to have a kid acting like a sloth or like a pug dog? Isn’t it hard? In your place I’d put it in a dedicated institute for problematics and pay a visit each year at max
06/01/2022 12:45:05 - INFO - __main__ - ['false']
06/01/2022 12:45:05 - INFO - __main__ -  [ethos-race] Watching this kid trying to step up and walk with that crutches is the best thing in the world. So idiot
06/01/2022 12:45:05 - INFO - __main__ - ['false']
06/01/2022 12:45:05 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:45:05 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:45:05 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:45:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:45:05 - INFO - __main__ - Starting training!
06/01/2022 12:45:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:45:16 - INFO - __main__ - Starting training!
06/01/2022 12:45:21 - INFO - __main__ - Step 10 Global step 10 Train loss 24.505562 on epoch=2
06/01/2022 12:45:26 - INFO - __main__ - Step 20 Global step 20 Train loss 21.783447 on epoch=4
06/01/2022 12:45:31 - INFO - __main__ - Step 30 Global step 30 Train loss 20.860548 on epoch=7
06/01/2022 12:45:36 - INFO - __main__ - Step 40 Global step 40 Train loss 18.556484 on epoch=9
06/01/2022 12:45:41 - INFO - __main__ - Step 50 Global step 50 Train loss 18.518677 on epoch=12
06/01/2022 12:46:01 - INFO - __main__ - Global step 50 Train loss 20.844944 Classification-F1 0.0 on epoch=12
06/01/2022 12:46:07 - INFO - __main__ - Step 60 Global step 60 Train loss 17.536392 on epoch=14
06/01/2022 12:46:12 - INFO - __main__ - Step 70 Global step 70 Train loss 17.988073 on epoch=17
06/01/2022 12:46:17 - INFO - __main__ - Step 80 Global step 80 Train loss 16.846180 on epoch=19
06/01/2022 12:46:22 - INFO - __main__ - Step 90 Global step 90 Train loss 17.338322 on epoch=22
06/01/2022 12:46:28 - INFO - __main__ - Step 100 Global step 100 Train loss 15.493794 on epoch=24
06/01/2022 12:46:46 - INFO - __main__ - Global step 100 Train loss 17.040552 Classification-F1 0.0 on epoch=24
06/01/2022 12:46:52 - INFO - __main__ - Step 110 Global step 110 Train loss 15.407709 on epoch=27
06/01/2022 12:46:57 - INFO - __main__ - Step 120 Global step 120 Train loss 15.927969 on epoch=29
06/01/2022 12:47:02 - INFO - __main__ - Step 130 Global step 130 Train loss 15.052620 on epoch=32
06/01/2022 12:47:07 - INFO - __main__ - Step 140 Global step 140 Train loss 15.170664 on epoch=34
06/01/2022 12:47:13 - INFO - __main__ - Step 150 Global step 150 Train loss 15.266126 on epoch=37
06/01/2022 12:47:31 - INFO - __main__ - Global step 150 Train loss 15.365018 Classification-F1 0.0 on epoch=37
06/01/2022 12:47:36 - INFO - __main__ - Step 160 Global step 160 Train loss 13.883031 on epoch=39
06/01/2022 12:47:42 - INFO - __main__ - Step 170 Global step 170 Train loss 13.469569 on epoch=42
06/01/2022 12:47:47 - INFO - __main__ - Step 180 Global step 180 Train loss 13.069570 on epoch=44
06/01/2022 12:47:52 - INFO - __main__ - Step 190 Global step 190 Train loss 12.771574 on epoch=47
06/01/2022 12:47:58 - INFO - __main__ - Step 200 Global step 200 Train loss 11.832936 on epoch=49
06/01/2022 12:48:14 - INFO - __main__ - Global step 200 Train loss 13.005336 Classification-F1 0.0 on epoch=49
06/01/2022 12:48:20 - INFO - __main__ - Step 210 Global step 210 Train loss 11.070764 on epoch=52
06/01/2022 12:48:25 - INFO - __main__ - Step 220 Global step 220 Train loss 10.715139 on epoch=54
06/01/2022 12:48:30 - INFO - __main__ - Step 230 Global step 230 Train loss 10.476733 on epoch=57
06/01/2022 12:48:36 - INFO - __main__ - Step 240 Global step 240 Train loss 8.865786 on epoch=59
06/01/2022 12:48:41 - INFO - __main__ - Step 250 Global step 250 Train loss 6.598830 on epoch=62
06/01/2022 12:48:51 - INFO - __main__ - Global step 250 Train loss 9.545450 Classification-F1 0.05147058823529411 on epoch=62
06/01/2022 12:48:57 - INFO - __main__ - Step 260 Global step 260 Train loss 5.321674 on epoch=64
06/01/2022 12:49:02 - INFO - __main__ - Step 270 Global step 270 Train loss 2.819867 on epoch=67
06/01/2022 12:49:08 - INFO - __main__ - Step 280 Global step 280 Train loss 2.031208 on epoch=69
06/01/2022 12:49:13 - INFO - __main__ - Step 290 Global step 290 Train loss 1.232620 on epoch=72
06/01/2022 12:49:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.996343 on epoch=74
06/01/2022 12:49:19 - INFO - __main__ - Global step 300 Train loss 2.480343 Classification-F1 0.5498749652681301 on epoch=74
06/01/2022 12:49:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.661325 on epoch=77
06/01/2022 12:49:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.840744 on epoch=79
06/01/2022 12:49:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.439983 on epoch=82
06/01/2022 12:49:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.554277 on epoch=84
06/01/2022 12:49:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.388346 on epoch=87
06/01/2022 12:49:47 - INFO - __main__ - Global step 350 Train loss 0.576935 Classification-F1 0.40199335548172754 on epoch=87
06/01/2022 12:49:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.364878 on epoch=89
06/01/2022 12:49:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.451762 on epoch=92
06/01/2022 12:50:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.352228 on epoch=94
06/01/2022 12:50:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.668636 on epoch=97
06/01/2022 12:50:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.375792 on epoch=99
06/01/2022 12:50:15 - INFO - __main__ - Global step 400 Train loss 0.442659 Classification-F1 0.6329254727474972 on epoch=99
06/01/2022 12:50:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.255480 on epoch=102
06/01/2022 12:50:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.409596 on epoch=104
06/01/2022 12:50:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.255921 on epoch=107
06/01/2022 12:50:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.272941 on epoch=109
06/01/2022 12:50:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.469284 on epoch=112
06/01/2022 12:50:43 - INFO - __main__ - Global step 450 Train loss 0.332645 Classification-F1 0.5775837792171219 on epoch=112
06/01/2022 12:50:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.264424 on epoch=114
06/01/2022 12:50:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.177959 on epoch=117
06/01/2022 12:50:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.197471 on epoch=119
06/01/2022 12:51:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.149520 on epoch=122
06/01/2022 12:51:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.399557 on epoch=124
06/01/2022 12:51:11 - INFO - __main__ - Global step 500 Train loss 0.237786 Classification-F1 0.6499027507641011 on epoch=124
06/01/2022 12:51:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.154804 on epoch=127
06/01/2022 12:51:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.139941 on epoch=129
06/01/2022 12:51:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.198968 on epoch=132
06/01/2022 12:51:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.358293 on epoch=134
06/01/2022 12:51:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.167177 on epoch=137
06/01/2022 12:51:39 - INFO - __main__ - Global step 550 Train loss 0.203837 Classification-F1 0.6666666666666666 on epoch=137
06/01/2022 12:51:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.170177 on epoch=139
06/01/2022 12:51:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.128123 on epoch=142
06/01/2022 12:51:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.138880 on epoch=144
06/01/2022 12:52:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.174213 on epoch=147
06/01/2022 12:52:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.119917 on epoch=149
06/01/2022 12:52:08 - INFO - __main__ - Global step 600 Train loss 0.146262 Classification-F1 0.6832453459294249 on epoch=149
06/01/2022 12:52:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.228448 on epoch=152
06/01/2022 12:52:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.159850 on epoch=154
06/01/2022 12:52:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.097862 on epoch=157
06/01/2022 12:52:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.092031 on epoch=159
06/01/2022 12:52:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.100376 on epoch=162
06/01/2022 12:52:36 - INFO - __main__ - Global step 650 Train loss 0.135713 Classification-F1 0.6666666666666666 on epoch=162
06/01/2022 12:52:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.132513 on epoch=164
06/01/2022 12:52:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.164980 on epoch=167
06/01/2022 12:52:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.107618 on epoch=169
06/01/2022 12:52:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.156907 on epoch=172
06/01/2022 12:53:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.132573 on epoch=174
06/01/2022 12:53:04 - INFO - __main__ - Global step 700 Train loss 0.138918 Classification-F1 0.6499027507641011 on epoch=174
06/01/2022 12:53:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.064054 on epoch=177
06/01/2022 12:53:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.107374 on epoch=179
06/01/2022 12:53:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.123049 on epoch=182
06/01/2022 12:53:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.096987 on epoch=184
06/01/2022 12:53:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.076469 on epoch=187
06/01/2022 12:53:31 - INFO - __main__ - Global step 750 Train loss 0.093587 Classification-F1 0.7465502675302733 on epoch=187
06/01/2022 12:53:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.089648 on epoch=189
06/01/2022 12:53:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.306103 on epoch=192
06/01/2022 12:53:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.054082 on epoch=194
06/01/2022 12:53:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.132834 on epoch=197
06/01/2022 12:53:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.099157 on epoch=199
06/01/2022 12:53:59 - INFO - __main__ - Global step 800 Train loss 0.136365 Classification-F1 0.7306397306397308 on epoch=199
06/01/2022 12:54:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.205003 on epoch=202
06/01/2022 12:54:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.111877 on epoch=204
06/01/2022 12:54:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.087912 on epoch=207
06/01/2022 12:54:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.079463 on epoch=209
06/01/2022 12:54:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.091336 on epoch=212
06/01/2022 12:54:27 - INFO - __main__ - Global step 850 Train loss 0.115118 Classification-F1 0.6811188811188811 on epoch=212
06/01/2022 12:54:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.100005 on epoch=214
06/01/2022 12:54:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.081614 on epoch=217
06/01/2022 12:54:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.092020 on epoch=219
06/01/2022 12:54:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.093676 on epoch=222
06/01/2022 12:54:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.102919 on epoch=224
06/01/2022 12:54:54 - INFO - __main__ - Global step 900 Train loss 0.094047 Classification-F1 0.6986607142857143 on epoch=224
06/01/2022 12:55:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.092931 on epoch=227
06/01/2022 12:55:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.114036 on epoch=229
06/01/2022 12:55:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.050191 on epoch=232
06/01/2022 12:55:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.064409 on epoch=234
06/01/2022 12:55:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.044745 on epoch=237
06/01/2022 12:55:22 - INFO - __main__ - Global step 950 Train loss 0.073262 Classification-F1 0.7165879410947484 on epoch=237
06/01/2022 12:55:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.068460 on epoch=239
06/01/2022 12:55:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.064058 on epoch=242
06/01/2022 12:55:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.048413 on epoch=244
06/01/2022 12:55:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.051262 on epoch=247
06/01/2022 12:55:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.168188 on epoch=249
06/01/2022 12:55:49 - INFO - __main__ - Global step 1000 Train loss 0.080076 Classification-F1 0.7499305362600723 on epoch=249
06/01/2022 12:55:50 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:55:50 - INFO - __main__ - Printing 3 examples
06/01/2022 12:55:50 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/01/2022 12:55:50 - INFO - __main__ - ['false']
06/01/2022 12:55:50 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 12:55:50 - INFO - __main__ - ['false']
06/01/2022 12:55:50 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/01/2022 12:55:50 - INFO - __main__ - ['false']
06/01/2022 12:55:50 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:55:50 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:55:50 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:55:50 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:55:50 - INFO - __main__ - Printing 3 examples
06/01/2022 12:55:50 - INFO - __main__ -  [ethos-race] Fucking idiots. Keep them out. They change the culture. Take over small towns. Vote in their own people to rule the town. And start enforcing their religious beliefs on everyone. It CANNOT FUCKING HAPPEN. Stop them now while you still can.
06/01/2022 12:55:50 - INFO - __main__ - ['false']
06/01/2022 12:55:50 - INFO - __main__ -  [ethos-race] I would beat the shit out of every Russian
06/01/2022 12:55:50 - INFO - __main__ - ['false']
06/01/2022 12:55:50 - INFO - __main__ -  [ethos-race] I can' stand feminist. I like men being manly and I like being treated like a lady. Dear men, if a woman asks for your help, ask if they are feminist. If they say yes, then remind them they can do everything without men and don' help them.
06/01/2022 12:55:50 - INFO - __main__ - ['false']
06/01/2022 12:55:50 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:55:50 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:55:50 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:55:50 - INFO - __main__ - save last model!
06/01/2022 12:55:57 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 12:55:58 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 12:55:58 - INFO - __main__ - Printing 3 examples
06/01/2022 12:55:58 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 12:55:58 - INFO - __main__ - ['true']
06/01/2022 12:55:58 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 12:55:58 - INFO - __main__ - ['false']
06/01/2022 12:55:58 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 12:55:58 - INFO - __main__ - ['false']
06/01/2022 12:55:58 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:55:58 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:55:58 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 12:56:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_42_0.0001_8_predictions.txt
06/01/2022 12:56:00 - INFO - __main__ - Classification-F1 on test data: 0.7288
06/01/2022 12:56:00 - INFO - __main__ - prefix=ethos-race_32_42, lr=0.0001, bsz=8, dev_performance=0.7499305362600723, test_performance=0.7287731523931781
06/01/2022 12:56:00 - INFO - __main__ - Running ... prefix=ethos-race_32_87, lr=0.0005, bsz=8 ...
06/01/2022 12:56:01 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 12:56:01 - INFO - __main__ - Printing 3 examples
06/01/2022 12:56:01 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/01/2022 12:56:01 - INFO - __main__ - ['false']
06/01/2022 12:56:01 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 12:56:01 - INFO - __main__ - ['false']
06/01/2022 12:56:01 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/01/2022 12:56:01 - INFO - __main__ - ['false']
06/01/2022 12:56:01 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:56:01 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:56:01 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 12:56:01 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 12:56:01 - INFO - __main__ - Printing 3 examples
06/01/2022 12:56:01 - INFO - __main__ -  [ethos-race] Fucking idiots. Keep them out. They change the culture. Take over small towns. Vote in their own people to rule the town. And start enforcing their religious beliefs on everyone. It CANNOT FUCKING HAPPEN. Stop them now while you still can.
06/01/2022 12:56:01 - INFO - __main__ - ['false']
06/01/2022 12:56:01 - INFO - __main__ -  [ethos-race] I would beat the shit out of every Russian
06/01/2022 12:56:01 - INFO - __main__ - ['false']
06/01/2022 12:56:01 - INFO - __main__ -  [ethos-race] I can' stand feminist. I like men being manly and I like being treated like a lady. Dear men, if a woman asks for your help, ask if they are feminist. If they say yes, then remind them they can do everything without men and don' help them.
06/01/2022 12:56:01 - INFO - __main__ - ['false']
06/01/2022 12:56:01 - INFO - __main__ - Tokenizing Input ...
06/01/2022 12:56:01 - INFO - __main__ - Tokenizing Output ...
06/01/2022 12:56:01 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 12:56:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:56:03 - INFO - __main__ - Starting training!
06/01/2022 12:56:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 12:56:14 - INFO - __main__ - Starting training!
06/01/2022 12:56:19 - INFO - __main__ - Step 10 Global step 10 Train loss 23.405569 on epoch=2
06/01/2022 12:56:24 - INFO - __main__ - Step 20 Global step 20 Train loss 20.545261 on epoch=4
06/01/2022 12:56:29 - INFO - __main__ - Step 30 Global step 30 Train loss 16.289091 on epoch=7
06/01/2022 12:56:35 - INFO - __main__ - Step 40 Global step 40 Train loss 14.353029 on epoch=9
06/01/2022 12:56:40 - INFO - __main__ - Step 50 Global step 50 Train loss 11.637793 on epoch=12
06/01/2022 12:57:04 - INFO - __main__ - Global step 50 Train loss 17.246149 Classification-F1 0.0014492753623188406 on epoch=12
06/01/2022 12:57:10 - INFO - __main__ - Step 60 Global step 60 Train loss 5.598473 on epoch=14
06/01/2022 12:57:15 - INFO - __main__ - Step 70 Global step 70 Train loss 1.417241 on epoch=17
06/01/2022 12:57:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.659960 on epoch=19
06/01/2022 12:57:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.946673 on epoch=22
06/01/2022 12:57:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.641375 on epoch=24
06/01/2022 12:57:32 - INFO - __main__ - Global step 100 Train loss 1.852744 Classification-F1 0.3478260869565218 on epoch=24
06/01/2022 12:57:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.604760 on epoch=27
06/01/2022 12:57:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.442746 on epoch=29
06/01/2022 12:57:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.582267 on epoch=32
06/01/2022 12:57:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.498547 on epoch=34
06/01/2022 12:57:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.484628 on epoch=37
06/01/2022 12:58:00 - INFO - __main__ - Global step 150 Train loss 0.522590 Classification-F1 0.5251661918328585 on epoch=37
06/01/2022 12:58:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.422545 on epoch=39
06/01/2022 12:58:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.504115 on epoch=42
06/01/2022 12:58:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.508989 on epoch=44
06/01/2022 12:58:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.433915 on epoch=47
06/01/2022 12:58:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.393417 on epoch=49
06/01/2022 12:58:29 - INFO - __main__ - Global step 200 Train loss 0.452596 Classification-F1 0.43823618242222895 on epoch=49
06/01/2022 12:58:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.420656 on epoch=52
06/01/2022 12:58:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.358659 on epoch=54
06/01/2022 12:58:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.474457 on epoch=57
06/01/2022 12:58:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.387900 on epoch=59
06/01/2022 12:58:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.424385 on epoch=62
06/01/2022 12:58:56 - INFO - __main__ - Global step 250 Train loss 0.413212 Classification-F1 0.3478260869565218 on epoch=62
06/01/2022 12:59:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.548622 on epoch=64
06/01/2022 12:59:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.446161 on epoch=67
06/01/2022 12:59:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.398459 on epoch=69
06/01/2022 12:59:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.359586 on epoch=72
06/01/2022 12:59:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.394128 on epoch=74
06/01/2022 12:59:24 - INFO - __main__ - Global step 300 Train loss 0.429391 Classification-F1 0.45632475534613987 on epoch=74
06/01/2022 12:59:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.413539 on epoch=77
06/01/2022 12:59:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.384358 on epoch=79
06/01/2022 12:59:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.348977 on epoch=82
06/01/2022 12:59:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.481268 on epoch=84
06/01/2022 12:59:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.371989 on epoch=87
06/01/2022 12:59:52 - INFO - __main__ - Global step 350 Train loss 0.400026 Classification-F1 0.3433292533659731 on epoch=87
06/01/2022 12:59:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.341958 on epoch=89
06/01/2022 13:00:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.395129 on epoch=92
06/01/2022 13:00:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.379797 on epoch=94
06/01/2022 13:00:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.310884 on epoch=97
06/01/2022 13:00:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.312519 on epoch=99
06/01/2022 13:00:20 - INFO - __main__ - Global step 400 Train loss 0.348057 Classification-F1 0.696969696969697 on epoch=99
06/01/2022 13:00:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.296356 on epoch=102
06/01/2022 13:00:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.286125 on epoch=104
06/01/2022 13:00:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.181584 on epoch=107
06/01/2022 13:00:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.241941 on epoch=109
06/01/2022 13:00:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.328209 on epoch=112
06/01/2022 13:00:48 - INFO - __main__ - Global step 450 Train loss 0.266843 Classification-F1 0.6327601282424948 on epoch=112
06/01/2022 13:00:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.266273 on epoch=114
06/01/2022 13:00:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.224316 on epoch=117
06/01/2022 13:01:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.277996 on epoch=119
06/01/2022 13:01:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.221514 on epoch=122
06/01/2022 13:01:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.292811 on epoch=124
06/01/2022 13:01:14 - INFO - __main__ - Global step 500 Train loss 0.256582 Classification-F1 0.783273131425396 on epoch=124
06/01/2022 13:01:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.228815 on epoch=127
06/01/2022 13:01:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.192921 on epoch=129
06/01/2022 13:01:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.267010 on epoch=132
06/01/2022 13:01:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.180067 on epoch=134
06/01/2022 13:01:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.236495 on epoch=137
06/01/2022 13:01:43 - INFO - __main__ - Global step 550 Train loss 0.221062 Classification-F1 0.5739425748687866 on epoch=137
06/01/2022 13:01:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.270248 on epoch=139
06/01/2022 13:01:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.237090 on epoch=142
06/01/2022 13:01:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.182219 on epoch=144
06/01/2022 13:02:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.134050 on epoch=147
06/01/2022 13:02:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.184495 on epoch=149
06/01/2022 13:02:09 - INFO - __main__ - Global step 600 Train loss 0.201620 Classification-F1 0.8124467178175618 on epoch=149
06/01/2022 13:02:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.175323 on epoch=152
06/01/2022 13:02:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.094755 on epoch=154
06/01/2022 13:02:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.121916 on epoch=157
06/01/2022 13:02:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.161504 on epoch=159
06/01/2022 13:02:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.169192 on epoch=162
06/01/2022 13:02:37 - INFO - __main__ - Global step 650 Train loss 0.144538 Classification-F1 0.7257142857142858 on epoch=162
06/01/2022 13:02:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.174166 on epoch=164
06/01/2022 13:02:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.094534 on epoch=167
06/01/2022 13:02:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.170071 on epoch=169
06/01/2022 13:02:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.115848 on epoch=172
06/01/2022 13:03:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.112041 on epoch=174
06/01/2022 13:03:04 - INFO - __main__ - Global step 700 Train loss 0.133332 Classification-F1 0.8153846153846154 on epoch=174
06/01/2022 13:03:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.065538 on epoch=177
06/01/2022 13:03:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.134910 on epoch=179
06/01/2022 13:03:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.123512 on epoch=182
06/01/2022 13:03:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.111895 on epoch=184
06/01/2022 13:03:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.046737 on epoch=187
06/01/2022 13:03:31 - INFO - __main__ - Global step 750 Train loss 0.096518 Classification-F1 0.9164578111946533 on epoch=187
06/01/2022 13:03:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.093610 on epoch=189
06/01/2022 13:03:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.028563 on epoch=192
06/01/2022 13:03:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.081892 on epoch=194
06/01/2022 13:03:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.074914 on epoch=197
06/01/2022 13:03:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.029252 on epoch=199
06/01/2022 13:03:59 - INFO - __main__ - Global step 800 Train loss 0.061646 Classification-F1 0.7257142857142858 on epoch=199
06/01/2022 13:04:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.014104 on epoch=202
06/01/2022 13:04:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.059546 on epoch=204
06/01/2022 13:04:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.037428 on epoch=207
06/01/2022 13:04:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.010323 on epoch=209
06/01/2022 13:04:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.043223 on epoch=212
06/01/2022 13:04:26 - INFO - __main__ - Global step 850 Train loss 0.032925 Classification-F1 0.8666666666666666 on epoch=212
06/01/2022 13:04:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.016908 on epoch=214
06/01/2022 13:04:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.032072 on epoch=217
06/01/2022 13:04:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.010570 on epoch=219
06/01/2022 13:04:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.056202 on epoch=222
06/01/2022 13:04:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.047179 on epoch=224
06/01/2022 13:04:53 - INFO - __main__ - Global step 900 Train loss 0.032586 Classification-F1 0.7442455242966752 on epoch=224
06/01/2022 13:04:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.008194 on epoch=227
06/01/2022 13:05:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.005490 on epoch=229
06/01/2022 13:05:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.034085 on epoch=232
06/01/2022 13:05:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.012365 on epoch=234
06/01/2022 13:05:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001675 on epoch=237
06/01/2022 13:05:20 - INFO - __main__ - Global step 950 Train loss 0.012362 Classification-F1 0.8499583217560432 on epoch=237
06/01/2022 13:05:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.030144 on epoch=239
06/01/2022 13:05:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.004264 on epoch=242
06/01/2022 13:05:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.024688 on epoch=244
06/01/2022 13:05:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.029761 on epoch=247
06/01/2022 13:05:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.002288 on epoch=249
06/01/2022 13:05:47 - INFO - __main__ - Global step 1000 Train loss 0.018229 Classification-F1 0.8331479421579533 on epoch=249
06/01/2022 13:05:47 - INFO - __main__ - save last model!
06/01/2022 13:05:47 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 13:05:47 - INFO - __main__ - Printing 3 examples
06/01/2022 13:05:47 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/01/2022 13:05:47 - INFO - __main__ - ['false']
06/01/2022 13:05:47 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 13:05:47 - INFO - __main__ - ['false']
06/01/2022 13:05:47 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/01/2022 13:05:47 - INFO - __main__ - ['false']
06/01/2022 13:05:47 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:05:47 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:05:47 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 13:05:47 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 13:05:47 - INFO - __main__ - Printing 3 examples
06/01/2022 13:05:47 - INFO - __main__ -  [ethos-race] Fucking idiots. Keep them out. They change the culture. Take over small towns. Vote in their own people to rule the town. And start enforcing their religious beliefs on everyone. It CANNOT FUCKING HAPPEN. Stop them now while you still can.
06/01/2022 13:05:47 - INFO - __main__ - ['false']
06/01/2022 13:05:47 - INFO - __main__ -  [ethos-race] I would beat the shit out of every Russian
06/01/2022 13:05:47 - INFO - __main__ - ['false']
06/01/2022 13:05:47 - INFO - __main__ -  [ethos-race] I can' stand feminist. I like men being manly and I like being treated like a lady. Dear men, if a woman asks for your help, ask if they are feminist. If they say yes, then remind them they can do everything without men and don' help them.
06/01/2022 13:05:47 - INFO - __main__ - ['false']
06/01/2022 13:05:47 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:05:47 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:05:47 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 13:05:54 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 13:05:55 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 13:05:55 - INFO - __main__ - Printing 3 examples
06/01/2022 13:05:55 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 13:05:55 - INFO - __main__ - ['true']
06/01/2022 13:05:55 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 13:05:55 - INFO - __main__ - ['false']
06/01/2022 13:05:55 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 13:05:55 - INFO - __main__ - ['false']
06/01/2022 13:05:55 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:05:55 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:05:55 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 13:05:57 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_87_0.0005_8_predictions.txt
06/01/2022 13:05:57 - INFO - __main__ - Classification-F1 on test data: 0.9065
06/01/2022 13:05:57 - INFO - __main__ - prefix=ethos-race_32_87, lr=0.0005, bsz=8, dev_performance=0.9164578111946533, test_performance=0.9065119277885235
06/01/2022 13:05:57 - INFO - __main__ - Running ... prefix=ethos-race_32_87, lr=0.0003, bsz=8 ...
06/01/2022 13:05:58 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 13:05:58 - INFO - __main__ - Printing 3 examples
06/01/2022 13:05:58 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/01/2022 13:05:58 - INFO - __main__ - ['false']
06/01/2022 13:05:58 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 13:05:58 - INFO - __main__ - ['false']
06/01/2022 13:05:58 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/01/2022 13:05:58 - INFO - __main__ - ['false']
06/01/2022 13:05:58 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:05:58 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:05:58 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 13:05:58 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 13:05:58 - INFO - __main__ - Printing 3 examples
06/01/2022 13:05:58 - INFO - __main__ -  [ethos-race] Fucking idiots. Keep them out. They change the culture. Take over small towns. Vote in their own people to rule the town. And start enforcing their religious beliefs on everyone. It CANNOT FUCKING HAPPEN. Stop them now while you still can.
06/01/2022 13:05:58 - INFO - __main__ - ['false']
06/01/2022 13:05:58 - INFO - __main__ -  [ethos-race] I would beat the shit out of every Russian
06/01/2022 13:05:58 - INFO - __main__ - ['false']
06/01/2022 13:05:58 - INFO - __main__ -  [ethos-race] I can' stand feminist. I like men being manly and I like being treated like a lady. Dear men, if a woman asks for your help, ask if they are feminist. If they say yes, then remind them they can do everything without men and don' help them.
06/01/2022 13:05:58 - INFO - __main__ - ['false']
06/01/2022 13:05:58 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:05:58 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:05:58 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 13:06:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:06:00 - INFO - __main__ - Starting training!
06/01/2022 13:06:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:06:11 - INFO - __main__ - Starting training!
06/01/2022 13:06:16 - INFO - __main__ - Step 10 Global step 10 Train loss 24.497982 on epoch=2
06/01/2022 13:06:21 - INFO - __main__ - Step 20 Global step 20 Train loss 19.281103 on epoch=4
06/01/2022 13:06:26 - INFO - __main__ - Step 30 Global step 30 Train loss 17.183523 on epoch=7
06/01/2022 13:06:31 - INFO - __main__ - Step 40 Global step 40 Train loss 16.477247 on epoch=9
06/01/2022 13:06:37 - INFO - __main__ - Step 50 Global step 50 Train loss 14.845197 on epoch=12
06/01/2022 13:06:55 - INFO - __main__ - Global step 50 Train loss 18.457008 Classification-F1 0.0 on epoch=12
06/01/2022 13:07:01 - INFO - __main__ - Step 60 Global step 60 Train loss 14.392817 on epoch=14
06/01/2022 13:07:06 - INFO - __main__ - Step 70 Global step 70 Train loss 12.792868 on epoch=17
06/01/2022 13:07:11 - INFO - __main__ - Step 80 Global step 80 Train loss 10.579396 on epoch=19
06/01/2022 13:07:17 - INFO - __main__ - Step 90 Global step 90 Train loss 6.140563 on epoch=22
06/01/2022 13:07:22 - INFO - __main__ - Step 100 Global step 100 Train loss 3.701799 on epoch=24
06/01/2022 13:07:23 - INFO - __main__ - Global step 100 Train loss 9.521487 Classification-F1 0.24120234604105573 on epoch=24
06/01/2022 13:07:29 - INFO - __main__ - Step 110 Global step 110 Train loss 2.987044 on epoch=27
06/01/2022 13:07:34 - INFO - __main__ - Step 120 Global step 120 Train loss 1.901809 on epoch=29
06/01/2022 13:07:40 - INFO - __main__ - Step 130 Global step 130 Train loss 1.355649 on epoch=32
06/01/2022 13:07:45 - INFO - __main__ - Step 140 Global step 140 Train loss 1.217100 on epoch=34
06/01/2022 13:07:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.756489 on epoch=37
06/01/2022 13:07:51 - INFO - __main__ - Global step 150 Train loss 1.643618 Classification-F1 0.38613111026904134 on epoch=37
06/01/2022 13:07:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.646591 on epoch=39
06/01/2022 13:08:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.365532 on epoch=42
06/01/2022 13:08:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.397294 on epoch=44
06/01/2022 13:08:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.235551 on epoch=47
06/01/2022 13:08:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.174656 on epoch=49
06/01/2022 13:08:19 - INFO - __main__ - Global step 200 Train loss 0.363925 Classification-F1 0.5368941031182464 on epoch=49
06/01/2022 13:08:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.266781 on epoch=52
06/01/2022 13:08:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.165234 on epoch=54
06/01/2022 13:08:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.114175 on epoch=57
06/01/2022 13:08:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.109091 on epoch=59
06/01/2022 13:08:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.153126 on epoch=62
06/01/2022 13:08:47 - INFO - __main__ - Global step 250 Train loss 0.161681 Classification-F1 0.8479301605181638 on epoch=62
06/01/2022 13:08:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.166045 on epoch=64
06/01/2022 13:08:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.059441 on epoch=67
06/01/2022 13:09:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.090535 on epoch=69
06/01/2022 13:09:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.082151 on epoch=72
06/01/2022 13:09:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.115727 on epoch=74
06/01/2022 13:09:15 - INFO - __main__ - Global step 300 Train loss 0.102780 Classification-F1 0.76 on epoch=74
06/01/2022 13:09:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.038405 on epoch=77
06/01/2022 13:09:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.044182 on epoch=79
06/01/2022 13:09:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.091291 on epoch=82
06/01/2022 13:09:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.017156 on epoch=84
06/01/2022 13:09:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.017876 on epoch=87
06/01/2022 13:09:42 - INFO - __main__ - Global step 350 Train loss 0.041782 Classification-F1 0.797979797979798 on epoch=87
06/01/2022 13:09:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.074347 on epoch=89
06/01/2022 13:09:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.024520 on epoch=92
06/01/2022 13:09:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.024362 on epoch=94
06/01/2022 13:10:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.004921 on epoch=97
06/01/2022 13:10:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.044772 on epoch=99
06/01/2022 13:10:10 - INFO - __main__ - Global step 400 Train loss 0.034585 Classification-F1 0.8665183537263627 on epoch=99
06/01/2022 13:10:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001632 on epoch=102
06/01/2022 13:10:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.006768 on epoch=104
06/01/2022 13:10:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.005722 on epoch=107
06/01/2022 13:10:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000890 on epoch=109
06/01/2022 13:10:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.008723 on epoch=112
06/01/2022 13:10:38 - INFO - __main__ - Global step 450 Train loss 0.004747 Classification-F1 0.765625 on epoch=112
06/01/2022 13:10:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.012039 on epoch=114
06/01/2022 13:10:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.010562 on epoch=117
06/01/2022 13:10:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.003069 on epoch=119
06/01/2022 13:10:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.025895 on epoch=122
06/01/2022 13:11:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.008771 on epoch=124
06/01/2022 13:11:05 - INFO - __main__ - Global step 500 Train loss 0.012067 Classification-F1 0.8499583217560434 on epoch=124
06/01/2022 13:11:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.005454 on epoch=127
06/01/2022 13:11:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000361 on epoch=129
06/01/2022 13:11:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.008166 on epoch=132
06/01/2022 13:11:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.074755 on epoch=134
06/01/2022 13:11:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003386 on epoch=137
06/01/2022 13:11:32 - INFO - __main__ - Global step 550 Train loss 0.018424 Classification-F1 0.797979797979798 on epoch=137
06/01/2022 13:11:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001920 on epoch=139
06/01/2022 13:11:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000618 on epoch=142
06/01/2022 13:11:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.024445 on epoch=144
06/01/2022 13:11:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001614 on epoch=147
06/01/2022 13:11:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001779 on epoch=149
06/01/2022 13:11:59 - INFO - __main__ - Global step 600 Train loss 0.006075 Classification-F1 0.797979797979798 on epoch=149
06/01/2022 13:12:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000068 on epoch=152
06/01/2022 13:12:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000347 on epoch=154
06/01/2022 13:12:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000181 on epoch=157
06/01/2022 13:12:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000609 on epoch=159
06/01/2022 13:12:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.014850 on epoch=162
06/01/2022 13:12:26 - INFO - __main__ - Global step 650 Train loss 0.003211 Classification-F1 0.8666666666666666 on epoch=162
06/01/2022 13:12:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000265 on epoch=164
06/01/2022 13:12:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000395 on epoch=167
06/01/2022 13:12:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000462 on epoch=169
06/01/2022 13:12:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000269 on epoch=172
06/01/2022 13:12:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000160 on epoch=174
06/01/2022 13:12:54 - INFO - __main__ - Global step 700 Train loss 0.000310 Classification-F1 0.8166157265907197 on epoch=174
06/01/2022 13:12:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000407 on epoch=177
06/01/2022 13:13:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.002211 on epoch=179
06/01/2022 13:13:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001633 on epoch=182
06/01/2022 13:13:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.078861 on epoch=184
06/01/2022 13:13:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001103 on epoch=187
06/01/2022 13:13:21 - INFO - __main__ - Global step 750 Train loss 0.016843 Classification-F1 0.8333333333333334 on epoch=187
06/01/2022 13:13:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001362 on epoch=189
06/01/2022 13:13:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000693 on epoch=192
06/01/2022 13:13:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000388 on epoch=194
06/01/2022 13:13:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000410 on epoch=197
06/01/2022 13:13:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002070 on epoch=199
06/01/2022 13:13:48 - INFO - __main__ - Global step 800 Train loss 0.000985 Classification-F1 0.706812302385743 on epoch=199
06/01/2022 13:13:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001045 on epoch=202
06/01/2022 13:13:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000330 on epoch=204
06/01/2022 13:14:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000082 on epoch=207
06/01/2022 13:14:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001855 on epoch=209
06/01/2022 13:14:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000500 on epoch=212
06/01/2022 13:14:15 - INFO - __main__ - Global step 850 Train loss 0.000762 Classification-F1 0.8166157265907197 on epoch=212
06/01/2022 13:14:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000173 on epoch=214
06/01/2022 13:14:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000054 on epoch=217
06/01/2022 13:14:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000236 on epoch=219
06/01/2022 13:14:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000227 on epoch=222
06/01/2022 13:14:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000423 on epoch=224
06/01/2022 13:14:43 - INFO - __main__ - Global step 900 Train loss 0.000223 Classification-F1 0.7827903091060986 on epoch=224
06/01/2022 13:14:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.018354 on epoch=227
06/01/2022 13:14:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.001388 on epoch=229
06/01/2022 13:14:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.006117 on epoch=232
06/01/2022 13:15:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000177 on epoch=234
06/01/2022 13:15:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000061 on epoch=237
06/01/2022 13:15:11 - INFO - __main__ - Global step 950 Train loss 0.005220 Classification-F1 0.849624060150376 on epoch=237
06/01/2022 13:15:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000695 on epoch=239
06/01/2022 13:15:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.006655 on epoch=242
06/01/2022 13:15:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000048 on epoch=244
06/01/2022 13:15:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000104 on epoch=247
06/01/2022 13:15:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000175 on epoch=249
06/01/2022 13:15:38 - INFO - __main__ - Global step 1000 Train loss 0.001535 Classification-F1 0.849624060150376 on epoch=249
06/01/2022 13:15:38 - INFO - __main__ - save last model!
06/01/2022 13:15:38 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 13:15:38 - INFO - __main__ - Printing 3 examples
06/01/2022 13:15:38 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/01/2022 13:15:38 - INFO - __main__ - ['false']
06/01/2022 13:15:38 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 13:15:38 - INFO - __main__ - ['false']
06/01/2022 13:15:38 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/01/2022 13:15:38 - INFO - __main__ - ['false']
06/01/2022 13:15:38 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:15:38 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:15:38 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 13:15:38 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 13:15:38 - INFO - __main__ - Printing 3 examples
06/01/2022 13:15:38 - INFO - __main__ -  [ethos-race] Fucking idiots. Keep them out. They change the culture. Take over small towns. Vote in their own people to rule the town. And start enforcing their religious beliefs on everyone. It CANNOT FUCKING HAPPEN. Stop them now while you still can.
06/01/2022 13:15:38 - INFO - __main__ - ['false']
06/01/2022 13:15:38 - INFO - __main__ -  [ethos-race] I would beat the shit out of every Russian
06/01/2022 13:15:38 - INFO - __main__ - ['false']
06/01/2022 13:15:38 - INFO - __main__ -  [ethos-race] I can' stand feminist. I like men being manly and I like being treated like a lady. Dear men, if a woman asks for your help, ask if they are feminist. If they say yes, then remind them they can do everything without men and don' help them.
06/01/2022 13:15:38 - INFO - __main__ - ['false']
06/01/2022 13:15:38 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:15:38 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:15:38 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 13:15:44 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 13:15:45 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 13:15:45 - INFO - __main__ - Printing 3 examples
06/01/2022 13:15:45 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 13:15:45 - INFO - __main__ - ['true']
06/01/2022 13:15:45 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 13:15:45 - INFO - __main__ - ['false']
06/01/2022 13:15:45 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 13:15:45 - INFO - __main__ - ['false']
06/01/2022 13:15:45 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:15:45 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:15:45 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 13:15:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_87_0.0003_8_predictions.txt
06/01/2022 13:15:47 - INFO - __main__ - Classification-F1 on test data: 0.8182
06/01/2022 13:15:47 - INFO - __main__ - prefix=ethos-race_32_87, lr=0.0003, bsz=8, dev_performance=0.8666666666666666, test_performance=0.8182336182336182
06/01/2022 13:15:47 - INFO - __main__ - Running ... prefix=ethos-race_32_87, lr=0.0002, bsz=8 ...
06/01/2022 13:15:48 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 13:15:48 - INFO - __main__ - Printing 3 examples
06/01/2022 13:15:48 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/01/2022 13:15:48 - INFO - __main__ - ['false']
06/01/2022 13:15:48 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 13:15:48 - INFO - __main__ - ['false']
06/01/2022 13:15:48 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/01/2022 13:15:48 - INFO - __main__ - ['false']
06/01/2022 13:15:48 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:15:48 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:15:48 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 13:15:48 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 13:15:48 - INFO - __main__ - Printing 3 examples
06/01/2022 13:15:48 - INFO - __main__ -  [ethos-race] Fucking idiots. Keep them out. They change the culture. Take over small towns. Vote in their own people to rule the town. And start enforcing their religious beliefs on everyone. It CANNOT FUCKING HAPPEN. Stop them now while you still can.
06/01/2022 13:15:48 - INFO - __main__ - ['false']
06/01/2022 13:15:48 - INFO - __main__ -  [ethos-race] I would beat the shit out of every Russian
06/01/2022 13:15:48 - INFO - __main__ - ['false']
06/01/2022 13:15:48 - INFO - __main__ -  [ethos-race] I can' stand feminist. I like men being manly and I like being treated like a lady. Dear men, if a woman asks for your help, ask if they are feminist. If they say yes, then remind them they can do everything without men and don' help them.
06/01/2022 13:15:48 - INFO - __main__ - ['false']
06/01/2022 13:15:48 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:15:48 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:15:48 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 13:15:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:15:50 - INFO - __main__ - Starting training!
06/01/2022 13:16:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:16:01 - INFO - __main__ - Starting training!
06/01/2022 13:16:06 - INFO - __main__ - Step 10 Global step 10 Train loss 24.239346 on epoch=2
06/01/2022 13:16:11 - INFO - __main__ - Step 20 Global step 20 Train loss 19.777672 on epoch=4
06/01/2022 13:16:16 - INFO - __main__ - Step 30 Global step 30 Train loss 17.350506 on epoch=7
06/01/2022 13:16:21 - INFO - __main__ - Step 40 Global step 40 Train loss 17.149975 on epoch=9
06/01/2022 13:16:27 - INFO - __main__ - Step 50 Global step 50 Train loss 16.907768 on epoch=12
06/01/2022 13:16:31 - INFO - __main__ - Global step 50 Train loss 19.085054 Classification-F1 0.0 on epoch=12
06/01/2022 13:16:36 - INFO - __main__ - Step 60 Global step 60 Train loss 16.032249 on epoch=14
06/01/2022 13:16:42 - INFO - __main__ - Step 70 Global step 70 Train loss 14.565478 on epoch=17
06/01/2022 13:16:47 - INFO - __main__ - Step 80 Global step 80 Train loss 14.640126 on epoch=19
06/01/2022 13:16:52 - INFO - __main__ - Step 90 Global step 90 Train loss 13.418678 on epoch=22
06/01/2022 13:16:58 - INFO - __main__ - Step 100 Global step 100 Train loss 12.180641 on epoch=24
06/01/2022 13:16:59 - INFO - __main__ - Global step 100 Train loss 14.167436 Classification-F1 0.009852216748768471 on epoch=24
06/01/2022 13:17:05 - INFO - __main__ - Step 110 Global step 110 Train loss 10.954201 on epoch=27
06/01/2022 13:17:10 - INFO - __main__ - Step 120 Global step 120 Train loss 7.997899 on epoch=29
06/01/2022 13:17:15 - INFO - __main__ - Step 130 Global step 130 Train loss 1.850954 on epoch=32
06/01/2022 13:17:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.664457 on epoch=34
06/01/2022 13:17:26 - INFO - __main__ - Step 150 Global step 150 Train loss 1.718987 on epoch=37
06/01/2022 13:17:26 - INFO - __main__ - Global step 150 Train loss 4.637300 Classification-F1 0.19409282700421945 on epoch=37
06/01/2022 13:17:32 - INFO - __main__ - Step 160 Global step 160 Train loss 1.775012 on epoch=39
06/01/2022 13:17:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.095057 on epoch=42
06/01/2022 13:17:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.458889 on epoch=44
06/01/2022 13:17:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.485406 on epoch=47
06/01/2022 13:17:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.433338 on epoch=49
06/01/2022 13:17:54 - INFO - __main__ - Global step 200 Train loss 0.849540 Classification-F1 0.6625000000000001 on epoch=49
06/01/2022 13:18:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.323103 on epoch=52
06/01/2022 13:18:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.315097 on epoch=54
06/01/2022 13:18:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.383401 on epoch=57
06/01/2022 13:18:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.244381 on epoch=59
06/01/2022 13:18:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.229948 on epoch=62
06/01/2022 13:18:22 - INFO - __main__ - Global step 250 Train loss 0.299186 Classification-F1 0.849624060150376 on epoch=62
06/01/2022 13:18:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.236765 on epoch=64
06/01/2022 13:18:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.233652 on epoch=67
06/01/2022 13:18:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.141145 on epoch=69
06/01/2022 13:18:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.228977 on epoch=72
06/01/2022 13:18:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.139925 on epoch=74
06/01/2022 13:18:50 - INFO - __main__ - Global step 300 Train loss 0.196093 Classification-F1 0.7997775305895439 on epoch=74
06/01/2022 13:18:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.125303 on epoch=77
06/01/2022 13:19:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.140924 on epoch=79
06/01/2022 13:19:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.086576 on epoch=82
06/01/2022 13:19:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.131752 on epoch=84
06/01/2022 13:19:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.054245 on epoch=87
06/01/2022 13:19:17 - INFO - __main__ - Global step 350 Train loss 0.107760 Classification-F1 0.8325892857142856 on epoch=87
06/01/2022 13:19:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.061675 on epoch=89
06/01/2022 13:19:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.090540 on epoch=92
06/01/2022 13:19:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.093802 on epoch=94
06/01/2022 13:19:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.096106 on epoch=97
06/01/2022 13:19:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.059995 on epoch=99
06/01/2022 13:19:44 - INFO - __main__ - Global step 400 Train loss 0.080423 Classification-F1 0.7643097643097643 on epoch=99
06/01/2022 13:19:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.043794 on epoch=102
06/01/2022 13:19:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.071957 on epoch=104
06/01/2022 13:20:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.028074 on epoch=107
06/01/2022 13:20:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.043730 on epoch=109
06/01/2022 13:20:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.071017 on epoch=112
06/01/2022 13:20:11 - INFO - __main__ - Global step 450 Train loss 0.051714 Classification-F1 0.8162071846282373 on epoch=112
06/01/2022 13:20:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.030580 on epoch=114
06/01/2022 13:20:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.013579 on epoch=117
06/01/2022 13:20:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.026973 on epoch=119
06/01/2022 13:20:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.017320 on epoch=122
06/01/2022 13:20:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.012781 on epoch=124
06/01/2022 13:20:38 - INFO - __main__ - Global step 500 Train loss 0.020247 Classification-F1 0.8333333333333334 on epoch=124
06/01/2022 13:20:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.010823 on epoch=127
06/01/2022 13:20:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.010233 on epoch=129
06/01/2022 13:20:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.010299 on epoch=132
06/01/2022 13:21:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.010771 on epoch=134
06/01/2022 13:21:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.016754 on epoch=137
06/01/2022 13:21:06 - INFO - __main__ - Global step 550 Train loss 0.011776 Classification-F1 0.7803435651929034 on epoch=137
06/01/2022 13:21:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.045910 on epoch=139
06/01/2022 13:21:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.042027 on epoch=142
06/01/2022 13:21:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.020044 on epoch=144
06/01/2022 13:21:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002823 on epoch=147
06/01/2022 13:21:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.022498 on epoch=149
06/01/2022 13:21:33 - INFO - __main__ - Global step 600 Train loss 0.026660 Classification-F1 0.5600150744299981 on epoch=149
06/01/2022 13:21:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001623 on epoch=152
06/01/2022 13:21:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.017458 on epoch=154
06/01/2022 13:21:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002762 on epoch=157
06/01/2022 13:21:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000755 on epoch=159
06/01/2022 13:21:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.015195 on epoch=162
06/01/2022 13:22:00 - INFO - __main__ - Global step 650 Train loss 0.007559 Classification-F1 0.5713747645951036 on epoch=162
06/01/2022 13:22:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.050224 on epoch=164
06/01/2022 13:22:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.020183 on epoch=167
06/01/2022 13:22:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.001707 on epoch=169
06/01/2022 13:22:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.002677 on epoch=172
06/01/2022 13:22:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000889 on epoch=174
06/01/2022 13:22:27 - INFO - __main__ - Global step 700 Train loss 0.015136 Classification-F1 0.5595238095238095 on epoch=174
06/01/2022 13:22:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.015621 on epoch=177
06/01/2022 13:22:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.013230 on epoch=179
06/01/2022 13:22:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.009993 on epoch=182
06/01/2022 13:22:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.002405 on epoch=184
06/01/2022 13:22:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001452 on epoch=187
06/01/2022 13:22:55 - INFO - __main__ - Global step 750 Train loss 0.008540 Classification-F1 0.8162071846282373 on epoch=187
06/01/2022 13:23:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000168 on epoch=189
06/01/2022 13:23:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000827 on epoch=192
06/01/2022 13:23:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003800 on epoch=194
06/01/2022 13:23:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000262 on epoch=197
06/01/2022 13:23:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000376 on epoch=199
06/01/2022 13:23:22 - INFO - __main__ - Global step 800 Train loss 0.001087 Classification-F1 0.8333333333333334 on epoch=199
06/01/2022 13:23:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001209 on epoch=202
06/01/2022 13:23:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000174 on epoch=204
06/01/2022 13:23:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001456 on epoch=207
06/01/2022 13:23:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000596 on epoch=209
06/01/2022 13:23:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001245 on epoch=212
06/01/2022 13:23:49 - INFO - __main__ - Global step 850 Train loss 0.000936 Classification-F1 0.8333333333333334 on epoch=212
06/01/2022 13:23:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000536 on epoch=214
06/01/2022 13:24:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000167 on epoch=217
06/01/2022 13:24:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001308 on epoch=219
06/01/2022 13:24:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000403 on epoch=222
06/01/2022 13:24:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000498 on epoch=224
06/01/2022 13:24:17 - INFO - __main__ - Global step 900 Train loss 0.000582 Classification-F1 0.8162071846282373 on epoch=224
06/01/2022 13:24:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001595 on epoch=227
06/01/2022 13:24:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000185 on epoch=229
06/01/2022 13:24:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.005319 on epoch=232
06/01/2022 13:24:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.007815 on epoch=234
06/01/2022 13:24:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.022389 on epoch=237
06/01/2022 13:24:44 - INFO - __main__ - Global step 950 Train loss 0.007461 Classification-F1 0.8162071846282373 on epoch=237
06/01/2022 13:24:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.008105 on epoch=239
06/01/2022 13:24:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.004985 on epoch=242
06/01/2022 13:25:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.023017 on epoch=244
06/01/2022 13:25:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000385 on epoch=247
06/01/2022 13:25:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000505 on epoch=249
06/01/2022 13:25:11 - INFO - __main__ - Global step 1000 Train loss 0.007399 Classification-F1 0.8333333333333334 on epoch=249
06/01/2022 13:25:11 - INFO - __main__ - save last model!
06/01/2022 13:25:11 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 13:25:11 - INFO - __main__ - Printing 3 examples
06/01/2022 13:25:11 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/01/2022 13:25:11 - INFO - __main__ - ['false']
06/01/2022 13:25:11 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 13:25:11 - INFO - __main__ - ['false']
06/01/2022 13:25:11 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/01/2022 13:25:11 - INFO - __main__ - ['false']
06/01/2022 13:25:11 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:25:11 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:25:11 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 13:25:11 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 13:25:11 - INFO - __main__ - Printing 3 examples
06/01/2022 13:25:11 - INFO - __main__ -  [ethos-race] Fucking idiots. Keep them out. They change the culture. Take over small towns. Vote in their own people to rule the town. And start enforcing their religious beliefs on everyone. It CANNOT FUCKING HAPPEN. Stop them now while you still can.
06/01/2022 13:25:11 - INFO - __main__ - ['false']
06/01/2022 13:25:11 - INFO - __main__ -  [ethos-race] I would beat the shit out of every Russian
06/01/2022 13:25:11 - INFO - __main__ - ['false']
06/01/2022 13:25:11 - INFO - __main__ -  [ethos-race] I can' stand feminist. I like men being manly and I like being treated like a lady. Dear men, if a woman asks for your help, ask if they are feminist. If they say yes, then remind them they can do everything without men and don' help them.
06/01/2022 13:25:11 - INFO - __main__ - ['false']
06/01/2022 13:25:11 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:25:11 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:25:11 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 13:25:17 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 13:25:18 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 13:25:18 - INFO - __main__ - Printing 3 examples
06/01/2022 13:25:18 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 13:25:18 - INFO - __main__ - ['true']
06/01/2022 13:25:18 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 13:25:18 - INFO - __main__ - ['false']
06/01/2022 13:25:18 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 13:25:18 - INFO - __main__ - ['false']
06/01/2022 13:25:18 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:25:18 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:25:18 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 13:25:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_87_0.0002_8_predictions.txt
06/01/2022 13:25:20 - INFO - __main__ - Classification-F1 on test data: 0.7374
06/01/2022 13:25:20 - INFO - __main__ - prefix=ethos-race_32_87, lr=0.0002, bsz=8, dev_performance=0.849624060150376, test_performance=0.7374400852121428
06/01/2022 13:25:20 - INFO - __main__ - Running ... prefix=ethos-race_32_87, lr=0.0001, bsz=8 ...
06/01/2022 13:25:21 - INFO - __main__ - Start tokenizing ... 64 instances
06/01/2022 13:25:21 - INFO - __main__ - Printing 3 examples
06/01/2022 13:25:21 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/01/2022 13:25:21 - INFO - __main__ - ['false']
06/01/2022 13:25:21 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
06/01/2022 13:25:21 - INFO - __main__ - ['false']
06/01/2022 13:25:21 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/01/2022 13:25:21 - INFO - __main__ - ['false']
06/01/2022 13:25:21 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:25:21 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:25:21 - INFO - __main__ - Loaded 64 examples from train data
06/01/2022 13:25:21 - INFO - __main__ - Start tokenizing ... 60 instances
06/01/2022 13:25:21 - INFO - __main__ - Printing 3 examples
06/01/2022 13:25:21 - INFO - __main__ -  [ethos-race] Fucking idiots. Keep them out. They change the culture. Take over small towns. Vote in their own people to rule the town. And start enforcing their religious beliefs on everyone. It CANNOT FUCKING HAPPEN. Stop them now while you still can.
06/01/2022 13:25:21 - INFO - __main__ - ['false']
06/01/2022 13:25:21 - INFO - __main__ -  [ethos-race] I would beat the shit out of every Russian
06/01/2022 13:25:21 - INFO - __main__ - ['false']
06/01/2022 13:25:21 - INFO - __main__ -  [ethos-race] I can' stand feminist. I like men being manly and I like being treated like a lady. Dear men, if a woman asks for your help, ask if they are feminist. If they say yes, then remind them they can do everything without men and don' help them.
06/01/2022 13:25:21 - INFO - __main__ - ['false']
06/01/2022 13:25:21 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:25:21 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:25:21 - INFO - __main__ - Loaded 60 examples from dev data
06/01/2022 13:25:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:25:22 - INFO - __main__ - Starting training!
06/01/2022 13:25:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:25:34 - INFO - __main__ - Starting training!
06/01/2022 13:25:39 - INFO - __main__ - Step 10 Global step 10 Train loss 24.646330 on epoch=2
06/01/2022 13:25:44 - INFO - __main__ - Step 20 Global step 20 Train loss 23.366497 on epoch=4
06/01/2022 13:25:49 - INFO - __main__ - Step 30 Global step 30 Train loss 20.178293 on epoch=7
06/01/2022 13:25:55 - INFO - __main__ - Step 40 Global step 40 Train loss 18.722675 on epoch=9
06/01/2022 13:26:00 - INFO - __main__ - Step 50 Global step 50 Train loss 18.129482 on epoch=12
06/01/2022 13:26:21 - INFO - __main__ - Global step 50 Train loss 21.008656 Classification-F1 0.0 on epoch=12
06/01/2022 13:26:26 - INFO - __main__ - Step 60 Global step 60 Train loss 17.853748 on epoch=14
06/01/2022 13:26:32 - INFO - __main__ - Step 70 Global step 70 Train loss 17.702850 on epoch=17
06/01/2022 13:26:37 - INFO - __main__ - Step 80 Global step 80 Train loss 17.217476 on epoch=19
06/01/2022 13:26:43 - INFO - __main__ - Step 90 Global step 90 Train loss 16.923626 on epoch=22
06/01/2022 13:26:48 - INFO - __main__ - Step 100 Global step 100 Train loss 16.757656 on epoch=24
06/01/2022 13:27:07 - INFO - __main__ - Global step 100 Train loss 17.291071 Classification-F1 0.0 on epoch=24
06/01/2022 13:27:12 - INFO - __main__ - Step 110 Global step 110 Train loss 15.697281 on epoch=27
06/01/2022 13:27:17 - INFO - __main__ - Step 120 Global step 120 Train loss 15.310289 on epoch=29
06/01/2022 13:27:22 - INFO - __main__ - Step 130 Global step 130 Train loss 14.650172 on epoch=32
06/01/2022 13:27:28 - INFO - __main__ - Step 140 Global step 140 Train loss 14.901209 on epoch=34
06/01/2022 13:27:33 - INFO - __main__ - Step 150 Global step 150 Train loss 14.061185 on epoch=37
06/01/2022 13:27:49 - INFO - __main__ - Global step 150 Train loss 14.924027 Classification-F1 0.0 on epoch=37
06/01/2022 13:27:54 - INFO - __main__ - Step 160 Global step 160 Train loss 13.806590 on epoch=39
06/01/2022 13:27:59 - INFO - __main__ - Step 170 Global step 170 Train loss 13.368196 on epoch=42
06/01/2022 13:28:04 - INFO - __main__ - Step 180 Global step 180 Train loss 13.537837 on epoch=44
06/01/2022 13:28:10 - INFO - __main__ - Step 190 Global step 190 Train loss 13.220708 on epoch=47
06/01/2022 13:28:15 - INFO - __main__ - Step 200 Global step 200 Train loss 12.792573 on epoch=49
06/01/2022 13:28:29 - INFO - __main__ - Global step 200 Train loss 13.345181 Classification-F1 0.0 on epoch=49
06/01/2022 13:28:34 - INFO - __main__ - Step 210 Global step 210 Train loss 12.534816 on epoch=52
06/01/2022 13:28:39 - INFO - __main__ - Step 220 Global step 220 Train loss 10.915746 on epoch=54
06/01/2022 13:28:45 - INFO - __main__ - Step 230 Global step 230 Train loss 10.431206 on epoch=57
06/01/2022 13:28:50 - INFO - __main__ - Step 240 Global step 240 Train loss 10.271852 on epoch=59
06/01/2022 13:28:55 - INFO - __main__ - Step 250 Global step 250 Train loss 8.752360 on epoch=62
06/01/2022 13:29:02 - INFO - __main__ - Global step 250 Train loss 10.581196 Classification-F1 0.0 on epoch=62
06/01/2022 13:29:07 - INFO - __main__ - Step 260 Global step 260 Train loss 7.808728 on epoch=64
06/01/2022 13:29:13 - INFO - __main__ - Step 270 Global step 270 Train loss 5.323376 on epoch=67
06/01/2022 13:29:18 - INFO - __main__ - Step 280 Global step 280 Train loss 3.694147 on epoch=69
06/01/2022 13:29:24 - INFO - __main__ - Step 290 Global step 290 Train loss 3.103048 on epoch=72
06/01/2022 13:29:29 - INFO - __main__ - Step 300 Global step 300 Train loss 3.439997 on epoch=74
06/01/2022 13:29:30 - INFO - __main__ - Global step 300 Train loss 4.673859 Classification-F1 0.2600255427841635 on epoch=74
06/01/2022 13:29:36 - INFO - __main__ - Step 310 Global step 310 Train loss 3.840652 on epoch=77
06/01/2022 13:29:41 - INFO - __main__ - Step 320 Global step 320 Train loss 3.236268 on epoch=79
06/01/2022 13:29:46 - INFO - __main__ - Step 330 Global step 330 Train loss 3.179444 on epoch=82
06/01/2022 13:29:52 - INFO - __main__ - Step 340 Global step 340 Train loss 1.668770 on epoch=84
06/01/2022 13:29:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.852165 on epoch=87
06/01/2022 13:29:58 - INFO - __main__ - Global step 350 Train loss 2.555460 Classification-F1 0.475 on epoch=87
06/01/2022 13:30:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.740451 on epoch=89
06/01/2022 13:30:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.562520 on epoch=92
06/01/2022 13:30:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.423148 on epoch=94
06/01/2022 13:30:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.355527 on epoch=97
06/01/2022 13:30:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.441447 on epoch=99
06/01/2022 13:30:26 - INFO - __main__ - Global step 400 Train loss 0.504618 Classification-F1 0.6832453459294249 on epoch=99
06/01/2022 13:30:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.436842 on epoch=102
06/01/2022 13:30:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.338583 on epoch=104
06/01/2022 13:30:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.300551 on epoch=107
06/01/2022 13:30:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.379674 on epoch=109
06/01/2022 13:30:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.334383 on epoch=112
06/01/2022 13:30:54 - INFO - __main__ - Global step 450 Train loss 0.358007 Classification-F1 0.7493734335839599 on epoch=112
06/01/2022 13:31:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.312560 on epoch=114
06/01/2022 13:31:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.306300 on epoch=117
06/01/2022 13:31:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.291246 on epoch=119
06/01/2022 13:31:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.275731 on epoch=122
06/01/2022 13:31:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.201460 on epoch=124
06/01/2022 13:31:22 - INFO - __main__ - Global step 500 Train loss 0.277459 Classification-F1 0.7442455242966752 on epoch=124
06/01/2022 13:31:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.193386 on epoch=127
06/01/2022 13:31:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.170605 on epoch=129
06/01/2022 13:31:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.170217 on epoch=132
06/01/2022 13:31:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.230882 on epoch=134
06/01/2022 13:31:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.150210 on epoch=137
06/01/2022 13:31:49 - INFO - __main__ - Global step 550 Train loss 0.183060 Classification-F1 0.5885714285714285 on epoch=137
06/01/2022 13:31:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.107361 on epoch=139
06/01/2022 13:31:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.307044 on epoch=142
06/01/2022 13:32:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.214132 on epoch=144
06/01/2022 13:32:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.089527 on epoch=147
06/01/2022 13:32:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.137758 on epoch=149
06/01/2022 13:32:16 - INFO - __main__ - Global step 600 Train loss 0.171164 Classification-F1 0.7499305362600722 on epoch=149
06/01/2022 13:32:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.115758 on epoch=152
06/01/2022 13:32:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.187322 on epoch=154
06/01/2022 13:32:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.070795 on epoch=157
06/01/2022 13:32:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.112014 on epoch=159
06/01/2022 13:32:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.130318 on epoch=162
06/01/2022 13:32:44 - INFO - __main__ - Global step 650 Train loss 0.123241 Classification-F1 0.765625 on epoch=162
06/01/2022 13:32:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.088594 on epoch=164
06/01/2022 13:32:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.094651 on epoch=167
06/01/2022 13:33:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.087432 on epoch=169
06/01/2022 13:33:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.085545 on epoch=172
06/01/2022 13:33:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.114459 on epoch=174
06/01/2022 13:33:12 - INFO - __main__ - Global step 700 Train loss 0.094136 Classification-F1 0.7159565580618211 on epoch=174
06/01/2022 13:33:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.066053 on epoch=177
06/01/2022 13:33:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.048481 on epoch=179
06/01/2022 13:33:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.055358 on epoch=182
06/01/2022 13:33:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.060032 on epoch=184
06/01/2022 13:33:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.114610 on epoch=187
06/01/2022 13:33:39 - INFO - __main__ - Global step 750 Train loss 0.068907 Classification-F1 0.7991071428571428 on epoch=187
06/01/2022 13:33:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.074339 on epoch=189
06/01/2022 13:33:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.059843 on epoch=192
06/01/2022 13:33:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.066613 on epoch=194
06/01/2022 13:34:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.055495 on epoch=197
06/01/2022 13:34:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.031833 on epoch=199
06/01/2022 13:34:06 - INFO - __main__ - Global step 800 Train loss 0.057625 Classification-F1 0.6811188811188811 on epoch=199
06/01/2022 13:34:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.052619 on epoch=202
06/01/2022 13:34:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.049794 on epoch=204
06/01/2022 13:34:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.026333 on epoch=207
06/01/2022 13:34:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.045944 on epoch=209
06/01/2022 13:34:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.050082 on epoch=212
06/01/2022 13:34:33 - INFO - __main__ - Global step 850 Train loss 0.044954 Classification-F1 0.7465502675302732 on epoch=212
06/01/2022 13:34:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.042101 on epoch=214
06/01/2022 13:34:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.060950 on epoch=217
06/01/2022 13:34:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.080089 on epoch=219
06/01/2022 13:34:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.030474 on epoch=222
06/01/2022 13:35:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.077281 on epoch=224
06/01/2022 13:35:01 - INFO - __main__ - Global step 900 Train loss 0.058179 Classification-F1 0.7643097643097644 on epoch=224
06/01/2022 13:35:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.059934 on epoch=227
06/01/2022 13:35:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.038763 on epoch=229
06/01/2022 13:35:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.112751 on epoch=232
06/01/2022 13:35:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.050097 on epoch=234
06/01/2022 13:35:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.054961 on epoch=237
06/01/2022 13:35:28 - INFO - __main__ - Global step 950 Train loss 0.063301 Classification-F1 0.7321428571428572 on epoch=237
06/01/2022 13:35:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.053969 on epoch=239
06/01/2022 13:35:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.057736 on epoch=242
06/01/2022 13:35:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.055783 on epoch=244
06/01/2022 13:35:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.064071 on epoch=247
06/01/2022 13:35:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.018449 on epoch=249
06/01/2022 13:35:55 - INFO - __main__ - Global step 1000 Train loss 0.050002 Classification-F1 0.7493734335839599 on epoch=249
06/01/2022 13:35:55 - INFO - __main__ - save last model!
06/01/2022 13:36:03 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 13:36:03 - INFO - __main__ - Start tokenizing ... 87 instances
06/01/2022 13:36:03 - INFO - __main__ - Printing 3 examples
06/01/2022 13:36:03 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/01/2022 13:36:03 - INFO - __main__ - ['true']
06/01/2022 13:36:03 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
06/01/2022 13:36:03 - INFO - __main__ - ['false']
06/01/2022 13:36:03 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
06/01/2022 13:36:03 - INFO - __main__ - ['false']
06/01/2022 13:36:03 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:36:03 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:36:03 - INFO - __main__ - Loaded 87 examples from test data
06/01/2022 13:36:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-ethos-race/ethos-race_32_87_0.0001_8_predictions.txt
06/01/2022 13:36:05 - INFO - __main__ - Classification-F1 on test data: 0.8182
06/01/2022 13:36:05 - INFO - __main__ - prefix=ethos-race_32_87, lr=0.0001, bsz=8, dev_performance=0.7991071428571428, test_performance=0.8182336182336182
