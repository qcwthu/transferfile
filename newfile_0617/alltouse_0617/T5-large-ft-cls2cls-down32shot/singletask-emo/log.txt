05/21/2022 21:20:48 - INFO - __main__ - Namespace(task_dir='data_32/emo/', task_name='emo', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:20:48 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-emo
05/21/2022 21:20:48 - INFO - __main__ - Namespace(task_dir='data_32/emo/', task_name='emo', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:20:48 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-emo
05/21/2022 21:20:50 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:20:50 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:20:50 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:20:50 - INFO - __main__ - Using 2 gpus
05/21/2022 21:20:50 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:20:50 - INFO - __main__ - Using 2 gpus
05/21/2022 21:20:50 - INFO - __main__ - Fine-tuning the following samples: ['emo_32_100', 'emo_32_13', 'emo_32_21', 'emo_32_42', 'emo_32_87']
05/21/2022 21:20:50 - INFO - __main__ - Fine-tuning the following samples: ['emo_32_100', 'emo_32_13', 'emo_32_21', 'emo_32_42', 'emo_32_87']
05/21/2022 21:20:55 - INFO - __main__ - Running ... prefix=emo_32_100, lr=0.0005, bsz=8 ...
05/31/2022 09:34:02 - INFO - __main__ - Namespace(task_dir='data_32/emo/', task_name='emo', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/31/2022 09:34:02 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-emo
05/31/2022 09:34:02 - INFO - __main__ - Namespace(task_dir='data_32/emo/', task_name='emo', identifier='T5-large-ft-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down32shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/31/2022 09:34:02 - INFO - __main__ - models/T5-large-ft-cls2cls-down32shot/singletask-emo
05/31/2022 09:34:02 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/31/2022 09:34:02 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/31/2022 09:34:02 - INFO - __main__ - args.device: cuda:0
05/31/2022 09:34:02 - INFO - __main__ - Using 2 gpus
05/31/2022 09:34:02 - INFO - __main__ - args.device: cuda:1
05/31/2022 09:34:02 - INFO - __main__ - Using 2 gpus
05/31/2022 09:34:02 - INFO - __main__ - Fine-tuning the following samples: ['emo_32_100', 'emo_32_13', 'emo_32_21', 'emo_32_42', 'emo_32_87']
05/31/2022 09:34:02 - INFO - __main__ - Fine-tuning the following samples: ['emo_32_100', 'emo_32_13', 'emo_32_21', 'emo_32_42', 'emo_32_87']
05/31/2022 09:34:07 - INFO - __main__ - Running ... prefix=emo_32_100, lr=0.0005, bsz=8 ...
05/31/2022 09:34:08 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:34:08 - INFO - __main__ - Printing 3 examples
05/31/2022 09:34:08 - INFO - __main__ -  [emo] how cause yes am listening
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:34:08 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:34:08 - INFO - __main__ - Printing 3 examples
05/31/2022 09:34:08 - INFO - __main__ -  [emo] how cause yes am listening
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:34:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:34:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:34:08 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 09:34:08 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:34:08 - INFO - __main__ - Printing 3 examples
05/31/2022 09:34:08 - INFO - __main__ -  [emo] do you like chicken who calls it soya chicken anyways  what i'm asking
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ -  [emo] i will i don't want to lose my bestie because i did not ask u wgere u r from
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ -  [emo] author name like that give me a description or summary hey that is author name yar
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:34:08 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 09:34:08 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:34:08 - INFO - __main__ - Printing 3 examples
05/31/2022 09:34:08 - INFO - __main__ -  [emo] do you like chicken who calls it soya chicken anyways  what i'm asking
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ -  [emo] i will i don't want to lose my bestie because i did not ask u wgere u r from
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ -  [emo] author name like that give me a description or summary hey that is author name yar
05/31/2022 09:34:08 - INFO - __main__ - ['others']
05/31/2022 09:34:08 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:34:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:34:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:34:08 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 09:34:08 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 09:34:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 09:34:21 - INFO - __main__ - Starting training!
05/31/2022 09:34:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 09:34:22 - INFO - __main__ - Starting training!
05/31/2022 09:34:26 - INFO - __main__ - Step 10 Global step 10 Train loss 24.943802 on epoch=1
05/31/2022 09:34:31 - INFO - __main__ - Step 20 Global step 20 Train loss 18.504263 on epoch=2
05/31/2022 09:34:36 - INFO - __main__ - Step 30 Global step 30 Train loss 16.680386 on epoch=3
05/31/2022 09:34:41 - INFO - __main__ - Step 40 Global step 40 Train loss 14.744168 on epoch=4
05/31/2022 09:34:46 - INFO - __main__ - Step 50 Global step 50 Train loss 13.275638 on epoch=6
05/31/2022 09:34:48 - INFO - __main__ - Global step 50 Train loss 17.629650 Classification-F1 0.0018365472910927456 on epoch=6
05/31/2022 09:34:53 - INFO - __main__ - Step 60 Global step 60 Train loss 11.053358 on epoch=7
05/31/2022 09:34:58 - INFO - __main__ - Step 70 Global step 70 Train loss 9.446592 on epoch=8
05/31/2022 09:35:03 - INFO - __main__ - Step 80 Global step 80 Train loss 5.216181 on epoch=9
05/31/2022 09:35:08 - INFO - __main__ - Step 90 Global step 90 Train loss 3.959331 on epoch=11
05/31/2022 09:35:13 - INFO - __main__ - Step 100 Global step 100 Train loss 3.655448 on epoch=12
05/31/2022 09:35:14 - INFO - __main__ - Global step 100 Train loss 6.666182 Classification-F1 0.1 on epoch=12
05/31/2022 09:35:21 - INFO - __main__ - Step 110 Global step 110 Train loss 4.039355 on epoch=13
05/31/2022 09:35:26 - INFO - __main__ - Step 120 Global step 120 Train loss 3.051652 on epoch=14
05/31/2022 09:35:31 - INFO - __main__ - Step 130 Global step 130 Train loss 3.203951 on epoch=16
05/31/2022 09:35:36 - INFO - __main__ - Step 140 Global step 140 Train loss 2.294821 on epoch=17
05/31/2022 09:35:41 - INFO - __main__ - Step 150 Global step 150 Train loss 2.446980 on epoch=18
05/31/2022 09:35:42 - INFO - __main__ - Global step 150 Train loss 3.007352 Classification-F1 0.2260989010989011 on epoch=18
05/31/2022 09:35:47 - INFO - __main__ - Step 160 Global step 160 Train loss 2.187787 on epoch=19
05/31/2022 09:35:52 - INFO - __main__ - Step 170 Global step 170 Train loss 2.802193 on epoch=21
05/31/2022 09:35:57 - INFO - __main__ - Step 180 Global step 180 Train loss 1.534154 on epoch=22
05/31/2022 09:36:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.840504 on epoch=23
05/31/2022 09:36:07 - INFO - __main__ - Step 200 Global step 200 Train loss 1.333652 on epoch=24
05/31/2022 09:36:08 - INFO - __main__ - Global step 200 Train loss 1.739658 Classification-F1 0.3077848996419006 on epoch=24
05/31/2022 09:36:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.796461 on epoch=26
05/31/2022 09:36:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.697714 on epoch=27
05/31/2022 09:36:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.712011 on epoch=28
05/31/2022 09:36:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.735362 on epoch=29
05/31/2022 09:36:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.743380 on epoch=31
05/31/2022 09:36:36 - INFO - __main__ - Global step 250 Train loss 0.736986 Classification-F1 0.3339686943102276 on epoch=31
05/31/2022 09:36:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.657974 on epoch=32
05/31/2022 09:36:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.616074 on epoch=33
05/31/2022 09:36:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.573288 on epoch=34
05/31/2022 09:36:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.603118 on epoch=36
05/31/2022 09:37:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.548222 on epoch=37
05/31/2022 09:37:03 - INFO - __main__ - Global step 300 Train loss 0.599735 Classification-F1 0.42358019369095673 on epoch=37
05/31/2022 09:37:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.589694 on epoch=38
05/31/2022 09:37:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.516440 on epoch=39
05/31/2022 09:37:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.416101 on epoch=41
05/31/2022 09:37:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.466397 on epoch=42
05/31/2022 09:37:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.367475 on epoch=43
05/31/2022 09:37:30 - INFO - __main__ - Global step 350 Train loss 0.471221 Classification-F1 0.7800375698119435 on epoch=43
05/31/2022 09:37:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.336423 on epoch=44
05/31/2022 09:37:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.363574 on epoch=46
05/31/2022 09:37:46 - INFO - __main__ - Step 380 Global step 380 Train loss 1.033883 on epoch=47
05/31/2022 09:37:51 - INFO - __main__ - Step 390 Global step 390 Train loss 1.750660 on epoch=48
05/31/2022 09:37:56 - INFO - __main__ - Step 400 Global step 400 Train loss 1.282013 on epoch=49
05/31/2022 09:37:57 - INFO - __main__ - Global step 400 Train loss 0.953311 Classification-F1 0.37974112625275414 on epoch=49
05/31/2022 09:38:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.363083 on epoch=51
05/31/2022 09:38:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.341737 on epoch=52
05/31/2022 09:38:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.246592 on epoch=53
05/31/2022 09:38:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.188780 on epoch=54
05/31/2022 09:38:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.252788 on epoch=56
05/31/2022 09:38:23 - INFO - __main__ - Global step 450 Train loss 0.278596 Classification-F1 0.6552654201590372 on epoch=56
05/31/2022 09:38:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.161716 on epoch=57
05/31/2022 09:38:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.136511 on epoch=58
05/31/2022 09:38:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.107716 on epoch=59
05/31/2022 09:38:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.055605 on epoch=61
05/31/2022 09:38:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.079569 on epoch=62
05/31/2022 09:38:49 - INFO - __main__ - Global step 500 Train loss 0.108223 Classification-F1 0.7770929635336414 on epoch=62
05/31/2022 09:38:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.061464 on epoch=63
05/31/2022 09:38:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.059152 on epoch=64
05/31/2022 09:39:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.058523 on epoch=66
05/31/2022 09:39:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.024966 on epoch=67
05/31/2022 09:39:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.076117 on epoch=68
05/31/2022 09:39:15 - INFO - __main__ - Global step 550 Train loss 0.056044 Classification-F1 0.651817003083824 on epoch=68
05/31/2022 09:39:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.023949 on epoch=69
05/31/2022 09:39:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.049480 on epoch=71
05/31/2022 09:39:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.052592 on epoch=72
05/31/2022 09:39:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.051166 on epoch=73
05/31/2022 09:39:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.007441 on epoch=74
05/31/2022 09:39:41 - INFO - __main__ - Global step 600 Train loss 0.036926 Classification-F1 0.5108197205604904 on epoch=74
05/31/2022 09:39:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.003111 on epoch=76
05/31/2022 09:39:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.016106 on epoch=77
05/31/2022 09:39:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.030704 on epoch=78
05/31/2022 09:40:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.038390 on epoch=79
05/31/2022 09:40:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.031735 on epoch=81
05/31/2022 09:40:07 - INFO - __main__ - Global step 650 Train loss 0.024009 Classification-F1 0.6112662265410578 on epoch=81
05/31/2022 09:40:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.029164 on epoch=82
05/31/2022 09:40:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.009090 on epoch=83
05/31/2022 09:40:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.005529 on epoch=84
05/31/2022 09:40:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.005061 on epoch=86
05/31/2022 09:40:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.005736 on epoch=87
05/31/2022 09:40:34 - INFO - __main__ - Global step 700 Train loss 0.010916 Classification-F1 0.6096042224773568 on epoch=87
05/31/2022 09:40:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.036913 on epoch=88
05/31/2022 09:40:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.090721 on epoch=89
05/31/2022 09:40:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001673 on epoch=91
05/31/2022 09:40:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.013738 on epoch=92
05/31/2022 09:40:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.007238 on epoch=93
05/31/2022 09:40:59 - INFO - __main__ - Global step 750 Train loss 0.030056 Classification-F1 0.7054238290185919 on epoch=93
05/31/2022 09:41:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.041355 on epoch=94
05/31/2022 09:41:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.057707 on epoch=96
05/31/2022 09:41:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.015052 on epoch=97
05/31/2022 09:41:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.019645 on epoch=98
05/31/2022 09:41:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.013857 on epoch=99
05/31/2022 09:41:26 - INFO - __main__ - Global step 800 Train loss 0.029523 Classification-F1 0.6065579710144927 on epoch=99
05/31/2022 09:41:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.020245 on epoch=101
05/31/2022 09:41:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.053323 on epoch=102
05/31/2022 09:41:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.078798 on epoch=103
05/31/2022 09:41:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.012528 on epoch=104
05/31/2022 09:41:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.018957 on epoch=106
05/31/2022 09:41:52 - INFO - __main__ - Global step 850 Train loss 0.036770 Classification-F1 0.49804535976184966 on epoch=106
05/31/2022 09:41:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.007364 on epoch=107
05/31/2022 09:42:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.009120 on epoch=108
05/31/2022 09:42:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.039418 on epoch=109
05/31/2022 09:42:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.012660 on epoch=111
05/31/2022 09:42:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000700 on epoch=112
05/31/2022 09:42:18 - INFO - __main__ - Global step 900 Train loss 0.013852 Classification-F1 0.62473604826546 on epoch=112
05/31/2022 09:42:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001153 on epoch=113
05/31/2022 09:42:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000370 on epoch=114
05/31/2022 09:42:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.017174 on epoch=116
05/31/2022 09:42:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.013577 on epoch=117
05/31/2022 09:42:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.003696 on epoch=118
05/31/2022 09:42:43 - INFO - __main__ - Global step 950 Train loss 0.007194 Classification-F1 0.6300097014363297 on epoch=118
05/31/2022 09:42:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000309 on epoch=119
05/31/2022 09:42:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.002450 on epoch=121
05/31/2022 09:42:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.003418 on epoch=122
05/31/2022 09:43:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.005583 on epoch=123
05/31/2022 09:43:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001048 on epoch=124
05/31/2022 09:43:09 - INFO - __main__ - Global step 1000 Train loss 0.002562 Classification-F1 0.5011241644253058 on epoch=124
05/31/2022 09:43:09 - INFO - __main__ - save last model!
05/31/2022 09:43:09 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:43:09 - INFO - __main__ - Printing 3 examples
05/31/2022 09:43:09 - INFO - __main__ -  [emo] how cause yes am listening
05/31/2022 09:43:09 - INFO - __main__ - ['others']
05/31/2022 09:43:09 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/31/2022 09:43:09 - INFO - __main__ - ['others']
05/31/2022 09:43:09 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/31/2022 09:43:09 - INFO - __main__ - ['others']
05/31/2022 09:43:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:43:09 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:43:10 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 09:43:10 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:43:10 - INFO - __main__ - Printing 3 examples
05/31/2022 09:43:10 - INFO - __main__ -  [emo] do you like chicken who calls it soya chicken anyways  what i'm asking
05/31/2022 09:43:10 - INFO - __main__ - ['others']
05/31/2022 09:43:10 - INFO - __main__ -  [emo] i will i don't want to lose my bestie because i did not ask u wgere u r from
05/31/2022 09:43:10 - INFO - __main__ - ['others']
05/31/2022 09:43:10 - INFO - __main__ -  [emo] author name like that give me a description or summary hey that is author name yar
05/31/2022 09:43:10 - INFO - __main__ - ['others']
05/31/2022 09:43:10 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:43:10 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:43:10 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 09:43:16 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 09:43:17 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 09:43:17 - INFO - __main__ - Printing 3 examples
05/31/2022 09:43:17 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 09:43:17 - INFO - __main__ - ['others']
05/31/2022 09:43:17 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 09:43:17 - INFO - __main__ - ['others']
05/31/2022 09:43:17 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 09:43:17 - INFO - __main__ - ['others']
05/31/2022 09:43:17 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:43:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:43:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 09:43:22 - INFO - __main__ - Starting training!
05/31/2022 09:43:25 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 09:44:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_100_0.0005_8_predictions.txt
05/31/2022 09:44:07 - INFO - __main__ - Classification-F1 on test data: 0.3466
05/31/2022 09:44:07 - INFO - __main__ - prefix=emo_32_100, lr=0.0005, bsz=8, dev_performance=0.7800375698119435, test_performance=0.3465955313187732
05/31/2022 09:44:07 - INFO - __main__ - Running ... prefix=emo_32_100, lr=0.0003, bsz=8 ...
05/31/2022 09:44:08 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:44:08 - INFO - __main__ - Printing 3 examples
05/31/2022 09:44:08 - INFO - __main__ -  [emo] how cause yes am listening
05/31/2022 09:44:08 - INFO - __main__ - ['others']
05/31/2022 09:44:08 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/31/2022 09:44:08 - INFO - __main__ - ['others']
05/31/2022 09:44:08 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/31/2022 09:44:08 - INFO - __main__ - ['others']
05/31/2022 09:44:08 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:44:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:44:08 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 09:44:08 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:44:08 - INFO - __main__ - Printing 3 examples
05/31/2022 09:44:08 - INFO - __main__ -  [emo] do you like chicken who calls it soya chicken anyways  what i'm asking
05/31/2022 09:44:08 - INFO - __main__ - ['others']
05/31/2022 09:44:08 - INFO - __main__ -  [emo] i will i don't want to lose my bestie because i did not ask u wgere u r from
05/31/2022 09:44:08 - INFO - __main__ - ['others']
05/31/2022 09:44:08 - INFO - __main__ -  [emo] author name like that give me a description or summary hey that is author name yar
05/31/2022 09:44:08 - INFO - __main__ - ['others']
05/31/2022 09:44:08 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:44:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:44:08 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 09:44:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 09:44:21 - INFO - __main__ - Starting training!
05/31/2022 09:44:26 - INFO - __main__ - Step 10 Global step 10 Train loss 26.017994 on epoch=1
05/31/2022 09:44:31 - INFO - __main__ - Step 20 Global step 20 Train loss 19.214918 on epoch=2
05/31/2022 09:44:36 - INFO - __main__ - Step 30 Global step 30 Train loss 18.418827 on epoch=3
05/31/2022 09:44:41 - INFO - __main__ - Step 40 Global step 40 Train loss 17.372858 on epoch=4
05/31/2022 09:44:46 - INFO - __main__ - Step 50 Global step 50 Train loss 16.225719 on epoch=6
05/31/2022 09:44:48 - INFO - __main__ - Global step 50 Train loss 19.450066 Classification-F1 0.0 on epoch=6
05/31/2022 09:44:53 - INFO - __main__ - Step 60 Global step 60 Train loss 14.683859 on epoch=7
05/31/2022 09:44:59 - INFO - __main__ - Step 70 Global step 70 Train loss 14.208066 on epoch=8
05/31/2022 09:45:04 - INFO - __main__ - Step 80 Global step 80 Train loss 12.023964 on epoch=9
05/31/2022 09:45:09 - INFO - __main__ - Step 90 Global step 90 Train loss 11.767748 on epoch=11
05/31/2022 09:45:14 - INFO - __main__ - Step 100 Global step 100 Train loss 10.976861 on epoch=12
05/31/2022 09:45:15 - INFO - __main__ - Global step 100 Train loss 12.732099 Classification-F1 0.0 on epoch=12
05/31/2022 09:45:21 - INFO - __main__ - Step 110 Global step 110 Train loss 9.383612 on epoch=13
05/31/2022 09:45:26 - INFO - __main__ - Step 120 Global step 120 Train loss 7.214456 on epoch=14
05/31/2022 09:45:31 - INFO - __main__ - Step 130 Global step 130 Train loss 4.686795 on epoch=16
05/31/2022 09:45:36 - INFO - __main__ - Step 140 Global step 140 Train loss 4.395871 on epoch=17
05/31/2022 09:45:41 - INFO - __main__ - Step 150 Global step 150 Train loss 3.535297 on epoch=18
05/31/2022 09:45:42 - INFO - __main__ - Global step 150 Train loss 5.843206 Classification-F1 0.23928571428571432 on epoch=18
05/31/2022 09:45:49 - INFO - __main__ - Step 160 Global step 160 Train loss 3.866955 on epoch=19
05/31/2022 09:45:54 - INFO - __main__ - Step 170 Global step 170 Train loss 3.180017 on epoch=21
05/31/2022 09:45:59 - INFO - __main__ - Step 180 Global step 180 Train loss 3.442999 on epoch=22
05/31/2022 09:46:04 - INFO - __main__ - Step 190 Global step 190 Train loss 2.663141 on epoch=23
05/31/2022 09:46:09 - INFO - __main__ - Step 200 Global step 200 Train loss 3.138596 on epoch=24
05/31/2022 09:46:10 - INFO - __main__ - Global step 200 Train loss 3.258342 Classification-F1 0.1 on epoch=24
05/31/2022 09:46:15 - INFO - __main__ - Step 210 Global step 210 Train loss 2.532415 on epoch=26
05/31/2022 09:46:20 - INFO - __main__ - Step 220 Global step 220 Train loss 2.759194 on epoch=27
05/31/2022 09:46:25 - INFO - __main__ - Step 230 Global step 230 Train loss 2.323348 on epoch=28
05/31/2022 09:46:30 - INFO - __main__ - Step 240 Global step 240 Train loss 2.075678 on epoch=29
05/31/2022 09:46:35 - INFO - __main__ - Step 250 Global step 250 Train loss 1.979563 on epoch=31
05/31/2022 09:46:37 - INFO - __main__ - Global step 250 Train loss 2.334039 Classification-F1 0.1581196581196581 on epoch=31
05/31/2022 09:46:42 - INFO - __main__ - Step 260 Global step 260 Train loss 1.955381 on epoch=32
05/31/2022 09:46:47 - INFO - __main__ - Step 270 Global step 270 Train loss 1.554361 on epoch=33
05/31/2022 09:46:52 - INFO - __main__ - Step 280 Global step 280 Train loss 1.507259 on epoch=34
05/31/2022 09:46:57 - INFO - __main__ - Step 290 Global step 290 Train loss 1.421534 on epoch=36
05/31/2022 09:47:02 - INFO - __main__ - Step 300 Global step 300 Train loss 1.490640 on epoch=37
05/31/2022 09:47:03 - INFO - __main__ - Global step 300 Train loss 1.585835 Classification-F1 0.3058922558922559 on epoch=37
05/31/2022 09:47:09 - INFO - __main__ - Step 310 Global step 310 Train loss 1.211186 on epoch=38
05/31/2022 09:47:14 - INFO - __main__ - Step 320 Global step 320 Train loss 1.178578 on epoch=39
05/31/2022 09:47:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.912363 on epoch=41
05/31/2022 09:47:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.931592 on epoch=42
05/31/2022 09:47:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.915141 on epoch=43
05/31/2022 09:47:31 - INFO - __main__ - Global step 350 Train loss 1.029772 Classification-F1 0.48753641997339475 on epoch=43
05/31/2022 09:47:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.736272 on epoch=44
05/31/2022 09:47:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.707757 on epoch=46
05/31/2022 09:47:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.748221 on epoch=47
05/31/2022 09:47:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.745561 on epoch=48
05/31/2022 09:47:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.677383 on epoch=49
05/31/2022 09:47:59 - INFO - __main__ - Global step 400 Train loss 0.723039 Classification-F1 0.6414395363408522 on epoch=49
05/31/2022 09:48:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.693370 on epoch=51
05/31/2022 09:48:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.537719 on epoch=52
05/31/2022 09:48:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.603663 on epoch=53
05/31/2022 09:48:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.619317 on epoch=54
05/31/2022 09:48:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.543047 on epoch=56
05/31/2022 09:48:26 - INFO - __main__ - Global step 450 Train loss 0.599423 Classification-F1 0.6217624507997179 on epoch=56
05/31/2022 09:48:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.472448 on epoch=57
05/31/2022 09:48:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.411452 on epoch=58
05/31/2022 09:48:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.313371 on epoch=59
05/31/2022 09:48:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.468187 on epoch=61
05/31/2022 09:48:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.293774 on epoch=62
05/31/2022 09:48:53 - INFO - __main__ - Global step 500 Train loss 0.391846 Classification-F1 0.6574940582293523 on epoch=62
05/31/2022 09:48:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.328439 on epoch=63
05/31/2022 09:49:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.323081 on epoch=64
05/31/2022 09:49:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.246339 on epoch=66
05/31/2022 09:49:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.164379 on epoch=67
05/31/2022 09:49:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.241654 on epoch=68
05/31/2022 09:49:21 - INFO - __main__ - Global step 550 Train loss 0.260778 Classification-F1 0.6946682500075159 on epoch=68
05/31/2022 09:49:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.158535 on epoch=69
05/31/2022 09:49:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.119540 on epoch=71
05/31/2022 09:49:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.130999 on epoch=72
05/31/2022 09:49:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.143237 on epoch=73
05/31/2022 09:49:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.123180 on epoch=74
05/31/2022 09:49:48 - INFO - __main__ - Global step 600 Train loss 0.135098 Classification-F1 0.6986861515766181 on epoch=74
05/31/2022 09:49:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.139260 on epoch=76
05/31/2022 09:49:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.113767 on epoch=77
05/31/2022 09:50:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.102013 on epoch=78
05/31/2022 09:50:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.108401 on epoch=79
05/31/2022 09:50:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.097445 on epoch=81
05/31/2022 09:50:15 - INFO - __main__ - Global step 650 Train loss 0.112177 Classification-F1 0.7702892589326796 on epoch=81
05/31/2022 09:50:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.062433 on epoch=82
05/31/2022 09:50:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.090152 on epoch=83
05/31/2022 09:50:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.140888 on epoch=84
05/31/2022 09:50:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.096745 on epoch=86
05/31/2022 09:50:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.069035 on epoch=87
05/31/2022 09:50:42 - INFO - __main__ - Global step 700 Train loss 0.091850 Classification-F1 0.7486425834701695 on epoch=87
05/31/2022 09:50:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.053898 on epoch=88
05/31/2022 09:50:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.090431 on epoch=89
05/31/2022 09:50:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.052000 on epoch=91
05/31/2022 09:51:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.052074 on epoch=92
05/31/2022 09:51:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.101910 on epoch=93
05/31/2022 09:51:09 - INFO - __main__ - Global step 750 Train loss 0.070063 Classification-F1 0.7209539335383296 on epoch=93
05/31/2022 09:51:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.118489 on epoch=94
05/31/2022 09:51:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.187117 on epoch=96
05/31/2022 09:51:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.090871 on epoch=97
05/31/2022 09:51:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.020539 on epoch=98
05/31/2022 09:51:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.090359 on epoch=99
05/31/2022 09:51:35 - INFO - __main__ - Global step 800 Train loss 0.101475 Classification-F1 0.7215422546318202 on epoch=99
05/31/2022 09:51:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.073492 on epoch=101
05/31/2022 09:51:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.039825 on epoch=102
05/31/2022 09:51:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.085818 on epoch=103
05/31/2022 09:51:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.034661 on epoch=104
05/31/2022 09:52:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.039571 on epoch=106
05/31/2022 09:52:01 - INFO - __main__ - Global step 850 Train loss 0.054674 Classification-F1 0.7080872154877705 on epoch=106
05/31/2022 09:52:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.023031 on epoch=107
05/31/2022 09:52:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.036987 on epoch=108
05/31/2022 09:52:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.032999 on epoch=109
05/31/2022 09:52:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.035712 on epoch=111
05/31/2022 09:52:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.009226 on epoch=112
05/31/2022 09:52:27 - INFO - __main__ - Global step 900 Train loss 0.027591 Classification-F1 0.7543955185868605 on epoch=112
05/31/2022 09:52:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.028200 on epoch=113
05/31/2022 09:52:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.035857 on epoch=114
05/31/2022 09:52:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.027844 on epoch=116
05/31/2022 09:52:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.006206 on epoch=117
05/31/2022 09:52:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.031855 on epoch=118
05/31/2022 09:52:54 - INFO - __main__ - Global step 950 Train loss 0.025992 Classification-F1 0.7502718658695469 on epoch=118
05/31/2022 09:52:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.034832 on epoch=119
05/31/2022 09:53:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.043621 on epoch=121
05/31/2022 09:53:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.013964 on epoch=122
05/31/2022 09:53:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.025006 on epoch=123
05/31/2022 09:53:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.049872 on epoch=124
05/31/2022 09:53:20 - INFO - __main__ - Global step 1000 Train loss 0.033459 Classification-F1 0.752149208031561 on epoch=124
05/31/2022 09:53:20 - INFO - __main__ - save last model!
05/31/2022 09:53:20 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:53:20 - INFO - __main__ - Printing 3 examples
05/31/2022 09:53:20 - INFO - __main__ -  [emo] how cause yes am listening
05/31/2022 09:53:20 - INFO - __main__ - ['others']
05/31/2022 09:53:20 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/31/2022 09:53:20 - INFO - __main__ - ['others']
05/31/2022 09:53:20 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/31/2022 09:53:20 - INFO - __main__ - ['others']
05/31/2022 09:53:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:53:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:53:20 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 09:53:20 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:53:20 - INFO - __main__ - Printing 3 examples
05/31/2022 09:53:20 - INFO - __main__ -  [emo] do you like chicken who calls it soya chicken anyways  what i'm asking
05/31/2022 09:53:20 - INFO - __main__ - ['others']
05/31/2022 09:53:20 - INFO - __main__ -  [emo] i will i don't want to lose my bestie because i did not ask u wgere u r from
05/31/2022 09:53:20 - INFO - __main__ - ['others']
05/31/2022 09:53:20 - INFO - __main__ -  [emo] author name like that give me a description or summary hey that is author name yar
05/31/2022 09:53:20 - INFO - __main__ - ['others']
05/31/2022 09:53:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:53:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:53:21 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 09:53:27 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 09:53:27 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 09:53:27 - INFO - __main__ - Printing 3 examples
05/31/2022 09:53:27 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 09:53:27 - INFO - __main__ - ['others']
05/31/2022 09:53:27 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 09:53:27 - INFO - __main__ - ['others']
05/31/2022 09:53:27 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 09:53:27 - INFO - __main__ - ['others']
05/31/2022 09:53:27 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:53:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:53:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 09:53:32 - INFO - __main__ - Starting training!
05/31/2022 09:53:35 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 09:54:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_100_0.0003_8_predictions.txt
05/31/2022 09:54:17 - INFO - __main__ - Classification-F1 on test data: 0.1432
05/31/2022 09:54:17 - INFO - __main__ - prefix=emo_32_100, lr=0.0003, bsz=8, dev_performance=0.7702892589326796, test_performance=0.14321139508116457
05/31/2022 09:54:17 - INFO - __main__ - Running ... prefix=emo_32_100, lr=0.0002, bsz=8 ...
05/31/2022 09:54:18 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:54:18 - INFO - __main__ - Printing 3 examples
05/31/2022 09:54:18 - INFO - __main__ -  [emo] how cause yes am listening
05/31/2022 09:54:18 - INFO - __main__ - ['others']
05/31/2022 09:54:18 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/31/2022 09:54:18 - INFO - __main__ - ['others']
05/31/2022 09:54:18 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/31/2022 09:54:18 - INFO - __main__ - ['others']
05/31/2022 09:54:18 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:54:18 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:54:18 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 09:54:18 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 09:54:18 - INFO - __main__ - Printing 3 examples
05/31/2022 09:54:18 - INFO - __main__ -  [emo] do you like chicken who calls it soya chicken anyways  what i'm asking
05/31/2022 09:54:18 - INFO - __main__ - ['others']
05/31/2022 09:54:18 - INFO - __main__ -  [emo] i will i don't want to lose my bestie because i did not ask u wgere u r from
05/31/2022 09:54:18 - INFO - __main__ - ['others']
05/31/2022 09:54:18 - INFO - __main__ -  [emo] author name like that give me a description or summary hey that is author name yar
05/31/2022 09:54:18 - INFO - __main__ - ['others']
05/31/2022 09:54:18 - INFO - __main__ - Tokenizing Input ...
05/31/2022 09:54:18 - INFO - __main__ - Tokenizing Output ...
05/31/2022 09:54:18 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 09:54:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 09:54:29 - INFO - __main__ - Starting training!
05/31/2022 09:54:33 - INFO - __main__ - Step 10 Global step 10 Train loss 25.123802 on epoch=1
05/31/2022 09:54:38 - INFO - __main__ - Step 20 Global step 20 Train loss 20.847275 on epoch=2
05/31/2022 09:54:43 - INFO - __main__ - Step 30 Global step 30 Train loss 19.473639 on epoch=3
05/31/2022 09:54:48 - INFO - __main__ - Step 40 Global step 40 Train loss 18.076351 on epoch=4
05/31/2022 09:54:53 - INFO - __main__ - Step 50 Global step 50 Train loss 16.945335 on epoch=6
05/31/2022 09:55:30 - INFO - __main__ - Global step 50 Train loss 20.093281 Classification-F1 0.0 on epoch=6
05/31/2022 09:55:36 - INFO - __main__ - Step 60 Global step 60 Train loss 16.262045 on epoch=7
05/31/2022 09:55:40 - INFO - __main__ - Step 70 Global step 70 Train loss 16.173796 on epoch=8
05/31/2022 09:55:45 - INFO - __main__ - Step 80 Global step 80 Train loss 14.844612 on epoch=9
05/31/2022 09:55:50 - INFO - __main__ - Step 90 Global step 90 Train loss 14.389279 on epoch=11
05/31/2022 09:55:55 - INFO - __main__ - Step 100 Global step 100 Train loss 13.156921 on epoch=12
05/31/2022 09:56:20 - INFO - __main__ - Global step 100 Train loss 14.965331 Classification-F1 0.000819000819000819 on epoch=12
05/31/2022 09:56:26 - INFO - __main__ - Step 110 Global step 110 Train loss 12.386532 on epoch=13
05/31/2022 09:56:31 - INFO - __main__ - Step 120 Global step 120 Train loss 13.437599 on epoch=14
05/31/2022 09:56:36 - INFO - __main__ - Step 130 Global step 130 Train loss 11.931379 on epoch=16
05/31/2022 09:56:41 - INFO - __main__ - Step 140 Global step 140 Train loss 11.263811 on epoch=17
05/31/2022 09:56:46 - INFO - __main__ - Step 150 Global step 150 Train loss 10.014123 on epoch=18
05/31/2022 09:56:49 - INFO - __main__ - Global step 150 Train loss 11.806689 Classification-F1 0.0 on epoch=18
05/31/2022 09:56:54 - INFO - __main__ - Step 160 Global step 160 Train loss 8.737082 on epoch=19
05/31/2022 09:56:59 - INFO - __main__ - Step 170 Global step 170 Train loss 7.060991 on epoch=21
05/31/2022 09:57:03 - INFO - __main__ - Step 180 Global step 180 Train loss 4.725131 on epoch=22
05/31/2022 09:57:08 - INFO - __main__ - Step 190 Global step 190 Train loss 4.535535 on epoch=23
05/31/2022 09:57:13 - INFO - __main__ - Step 200 Global step 200 Train loss 3.032200 on epoch=24
05/31/2022 09:57:14 - INFO - __main__ - Global step 200 Train loss 5.618188 Classification-F1 0.11307391734358026 on epoch=24
05/31/2022 09:57:20 - INFO - __main__ - Step 210 Global step 210 Train loss 3.786617 on epoch=26
05/31/2022 09:57:25 - INFO - __main__ - Step 220 Global step 220 Train loss 2.414616 on epoch=27
05/31/2022 09:57:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.884265 on epoch=28
05/31/2022 09:57:35 - INFO - __main__ - Step 240 Global step 240 Train loss 1.052067 on epoch=29
05/31/2022 09:57:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.854981 on epoch=31
05/31/2022 09:57:41 - INFO - __main__ - Global step 250 Train loss 1.798509 Classification-F1 0.26989347011244824 on epoch=31
05/31/2022 09:57:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.731366 on epoch=32
05/31/2022 09:57:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.887963 on epoch=33
05/31/2022 09:57:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.760689 on epoch=34
05/31/2022 09:58:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.849180 on epoch=36
05/31/2022 09:58:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.774274 on epoch=37
05/31/2022 09:58:08 - INFO - __main__ - Global step 300 Train loss 0.800694 Classification-F1 0.49256297465974885 on epoch=37
05/31/2022 09:58:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.758406 on epoch=38
05/31/2022 09:58:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.723044 on epoch=39
05/31/2022 09:58:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.715295 on epoch=41
05/31/2022 09:58:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.680171 on epoch=42
05/31/2022 09:58:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.676048 on epoch=43
05/31/2022 09:58:35 - INFO - __main__ - Global step 350 Train loss 0.710593 Classification-F1 0.5316002770435247 on epoch=43
05/31/2022 09:58:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.604779 on epoch=44
05/31/2022 09:58:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.645703 on epoch=46
05/31/2022 09:58:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.629431 on epoch=47
05/31/2022 09:58:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.612672 on epoch=48
05/31/2022 09:59:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.647158 on epoch=49
05/31/2022 09:59:02 - INFO - __main__ - Global step 400 Train loss 0.627948 Classification-F1 0.5307175932175933 on epoch=49
05/31/2022 09:59:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.611191 on epoch=51
05/31/2022 09:59:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.565690 on epoch=52
05/31/2022 09:59:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.585275 on epoch=53
05/31/2022 09:59:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.586912 on epoch=54
05/31/2022 09:59:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.555150 on epoch=56
05/31/2022 09:59:28 - INFO - __main__ - Global step 450 Train loss 0.580844 Classification-F1 0.718436580994518 on epoch=56
05/31/2022 09:59:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.458962 on epoch=57
05/31/2022 09:59:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.504483 on epoch=58
05/31/2022 09:59:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.380349 on epoch=59
05/31/2022 09:59:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.428031 on epoch=61
05/31/2022 09:59:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.475393 on epoch=62
05/31/2022 09:59:56 - INFO - __main__ - Global step 500 Train loss 0.449444 Classification-F1 0.5709618110933901 on epoch=62
05/31/2022 10:00:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.458178 on epoch=63
05/31/2022 10:00:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.441546 on epoch=64
05/31/2022 10:00:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.437895 on epoch=66
05/31/2022 10:00:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.485000 on epoch=67
05/31/2022 10:00:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.403726 on epoch=68
05/31/2022 10:00:22 - INFO - __main__ - Global step 550 Train loss 0.445269 Classification-F1 0.7114774114774115 on epoch=68
05/31/2022 10:00:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.512634 on epoch=69
05/31/2022 10:00:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.445038 on epoch=71
05/31/2022 10:00:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.484156 on epoch=72
05/31/2022 10:00:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.539302 on epoch=73
05/31/2022 10:00:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.488097 on epoch=74
05/31/2022 10:00:49 - INFO - __main__ - Global step 600 Train loss 0.493845 Classification-F1 0.5701452047648031 on epoch=74
05/31/2022 10:00:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.520084 on epoch=76
05/31/2022 10:00:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.474800 on epoch=77
05/31/2022 10:01:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.544647 on epoch=78
05/31/2022 10:01:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.455844 on epoch=79
05/31/2022 10:01:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.552430 on epoch=81
05/31/2022 10:01:15 - INFO - __main__ - Global step 650 Train loss 0.509561 Classification-F1 0.5076638850832399 on epoch=81
05/31/2022 10:01:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.407015 on epoch=82
05/31/2022 10:01:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.583840 on epoch=83
05/31/2022 10:01:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.448873 on epoch=84
05/31/2022 10:01:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.505639 on epoch=86
05/31/2022 10:01:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.479940 on epoch=87
05/31/2022 10:01:42 - INFO - __main__ - Global step 700 Train loss 0.485061 Classification-F1 0.48801304694350633 on epoch=87
05/31/2022 10:01:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.417113 on epoch=88
05/31/2022 10:01:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.494180 on epoch=89
05/31/2022 10:01:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.412584 on epoch=91
05/31/2022 10:02:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.299835 on epoch=92
05/31/2022 10:02:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.443189 on epoch=93
05/31/2022 10:02:09 - INFO - __main__ - Global step 750 Train loss 0.413380 Classification-F1 0.5505789609718275 on epoch=93
05/31/2022 10:02:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.422029 on epoch=94
05/31/2022 10:02:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.424637 on epoch=96
05/31/2022 10:02:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.284097 on epoch=97
05/31/2022 10:02:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.307348 on epoch=98
05/31/2022 10:02:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.437032 on epoch=99
05/31/2022 10:02:35 - INFO - __main__ - Global step 800 Train loss 0.375028 Classification-F1 0.7272586065689515 on epoch=99
05/31/2022 10:02:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.430428 on epoch=101
05/31/2022 10:02:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.275992 on epoch=102
05/31/2022 10:02:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.465106 on epoch=103
05/31/2022 10:02:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.457515 on epoch=104
05/31/2022 10:03:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.339942 on epoch=106
05/31/2022 10:03:02 - INFO - __main__ - Global step 850 Train loss 0.393796 Classification-F1 0.5646207059938404 on epoch=106
05/31/2022 10:03:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.281786 on epoch=107
05/31/2022 10:03:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.319065 on epoch=108
05/31/2022 10:03:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.242367 on epoch=109
05/31/2022 10:03:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.378379 on epoch=111
05/31/2022 10:03:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.242358 on epoch=112
05/31/2022 10:03:29 - INFO - __main__ - Global step 900 Train loss 0.292791 Classification-F1 0.6275633610553737 on epoch=112
05/31/2022 10:03:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.290969 on epoch=113
05/31/2022 10:03:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.362766 on epoch=114
05/31/2022 10:03:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.360042 on epoch=116
05/31/2022 10:03:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.315521 on epoch=117
05/31/2022 10:03:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.263570 on epoch=118
05/31/2022 10:03:56 - INFO - __main__ - Global step 950 Train loss 0.318574 Classification-F1 0.7487008045227223 on epoch=118
05/31/2022 10:04:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.271646 on epoch=119
05/31/2022 10:04:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.277035 on epoch=121
05/31/2022 10:04:12 - INFO - __main__ - Step 980 Global step 980 Train loss 2.500782 on epoch=122
05/31/2022 10:04:17 - INFO - __main__ - Step 990 Global step 990 Train loss 2.393965 on epoch=123
05/31/2022 10:04:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.213093 on epoch=124
05/31/2022 10:04:23 - INFO - __main__ - Global step 1000 Train loss 1.131305 Classification-F1 0.7038215174129353 on epoch=124
05/31/2022 10:04:23 - INFO - __main__ - save last model!
05/31/2022 10:04:23 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:04:23 - INFO - __main__ - Printing 3 examples
05/31/2022 10:04:23 - INFO - __main__ -  [emo] how cause yes am listening
05/31/2022 10:04:23 - INFO - __main__ - ['others']
05/31/2022 10:04:23 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/31/2022 10:04:23 - INFO - __main__ - ['others']
05/31/2022 10:04:23 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/31/2022 10:04:23 - INFO - __main__ - ['others']
05/31/2022 10:04:23 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:04:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:04:23 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 10:04:23 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:04:23 - INFO - __main__ - Printing 3 examples
05/31/2022 10:04:23 - INFO - __main__ -  [emo] do you like chicken who calls it soya chicken anyways  what i'm asking
05/31/2022 10:04:23 - INFO - __main__ - ['others']
05/31/2022 10:04:23 - INFO - __main__ -  [emo] i will i don't want to lose my bestie because i did not ask u wgere u r from
05/31/2022 10:04:23 - INFO - __main__ - ['others']
05/31/2022 10:04:23 - INFO - __main__ -  [emo] author name like that give me a description or summary hey that is author name yar
05/31/2022 10:04:23 - INFO - __main__ - ['others']
05/31/2022 10:04:23 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:04:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:04:23 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 10:04:30 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 10:04:31 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 10:04:31 - INFO - __main__ - Printing 3 examples
05/31/2022 10:04:31 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 10:04:31 - INFO - __main__ - ['others']
05/31/2022 10:04:31 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 10:04:31 - INFO - __main__ - ['others']
05/31/2022 10:04:31 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 10:04:31 - INFO - __main__ - ['others']
05/31/2022 10:04:31 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:04:33 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:04:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 10:04:36 - INFO - __main__ - Starting training!
05/31/2022 10:04:38 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 10:05:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_100_0.0002_8_predictions.txt
05/31/2022 10:05:21 - INFO - __main__ - Classification-F1 on test data: 0.3117
05/31/2022 10:05:22 - INFO - __main__ - prefix=emo_32_100, lr=0.0002, bsz=8, dev_performance=0.7487008045227223, test_performance=0.31165768562160384
05/31/2022 10:05:22 - INFO - __main__ - Running ... prefix=emo_32_100, lr=0.0001, bsz=8 ...
05/31/2022 10:05:23 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:05:23 - INFO - __main__ - Printing 3 examples
05/31/2022 10:05:23 - INFO - __main__ -  [emo] how cause yes am listening
05/31/2022 10:05:23 - INFO - __main__ - ['others']
05/31/2022 10:05:23 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/31/2022 10:05:23 - INFO - __main__ - ['others']
05/31/2022 10:05:23 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/31/2022 10:05:23 - INFO - __main__ - ['others']
05/31/2022 10:05:23 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:05:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:05:23 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 10:05:23 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:05:23 - INFO - __main__ - Printing 3 examples
05/31/2022 10:05:23 - INFO - __main__ -  [emo] do you like chicken who calls it soya chicken anyways  what i'm asking
05/31/2022 10:05:23 - INFO - __main__ - ['others']
05/31/2022 10:05:23 - INFO - __main__ -  [emo] i will i don't want to lose my bestie because i did not ask u wgere u r from
05/31/2022 10:05:23 - INFO - __main__ - ['others']
05/31/2022 10:05:23 - INFO - __main__ -  [emo] author name like that give me a description or summary hey that is author name yar
05/31/2022 10:05:23 - INFO - __main__ - ['others']
05/31/2022 10:05:23 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:05:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:05:23 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 10:05:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 10:05:33 - INFO - __main__ - Starting training!
05/31/2022 10:05:38 - INFO - __main__ - Step 10 Global step 10 Train loss 25.404856 on epoch=1
05/31/2022 10:05:42 - INFO - __main__ - Step 20 Global step 20 Train loss 22.044207 on epoch=2
05/31/2022 10:05:47 - INFO - __main__ - Step 30 Global step 30 Train loss 20.828371 on epoch=3
05/31/2022 10:05:53 - INFO - __main__ - Step 40 Global step 40 Train loss 19.362253 on epoch=4
05/31/2022 10:05:58 - INFO - __main__ - Step 50 Global step 50 Train loss 19.625534 on epoch=6
05/31/2022 10:06:36 - INFO - __main__ - Global step 50 Train loss 21.453045 Classification-F1 0.0 on epoch=6
05/31/2022 10:06:42 - INFO - __main__ - Step 60 Global step 60 Train loss 18.379297 on epoch=7
05/31/2022 10:06:47 - INFO - __main__ - Step 70 Global step 70 Train loss 18.002834 on epoch=8
05/31/2022 10:06:52 - INFO - __main__ - Step 80 Global step 80 Train loss 16.878611 on epoch=9
05/31/2022 10:06:57 - INFO - __main__ - Step 90 Global step 90 Train loss 17.215191 on epoch=11
05/31/2022 10:07:02 - INFO - __main__ - Step 100 Global step 100 Train loss 16.261143 on epoch=12
05/31/2022 10:07:28 - INFO - __main__ - Global step 100 Train loss 17.347414 Classification-F1 0.0 on epoch=12
05/31/2022 10:07:33 - INFO - __main__ - Step 110 Global step 110 Train loss 17.197720 on epoch=13
05/31/2022 10:07:38 - INFO - __main__ - Step 120 Global step 120 Train loss 16.015160 on epoch=14
05/31/2022 10:07:43 - INFO - __main__ - Step 130 Global step 130 Train loss 15.686769 on epoch=16
05/31/2022 10:07:48 - INFO - __main__ - Step 140 Global step 140 Train loss 14.839244 on epoch=17
05/31/2022 10:07:53 - INFO - __main__ - Step 150 Global step 150 Train loss 14.360135 on epoch=18
05/31/2022 10:08:07 - INFO - __main__ - Global step 150 Train loss 15.619806 Classification-F1 0.0013468013468013469 on epoch=18
05/31/2022 10:08:13 - INFO - __main__ - Step 160 Global step 160 Train loss 14.083046 on epoch=19
05/31/2022 10:08:18 - INFO - __main__ - Step 170 Global step 170 Train loss 13.985870 on epoch=21
05/31/2022 10:08:23 - INFO - __main__ - Step 180 Global step 180 Train loss 13.369745 on epoch=22
05/31/2022 10:08:28 - INFO - __main__ - Step 190 Global step 190 Train loss 13.382803 on epoch=23
05/31/2022 10:08:33 - INFO - __main__ - Step 200 Global step 200 Train loss 13.562784 on epoch=24
05/31/2022 10:08:44 - INFO - __main__ - Global step 200 Train loss 13.676849 Classification-F1 0.001638001638001638 on epoch=24
05/31/2022 10:08:50 - INFO - __main__ - Step 210 Global step 210 Train loss 12.991412 on epoch=26
05/31/2022 10:08:55 - INFO - __main__ - Step 220 Global step 220 Train loss 12.599655 on epoch=27
05/31/2022 10:09:00 - INFO - __main__ - Step 230 Global step 230 Train loss 12.078912 on epoch=28
05/31/2022 10:09:05 - INFO - __main__ - Step 240 Global step 240 Train loss 12.687424 on epoch=29
05/31/2022 10:09:10 - INFO - __main__ - Step 250 Global step 250 Train loss 12.173136 on epoch=31
05/31/2022 10:09:14 - INFO - __main__ - Global step 250 Train loss 12.506107 Classification-F1 0.0 on epoch=31
05/31/2022 10:09:20 - INFO - __main__ - Step 260 Global step 260 Train loss 11.651133 on epoch=32
05/31/2022 10:09:25 - INFO - __main__ - Step 270 Global step 270 Train loss 11.369680 on epoch=33
05/31/2022 10:09:30 - INFO - __main__ - Step 280 Global step 280 Train loss 11.248190 on epoch=34
05/31/2022 10:09:35 - INFO - __main__ - Step 290 Global step 290 Train loss 11.579872 on epoch=36
05/31/2022 10:09:40 - INFO - __main__ - Step 300 Global step 300 Train loss 10.832561 on epoch=37
05/31/2022 10:09:42 - INFO - __main__ - Global step 300 Train loss 11.336287 Classification-F1 0.0 on epoch=37
05/31/2022 10:09:47 - INFO - __main__ - Step 310 Global step 310 Train loss 9.545273 on epoch=38
05/31/2022 10:09:52 - INFO - __main__ - Step 320 Global step 320 Train loss 8.398612 on epoch=39
05/31/2022 10:09:58 - INFO - __main__ - Step 330 Global step 330 Train loss 8.820833 on epoch=41
05/31/2022 10:10:03 - INFO - __main__ - Step 340 Global step 340 Train loss 7.730904 on epoch=42
05/31/2022 10:10:08 - INFO - __main__ - Step 350 Global step 350 Train loss 7.522760 on epoch=43
05/31/2022 10:10:09 - INFO - __main__ - Global step 350 Train loss 8.403677 Classification-F1 0.0 on epoch=43
05/31/2022 10:10:14 - INFO - __main__ - Step 360 Global step 360 Train loss 6.916570 on epoch=44
05/31/2022 10:10:20 - INFO - __main__ - Step 370 Global step 370 Train loss 6.659471 on epoch=46
05/31/2022 10:10:25 - INFO - __main__ - Step 380 Global step 380 Train loss 5.219968 on epoch=47
05/31/2022 10:10:30 - INFO - __main__ - Step 390 Global step 390 Train loss 5.483529 on epoch=48
05/31/2022 10:10:35 - INFO - __main__ - Step 400 Global step 400 Train loss 5.273046 on epoch=49
05/31/2022 10:10:36 - INFO - __main__ - Global step 400 Train loss 5.910517 Classification-F1 0.0 on epoch=49
05/31/2022 10:10:41 - INFO - __main__ - Step 410 Global step 410 Train loss 4.952740 on epoch=51
05/31/2022 10:10:46 - INFO - __main__ - Step 420 Global step 420 Train loss 4.208267 on epoch=52
05/31/2022 10:10:51 - INFO - __main__ - Step 430 Global step 430 Train loss 4.893454 on epoch=53
05/31/2022 10:10:56 - INFO - __main__ - Step 440 Global step 440 Train loss 3.540166 on epoch=54
05/31/2022 10:11:01 - INFO - __main__ - Step 450 Global step 450 Train loss 3.227606 on epoch=56
05/31/2022 10:11:02 - INFO - __main__ - Global step 450 Train loss 4.164447 Classification-F1 0.1 on epoch=56
05/31/2022 10:11:08 - INFO - __main__ - Step 460 Global step 460 Train loss 3.116768 on epoch=57
05/31/2022 10:11:13 - INFO - __main__ - Step 470 Global step 470 Train loss 3.529059 on epoch=58
05/31/2022 10:11:18 - INFO - __main__ - Step 480 Global step 480 Train loss 3.228443 on epoch=59
05/31/2022 10:11:24 - INFO - __main__ - Step 490 Global step 490 Train loss 3.054525 on epoch=61
05/31/2022 10:11:29 - INFO - __main__ - Step 500 Global step 500 Train loss 3.598886 on epoch=62
05/31/2022 10:11:30 - INFO - __main__ - Global step 500 Train loss 3.305536 Classification-F1 0.1 on epoch=62
05/31/2022 10:11:35 - INFO - __main__ - Step 510 Global step 510 Train loss 2.653131 on epoch=63
05/31/2022 10:11:40 - INFO - __main__ - Step 520 Global step 520 Train loss 3.040584 on epoch=64
05/31/2022 10:11:45 - INFO - __main__ - Step 530 Global step 530 Train loss 3.270660 on epoch=66
05/31/2022 10:11:50 - INFO - __main__ - Step 540 Global step 540 Train loss 2.886441 on epoch=67
05/31/2022 10:11:55 - INFO - __main__ - Step 550 Global step 550 Train loss 2.564150 on epoch=68
05/31/2022 10:11:56 - INFO - __main__ - Global step 550 Train loss 2.882993 Classification-F1 0.11578044596912522 on epoch=68
05/31/2022 10:12:02 - INFO - __main__ - Step 560 Global step 560 Train loss 2.795260 on epoch=69
05/31/2022 10:12:07 - INFO - __main__ - Step 570 Global step 570 Train loss 2.792533 on epoch=71
05/31/2022 10:12:12 - INFO - __main__ - Step 580 Global step 580 Train loss 3.297277 on epoch=72
05/31/2022 10:12:17 - INFO - __main__ - Step 590 Global step 590 Train loss 3.391779 on epoch=73
05/31/2022 10:12:22 - INFO - __main__ - Step 600 Global step 600 Train loss 3.380877 on epoch=74
05/31/2022 10:12:23 - INFO - __main__ - Global step 600 Train loss 3.131545 Classification-F1 0.11578044596912522 on epoch=74
05/31/2022 10:12:28 - INFO - __main__ - Step 610 Global step 610 Train loss 3.197549 on epoch=76
05/31/2022 10:12:33 - INFO - __main__ - Step 620 Global step 620 Train loss 2.361536 on epoch=77
05/31/2022 10:12:38 - INFO - __main__ - Step 630 Global step 630 Train loss 3.001545 on epoch=78
05/31/2022 10:12:44 - INFO - __main__ - Step 640 Global step 640 Train loss 2.586378 on epoch=79
05/31/2022 10:12:49 - INFO - __main__ - Step 650 Global step 650 Train loss 2.305175 on epoch=81
05/31/2022 10:12:50 - INFO - __main__ - Global step 650 Train loss 2.690436 Classification-F1 0.2641303096581098 on epoch=81
05/31/2022 10:12:56 - INFO - __main__ - Step 660 Global step 660 Train loss 2.352194 on epoch=82
05/31/2022 10:13:01 - INFO - __main__ - Step 670 Global step 670 Train loss 2.172472 on epoch=83
05/31/2022 10:13:06 - INFO - __main__ - Step 680 Global step 680 Train loss 2.554943 on epoch=84
05/31/2022 10:13:11 - INFO - __main__ - Step 690 Global step 690 Train loss 2.410322 on epoch=86
05/31/2022 10:13:16 - INFO - __main__ - Step 700 Global step 700 Train loss 2.495672 on epoch=87
05/31/2022 10:13:17 - INFO - __main__ - Global step 700 Train loss 2.397121 Classification-F1 0.14476797088262056 on epoch=87
05/31/2022 10:13:22 - INFO - __main__ - Step 710 Global step 710 Train loss 2.443484 on epoch=88
05/31/2022 10:13:27 - INFO - __main__ - Step 720 Global step 720 Train loss 2.509181 on epoch=89
05/31/2022 10:13:32 - INFO - __main__ - Step 730 Global step 730 Train loss 2.464298 on epoch=91
05/31/2022 10:13:37 - INFO - __main__ - Step 740 Global step 740 Train loss 2.496843 on epoch=92
05/31/2022 10:13:42 - INFO - __main__ - Step 750 Global step 750 Train loss 2.315814 on epoch=93
05/31/2022 10:13:43 - INFO - __main__ - Global step 750 Train loss 2.445924 Classification-F1 0.1581196581196581 on epoch=93
05/31/2022 10:13:49 - INFO - __main__ - Step 760 Global step 760 Train loss 2.330161 on epoch=94
05/31/2022 10:13:54 - INFO - __main__ - Step 770 Global step 770 Train loss 2.407401 on epoch=96
05/31/2022 10:13:59 - INFO - __main__ - Step 780 Global step 780 Train loss 2.202616 on epoch=97
05/31/2022 10:14:04 - INFO - __main__ - Step 790 Global step 790 Train loss 2.398232 on epoch=98
05/31/2022 10:14:09 - INFO - __main__ - Step 800 Global step 800 Train loss 1.929240 on epoch=99
05/31/2022 10:14:10 - INFO - __main__ - Global step 800 Train loss 2.253530 Classification-F1 0.13067758749069247 on epoch=99
05/31/2022 10:14:15 - INFO - __main__ - Step 810 Global step 810 Train loss 1.991376 on epoch=101
05/31/2022 10:14:20 - INFO - __main__ - Step 820 Global step 820 Train loss 1.783666 on epoch=102
05/31/2022 10:14:25 - INFO - __main__ - Step 830 Global step 830 Train loss 2.268222 on epoch=103
05/31/2022 10:14:30 - INFO - __main__ - Step 840 Global step 840 Train loss 1.530258 on epoch=104
05/31/2022 10:14:36 - INFO - __main__ - Step 850 Global step 850 Train loss 1.489926 on epoch=106
05/31/2022 10:14:37 - INFO - __main__ - Global step 850 Train loss 1.812690 Classification-F1 0.4050077022460743 on epoch=106
05/31/2022 10:14:43 - INFO - __main__ - Step 860 Global step 860 Train loss 1.534058 on epoch=107
05/31/2022 10:14:48 - INFO - __main__ - Step 870 Global step 870 Train loss 1.629005 on epoch=108
05/31/2022 10:14:53 - INFO - __main__ - Step 880 Global step 880 Train loss 1.915534 on epoch=109
05/31/2022 10:14:58 - INFO - __main__ - Step 890 Global step 890 Train loss 1.353258 on epoch=111
05/31/2022 10:15:03 - INFO - __main__ - Step 900 Global step 900 Train loss 1.585961 on epoch=112
05/31/2022 10:15:04 - INFO - __main__ - Global step 900 Train loss 1.603563 Classification-F1 0.28491075917546504 on epoch=112
05/31/2022 10:15:09 - INFO - __main__ - Step 910 Global step 910 Train loss 1.915121 on epoch=113
05/31/2022 10:15:14 - INFO - __main__ - Step 920 Global step 920 Train loss 1.563818 on epoch=114
05/31/2022 10:15:19 - INFO - __main__ - Step 930 Global step 930 Train loss 1.315485 on epoch=116
05/31/2022 10:15:24 - INFO - __main__ - Step 940 Global step 940 Train loss 1.467833 on epoch=117
05/31/2022 10:15:29 - INFO - __main__ - Step 950 Global step 950 Train loss 1.551058 on epoch=118
05/31/2022 10:15:30 - INFO - __main__ - Global step 950 Train loss 1.562663 Classification-F1 0.2868531716170156 on epoch=118
05/31/2022 10:15:35 - INFO - __main__ - Step 960 Global step 960 Train loss 1.370477 on epoch=119
05/31/2022 10:15:41 - INFO - __main__ - Step 970 Global step 970 Train loss 1.253094 on epoch=121
05/31/2022 10:15:46 - INFO - __main__ - Step 980 Global step 980 Train loss 1.362944 on epoch=122
05/31/2022 10:15:51 - INFO - __main__ - Step 990 Global step 990 Train loss 1.257103 on epoch=123
05/31/2022 10:15:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.266839 on epoch=124
05/31/2022 10:15:57 - INFO - __main__ - Global step 1000 Train loss 1.302091 Classification-F1 0.4736020725065156 on epoch=124
05/31/2022 10:15:57 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:15:57 - INFO - __main__ - Printing 3 examples
05/31/2022 10:15:57 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/31/2022 10:15:57 - INFO - __main__ - ['others']
05/31/2022 10:15:57 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/31/2022 10:15:57 - INFO - __main__ - ['others']
05/31/2022 10:15:57 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/31/2022 10:15:57 - INFO - __main__ - ['others']
05/31/2022 10:15:57 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:15:57 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:15:57 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 10:15:57 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:15:57 - INFO - __main__ - Printing 3 examples
05/31/2022 10:15:57 - INFO - __main__ -  [emo] please be my other half your who sorry don't be like that okkkk then my gi
05/31/2022 10:15:57 - INFO - __main__ - ['others']
05/31/2022 10:15:57 - INFO - __main__ -  [emo] i'll catch you after a short  what if you don't can i
05/31/2022 10:15:57 - INFO - __main__ - ['others']
05/31/2022 10:15:57 - INFO - __main__ -  [emo] u oly first  no you no u
05/31/2022 10:15:57 - INFO - __main__ - ['others']
05/31/2022 10:15:57 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:15:57 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:15:57 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 10:15:58 - INFO - __main__ - save last model!
05/31/2022 10:16:05 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 10:16:06 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 10:16:06 - INFO - __main__ - Printing 3 examples
05/31/2022 10:16:06 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 10:16:06 - INFO - __main__ - ['others']
05/31/2022 10:16:06 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 10:16:06 - INFO - __main__ - ['others']
05/31/2022 10:16:06 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 10:16:06 - INFO - __main__ - ['others']
05/31/2022 10:16:06 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:16:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:16:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 10:16:10 - INFO - __main__ - Starting training!
05/31/2022 10:16:13 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 10:16:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_100_0.0001_8_predictions.txt
05/31/2022 10:16:56 - INFO - __main__ - Classification-F1 on test data: 0.1799
05/31/2022 10:16:56 - INFO - __main__ - prefix=emo_32_100, lr=0.0001, bsz=8, dev_performance=0.4736020725065156, test_performance=0.17986488815429288
05/31/2022 10:16:56 - INFO - __main__ - Running ... prefix=emo_32_13, lr=0.0005, bsz=8 ...
05/31/2022 10:16:57 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:16:57 - INFO - __main__ - Printing 3 examples
05/31/2022 10:16:57 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/31/2022 10:16:57 - INFO - __main__ - ['others']
05/31/2022 10:16:57 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/31/2022 10:16:57 - INFO - __main__ - ['others']
05/31/2022 10:16:57 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/31/2022 10:16:57 - INFO - __main__ - ['others']
05/31/2022 10:16:57 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:16:57 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:16:57 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 10:16:57 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:16:57 - INFO - __main__ - Printing 3 examples
05/31/2022 10:16:57 - INFO - __main__ -  [emo] please be my other half your who sorry don't be like that okkkk then my gi
05/31/2022 10:16:57 - INFO - __main__ - ['others']
05/31/2022 10:16:57 - INFO - __main__ -  [emo] i'll catch you after a short  what if you don't can i
05/31/2022 10:16:57 - INFO - __main__ - ['others']
05/31/2022 10:16:57 - INFO - __main__ -  [emo] u oly first  no you no u
05/31/2022 10:16:57 - INFO - __main__ - ['others']
05/31/2022 10:16:57 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:16:57 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:16:58 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 10:17:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 10:17:08 - INFO - __main__ - Starting training!
05/31/2022 10:17:12 - INFO - __main__ - Step 10 Global step 10 Train loss 25.176449 on epoch=1
05/31/2022 10:17:17 - INFO - __main__ - Step 20 Global step 20 Train loss 18.760242 on epoch=2
05/31/2022 10:17:22 - INFO - __main__ - Step 30 Global step 30 Train loss 17.250839 on epoch=3
05/31/2022 10:17:27 - INFO - __main__ - Step 40 Global step 40 Train loss 14.662537 on epoch=4
05/31/2022 10:17:32 - INFO - __main__ - Step 50 Global step 50 Train loss 13.929018 on epoch=6
05/31/2022 10:17:34 - INFO - __main__ - Global step 50 Train loss 17.955816 Classification-F1 0.0 on epoch=6
05/31/2022 10:17:39 - INFO - __main__ - Step 60 Global step 60 Train loss 12.119122 on epoch=7
05/31/2022 10:17:44 - INFO - __main__ - Step 70 Global step 70 Train loss 10.528910 on epoch=8
05/31/2022 10:17:50 - INFO - __main__ - Step 80 Global step 80 Train loss 7.252949 on epoch=9
05/31/2022 10:17:55 - INFO - __main__ - Step 90 Global step 90 Train loss 3.581513 on epoch=11
05/31/2022 10:18:00 - INFO - __main__ - Step 100 Global step 100 Train loss 1.158432 on epoch=12
05/31/2022 10:18:01 - INFO - __main__ - Global step 100 Train loss 6.928185 Classification-F1 0.19559243808609522 on epoch=12
05/31/2022 10:18:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.677114 on epoch=13
05/31/2022 10:18:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.856223 on epoch=14
05/31/2022 10:18:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.855365 on epoch=16
05/31/2022 10:18:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.928617 on epoch=17
05/31/2022 10:18:27 - INFO - __main__ - Step 150 Global step 150 Train loss 1.143522 on epoch=18
05/31/2022 10:18:28 - INFO - __main__ - Global step 150 Train loss 0.892168 Classification-F1 0.2911826172475259 on epoch=18
05/31/2022 10:18:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.788181 on epoch=19
05/31/2022 10:18:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.890501 on epoch=21
05/31/2022 10:18:44 - INFO - __main__ - Step 180 Global step 180 Train loss 3.611919 on epoch=22
05/31/2022 10:18:49 - INFO - __main__ - Step 190 Global step 190 Train loss 1.929821 on epoch=23
05/31/2022 10:18:54 - INFO - __main__ - Step 200 Global step 200 Train loss 1.385656 on epoch=24
05/31/2022 10:18:55 - INFO - __main__ - Global step 200 Train loss 1.721216 Classification-F1 0.34762438009918406 on epoch=24
05/31/2022 10:19:01 - INFO - __main__ - Step 210 Global step 210 Train loss 1.460763 on epoch=26
05/31/2022 10:19:06 - INFO - __main__ - Step 220 Global step 220 Train loss 1.645971 on epoch=27
05/31/2022 10:19:11 - INFO - __main__ - Step 230 Global step 230 Train loss 1.254702 on epoch=28
05/31/2022 10:19:16 - INFO - __main__ - Step 240 Global step 240 Train loss 1.082675 on epoch=29
05/31/2022 10:19:21 - INFO - __main__ - Step 250 Global step 250 Train loss 1.384727 on epoch=31
05/31/2022 10:19:22 - INFO - __main__ - Global step 250 Train loss 1.365767 Classification-F1 0.4453502415458937 on epoch=31
05/31/2022 10:19:28 - INFO - __main__ - Step 260 Global step 260 Train loss 1.003904 on epoch=32
05/31/2022 10:19:33 - INFO - __main__ - Step 270 Global step 270 Train loss 1.415806 on epoch=33
05/31/2022 10:19:38 - INFO - __main__ - Step 280 Global step 280 Train loss 1.598969 on epoch=34
05/31/2022 10:19:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.946391 on epoch=36
05/31/2022 10:19:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.726406 on epoch=37
05/31/2022 10:19:49 - INFO - __main__ - Global step 300 Train loss 1.138295 Classification-F1 0.49806653491436104 on epoch=37
05/31/2022 10:19:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.679476 on epoch=38
05/31/2022 10:20:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.817526 on epoch=39
05/31/2022 10:20:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.776137 on epoch=41
05/31/2022 10:20:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.578239 on epoch=42
05/31/2022 10:20:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.599294 on epoch=43
05/31/2022 10:20:16 - INFO - __main__ - Global step 350 Train loss 0.690134 Classification-F1 0.5651190476190476 on epoch=43
05/31/2022 10:20:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.453428 on epoch=44
05/31/2022 10:20:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.556071 on epoch=46
05/31/2022 10:20:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.490058 on epoch=47
05/31/2022 10:20:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.455602 on epoch=48
05/31/2022 10:20:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.424511 on epoch=49
05/31/2022 10:20:43 - INFO - __main__ - Global step 400 Train loss 0.475934 Classification-F1 0.5536705195469024 on epoch=49
05/31/2022 10:20:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.495231 on epoch=51
05/31/2022 10:20:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.433791 on epoch=52
05/31/2022 10:20:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.340467 on epoch=53
05/31/2022 10:21:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.388260 on epoch=54
05/31/2022 10:21:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.279374 on epoch=56
05/31/2022 10:21:10 - INFO - __main__ - Global step 450 Train loss 0.387425 Classification-F1 0.6120279115690281 on epoch=56
05/31/2022 10:21:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.389860 on epoch=57
05/31/2022 10:21:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.332916 on epoch=58
05/31/2022 10:21:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.267513 on epoch=59
05/31/2022 10:21:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.291298 on epoch=61
05/31/2022 10:21:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.299160 on epoch=62
05/31/2022 10:21:37 - INFO - __main__ - Global step 500 Train loss 0.316149 Classification-F1 0.6309118456704716 on epoch=62
05/31/2022 10:21:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.294316 on epoch=63
05/31/2022 10:21:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.248042 on epoch=64
05/31/2022 10:21:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.266237 on epoch=66
05/31/2022 10:21:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.259493 on epoch=67
05/31/2022 10:22:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.265146 on epoch=68
05/31/2022 10:22:04 - INFO - __main__ - Global step 550 Train loss 0.266647 Classification-F1 0.608618881432973 on epoch=68
05/31/2022 10:22:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.264121 on epoch=69
05/31/2022 10:22:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.344239 on epoch=71
05/31/2022 10:22:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.321281 on epoch=72
05/31/2022 10:22:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.252727 on epoch=73
05/31/2022 10:22:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.223005 on epoch=74
05/31/2022 10:22:30 - INFO - __main__ - Global step 600 Train loss 0.281075 Classification-F1 0.6408053878828527 on epoch=74
05/31/2022 10:22:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.252586 on epoch=76
05/31/2022 10:22:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.233076 on epoch=77
05/31/2022 10:22:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.292087 on epoch=78
05/31/2022 10:22:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.212843 on epoch=79
05/31/2022 10:22:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.245721 on epoch=81
05/31/2022 10:22:57 - INFO - __main__ - Global step 650 Train loss 0.247263 Classification-F1 0.6106203007518797 on epoch=81
05/31/2022 10:23:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.145322 on epoch=82
05/31/2022 10:23:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.166443 on epoch=83
05/31/2022 10:23:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.141341 on epoch=84
05/31/2022 10:23:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.170149 on epoch=86
05/31/2022 10:23:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.215042 on epoch=87
05/31/2022 10:23:24 - INFO - __main__ - Global step 700 Train loss 0.167660 Classification-F1 0.6179081777023203 on epoch=87
05/31/2022 10:23:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.151027 on epoch=88
05/31/2022 10:23:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.141730 on epoch=89
05/31/2022 10:23:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.205883 on epoch=91
05/31/2022 10:23:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.192848 on epoch=92
05/31/2022 10:23:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.171756 on epoch=93
05/31/2022 10:23:51 - INFO - __main__ - Global step 750 Train loss 0.172649 Classification-F1 0.5381264348065385 on epoch=93
05/31/2022 10:23:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.146187 on epoch=94
05/31/2022 10:24:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.136753 on epoch=96
05/31/2022 10:24:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.128879 on epoch=97
05/31/2022 10:24:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.097155 on epoch=98
05/31/2022 10:24:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.124442 on epoch=99
05/31/2022 10:24:17 - INFO - __main__ - Global step 800 Train loss 0.126683 Classification-F1 0.6333526831785345 on epoch=99
05/31/2022 10:24:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.062903 on epoch=101
05/31/2022 10:24:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.131814 on epoch=102
05/31/2022 10:24:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.224918 on epoch=103
05/31/2022 10:24:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.114236 on epoch=104
05/31/2022 10:24:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.090695 on epoch=106
05/31/2022 10:24:43 - INFO - __main__ - Global step 850 Train loss 0.124913 Classification-F1 0.6373559394745836 on epoch=106
05/31/2022 10:24:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.094921 on epoch=107
05/31/2022 10:24:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.086093 on epoch=108
05/31/2022 10:24:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.099228 on epoch=109
05/31/2022 10:25:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.078917 on epoch=111
05/31/2022 10:25:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.097256 on epoch=112
05/31/2022 10:25:10 - INFO - __main__ - Global step 900 Train loss 0.091283 Classification-F1 0.6840822541025305 on epoch=112
05/31/2022 10:25:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.116229 on epoch=113
05/31/2022 10:25:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.095482 on epoch=114
05/31/2022 10:25:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.058179 on epoch=116
05/31/2022 10:25:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.047706 on epoch=117
05/31/2022 10:25:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.069745 on epoch=118
05/31/2022 10:25:37 - INFO - __main__ - Global step 950 Train loss 0.077468 Classification-F1 0.6005787690570299 on epoch=118
05/31/2022 10:25:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.088692 on epoch=119
05/31/2022 10:25:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.064349 on epoch=121
05/31/2022 10:25:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.164015 on epoch=122
05/31/2022 10:25:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.071048 on epoch=123
05/31/2022 10:26:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.059473 on epoch=124
05/31/2022 10:26:04 - INFO - __main__ - Global step 1000 Train loss 0.089515 Classification-F1 0.5912130127555659 on epoch=124
05/31/2022 10:26:04 - INFO - __main__ - save last model!
05/31/2022 10:26:04 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:26:04 - INFO - __main__ - Printing 3 examples
05/31/2022 10:26:04 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/31/2022 10:26:04 - INFO - __main__ - ['others']
05/31/2022 10:26:04 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/31/2022 10:26:04 - INFO - __main__ - ['others']
05/31/2022 10:26:04 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/31/2022 10:26:04 - INFO - __main__ - ['others']
05/31/2022 10:26:04 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:26:04 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:26:04 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 10:26:04 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:26:04 - INFO - __main__ - Printing 3 examples
05/31/2022 10:26:04 - INFO - __main__ -  [emo] please be my other half your who sorry don't be like that okkkk then my gi
05/31/2022 10:26:04 - INFO - __main__ - ['others']
05/31/2022 10:26:04 - INFO - __main__ -  [emo] i'll catch you after a short  what if you don't can i
05/31/2022 10:26:04 - INFO - __main__ - ['others']
05/31/2022 10:26:04 - INFO - __main__ -  [emo] u oly first  no you no u
05/31/2022 10:26:04 - INFO - __main__ - ['others']
05/31/2022 10:26:04 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:26:04 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:26:04 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 10:26:11 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 10:26:12 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 10:26:12 - INFO - __main__ - Printing 3 examples
05/31/2022 10:26:12 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 10:26:12 - INFO - __main__ - ['others']
05/31/2022 10:26:12 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 10:26:12 - INFO - __main__ - ['others']
05/31/2022 10:26:12 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 10:26:12 - INFO - __main__ - ['others']
05/31/2022 10:26:12 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:26:14 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:26:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 10:26:17 - INFO - __main__ - Starting training!
05/31/2022 10:26:19 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 10:27:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_13_0.0005_8_predictions.txt
05/31/2022 10:27:02 - INFO - __main__ - Classification-F1 on test data: 0.0796
05/31/2022 10:27:02 - INFO - __main__ - prefix=emo_32_13, lr=0.0005, bsz=8, dev_performance=0.6840822541025305, test_performance=0.07960405988263933
05/31/2022 10:27:02 - INFO - __main__ - Running ... prefix=emo_32_13, lr=0.0003, bsz=8 ...
05/31/2022 10:27:03 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:27:03 - INFO - __main__ - Printing 3 examples
05/31/2022 10:27:03 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/31/2022 10:27:03 - INFO - __main__ - ['others']
05/31/2022 10:27:03 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/31/2022 10:27:03 - INFO - __main__ - ['others']
05/31/2022 10:27:03 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/31/2022 10:27:03 - INFO - __main__ - ['others']
05/31/2022 10:27:03 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:27:03 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:27:03 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 10:27:03 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:27:03 - INFO - __main__ - Printing 3 examples
05/31/2022 10:27:03 - INFO - __main__ -  [emo] please be my other half your who sorry don't be like that okkkk then my gi
05/31/2022 10:27:03 - INFO - __main__ - ['others']
05/31/2022 10:27:03 - INFO - __main__ -  [emo] i'll catch you after a short  what if you don't can i
05/31/2022 10:27:03 - INFO - __main__ - ['others']
05/31/2022 10:27:03 - INFO - __main__ -  [emo] u oly first  no you no u
05/31/2022 10:27:03 - INFO - __main__ - ['others']
05/31/2022 10:27:03 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:27:03 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:27:03 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 10:27:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 10:27:16 - INFO - __main__ - Starting training!
05/31/2022 10:27:20 - INFO - __main__ - Step 10 Global step 10 Train loss 25.625469 on epoch=1
05/31/2022 10:27:26 - INFO - __main__ - Step 20 Global step 20 Train loss 19.790607 on epoch=2
05/31/2022 10:27:31 - INFO - __main__ - Step 30 Global step 30 Train loss 18.420734 on epoch=3
05/31/2022 10:27:36 - INFO - __main__ - Step 40 Global step 40 Train loss 17.198132 on epoch=4
05/31/2022 10:27:41 - INFO - __main__ - Step 50 Global step 50 Train loss 16.608799 on epoch=6
05/31/2022 10:27:42 - INFO - __main__ - Global step 50 Train loss 19.528749 Classification-F1 0.0 on epoch=6
05/31/2022 10:27:48 - INFO - __main__ - Step 60 Global step 60 Train loss 14.117434 on epoch=7
05/31/2022 10:27:53 - INFO - __main__ - Step 70 Global step 70 Train loss 13.747732 on epoch=8
05/31/2022 10:27:58 - INFO - __main__ - Step 80 Global step 80 Train loss 12.867620 on epoch=9
05/31/2022 10:28:03 - INFO - __main__ - Step 90 Global step 90 Train loss 12.298473 on epoch=11
05/31/2022 10:28:08 - INFO - __main__ - Step 100 Global step 100 Train loss 10.988802 on epoch=12
05/31/2022 10:28:10 - INFO - __main__ - Global step 100 Train loss 12.804012 Classification-F1 0.0 on epoch=12
05/31/2022 10:28:15 - INFO - __main__ - Step 110 Global step 110 Train loss 9.444844 on epoch=13
05/31/2022 10:28:20 - INFO - __main__ - Step 120 Global step 120 Train loss 7.513485 on epoch=14
05/31/2022 10:28:25 - INFO - __main__ - Step 130 Global step 130 Train loss 7.613049 on epoch=16
05/31/2022 10:28:29 - INFO - __main__ - Step 140 Global step 140 Train loss 3.315156 on epoch=17
05/31/2022 10:28:34 - INFO - __main__ - Step 150 Global step 150 Train loss 2.619441 on epoch=18
05/31/2022 10:28:36 - INFO - __main__ - Global step 150 Train loss 6.101195 Classification-F1 0.12565826330532212 on epoch=18
05/31/2022 10:28:41 - INFO - __main__ - Step 160 Global step 160 Train loss 1.647988 on epoch=19
05/31/2022 10:28:46 - INFO - __main__ - Step 170 Global step 170 Train loss 1.037019 on epoch=21
05/31/2022 10:28:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.897575 on epoch=22
05/31/2022 10:28:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.939550 on epoch=23
05/31/2022 10:29:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.910021 on epoch=24
05/31/2022 10:29:03 - INFO - __main__ - Global step 200 Train loss 1.086431 Classification-F1 0.19418617676711097 on epoch=24
05/31/2022 10:29:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.967605 on epoch=26
05/31/2022 10:29:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.784795 on epoch=27
05/31/2022 10:29:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.820443 on epoch=28
05/31/2022 10:29:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.792414 on epoch=29
05/31/2022 10:29:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.852335 on epoch=31
05/31/2022 10:29:30 - INFO - __main__ - Global step 250 Train loss 0.843518 Classification-F1 0.12922077922077924 on epoch=31
05/31/2022 10:29:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.787557 on epoch=32
05/31/2022 10:29:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.768924 on epoch=33
05/31/2022 10:29:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.778148 on epoch=34
05/31/2022 10:29:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.797470 on epoch=36
05/31/2022 10:29:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.704503 on epoch=37
05/31/2022 10:29:56 - INFO - __main__ - Global step 300 Train loss 0.767320 Classification-F1 0.485797853928695 on epoch=37
05/31/2022 10:30:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.737140 on epoch=38
05/31/2022 10:30:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.682111 on epoch=39
05/31/2022 10:30:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.656121 on epoch=41
05/31/2022 10:30:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.676961 on epoch=42
05/31/2022 10:30:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.587522 on epoch=43
05/31/2022 10:30:23 - INFO - __main__ - Global step 350 Train loss 0.667971 Classification-F1 0.39009301509301514 on epoch=43
05/31/2022 10:30:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.596004 on epoch=44
05/31/2022 10:30:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.682158 on epoch=46
05/31/2022 10:30:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.657816 on epoch=47
05/31/2022 10:30:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.685222 on epoch=48
05/31/2022 10:30:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.635508 on epoch=49
05/31/2022 10:30:50 - INFO - __main__ - Global step 400 Train loss 0.651342 Classification-F1 0.5439561280614362 on epoch=49
05/31/2022 10:30:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.562875 on epoch=51
05/31/2022 10:31:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.708312 on epoch=52
05/31/2022 10:31:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.588797 on epoch=53
05/31/2022 10:31:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.549015 on epoch=54
05/31/2022 10:31:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.624125 on epoch=56
05/31/2022 10:31:17 - INFO - __main__ - Global step 450 Train loss 0.606625 Classification-F1 0.47358438954025317 on epoch=56
05/31/2022 10:31:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.535321 on epoch=57
05/31/2022 10:31:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.539770 on epoch=58
05/31/2022 10:31:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.523433 on epoch=59
05/31/2022 10:31:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.585390 on epoch=61
05/31/2022 10:31:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.459974 on epoch=62
05/31/2022 10:31:43 - INFO - __main__ - Global step 500 Train loss 0.528777 Classification-F1 0.6291146079556966 on epoch=62
05/31/2022 10:31:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.572526 on epoch=63
05/31/2022 10:31:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.473684 on epoch=64
05/31/2022 10:31:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.424487 on epoch=66
05/31/2022 10:32:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.509149 on epoch=67
05/31/2022 10:32:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.484188 on epoch=68
05/31/2022 10:32:10 - INFO - __main__ - Global step 550 Train loss 0.492807 Classification-F1 0.6671742576672154 on epoch=68
05/31/2022 10:32:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.380437 on epoch=69
05/31/2022 10:32:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.397098 on epoch=71
05/31/2022 10:32:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.296846 on epoch=72
05/31/2022 10:32:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.317791 on epoch=73
05/31/2022 10:32:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.195933 on epoch=74
05/31/2022 10:32:37 - INFO - __main__ - Global step 600 Train loss 0.317621 Classification-F1 0.6674994365562318 on epoch=74
05/31/2022 10:32:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.224277 on epoch=76
05/31/2022 10:32:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.318817 on epoch=77
05/31/2022 10:32:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.186786 on epoch=78
05/31/2022 10:32:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.170069 on epoch=79
05/31/2022 10:33:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.180146 on epoch=81
05/31/2022 10:33:04 - INFO - __main__ - Global step 650 Train loss 0.216019 Classification-F1 0.6789191538684438 on epoch=81
05/31/2022 10:33:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.161797 on epoch=82
05/31/2022 10:33:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.213466 on epoch=83
05/31/2022 10:33:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.256297 on epoch=84
05/31/2022 10:33:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.239828 on epoch=86
05/31/2022 10:33:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.269676 on epoch=87
05/31/2022 10:33:31 - INFO - __main__ - Global step 700 Train loss 0.228213 Classification-F1 0.6956108686609466 on epoch=87
05/31/2022 10:33:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.162100 on epoch=88
05/31/2022 10:33:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.158285 on epoch=89
05/31/2022 10:33:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.262184 on epoch=91
05/31/2022 10:33:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.130875 on epoch=92
05/31/2022 10:33:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.157929 on epoch=93
05/31/2022 10:33:59 - INFO - __main__ - Global step 750 Train loss 0.174275 Classification-F1 0.7013195285176218 on epoch=93
05/31/2022 10:34:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.206686 on epoch=94
05/31/2022 10:34:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.079573 on epoch=96
05/31/2022 10:34:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.097176 on epoch=97
05/31/2022 10:34:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.078867 on epoch=98
05/31/2022 10:34:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.044998 on epoch=99
05/31/2022 10:34:26 - INFO - __main__ - Global step 800 Train loss 0.101460 Classification-F1 0.5750909060498101 on epoch=99
05/31/2022 10:34:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.059001 on epoch=101
05/31/2022 10:34:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.096444 on epoch=102
05/31/2022 10:34:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.095487 on epoch=103
05/31/2022 10:34:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.068925 on epoch=104
05/31/2022 10:34:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.110804 on epoch=106
05/31/2022 10:34:52 - INFO - __main__ - Global step 850 Train loss 0.086132 Classification-F1 0.6638804885368843 on epoch=106
05/31/2022 10:34:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.058540 on epoch=107
05/31/2022 10:35:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.101337 on epoch=108
05/31/2022 10:35:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.050056 on epoch=109
05/31/2022 10:35:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.097969 on epoch=111
05/31/2022 10:35:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.139680 on epoch=112
05/31/2022 10:35:19 - INFO - __main__ - Global step 900 Train loss 0.089516 Classification-F1 0.6646478727973437 on epoch=112
05/31/2022 10:35:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.077082 on epoch=113
05/31/2022 10:35:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.089024 on epoch=114
05/31/2022 10:35:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.041485 on epoch=116
05/31/2022 10:35:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.065517 on epoch=117
05/31/2022 10:35:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.034643 on epoch=118
05/31/2022 10:35:46 - INFO - __main__ - Global step 950 Train loss 0.061550 Classification-F1 0.4586138478379858 on epoch=118
05/31/2022 10:35:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.033960 on epoch=119
05/31/2022 10:35:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.052130 on epoch=121
05/31/2022 10:36:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.042686 on epoch=122
05/31/2022 10:36:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.064018 on epoch=123
05/31/2022 10:36:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.061572 on epoch=124
05/31/2022 10:36:12 - INFO - __main__ - Global step 1000 Train loss 0.050873 Classification-F1 0.5311637831511397 on epoch=124
05/31/2022 10:36:12 - INFO - __main__ - save last model!
05/31/2022 10:36:12 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:36:12 - INFO - __main__ - Printing 3 examples
05/31/2022 10:36:12 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/31/2022 10:36:12 - INFO - __main__ - ['others']
05/31/2022 10:36:12 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/31/2022 10:36:12 - INFO - __main__ - ['others']
05/31/2022 10:36:12 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/31/2022 10:36:12 - INFO - __main__ - ['others']
05/31/2022 10:36:12 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:36:12 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:36:12 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 10:36:12 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:36:12 - INFO - __main__ - Printing 3 examples
05/31/2022 10:36:12 - INFO - __main__ -  [emo] please be my other half your who sorry don't be like that okkkk then my gi
05/31/2022 10:36:12 - INFO - __main__ - ['others']
05/31/2022 10:36:12 - INFO - __main__ -  [emo] i'll catch you after a short  what if you don't can i
05/31/2022 10:36:12 - INFO - __main__ - ['others']
05/31/2022 10:36:12 - INFO - __main__ -  [emo] u oly first  no you no u
05/31/2022 10:36:12 - INFO - __main__ - ['others']
05/31/2022 10:36:12 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:36:12 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:36:13 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 10:36:19 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 10:36:19 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 10:36:19 - INFO - __main__ - Printing 3 examples
05/31/2022 10:36:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 10:36:19 - INFO - __main__ - ['others']
05/31/2022 10:36:19 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 10:36:19 - INFO - __main__ - ['others']
05/31/2022 10:36:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 10:36:19 - INFO - __main__ - ['others']
05/31/2022 10:36:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:36:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:36:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 10:36:25 - INFO - __main__ - Starting training!
05/31/2022 10:36:26 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 10:37:09 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_13_0.0003_8_predictions.txt
05/31/2022 10:37:09 - INFO - __main__ - Classification-F1 on test data: 0.0676
05/31/2022 10:37:09 - INFO - __main__ - prefix=emo_32_13, lr=0.0003, bsz=8, dev_performance=0.7013195285176218, test_performance=0.06762187870966714
05/31/2022 10:37:09 - INFO - __main__ - Running ... prefix=emo_32_13, lr=0.0002, bsz=8 ...
05/31/2022 10:37:10 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:37:10 - INFO - __main__ - Printing 3 examples
05/31/2022 10:37:10 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/31/2022 10:37:10 - INFO - __main__ - ['others']
05/31/2022 10:37:10 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/31/2022 10:37:10 - INFO - __main__ - ['others']
05/31/2022 10:37:10 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/31/2022 10:37:10 - INFO - __main__ - ['others']
05/31/2022 10:37:10 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:37:10 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:37:10 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 10:37:10 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:37:10 - INFO - __main__ - Printing 3 examples
05/31/2022 10:37:10 - INFO - __main__ -  [emo] please be my other half your who sorry don't be like that okkkk then my gi
05/31/2022 10:37:10 - INFO - __main__ - ['others']
05/31/2022 10:37:10 - INFO - __main__ -  [emo] i'll catch you after a short  what if you don't can i
05/31/2022 10:37:10 - INFO - __main__ - ['others']
05/31/2022 10:37:10 - INFO - __main__ -  [emo] u oly first  no you no u
05/31/2022 10:37:10 - INFO - __main__ - ['others']
05/31/2022 10:37:10 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:37:10 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:37:10 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 10:37:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 10:37:23 - INFO - __main__ - Starting training!
05/31/2022 10:37:27 - INFO - __main__ - Step 10 Global step 10 Train loss 25.222128 on epoch=1
05/31/2022 10:37:32 - INFO - __main__ - Step 20 Global step 20 Train loss 20.550653 on epoch=2
05/31/2022 10:37:37 - INFO - __main__ - Step 30 Global step 30 Train loss 18.997007 on epoch=3
05/31/2022 10:37:42 - INFO - __main__ - Step 40 Global step 40 Train loss 17.544094 on epoch=4
05/31/2022 10:37:48 - INFO - __main__ - Step 50 Global step 50 Train loss 17.555538 on epoch=6
05/31/2022 10:38:24 - INFO - __main__ - Global step 50 Train loss 19.973885 Classification-F1 0.0 on epoch=6
05/31/2022 10:38:29 - INFO - __main__ - Step 60 Global step 60 Train loss 15.792148 on epoch=7
05/31/2022 10:38:34 - INFO - __main__ - Step 70 Global step 70 Train loss 15.143282 on epoch=8
05/31/2022 10:38:39 - INFO - __main__ - Step 80 Global step 80 Train loss 14.134256 on epoch=9
05/31/2022 10:38:45 - INFO - __main__ - Step 90 Global step 90 Train loss 14.628085 on epoch=11
05/31/2022 10:38:50 - INFO - __main__ - Step 100 Global step 100 Train loss 13.737276 on epoch=12
05/31/2022 10:38:51 - INFO - __main__ - Global step 100 Train loss 14.687009 Classification-F1 0.0 on epoch=12
05/31/2022 10:38:56 - INFO - __main__ - Step 110 Global step 110 Train loss 13.577154 on epoch=13
05/31/2022 10:39:01 - INFO - __main__ - Step 120 Global step 120 Train loss 12.606101 on epoch=14
05/31/2022 10:39:06 - INFO - __main__ - Step 130 Global step 130 Train loss 12.409525 on epoch=16
05/31/2022 10:39:11 - INFO - __main__ - Step 140 Global step 140 Train loss 10.986204 on epoch=17
05/31/2022 10:39:16 - INFO - __main__ - Step 150 Global step 150 Train loss 10.856951 on epoch=18
05/31/2022 10:39:18 - INFO - __main__ - Global step 150 Train loss 12.087188 Classification-F1 0.0 on epoch=18
05/31/2022 10:39:23 - INFO - __main__ - Step 160 Global step 160 Train loss 10.440100 on epoch=19
05/31/2022 10:39:28 - INFO - __main__ - Step 170 Global step 170 Train loss 9.013003 on epoch=21
05/31/2022 10:39:33 - INFO - __main__ - Step 180 Global step 180 Train loss 6.968721 on epoch=22
05/31/2022 10:39:38 - INFO - __main__ - Step 190 Global step 190 Train loss 4.211574 on epoch=23
05/31/2022 10:39:43 - INFO - __main__ - Step 200 Global step 200 Train loss 1.279506 on epoch=24
05/31/2022 10:40:22 - INFO - __main__ - Global step 200 Train loss 6.382581 Classification-F1 0.03926305254430254 on epoch=24
05/31/2022 10:40:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.905932 on epoch=26
05/31/2022 10:40:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.687593 on epoch=27
05/31/2022 10:40:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.786088 on epoch=28
05/31/2022 10:40:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.544685 on epoch=29
05/31/2022 10:40:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.568190 on epoch=31
05/31/2022 10:40:49 - INFO - __main__ - Global step 250 Train loss 0.698498 Classification-F1 0.6010077675587611 on epoch=31
05/31/2022 10:40:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.497111 on epoch=32
05/31/2022 10:41:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.514109 on epoch=33
05/31/2022 10:41:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.369861 on epoch=34
05/31/2022 10:41:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.390627 on epoch=36
05/31/2022 10:41:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.383664 on epoch=37
05/31/2022 10:41:16 - INFO - __main__ - Global step 300 Train loss 0.431074 Classification-F1 0.7009906197948628 on epoch=37
05/31/2022 10:41:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.324149 on epoch=38
05/31/2022 10:41:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.231362 on epoch=39
05/31/2022 10:41:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.254674 on epoch=41
05/31/2022 10:41:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.196304 on epoch=42
05/31/2022 10:41:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.203788 on epoch=43
05/31/2022 10:41:43 - INFO - __main__ - Global step 350 Train loss 0.242055 Classification-F1 0.6779193561482564 on epoch=43
05/31/2022 10:41:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.141867 on epoch=44
05/31/2022 10:41:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.175148 on epoch=46
05/31/2022 10:41:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.192591 on epoch=47
05/31/2022 10:42:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.168082 on epoch=48
05/31/2022 10:42:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.089361 on epoch=49
05/31/2022 10:42:10 - INFO - __main__ - Global step 400 Train loss 0.153410 Classification-F1 0.7175498992758482 on epoch=49
05/31/2022 10:42:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.118384 on epoch=51
05/31/2022 10:42:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.109081 on epoch=52
05/31/2022 10:42:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.107514 on epoch=53
05/31/2022 10:42:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.123255 on epoch=54
05/31/2022 10:42:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.399894 on epoch=56
05/31/2022 10:42:37 - INFO - __main__ - Global step 450 Train loss 0.171626 Classification-F1 0.6875894812310036 on epoch=56
05/31/2022 10:42:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.063980 on epoch=57
05/31/2022 10:42:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.077787 on epoch=58
05/31/2022 10:42:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.095589 on epoch=59
05/31/2022 10:42:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.105521 on epoch=61
05/31/2022 10:43:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.296173 on epoch=62
05/31/2022 10:43:04 - INFO - __main__ - Global step 500 Train loss 0.127810 Classification-F1 0.6818594541052169 on epoch=62
05/31/2022 10:43:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.025314 on epoch=63
05/31/2022 10:43:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.043263 on epoch=64
05/31/2022 10:43:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.098439 on epoch=66
05/31/2022 10:43:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.093470 on epoch=67
05/31/2022 10:43:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.025007 on epoch=68
05/31/2022 10:43:31 - INFO - __main__ - Global step 550 Train loss 0.057098 Classification-F1 0.6859519619271699 on epoch=68
05/31/2022 10:43:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.052761 on epoch=69
05/31/2022 10:43:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.021756 on epoch=71
05/31/2022 10:43:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.050210 on epoch=72
05/31/2022 10:43:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.045687 on epoch=73
05/31/2022 10:43:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.010101 on epoch=74
05/31/2022 10:43:58 - INFO - __main__ - Global step 600 Train loss 0.036103 Classification-F1 0.7459438314666325 on epoch=74
05/31/2022 10:44:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.012602 on epoch=76
05/31/2022 10:44:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.059902 on epoch=77
05/31/2022 10:44:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.027654 on epoch=78
05/31/2022 10:44:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.055913 on epoch=79
05/31/2022 10:44:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.005983 on epoch=81
05/31/2022 10:44:26 - INFO - __main__ - Global step 650 Train loss 0.032411 Classification-F1 0.7286273948764738 on epoch=81
05/31/2022 10:44:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.002689 on epoch=82
05/31/2022 10:44:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.013572 on epoch=83
05/31/2022 10:44:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.041863 on epoch=84
05/31/2022 10:44:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.006941 on epoch=86
05/31/2022 10:44:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.005481 on epoch=87
05/31/2022 10:44:53 - INFO - __main__ - Global step 700 Train loss 0.014109 Classification-F1 0.7293400588482555 on epoch=87
05/31/2022 10:44:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.014593 on epoch=88
05/31/2022 10:45:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.021960 on epoch=89
05/31/2022 10:45:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.024901 on epoch=91
05/31/2022 10:45:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.016640 on epoch=92
05/31/2022 10:45:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.020347 on epoch=93
05/31/2022 10:45:20 - INFO - __main__ - Global step 750 Train loss 0.019688 Classification-F1 0.7183649683649684 on epoch=93
05/31/2022 10:45:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.113793 on epoch=94
05/31/2022 10:45:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.002548 on epoch=96
05/31/2022 10:45:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.024769 on epoch=97
05/31/2022 10:45:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.010209 on epoch=98
05/31/2022 10:45:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.012423 on epoch=99
05/31/2022 10:45:47 - INFO - __main__ - Global step 800 Train loss 0.032748 Classification-F1 0.5786322624167299 on epoch=99
05/31/2022 10:45:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.031637 on epoch=101
05/31/2022 10:45:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.023824 on epoch=102
05/31/2022 10:46:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.003400 on epoch=103
05/31/2022 10:46:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.012283 on epoch=104
05/31/2022 10:46:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.009701 on epoch=106
05/31/2022 10:46:14 - INFO - __main__ - Global step 850 Train loss 0.016169 Classification-F1 0.6511814638169412 on epoch=106
05/31/2022 10:46:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.025571 on epoch=107
05/31/2022 10:46:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.002320 on epoch=108
05/31/2022 10:46:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.008927 on epoch=109
05/31/2022 10:46:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.004777 on epoch=111
05/31/2022 10:46:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.008551 on epoch=112
05/31/2022 10:46:42 - INFO - __main__ - Global step 900 Train loss 0.010029 Classification-F1 0.7134602172113239 on epoch=112
05/31/2022 10:46:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001191 on epoch=113
05/31/2022 10:46:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.002887 on epoch=114
05/31/2022 10:46:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.074016 on epoch=116
05/31/2022 10:47:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.003592 on epoch=117
05/31/2022 10:47:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.002605 on epoch=118
05/31/2022 10:47:09 - INFO - __main__ - Global step 950 Train loss 0.016858 Classification-F1 0.5601228878648234 on epoch=118
05/31/2022 10:47:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.004133 on epoch=119
05/31/2022 10:47:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.018555 on epoch=121
05/31/2022 10:47:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001232 on epoch=122
05/31/2022 10:47:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000479 on epoch=123
05/31/2022 10:47:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001731 on epoch=124
05/31/2022 10:47:36 - INFO - __main__ - Global step 1000 Train loss 0.005226 Classification-F1 0.705228145469888 on epoch=124
05/31/2022 10:47:36 - INFO - __main__ - save last model!
05/31/2022 10:47:36 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:47:36 - INFO - __main__ - Printing 3 examples
05/31/2022 10:47:36 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/31/2022 10:47:36 - INFO - __main__ - ['others']
05/31/2022 10:47:36 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/31/2022 10:47:36 - INFO - __main__ - ['others']
05/31/2022 10:47:36 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/31/2022 10:47:36 - INFO - __main__ - ['others']
05/31/2022 10:47:36 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:47:36 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:47:36 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 10:47:36 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:47:36 - INFO - __main__ - Printing 3 examples
05/31/2022 10:47:36 - INFO - __main__ -  [emo] please be my other half your who sorry don't be like that okkkk then my gi
05/31/2022 10:47:36 - INFO - __main__ - ['others']
05/31/2022 10:47:36 - INFO - __main__ -  [emo] i'll catch you after a short  what if you don't can i
05/31/2022 10:47:36 - INFO - __main__ - ['others']
05/31/2022 10:47:36 - INFO - __main__ -  [emo] u oly first  no you no u
05/31/2022 10:47:36 - INFO - __main__ - ['others']
05/31/2022 10:47:36 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:47:36 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:47:37 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 10:47:43 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 10:47:44 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 10:47:44 - INFO - __main__ - Printing 3 examples
05/31/2022 10:47:44 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 10:47:44 - INFO - __main__ - ['others']
05/31/2022 10:47:44 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 10:47:44 - INFO - __main__ - ['others']
05/31/2022 10:47:44 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 10:47:44 - INFO - __main__ - ['others']
05/31/2022 10:47:44 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:47:46 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:47:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 10:47:48 - INFO - __main__ - Starting training!
05/31/2022 10:47:52 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 10:48:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_13_0.0002_8_predictions.txt
05/31/2022 10:48:34 - INFO - __main__ - Classification-F1 on test data: 0.0234
05/31/2022 10:48:35 - INFO - __main__ - prefix=emo_32_13, lr=0.0002, bsz=8, dev_performance=0.7459438314666325, test_performance=0.023424629641323986
05/31/2022 10:48:35 - INFO - __main__ - Running ... prefix=emo_32_13, lr=0.0001, bsz=8 ...
05/31/2022 10:48:36 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:48:36 - INFO - __main__ - Printing 3 examples
05/31/2022 10:48:36 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/31/2022 10:48:36 - INFO - __main__ - ['others']
05/31/2022 10:48:36 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/31/2022 10:48:36 - INFO - __main__ - ['others']
05/31/2022 10:48:36 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/31/2022 10:48:36 - INFO - __main__ - ['others']
05/31/2022 10:48:36 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:48:36 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:48:36 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 10:48:36 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 10:48:36 - INFO - __main__ - Printing 3 examples
05/31/2022 10:48:36 - INFO - __main__ -  [emo] please be my other half your who sorry don't be like that okkkk then my gi
05/31/2022 10:48:36 - INFO - __main__ - ['others']
05/31/2022 10:48:36 - INFO - __main__ -  [emo] i'll catch you after a short  what if you don't can i
05/31/2022 10:48:36 - INFO - __main__ - ['others']
05/31/2022 10:48:36 - INFO - __main__ -  [emo] u oly first  no you no u
05/31/2022 10:48:36 - INFO - __main__ - ['others']
05/31/2022 10:48:36 - INFO - __main__ - Tokenizing Input ...
05/31/2022 10:48:36 - INFO - __main__ - Tokenizing Output ...
05/31/2022 10:48:36 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 10:48:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 10:48:49 - INFO - __main__ - Starting training!
05/31/2022 10:48:53 - INFO - __main__ - Step 10 Global step 10 Train loss 25.933294 on epoch=1
05/31/2022 10:48:58 - INFO - __main__ - Step 20 Global step 20 Train loss 21.866779 on epoch=2
05/31/2022 10:49:03 - INFO - __main__ - Step 30 Global step 30 Train loss 20.122816 on epoch=3
05/31/2022 10:49:09 - INFO - __main__ - Step 40 Global step 40 Train loss 19.425663 on epoch=4
05/31/2022 10:49:14 - INFO - __main__ - Step 50 Global step 50 Train loss 18.488699 on epoch=6
05/31/2022 10:49:54 - INFO - __main__ - Global step 50 Train loss 21.167452 Classification-F1 0.0 on epoch=6
05/31/2022 10:50:00 - INFO - __main__ - Step 60 Global step 60 Train loss 17.601122 on epoch=7
05/31/2022 10:50:05 - INFO - __main__ - Step 70 Global step 70 Train loss 17.684038 on epoch=8
05/31/2022 10:50:10 - INFO - __main__ - Step 80 Global step 80 Train loss 16.919628 on epoch=9
05/31/2022 10:50:15 - INFO - __main__ - Step 90 Global step 90 Train loss 17.514263 on epoch=11
05/31/2022 10:50:20 - INFO - __main__ - Step 100 Global step 100 Train loss 15.814835 on epoch=12
05/31/2022 10:50:57 - INFO - __main__ - Global step 100 Train loss 17.106775 Classification-F1 0.0 on epoch=12
05/31/2022 10:51:02 - INFO - __main__ - Step 110 Global step 110 Train loss 15.824995 on epoch=13
05/31/2022 10:51:07 - INFO - __main__ - Step 120 Global step 120 Train loss 15.265242 on epoch=14
05/31/2022 10:51:12 - INFO - __main__ - Step 130 Global step 130 Train loss 16.219423 on epoch=16
05/31/2022 10:51:17 - INFO - __main__ - Step 140 Global step 140 Train loss 15.774620 on epoch=17
05/31/2022 10:51:22 - INFO - __main__ - Step 150 Global step 150 Train loss 14.934752 on epoch=18
05/31/2022 10:51:47 - INFO - __main__ - Global step 150 Train loss 15.603806 Classification-F1 0.0 on epoch=18
05/31/2022 10:51:52 - INFO - __main__ - Step 160 Global step 160 Train loss 14.136015 on epoch=19
05/31/2022 10:51:57 - INFO - __main__ - Step 170 Global step 170 Train loss 14.209589 on epoch=21
05/31/2022 10:52:03 - INFO - __main__ - Step 180 Global step 180 Train loss 14.040337 on epoch=22
05/31/2022 10:52:08 - INFO - __main__ - Step 190 Global step 190 Train loss 13.495898 on epoch=23
05/31/2022 10:52:13 - INFO - __main__ - Step 200 Global step 200 Train loss 13.523026 on epoch=24
05/31/2022 10:52:31 - INFO - __main__ - Global step 200 Train loss 13.880974 Classification-F1 0.0016570008285004142 on epoch=24
05/31/2022 10:52:37 - INFO - __main__ - Step 210 Global step 210 Train loss 12.759175 on epoch=26
05/31/2022 10:52:42 - INFO - __main__ - Step 220 Global step 220 Train loss 12.965631 on epoch=27
05/31/2022 10:52:47 - INFO - __main__ - Step 230 Global step 230 Train loss 12.060687 on epoch=28
05/31/2022 10:52:53 - INFO - __main__ - Step 240 Global step 240 Train loss 12.061440 on epoch=29
05/31/2022 10:52:58 - INFO - __main__ - Step 250 Global step 250 Train loss 11.638914 on epoch=31
05/31/2022 10:53:07 - INFO - __main__ - Global step 250 Train loss 12.297169 Classification-F1 0.0 on epoch=31
05/31/2022 10:53:12 - INFO - __main__ - Step 260 Global step 260 Train loss 11.446991 on epoch=32
05/31/2022 10:53:17 - INFO - __main__ - Step 270 Global step 270 Train loss 10.620595 on epoch=33
05/31/2022 10:53:23 - INFO - __main__ - Step 280 Global step 280 Train loss 11.103910 on epoch=34
05/31/2022 10:53:28 - INFO - __main__ - Step 290 Global step 290 Train loss 10.133315 on epoch=36
05/31/2022 10:53:33 - INFO - __main__ - Step 300 Global step 300 Train loss 9.729016 on epoch=37
05/31/2022 10:53:40 - INFO - __main__ - Global step 300 Train loss 10.606767 Classification-F1 0.0 on epoch=37
05/31/2022 10:53:45 - INFO - __main__ - Step 310 Global step 310 Train loss 8.591776 on epoch=38
05/31/2022 10:53:50 - INFO - __main__ - Step 320 Global step 320 Train loss 8.797890 on epoch=39
05/31/2022 10:53:55 - INFO - __main__ - Step 330 Global step 330 Train loss 7.730151 on epoch=41
05/31/2022 10:54:00 - INFO - __main__ - Step 340 Global step 340 Train loss 8.223967 on epoch=42
05/31/2022 10:54:05 - INFO - __main__ - Step 350 Global step 350 Train loss 6.663053 on epoch=43
05/31/2022 10:54:07 - INFO - __main__ - Global step 350 Train loss 8.001368 Classification-F1 0.0 on epoch=43
05/31/2022 10:54:12 - INFO - __main__ - Step 360 Global step 360 Train loss 5.973567 on epoch=44
05/31/2022 10:54:17 - INFO - __main__ - Step 370 Global step 370 Train loss 4.997699 on epoch=46
05/31/2022 10:54:23 - INFO - __main__ - Step 380 Global step 380 Train loss 4.239889 on epoch=47
05/31/2022 10:54:28 - INFO - __main__ - Step 390 Global step 390 Train loss 5.309294 on epoch=48
05/31/2022 10:54:33 - INFO - __main__ - Step 400 Global step 400 Train loss 3.889207 on epoch=49
05/31/2022 10:54:34 - INFO - __main__ - Global step 400 Train loss 4.881931 Classification-F1 0.1 on epoch=49
05/31/2022 10:54:40 - INFO - __main__ - Step 410 Global step 410 Train loss 4.456044 on epoch=51
05/31/2022 10:54:45 - INFO - __main__ - Step 420 Global step 420 Train loss 4.281818 on epoch=52
05/31/2022 10:54:50 - INFO - __main__ - Step 430 Global step 430 Train loss 4.202975 on epoch=53
05/31/2022 10:54:55 - INFO - __main__ - Step 440 Global step 440 Train loss 3.731482 on epoch=54
05/31/2022 10:55:00 - INFO - __main__ - Step 450 Global step 450 Train loss 4.019523 on epoch=56
05/31/2022 10:55:01 - INFO - __main__ - Global step 450 Train loss 4.138369 Classification-F1 0.11187803187803189 on epoch=56
05/31/2022 10:55:07 - INFO - __main__ - Step 460 Global step 460 Train loss 3.590410 on epoch=57
05/31/2022 10:55:12 - INFO - __main__ - Step 470 Global step 470 Train loss 3.389667 on epoch=58
05/31/2022 10:55:17 - INFO - __main__ - Step 480 Global step 480 Train loss 4.240303 on epoch=59
05/31/2022 10:55:22 - INFO - __main__ - Step 490 Global step 490 Train loss 2.945094 on epoch=61
05/31/2022 10:55:28 - INFO - __main__ - Step 500 Global step 500 Train loss 3.218641 on epoch=62
05/31/2022 10:55:29 - INFO - __main__ - Global step 500 Train loss 3.476823 Classification-F1 0.14054054054054055 on epoch=62
05/31/2022 10:55:34 - INFO - __main__ - Step 510 Global step 510 Train loss 3.113264 on epoch=63
05/31/2022 10:55:39 - INFO - __main__ - Step 520 Global step 520 Train loss 3.706547 on epoch=64
05/31/2022 10:55:44 - INFO - __main__ - Step 530 Global step 530 Train loss 3.339153 on epoch=66
05/31/2022 10:55:50 - INFO - __main__ - Step 540 Global step 540 Train loss 3.440054 on epoch=67
05/31/2022 10:55:55 - INFO - __main__ - Step 550 Global step 550 Train loss 3.570473 on epoch=68
05/31/2022 10:56:05 - INFO - __main__ - Global step 550 Train loss 3.433898 Classification-F1 0.11634293634293635 on epoch=68
05/31/2022 10:56:10 - INFO - __main__ - Step 560 Global step 560 Train loss 3.265423 on epoch=69
05/31/2022 10:56:15 - INFO - __main__ - Step 570 Global step 570 Train loss 3.659703 on epoch=71
05/31/2022 10:56:20 - INFO - __main__ - Step 580 Global step 580 Train loss 2.599709 on epoch=72
05/31/2022 10:56:26 - INFO - __main__ - Step 590 Global step 590 Train loss 2.885595 on epoch=73
05/31/2022 10:56:31 - INFO - __main__ - Step 600 Global step 600 Train loss 3.051093 on epoch=74
05/31/2022 10:56:32 - INFO - __main__ - Global step 600 Train loss 3.092304 Classification-F1 0.15727986050566695 on epoch=74
05/31/2022 10:56:37 - INFO - __main__ - Step 610 Global step 610 Train loss 3.049277 on epoch=76
05/31/2022 10:56:43 - INFO - __main__ - Step 620 Global step 620 Train loss 2.753150 on epoch=77
05/31/2022 10:56:48 - INFO - __main__ - Step 630 Global step 630 Train loss 2.885808 on epoch=78
05/31/2022 10:56:53 - INFO - __main__ - Step 640 Global step 640 Train loss 2.922884 on epoch=79
05/31/2022 10:56:58 - INFO - __main__ - Step 650 Global step 650 Train loss 2.470361 on epoch=81
05/31/2022 10:56:59 - INFO - __main__ - Global step 650 Train loss 2.816296 Classification-F1 0.1 on epoch=81
05/31/2022 10:57:04 - INFO - __main__ - Step 660 Global step 660 Train loss 2.255483 on epoch=82
05/31/2022 10:57:09 - INFO - __main__ - Step 670 Global step 670 Train loss 2.864157 on epoch=83
05/31/2022 10:57:15 - INFO - __main__ - Step 680 Global step 680 Train loss 3.120207 on epoch=84
05/31/2022 10:57:20 - INFO - __main__ - Step 690 Global step 690 Train loss 2.667116 on epoch=86
05/31/2022 10:57:25 - INFO - __main__ - Step 700 Global step 700 Train loss 2.867422 on epoch=87
05/31/2022 10:57:26 - INFO - __main__ - Global step 700 Train loss 2.754877 Classification-F1 0.17480643240023822 on epoch=87
05/31/2022 10:57:31 - INFO - __main__ - Step 710 Global step 710 Train loss 2.458046 on epoch=88
05/31/2022 10:57:37 - INFO - __main__ - Step 720 Global step 720 Train loss 2.749675 on epoch=89
05/31/2022 10:57:42 - INFO - __main__ - Step 730 Global step 730 Train loss 2.638495 on epoch=91
05/31/2022 10:57:47 - INFO - __main__ - Step 740 Global step 740 Train loss 1.954778 on epoch=92
05/31/2022 10:57:52 - INFO - __main__ - Step 750 Global step 750 Train loss 2.495788 on epoch=93
05/31/2022 10:57:53 - INFO - __main__ - Global step 750 Train loss 2.459356 Classification-F1 0.14279332936271588 on epoch=93
05/31/2022 10:57:59 - INFO - __main__ - Step 760 Global step 760 Train loss 2.287817 on epoch=94
05/31/2022 10:58:04 - INFO - __main__ - Step 770 Global step 770 Train loss 2.279758 on epoch=96
05/31/2022 10:58:09 - INFO - __main__ - Step 780 Global step 780 Train loss 2.608948 on epoch=97
05/31/2022 10:58:14 - INFO - __main__ - Step 790 Global step 790 Train loss 2.239023 on epoch=98
05/31/2022 10:58:19 - INFO - __main__ - Step 800 Global step 800 Train loss 2.542099 on epoch=99
05/31/2022 10:58:20 - INFO - __main__ - Global step 800 Train loss 2.391529 Classification-F1 0.24450967601652535 on epoch=99
05/31/2022 10:58:26 - INFO - __main__ - Step 810 Global step 810 Train loss 2.658855 on epoch=101
05/31/2022 10:58:31 - INFO - __main__ - Step 820 Global step 820 Train loss 2.261588 on epoch=102
05/31/2022 10:58:36 - INFO - __main__ - Step 830 Global step 830 Train loss 2.352165 on epoch=103
05/31/2022 10:58:41 - INFO - __main__ - Step 840 Global step 840 Train loss 2.464542 on epoch=104
05/31/2022 10:58:46 - INFO - __main__ - Step 850 Global step 850 Train loss 2.616041 on epoch=106
05/31/2022 10:58:47 - INFO - __main__ - Global step 850 Train loss 2.470638 Classification-F1 0.1304822565969063 on epoch=106
05/31/2022 10:58:53 - INFO - __main__ - Step 860 Global step 860 Train loss 2.091579 on epoch=107
05/31/2022 10:58:58 - INFO - __main__ - Step 870 Global step 870 Train loss 1.970800 on epoch=108
05/31/2022 10:59:03 - INFO - __main__ - Step 880 Global step 880 Train loss 2.053851 on epoch=109
05/31/2022 10:59:08 - INFO - __main__ - Step 890 Global step 890 Train loss 2.235039 on epoch=111
05/31/2022 10:59:13 - INFO - __main__ - Step 900 Global step 900 Train loss 1.773685 on epoch=112
05/31/2022 10:59:14 - INFO - __main__ - Global step 900 Train loss 2.024991 Classification-F1 0.2439660138248848 on epoch=112
05/31/2022 10:59:20 - INFO - __main__ - Step 910 Global step 910 Train loss 2.184698 on epoch=113
05/31/2022 10:59:25 - INFO - __main__ - Step 920 Global step 920 Train loss 1.732707 on epoch=114
05/31/2022 10:59:30 - INFO - __main__ - Step 930 Global step 930 Train loss 1.825537 on epoch=116
05/31/2022 10:59:35 - INFO - __main__ - Step 940 Global step 940 Train loss 1.619148 on epoch=117
05/31/2022 10:59:41 - INFO - __main__ - Step 950 Global step 950 Train loss 2.270736 on epoch=118
05/31/2022 10:59:41 - INFO - __main__ - Global step 950 Train loss 1.926565 Classification-F1 0.19952670723461802 on epoch=118
05/31/2022 10:59:47 - INFO - __main__ - Step 960 Global step 960 Train loss 1.875762 on epoch=119
05/31/2022 10:59:52 - INFO - __main__ - Step 970 Global step 970 Train loss 1.729329 on epoch=121
05/31/2022 10:59:57 - INFO - __main__ - Step 980 Global step 980 Train loss 1.962956 on epoch=122
05/31/2022 11:00:02 - INFO - __main__ - Step 990 Global step 990 Train loss 2.023084 on epoch=123
05/31/2022 11:00:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.878935 on epoch=124
05/31/2022 11:00:09 - INFO - __main__ - Global step 1000 Train loss 1.894014 Classification-F1 0.2658395989974937 on epoch=124
05/31/2022 11:00:09 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:00:09 - INFO - __main__ - Printing 3 examples
05/31/2022 11:00:09 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/31/2022 11:00:09 - INFO - __main__ - ['sad']
05/31/2022 11:00:09 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/31/2022 11:00:09 - INFO - __main__ - ['sad']
05/31/2022 11:00:09 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/31/2022 11:00:09 - INFO - __main__ - ['sad']
05/31/2022 11:00:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:00:09 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:00:09 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:00:09 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:00:09 - INFO - __main__ - Printing 3 examples
05/31/2022 11:00:09 - INFO - __main__ -  [emo] i am not ok why  down with fever
05/31/2022 11:00:09 - INFO - __main__ - ['sad']
05/31/2022 11:00:09 - INFO - __main__ -  [emo] i'm disappointed such people stink that's rude to generalise
05/31/2022 11:00:09 - INFO - __main__ - ['sad']
05/31/2022 11:00:09 - INFO - __main__ -  [emo] o go to hell how are u  miserable
05/31/2022 11:00:09 - INFO - __main__ - ['sad']
05/31/2022 11:00:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:00:09 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:00:09 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:00:09 - INFO - __main__ - save last model!
05/31/2022 11:00:16 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 11:00:17 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 11:00:17 - INFO - __main__ - Printing 3 examples
05/31/2022 11:00:17 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 11:00:17 - INFO - __main__ - ['others']
05/31/2022 11:00:17 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 11:00:17 - INFO - __main__ - ['others']
05/31/2022 11:00:17 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 11:00:17 - INFO - __main__ - ['others']
05/31/2022 11:00:17 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:00:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:00:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:00:20 - INFO - __main__ - Starting training!
05/31/2022 11:00:24 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 11:01:06 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_13_0.0001_8_predictions.txt
05/31/2022 11:01:06 - INFO - __main__ - Classification-F1 on test data: 0.1074
05/31/2022 11:01:07 - INFO - __main__ - prefix=emo_32_13, lr=0.0001, bsz=8, dev_performance=0.2658395989974937, test_performance=0.10743341999872585
05/31/2022 11:01:07 - INFO - __main__ - Running ... prefix=emo_32_21, lr=0.0005, bsz=8 ...
05/31/2022 11:01:08 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:01:08 - INFO - __main__ - Printing 3 examples
05/31/2022 11:01:08 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/31/2022 11:01:08 - INFO - __main__ - ['sad']
05/31/2022 11:01:08 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/31/2022 11:01:08 - INFO - __main__ - ['sad']
05/31/2022 11:01:08 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/31/2022 11:01:08 - INFO - __main__ - ['sad']
05/31/2022 11:01:08 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:01:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:01:08 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:01:08 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:01:08 - INFO - __main__ - Printing 3 examples
05/31/2022 11:01:08 - INFO - __main__ -  [emo] i am not ok why  down with fever
05/31/2022 11:01:08 - INFO - __main__ - ['sad']
05/31/2022 11:01:08 - INFO - __main__ -  [emo] i'm disappointed such people stink that's rude to generalise
05/31/2022 11:01:08 - INFO - __main__ - ['sad']
05/31/2022 11:01:08 - INFO - __main__ -  [emo] o go to hell how are u  miserable
05/31/2022 11:01:08 - INFO - __main__ - ['sad']
05/31/2022 11:01:08 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:01:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:01:08 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:01:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:01:21 - INFO - __main__ - Starting training!
05/31/2022 11:01:25 - INFO - __main__ - Step 10 Global step 10 Train loss 24.803339 on epoch=1
05/31/2022 11:01:30 - INFO - __main__ - Step 20 Global step 20 Train loss 18.484879 on epoch=2
05/31/2022 11:01:36 - INFO - __main__ - Step 30 Global step 30 Train loss 16.633251 on epoch=3
05/31/2022 11:01:41 - INFO - __main__ - Step 40 Global step 40 Train loss 14.253139 on epoch=4
05/31/2022 11:01:46 - INFO - __main__ - Step 50 Global step 50 Train loss 13.197392 on epoch=6
05/31/2022 11:01:47 - INFO - __main__ - Global step 50 Train loss 17.474401 Classification-F1 0.0 on epoch=6
05/31/2022 11:01:53 - INFO - __main__ - Step 60 Global step 60 Train loss 11.430510 on epoch=7
05/31/2022 11:01:58 - INFO - __main__ - Step 70 Global step 70 Train loss 10.015354 on epoch=8
05/31/2022 11:02:03 - INFO - __main__ - Step 80 Global step 80 Train loss 5.520062 on epoch=9
05/31/2022 11:02:08 - INFO - __main__ - Step 90 Global step 90 Train loss 2.745765 on epoch=11
05/31/2022 11:02:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.648719 on epoch=12
05/31/2022 11:02:14 - INFO - __main__ - Global step 100 Train loss 6.072082 Classification-F1 0.503255701143331 on epoch=12
05/31/2022 11:02:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.658661 on epoch=13
05/31/2022 11:02:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.543000 on epoch=14
05/31/2022 11:02:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.466235 on epoch=16
05/31/2022 11:02:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.304913 on epoch=17
05/31/2022 11:02:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.322650 on epoch=18
05/31/2022 11:02:42 - INFO - __main__ - Global step 150 Train loss 0.459092 Classification-F1 0.48663777089783283 on epoch=18
05/31/2022 11:02:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.283718 on epoch=19
05/31/2022 11:02:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.301025 on epoch=21
05/31/2022 11:02:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.166721 on epoch=22
05/31/2022 11:03:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.126989 on epoch=23
05/31/2022 11:03:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.084687 on epoch=24
05/31/2022 11:03:09 - INFO - __main__ - Global step 200 Train loss 0.192628 Classification-F1 0.6861336866514995 on epoch=24
05/31/2022 11:03:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.027656 on epoch=26
05/31/2022 11:03:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.062493 on epoch=27
05/31/2022 11:03:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.095509 on epoch=28
05/31/2022 11:03:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.020492 on epoch=29
05/31/2022 11:03:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.034583 on epoch=31
05/31/2022 11:03:36 - INFO - __main__ - Global step 250 Train loss 0.048147 Classification-F1 0.7416488983653163 on epoch=31
05/31/2022 11:03:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.014524 on epoch=32
05/31/2022 11:03:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.046490 on epoch=33
05/31/2022 11:03:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.223308 on epoch=34
05/31/2022 11:03:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.019843 on epoch=36
05/31/2022 11:04:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.021105 on epoch=37
05/31/2022 11:04:04 - INFO - __main__ - Global step 300 Train loss 0.065054 Classification-F1 0.6616580525608464 on epoch=37
05/31/2022 11:04:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.006333 on epoch=38
05/31/2022 11:04:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.002984 on epoch=39
05/31/2022 11:04:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.004215 on epoch=41
05/31/2022 11:04:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.002574 on epoch=42
05/31/2022 11:04:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.023238 on epoch=43
05/31/2022 11:04:31 - INFO - __main__ - Global step 350 Train loss 0.007869 Classification-F1 0.5769836523125997 on epoch=43
05/31/2022 11:04:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.004533 on epoch=44
05/31/2022 11:04:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.004185 on epoch=46
05/31/2022 11:04:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.012903 on epoch=47
05/31/2022 11:04:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.045768 on epoch=48
05/31/2022 11:04:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.002796 on epoch=49
05/31/2022 11:04:57 - INFO - __main__ - Global step 400 Train loss 0.014037 Classification-F1 0.7582944385662634 on epoch=49
05/31/2022 11:05:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000521 on epoch=51
05/31/2022 11:05:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000592 on epoch=52
05/31/2022 11:05:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.007025 on epoch=53
05/31/2022 11:05:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.063749 on epoch=54
05/31/2022 11:05:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.003168 on epoch=56
05/31/2022 11:05:25 - INFO - __main__ - Global step 450 Train loss 0.015011 Classification-F1 0.7363949793622313 on epoch=56
05/31/2022 11:05:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.007573 on epoch=57
05/31/2022 11:05:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003668 on epoch=58
05/31/2022 11:05:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000985 on epoch=59
05/31/2022 11:05:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000430 on epoch=61
05/31/2022 11:05:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000156 on epoch=62
05/31/2022 11:05:52 - INFO - __main__ - Global step 500 Train loss 0.002563 Classification-F1 0.7574154011510992 on epoch=62
05/31/2022 11:05:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.009581 on epoch=63
05/31/2022 11:06:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000618 on epoch=64
05/31/2022 11:06:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.005954 on epoch=66
05/31/2022 11:06:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.031017 on epoch=67
05/31/2022 11:06:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.009695 on epoch=68
05/31/2022 11:06:19 - INFO - __main__ - Global step 550 Train loss 0.011373 Classification-F1 0.7069930487513588 on epoch=68
05/31/2022 11:06:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.113679 on epoch=69
05/31/2022 11:06:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.266236 on epoch=71
05/31/2022 11:06:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000694 on epoch=72
05/31/2022 11:06:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002018 on epoch=73
05/31/2022 11:06:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001220 on epoch=74
05/31/2022 11:06:45 - INFO - __main__ - Global step 600 Train loss 0.076769 Classification-F1 0.7244075788079238 on epoch=74
05/31/2022 11:06:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000788 on epoch=76
05/31/2022 11:06:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.017725 on epoch=77
05/31/2022 11:07:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000194 on epoch=78
05/31/2022 11:07:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.003432 on epoch=79
05/31/2022 11:07:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001961 on epoch=81
05/31/2022 11:07:12 - INFO - __main__ - Global step 650 Train loss 0.004820 Classification-F1 0.7146214896214896 on epoch=81
05/31/2022 11:07:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.024037 on epoch=82
05/31/2022 11:07:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000466 on epoch=83
05/31/2022 11:07:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000076 on epoch=84
05/31/2022 11:07:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000021 on epoch=86
05/31/2022 11:07:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000471 on epoch=87
05/31/2022 11:07:39 - INFO - __main__ - Global step 700 Train loss 0.005014 Classification-F1 0.747914548221227 on epoch=87
05/31/2022 11:07:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000035 on epoch=88
05/31/2022 11:07:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.022830 on epoch=89
05/31/2022 11:07:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000114 on epoch=91
05/31/2022 11:08:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000698 on epoch=92
05/31/2022 11:08:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000103 on epoch=93
05/31/2022 11:08:06 - INFO - __main__ - Global step 750 Train loss 0.004756 Classification-F1 0.7503081541781232 on epoch=93
05/31/2022 11:08:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000151 on epoch=94
05/31/2022 11:08:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000041 on epoch=96
05/31/2022 11:08:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000094 on epoch=97
05/31/2022 11:08:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000136 on epoch=98
05/31/2022 11:08:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.073889 on epoch=99
05/31/2022 11:08:33 - INFO - __main__ - Global step 800 Train loss 0.014862 Classification-F1 0.7700909025061567 on epoch=99
05/31/2022 11:08:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.007127 on epoch=101
05/31/2022 11:08:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000278 on epoch=102
05/31/2022 11:08:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.003536 on epoch=103
05/31/2022 11:08:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000191 on epoch=104
05/31/2022 11:08:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000097 on epoch=106
05/31/2022 11:09:00 - INFO - __main__ - Global step 850 Train loss 0.002246 Classification-F1 0.743199441687345 on epoch=106
05/31/2022 11:09:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.003091 on epoch=107
05/31/2022 11:09:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000287 on epoch=108
05/31/2022 11:09:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000799 on epoch=109
05/31/2022 11:09:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.011055 on epoch=111
05/31/2022 11:09:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000120 on epoch=112
05/31/2022 11:09:27 - INFO - __main__ - Global step 900 Train loss 0.003070 Classification-F1 0.5751807078763709 on epoch=112
05/31/2022 11:09:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000062 on epoch=113
05/31/2022 11:09:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000427 on epoch=114
05/31/2022 11:09:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000072 on epoch=116
05/31/2022 11:09:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000023 on epoch=117
05/31/2022 11:09:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000040 on epoch=118
05/31/2022 11:09:54 - INFO - __main__ - Global step 950 Train loss 0.000125 Classification-F1 0.7402835777213193 on epoch=118
05/31/2022 11:09:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000033 on epoch=119
05/31/2022 11:10:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000021 on epoch=121
05/31/2022 11:10:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000029 on epoch=122
05/31/2022 11:10:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000016 on epoch=123
05/31/2022 11:10:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000015 on epoch=124
05/31/2022 11:10:21 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:10:21 - INFO - __main__ - Printing 3 examples
05/31/2022 11:10:21 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/31/2022 11:10:21 - INFO - __main__ - ['sad']
05/31/2022 11:10:21 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/31/2022 11:10:21 - INFO - __main__ - ['sad']
05/31/2022 11:10:21 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/31/2022 11:10:21 - INFO - __main__ - ['sad']
05/31/2022 11:10:21 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:10:21 - INFO - __main__ - Global step 1000 Train loss 0.000023 Classification-F1 0.7485422135335036 on epoch=124
05/31/2022 11:10:21 - INFO - __main__ - save last model!
05/31/2022 11:10:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:10:21 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:10:21 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:10:21 - INFO - __main__ - Printing 3 examples
05/31/2022 11:10:21 - INFO - __main__ -  [emo] i am not ok why  down with fever
05/31/2022 11:10:21 - INFO - __main__ - ['sad']
05/31/2022 11:10:21 - INFO - __main__ -  [emo] i'm disappointed such people stink that's rude to generalise
05/31/2022 11:10:21 - INFO - __main__ - ['sad']
05/31/2022 11:10:21 - INFO - __main__ -  [emo] o go to hell how are u  miserable
05/31/2022 11:10:21 - INFO - __main__ - ['sad']
05/31/2022 11:10:21 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:10:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:10:22 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:10:29 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 11:10:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 11:10:30 - INFO - __main__ - Printing 3 examples
05/31/2022 11:10:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 11:10:30 - INFO - __main__ - ['others']
05/31/2022 11:10:30 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 11:10:30 - INFO - __main__ - ['others']
05/31/2022 11:10:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 11:10:30 - INFO - __main__ - ['others']
05/31/2022 11:10:30 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:10:32 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:10:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:10:34 - INFO - __main__ - Starting training!
05/31/2022 11:10:37 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 11:11:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_21_0.0005_8_predictions.txt
05/31/2022 11:11:32 - INFO - __main__ - Classification-F1 on test data: 0.0394
05/31/2022 11:11:32 - INFO - __main__ - prefix=emo_32_21, lr=0.0005, bsz=8, dev_performance=0.7700909025061567, test_performance=0.039428981151203975
05/31/2022 11:11:32 - INFO - __main__ - Running ... prefix=emo_32_21, lr=0.0003, bsz=8 ...
05/31/2022 11:11:33 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:11:33 - INFO - __main__ - Printing 3 examples
05/31/2022 11:11:33 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/31/2022 11:11:33 - INFO - __main__ - ['sad']
05/31/2022 11:11:33 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/31/2022 11:11:33 - INFO - __main__ - ['sad']
05/31/2022 11:11:33 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/31/2022 11:11:33 - INFO - __main__ - ['sad']
05/31/2022 11:11:33 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:11:33 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:11:33 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:11:33 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:11:33 - INFO - __main__ - Printing 3 examples
05/31/2022 11:11:33 - INFO - __main__ -  [emo] i am not ok why  down with fever
05/31/2022 11:11:33 - INFO - __main__ - ['sad']
05/31/2022 11:11:33 - INFO - __main__ -  [emo] i'm disappointed such people stink that's rude to generalise
05/31/2022 11:11:33 - INFO - __main__ - ['sad']
05/31/2022 11:11:33 - INFO - __main__ -  [emo] o go to hell how are u  miserable
05/31/2022 11:11:33 - INFO - __main__ - ['sad']
05/31/2022 11:11:33 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:11:33 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:11:34 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:11:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:11:46 - INFO - __main__ - Starting training!
05/31/2022 11:11:51 - INFO - __main__ - Step 10 Global step 10 Train loss 24.043583 on epoch=1
05/31/2022 11:11:56 - INFO - __main__ - Step 20 Global step 20 Train loss 20.429705 on epoch=2
05/31/2022 11:12:01 - INFO - __main__ - Step 30 Global step 30 Train loss 17.953526 on epoch=3
05/31/2022 11:12:06 - INFO - __main__ - Step 40 Global step 40 Train loss 17.125431 on epoch=4
05/31/2022 11:12:11 - INFO - __main__ - Step 50 Global step 50 Train loss 15.215715 on epoch=6
05/31/2022 11:12:21 - INFO - __main__ - Global step 50 Train loss 18.953592 Classification-F1 0.0 on epoch=6
05/31/2022 11:12:26 - INFO - __main__ - Step 60 Global step 60 Train loss 15.184568 on epoch=7
05/31/2022 11:12:31 - INFO - __main__ - Step 70 Global step 70 Train loss 14.378939 on epoch=8
05/31/2022 11:12:36 - INFO - __main__ - Step 80 Global step 80 Train loss 13.419699 on epoch=9
05/31/2022 11:12:41 - INFO - __main__ - Step 90 Global step 90 Train loss 11.750635 on epoch=11
05/31/2022 11:12:46 - INFO - __main__ - Step 100 Global step 100 Train loss 11.377394 on epoch=12
05/31/2022 11:12:49 - INFO - __main__ - Global step 100 Train loss 13.222246 Classification-F1 0.0 on epoch=12
05/31/2022 11:12:54 - INFO - __main__ - Step 110 Global step 110 Train loss 11.369580 on epoch=13
05/31/2022 11:12:59 - INFO - __main__ - Step 120 Global step 120 Train loss 9.825772 on epoch=14
05/31/2022 11:13:04 - INFO - __main__ - Step 130 Global step 130 Train loss 8.504438 on epoch=16
05/31/2022 11:13:09 - INFO - __main__ - Step 140 Global step 140 Train loss 6.795786 on epoch=17
05/31/2022 11:13:14 - INFO - __main__ - Step 150 Global step 150 Train loss 5.792782 on epoch=18
05/31/2022 11:13:15 - INFO - __main__ - Global step 150 Train loss 8.457672 Classification-F1 0.011428571428571429 on epoch=18
05/31/2022 11:13:21 - INFO - __main__ - Step 160 Global step 160 Train loss 3.379625 on epoch=19
05/31/2022 11:13:26 - INFO - __main__ - Step 170 Global step 170 Train loss 3.161990 on epoch=21
05/31/2022 11:13:31 - INFO - __main__ - Step 180 Global step 180 Train loss 3.580527 on epoch=22
05/31/2022 11:13:36 - INFO - __main__ - Step 190 Global step 190 Train loss 3.344893 on epoch=23
05/31/2022 11:13:41 - INFO - __main__ - Step 200 Global step 200 Train loss 3.021223 on epoch=24
05/31/2022 11:13:42 - INFO - __main__ - Global step 200 Train loss 3.297651 Classification-F1 0.1 on epoch=24
05/31/2022 11:13:48 - INFO - __main__ - Step 210 Global step 210 Train loss 2.624457 on epoch=26
05/31/2022 11:13:53 - INFO - __main__ - Step 220 Global step 220 Train loss 2.554720 on epoch=27
05/31/2022 11:13:58 - INFO - __main__ - Step 230 Global step 230 Train loss 2.907673 on epoch=28
05/31/2022 11:14:03 - INFO - __main__ - Step 240 Global step 240 Train loss 1.899200 on epoch=29
05/31/2022 11:14:08 - INFO - __main__ - Step 250 Global step 250 Train loss 1.899804 on epoch=31
05/31/2022 11:14:09 - INFO - __main__ - Global step 250 Train loss 2.377171 Classification-F1 0.3026143790849673 on epoch=31
05/31/2022 11:14:15 - INFO - __main__ - Step 260 Global step 260 Train loss 2.217583 on epoch=32
05/31/2022 11:14:20 - INFO - __main__ - Step 270 Global step 270 Train loss 2.165233 on epoch=33
05/31/2022 11:14:25 - INFO - __main__ - Step 280 Global step 280 Train loss 2.058253 on epoch=34
05/31/2022 11:14:30 - INFO - __main__ - Step 290 Global step 290 Train loss 2.392647 on epoch=36
05/31/2022 11:14:35 - INFO - __main__ - Step 300 Global step 300 Train loss 2.070429 on epoch=37
05/31/2022 11:14:36 - INFO - __main__ - Global step 300 Train loss 2.180829 Classification-F1 0.32854219948849106 on epoch=37
05/31/2022 11:14:42 - INFO - __main__ - Step 310 Global step 310 Train loss 1.843924 on epoch=38
05/31/2022 11:14:47 - INFO - __main__ - Step 320 Global step 320 Train loss 1.494205 on epoch=39
05/31/2022 11:14:52 - INFO - __main__ - Step 330 Global step 330 Train loss 1.164997 on epoch=41
05/31/2022 11:14:57 - INFO - __main__ - Step 340 Global step 340 Train loss 1.300919 on epoch=42
05/31/2022 11:15:02 - INFO - __main__ - Step 350 Global step 350 Train loss 1.455019 on epoch=43
05/31/2022 11:15:03 - INFO - __main__ - Global step 350 Train loss 1.451813 Classification-F1 0.28112728112728114 on epoch=43
05/31/2022 11:15:09 - INFO - __main__ - Step 360 Global step 360 Train loss 1.309868 on epoch=44
05/31/2022 11:15:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.804750 on epoch=46
05/31/2022 11:15:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.820337 on epoch=47
05/31/2022 11:15:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.813879 on epoch=48
05/31/2022 11:15:29 - INFO - __main__ - Step 400 Global step 400 Train loss 1.090183 on epoch=49
05/31/2022 11:15:30 - INFO - __main__ - Global step 400 Train loss 0.967803 Classification-F1 0.5327597840755736 on epoch=49
05/31/2022 11:15:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.705668 on epoch=51
05/31/2022 11:15:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.669449 on epoch=52
05/31/2022 11:15:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.659878 on epoch=53
05/31/2022 11:15:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.734795 on epoch=54
05/31/2022 11:15:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.479485 on epoch=56
05/31/2022 11:15:57 - INFO - __main__ - Global step 450 Train loss 0.649855 Classification-F1 0.6895709988540871 on epoch=56
05/31/2022 11:16:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.410917 on epoch=57
05/31/2022 11:16:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.524472 on epoch=58
05/31/2022 11:16:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.433710 on epoch=59
05/31/2022 11:16:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.334407 on epoch=61
05/31/2022 11:16:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.211461 on epoch=62
05/31/2022 11:16:25 - INFO - __main__ - Global step 500 Train loss 0.382994 Classification-F1 0.7421169283192334 on epoch=62
05/31/2022 11:16:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.282307 on epoch=63
05/31/2022 11:16:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.257976 on epoch=64
05/31/2022 11:16:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.168557 on epoch=66
05/31/2022 11:16:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.127380 on epoch=67
05/31/2022 11:16:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.171449 on epoch=68
05/31/2022 11:16:52 - INFO - __main__ - Global step 550 Train loss 0.201534 Classification-F1 0.6849629133345687 on epoch=68
05/31/2022 11:16:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.124946 on epoch=69
05/31/2022 11:17:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.117255 on epoch=71
05/31/2022 11:17:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.135692 on epoch=72
05/31/2022 11:17:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.079851 on epoch=73
05/31/2022 11:17:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.052518 on epoch=74
05/31/2022 11:17:18 - INFO - __main__ - Global step 600 Train loss 0.102052 Classification-F1 0.7189742869919731 on epoch=74
05/31/2022 11:17:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.123762 on epoch=76
05/31/2022 11:17:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.078800 on epoch=77
05/31/2022 11:17:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.093150 on epoch=78
05/31/2022 11:17:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.061334 on epoch=79
05/31/2022 11:17:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.068798 on epoch=81
05/31/2022 11:17:45 - INFO - __main__ - Global step 650 Train loss 0.085169 Classification-F1 0.7232358870967742 on epoch=81
05/31/2022 11:17:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.042373 on epoch=82
05/31/2022 11:17:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.033985 on epoch=83
05/31/2022 11:18:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.039377 on epoch=84
05/31/2022 11:18:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.034487 on epoch=86
05/31/2022 11:18:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.018456 on epoch=87
05/31/2022 11:18:12 - INFO - __main__ - Global step 700 Train loss 0.033736 Classification-F1 0.7177460875323808 on epoch=87
05/31/2022 11:18:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.033343 on epoch=88
05/31/2022 11:18:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.008589 on epoch=89
05/31/2022 11:18:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.022236 on epoch=91
05/31/2022 11:18:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.012739 on epoch=92
05/31/2022 11:18:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.010558 on epoch=93
05/31/2022 11:18:39 - INFO - __main__ - Global step 750 Train loss 0.017493 Classification-F1 0.7225503119452954 on epoch=93
05/31/2022 11:18:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.040070 on epoch=94
05/31/2022 11:18:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.007624 on epoch=96
05/31/2022 11:18:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003634 on epoch=97
05/31/2022 11:18:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.003858 on epoch=98
05/31/2022 11:19:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.019246 on epoch=99
05/31/2022 11:19:05 - INFO - __main__ - Global step 800 Train loss 0.014886 Classification-F1 0.7636948529411766 on epoch=99
05/31/2022 11:19:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.009163 on epoch=101
05/31/2022 11:19:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.003049 on epoch=102
05/31/2022 11:19:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.009541 on epoch=103
05/31/2022 11:19:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.002201 on epoch=104
05/31/2022 11:19:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.043026 on epoch=106
05/31/2022 11:19:33 - INFO - __main__ - Global step 850 Train loss 0.013396 Classification-F1 0.7200095605546336 on epoch=106
05/31/2022 11:19:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.006898 on epoch=107
05/31/2022 11:19:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.008004 on epoch=108
05/31/2022 11:19:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.002541 on epoch=109
05/31/2022 11:19:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001627 on epoch=111
05/31/2022 11:19:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001778 on epoch=112
05/31/2022 11:19:59 - INFO - __main__ - Global step 900 Train loss 0.004169 Classification-F1 0.7465789973560129 on epoch=112
05/31/2022 11:20:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001160 on epoch=113
05/31/2022 11:20:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.002552 on epoch=114
05/31/2022 11:20:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000561 on epoch=116
05/31/2022 11:20:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000376 on epoch=117
05/31/2022 11:20:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.023063 on epoch=118
05/31/2022 11:20:26 - INFO - __main__ - Global step 950 Train loss 0.005542 Classification-F1 0.7387895998395588 on epoch=118
05/31/2022 11:20:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.010372 on epoch=119
05/31/2022 11:20:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001230 on epoch=121
05/31/2022 11:20:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.003765 on epoch=122
05/31/2022 11:20:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.020163 on epoch=123
05/31/2022 11:20:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.002994 on epoch=124
05/31/2022 11:20:53 - INFO - __main__ - Global step 1000 Train loss 0.007705 Classification-F1 0.7408123455719797 on epoch=124
05/31/2022 11:20:53 - INFO - __main__ - save last model!
05/31/2022 11:20:53 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:20:53 - INFO - __main__ - Printing 3 examples
05/31/2022 11:20:53 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/31/2022 11:20:53 - INFO - __main__ - ['sad']
05/31/2022 11:20:53 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/31/2022 11:20:53 - INFO - __main__ - ['sad']
05/31/2022 11:20:53 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/31/2022 11:20:53 - INFO - __main__ - ['sad']
05/31/2022 11:20:53 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:20:53 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:20:53 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:20:53 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:20:53 - INFO - __main__ - Printing 3 examples
05/31/2022 11:20:53 - INFO - __main__ -  [emo] i am not ok why  down with fever
05/31/2022 11:20:53 - INFO - __main__ - ['sad']
05/31/2022 11:20:53 - INFO - __main__ -  [emo] i'm disappointed such people stink that's rude to generalise
05/31/2022 11:20:53 - INFO - __main__ - ['sad']
05/31/2022 11:20:53 - INFO - __main__ -  [emo] o go to hell how are u  miserable
05/31/2022 11:20:53 - INFO - __main__ - ['sad']
05/31/2022 11:20:53 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:20:53 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:20:53 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:21:00 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 11:21:01 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 11:21:01 - INFO - __main__ - Printing 3 examples
05/31/2022 11:21:01 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 11:21:01 - INFO - __main__ - ['others']
05/31/2022 11:21:01 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 11:21:01 - INFO - __main__ - ['others']
05/31/2022 11:21:01 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 11:21:01 - INFO - __main__ - ['others']
05/31/2022 11:21:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:21:03 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:21:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:21:04 - INFO - __main__ - Starting training!
05/31/2022 11:21:08 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 11:21:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_21_0.0003_8_predictions.txt
05/31/2022 11:21:52 - INFO - __main__ - Classification-F1 on test data: 0.4695
05/31/2022 11:21:52 - INFO - __main__ - prefix=emo_32_21, lr=0.0003, bsz=8, dev_performance=0.7636948529411766, test_performance=0.4695384704765091
05/31/2022 11:21:52 - INFO - __main__ - Running ... prefix=emo_32_21, lr=0.0002, bsz=8 ...
05/31/2022 11:21:53 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:21:53 - INFO - __main__ - Printing 3 examples
05/31/2022 11:21:53 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/31/2022 11:21:53 - INFO - __main__ - ['sad']
05/31/2022 11:21:53 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/31/2022 11:21:53 - INFO - __main__ - ['sad']
05/31/2022 11:21:53 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/31/2022 11:21:53 - INFO - __main__ - ['sad']
05/31/2022 11:21:53 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:21:53 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:21:53 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:21:53 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:21:53 - INFO - __main__ - Printing 3 examples
05/31/2022 11:21:53 - INFO - __main__ -  [emo] i am not ok why  down with fever
05/31/2022 11:21:53 - INFO - __main__ - ['sad']
05/31/2022 11:21:53 - INFO - __main__ -  [emo] i'm disappointed such people stink that's rude to generalise
05/31/2022 11:21:53 - INFO - __main__ - ['sad']
05/31/2022 11:21:53 - INFO - __main__ -  [emo] o go to hell how are u  miserable
05/31/2022 11:21:53 - INFO - __main__ - ['sad']
05/31/2022 11:21:53 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:21:53 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:21:53 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:22:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:22:04 - INFO - __main__ - Starting training!
05/31/2022 11:22:08 - INFO - __main__ - Step 10 Global step 10 Train loss 24.192755 on epoch=1
05/31/2022 11:22:13 - INFO - __main__ - Step 20 Global step 20 Train loss 20.899830 on epoch=2
05/31/2022 11:22:18 - INFO - __main__ - Step 30 Global step 30 Train loss 17.680563 on epoch=3
05/31/2022 11:22:23 - INFO - __main__ - Step 40 Global step 40 Train loss 17.945312 on epoch=4
05/31/2022 11:22:28 - INFO - __main__ - Step 50 Global step 50 Train loss 16.121792 on epoch=6
05/31/2022 11:22:47 - INFO - __main__ - Global step 50 Train loss 19.368048 Classification-F1 0.0 on epoch=6
05/31/2022 11:22:53 - INFO - __main__ - Step 60 Global step 60 Train loss 15.786011 on epoch=7
05/31/2022 11:22:58 - INFO - __main__ - Step 70 Global step 70 Train loss 14.878119 on epoch=8
05/31/2022 11:23:03 - INFO - __main__ - Step 80 Global step 80 Train loss 14.565306 on epoch=9
05/31/2022 11:23:08 - INFO - __main__ - Step 90 Global step 90 Train loss 14.053757 on epoch=11
05/31/2022 11:23:13 - INFO - __main__ - Step 100 Global step 100 Train loss 12.540741 on epoch=12
05/31/2022 11:23:17 - INFO - __main__ - Global step 100 Train loss 14.364786 Classification-F1 0.0 on epoch=12
05/31/2022 11:23:22 - INFO - __main__ - Step 110 Global step 110 Train loss 13.060278 on epoch=13
05/31/2022 11:23:27 - INFO - __main__ - Step 120 Global step 120 Train loss 12.128159 on epoch=14
05/31/2022 11:23:32 - INFO - __main__ - Step 130 Global step 130 Train loss 11.509321 on epoch=16
05/31/2022 11:23:37 - INFO - __main__ - Step 140 Global step 140 Train loss 11.018619 on epoch=17
05/31/2022 11:23:43 - INFO - __main__ - Step 150 Global step 150 Train loss 9.430399 on epoch=18
05/31/2022 11:23:45 - INFO - __main__ - Global step 150 Train loss 11.429356 Classification-F1 0.0 on epoch=18
05/31/2022 11:23:50 - INFO - __main__ - Step 160 Global step 160 Train loss 8.310726 on epoch=19
05/31/2022 11:23:54 - INFO - __main__ - Step 170 Global step 170 Train loss 5.583582 on epoch=21
05/31/2022 11:23:59 - INFO - __main__ - Step 180 Global step 180 Train loss 3.240654 on epoch=22
05/31/2022 11:24:04 - INFO - __main__ - Step 190 Global step 190 Train loss 1.631407 on epoch=23
05/31/2022 11:24:09 - INFO - __main__ - Step 200 Global step 200 Train loss 1.893930 on epoch=24
05/31/2022 11:24:10 - INFO - __main__ - Global step 200 Train loss 4.132060 Classification-F1 0.2900142644026667 on epoch=24
05/31/2022 11:24:16 - INFO - __main__ - Step 210 Global step 210 Train loss 1.604201 on epoch=26
05/31/2022 11:24:21 - INFO - __main__ - Step 220 Global step 220 Train loss 1.121791 on epoch=27
05/31/2022 11:24:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.927208 on epoch=28
05/31/2022 11:24:31 - INFO - __main__ - Step 240 Global step 240 Train loss 1.106553 on epoch=29
05/31/2022 11:24:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.489239 on epoch=31
05/31/2022 11:24:37 - INFO - __main__ - Global step 250 Train loss 1.049799 Classification-F1 0.5941308115726721 on epoch=31
05/31/2022 11:24:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.451728 on epoch=32
05/31/2022 11:24:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.385903 on epoch=33
05/31/2022 11:24:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.545564 on epoch=34
05/31/2022 11:24:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.434883 on epoch=36
05/31/2022 11:25:03 - INFO - __main__ - Step 300 Global step 300 Train loss 1.713232 on epoch=37
05/31/2022 11:25:04 - INFO - __main__ - Global step 300 Train loss 0.706262 Classification-F1 0.6575994972769167 on epoch=37
05/31/2022 11:25:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.618681 on epoch=38
05/31/2022 11:25:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.620204 on epoch=39
05/31/2022 11:25:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.482235 on epoch=41
05/31/2022 11:25:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.434713 on epoch=42
05/31/2022 11:25:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.296031 on epoch=43
05/31/2022 11:25:30 - INFO - __main__ - Global step 350 Train loss 0.490373 Classification-F1 0.6584473497516975 on epoch=43
05/31/2022 11:25:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.211515 on epoch=44
05/31/2022 11:25:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.202332 on epoch=46
05/31/2022 11:25:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.154023 on epoch=47
05/31/2022 11:25:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.204467 on epoch=48
05/31/2022 11:25:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.147380 on epoch=49
05/31/2022 11:25:57 - INFO - __main__ - Global step 400 Train loss 0.183943 Classification-F1 0.6835650274743834 on epoch=49
05/31/2022 11:26:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.126571 on epoch=51
05/31/2022 11:26:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.153532 on epoch=52
05/31/2022 11:26:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.107645 on epoch=53
05/31/2022 11:26:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.112143 on epoch=54
05/31/2022 11:26:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.149643 on epoch=56
05/31/2022 11:26:24 - INFO - __main__ - Global step 450 Train loss 0.129907 Classification-F1 0.5116642030610157 on epoch=56
05/31/2022 11:26:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.072038 on epoch=57
05/31/2022 11:26:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.124701 on epoch=58
05/31/2022 11:26:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.278050 on epoch=59
05/31/2022 11:26:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.048937 on epoch=61
05/31/2022 11:26:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.040603 on epoch=62
05/31/2022 11:26:50 - INFO - __main__ - Global step 500 Train loss 0.112866 Classification-F1 0.5847937411095306 on epoch=62
05/31/2022 11:26:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.077607 on epoch=63
05/31/2022 11:27:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.114097 on epoch=64
05/31/2022 11:27:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.038650 on epoch=66
05/31/2022 11:27:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.043900 on epoch=67
05/31/2022 11:27:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.023840 on epoch=68
05/31/2022 11:27:16 - INFO - __main__ - Global step 550 Train loss 0.059619 Classification-F1 0.5480096639904847 on epoch=68
05/31/2022 11:27:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.035136 on epoch=69
05/31/2022 11:27:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.042854 on epoch=71
05/31/2022 11:27:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.046014 on epoch=72
05/31/2022 11:27:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.040858 on epoch=73
05/31/2022 11:27:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.006814 on epoch=74
05/31/2022 11:27:42 - INFO - __main__ - Global step 600 Train loss 0.034335 Classification-F1 0.48692899140660334 on epoch=74
05/31/2022 11:27:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.031026 on epoch=76
05/31/2022 11:27:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.042327 on epoch=77
05/31/2022 11:27:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.027734 on epoch=78
05/31/2022 11:28:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.100748 on epoch=79
05/31/2022 11:28:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.038167 on epoch=81
05/31/2022 11:28:08 - INFO - __main__ - Global step 650 Train loss 0.048000 Classification-F1 0.5604761639666722 on epoch=81
05/31/2022 11:28:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.011613 on epoch=82
05/31/2022 11:28:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.022410 on epoch=83
05/31/2022 11:28:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.002069 on epoch=84
05/31/2022 11:28:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.026222 on epoch=86
05/31/2022 11:28:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.033401 on epoch=87
05/31/2022 11:28:34 - INFO - __main__ - Global step 700 Train loss 0.019143 Classification-F1 0.5811789497155238 on epoch=87
05/31/2022 11:28:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.009243 on epoch=88
05/31/2022 11:28:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.001107 on epoch=89
05/31/2022 11:28:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.005309 on epoch=91
05/31/2022 11:28:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.006486 on epoch=92
05/31/2022 11:28:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.019505 on epoch=93
05/31/2022 11:29:00 - INFO - __main__ - Global step 750 Train loss 0.008330 Classification-F1 0.5743525480367586 on epoch=93
05/31/2022 11:29:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000477 on epoch=94
05/31/2022 11:29:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.002352 on epoch=96
05/31/2022 11:29:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.011720 on epoch=97
05/31/2022 11:29:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.008044 on epoch=98
05/31/2022 11:29:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.004629 on epoch=99
05/31/2022 11:29:26 - INFO - __main__ - Global step 800 Train loss 0.005444 Classification-F1 0.4818711066516159 on epoch=99
05/31/2022 11:29:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001078 on epoch=101
05/31/2022 11:29:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001992 on epoch=102
05/31/2022 11:29:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.009772 on epoch=103
05/31/2022 11:29:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.002916 on epoch=104
05/31/2022 11:29:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.038856 on epoch=106
05/31/2022 11:29:53 - INFO - __main__ - Global step 850 Train loss 0.010923 Classification-F1 0.5395519100782259 on epoch=106
05/31/2022 11:29:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.065015 on epoch=107
05/31/2022 11:30:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003876 on epoch=108
05/31/2022 11:30:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.017561 on epoch=109
05/31/2022 11:30:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.014367 on epoch=111
05/31/2022 11:30:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.036426 on epoch=112
05/31/2022 11:30:19 - INFO - __main__ - Global step 900 Train loss 0.027449 Classification-F1 0.5538342066284057 on epoch=112
05/31/2022 11:30:24 - INFO - __main__ - Step 910 Global step 910 Train loss 1.077389 on epoch=113
05/31/2022 11:30:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.260665 on epoch=114
05/31/2022 11:30:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.011232 on epoch=116
05/31/2022 11:30:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.023253 on epoch=117
05/31/2022 11:30:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.006588 on epoch=118
05/31/2022 11:30:45 - INFO - __main__ - Global step 950 Train loss 0.275826 Classification-F1 0.4931644026471613 on epoch=118
05/31/2022 11:30:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.065588 on epoch=119
05/31/2022 11:30:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.091484 on epoch=121
05/31/2022 11:31:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.155665 on epoch=122
05/31/2022 11:31:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.203234 on epoch=123
05/31/2022 11:31:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.015702 on epoch=124
05/31/2022 11:31:11 - INFO - __main__ - Global step 1000 Train loss 0.106335 Classification-F1 0.48932634829881455 on epoch=124
05/31/2022 11:31:11 - INFO - __main__ - save last model!
05/31/2022 11:31:11 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:31:11 - INFO - __main__ - Printing 3 examples
05/31/2022 11:31:11 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/31/2022 11:31:11 - INFO - __main__ - ['sad']
05/31/2022 11:31:11 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/31/2022 11:31:11 - INFO - __main__ - ['sad']
05/31/2022 11:31:11 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/31/2022 11:31:11 - INFO - __main__ - ['sad']
05/31/2022 11:31:11 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:31:11 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:31:11 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:31:11 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:31:11 - INFO - __main__ - Printing 3 examples
05/31/2022 11:31:11 - INFO - __main__ -  [emo] i am not ok why  down with fever
05/31/2022 11:31:11 - INFO - __main__ - ['sad']
05/31/2022 11:31:11 - INFO - __main__ -  [emo] i'm disappointed such people stink that's rude to generalise
05/31/2022 11:31:11 - INFO - __main__ - ['sad']
05/31/2022 11:31:11 - INFO - __main__ -  [emo] o go to hell how are u  miserable
05/31/2022 11:31:11 - INFO - __main__ - ['sad']
05/31/2022 11:31:11 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:31:11 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:31:11 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:31:18 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 11:31:18 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 11:31:18 - INFO - __main__ - Printing 3 examples
05/31/2022 11:31:18 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 11:31:18 - INFO - __main__ - ['others']
05/31/2022 11:31:18 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 11:31:18 - INFO - __main__ - ['others']
05/31/2022 11:31:18 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 11:31:18 - INFO - __main__ - ['others']
05/31/2022 11:31:18 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:31:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:31:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:31:22 - INFO - __main__ - Starting training!
05/31/2022 11:31:26 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 11:32:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_21_0.0002_8_predictions.txt
05/31/2022 11:32:11 - INFO - __main__ - Classification-F1 on test data: 0.0992
05/31/2022 11:32:11 - INFO - __main__ - prefix=emo_32_21, lr=0.0002, bsz=8, dev_performance=0.6835650274743834, test_performance=0.09921950992909437
05/31/2022 11:32:11 - INFO - __main__ - Running ... prefix=emo_32_21, lr=0.0001, bsz=8 ...
05/31/2022 11:32:12 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:32:12 - INFO - __main__ - Printing 3 examples
05/31/2022 11:32:12 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/31/2022 11:32:12 - INFO - __main__ - ['sad']
05/31/2022 11:32:12 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/31/2022 11:32:12 - INFO - __main__ - ['sad']
05/31/2022 11:32:12 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/31/2022 11:32:12 - INFO - __main__ - ['sad']
05/31/2022 11:32:12 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:32:12 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:32:12 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:32:12 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:32:12 - INFO - __main__ - Printing 3 examples
05/31/2022 11:32:12 - INFO - __main__ -  [emo] i am not ok why  down with fever
05/31/2022 11:32:12 - INFO - __main__ - ['sad']
05/31/2022 11:32:12 - INFO - __main__ -  [emo] i'm disappointed such people stink that's rude to generalise
05/31/2022 11:32:12 - INFO - __main__ - ['sad']
05/31/2022 11:32:12 - INFO - __main__ -  [emo] o go to hell how are u  miserable
05/31/2022 11:32:12 - INFO - __main__ - ['sad']
05/31/2022 11:32:12 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:32:12 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:32:13 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:32:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:32:25 - INFO - __main__ - Starting training!
05/31/2022 11:32:30 - INFO - __main__ - Step 10 Global step 10 Train loss 25.043558 on epoch=1
05/31/2022 11:32:35 - INFO - __main__ - Step 20 Global step 20 Train loss 21.742880 on epoch=2
05/31/2022 11:32:40 - INFO - __main__ - Step 30 Global step 30 Train loss 19.931547 on epoch=3
05/31/2022 11:32:45 - INFO - __main__ - Step 40 Global step 40 Train loss 18.295204 on epoch=4
05/31/2022 11:32:50 - INFO - __main__ - Step 50 Global step 50 Train loss 18.321325 on epoch=6
05/31/2022 11:33:29 - INFO - __main__ - Global step 50 Train loss 20.666903 Classification-F1 0.0 on epoch=6
05/31/2022 11:33:34 - INFO - __main__ - Step 60 Global step 60 Train loss 17.047638 on epoch=7
05/31/2022 11:33:40 - INFO - __main__ - Step 70 Global step 70 Train loss 17.496580 on epoch=8
05/31/2022 11:33:45 - INFO - __main__ - Step 80 Global step 80 Train loss 17.259119 on epoch=9
05/31/2022 11:33:50 - INFO - __main__ - Step 90 Global step 90 Train loss 15.439829 on epoch=11
05/31/2022 11:33:55 - INFO - __main__ - Step 100 Global step 100 Train loss 16.435879 on epoch=12
05/31/2022 11:34:22 - INFO - __main__ - Global step 100 Train loss 16.735807 Classification-F1 0.0 on epoch=12
05/31/2022 11:34:27 - INFO - __main__ - Step 110 Global step 110 Train loss 16.954216 on epoch=13
05/31/2022 11:34:33 - INFO - __main__ - Step 120 Global step 120 Train loss 15.492505 on epoch=14
05/31/2022 11:34:38 - INFO - __main__ - Step 130 Global step 130 Train loss 15.549089 on epoch=16
05/31/2022 11:34:43 - INFO - __main__ - Step 140 Global step 140 Train loss 14.502461 on epoch=17
05/31/2022 11:34:48 - INFO - __main__ - Step 150 Global step 150 Train loss 14.693563 on epoch=18
05/31/2022 11:34:59 - INFO - __main__ - Global step 150 Train loss 15.438367 Classification-F1 0.0 on epoch=18
05/31/2022 11:35:04 - INFO - __main__ - Step 160 Global step 160 Train loss 14.488951 on epoch=19
05/31/2022 11:35:09 - INFO - __main__ - Step 170 Global step 170 Train loss 13.714478 on epoch=21
05/31/2022 11:35:14 - INFO - __main__ - Step 180 Global step 180 Train loss 13.322496 on epoch=22
05/31/2022 11:35:19 - INFO - __main__ - Step 190 Global step 190 Train loss 14.258591 on epoch=23
05/31/2022 11:35:24 - INFO - __main__ - Step 200 Global step 200 Train loss 13.479902 on epoch=24
05/31/2022 11:35:33 - INFO - __main__ - Global step 200 Train loss 13.852883 Classification-F1 0.0 on epoch=24
05/31/2022 11:35:38 - INFO - __main__ - Step 210 Global step 210 Train loss 13.099055 on epoch=26
05/31/2022 11:35:43 - INFO - __main__ - Step 220 Global step 220 Train loss 12.381842 on epoch=27
05/31/2022 11:35:48 - INFO - __main__ - Step 230 Global step 230 Train loss 11.501524 on epoch=28
05/31/2022 11:35:53 - INFO - __main__ - Step 240 Global step 240 Train loss 11.891547 on epoch=29
05/31/2022 11:35:58 - INFO - __main__ - Step 250 Global step 250 Train loss 11.384713 on epoch=31
05/31/2022 11:36:04 - INFO - __main__ - Global step 250 Train loss 12.051735 Classification-F1 0.0 on epoch=31
05/31/2022 11:36:09 - INFO - __main__ - Step 260 Global step 260 Train loss 12.107339 on epoch=32
05/31/2022 11:36:14 - INFO - __main__ - Step 270 Global step 270 Train loss 10.778469 on epoch=33
05/31/2022 11:36:19 - INFO - __main__ - Step 280 Global step 280 Train loss 11.166739 on epoch=34
05/31/2022 11:36:24 - INFO - __main__ - Step 290 Global step 290 Train loss 10.363730 on epoch=36
05/31/2022 11:36:29 - INFO - __main__ - Step 300 Global step 300 Train loss 9.999146 on epoch=37
05/31/2022 11:36:32 - INFO - __main__ - Global step 300 Train loss 10.883083 Classification-F1 0.0 on epoch=37
05/31/2022 11:36:37 - INFO - __main__ - Step 310 Global step 310 Train loss 10.190273 on epoch=38
05/31/2022 11:36:42 - INFO - __main__ - Step 320 Global step 320 Train loss 8.853288 on epoch=39
05/31/2022 11:36:47 - INFO - __main__ - Step 330 Global step 330 Train loss 8.192739 on epoch=41
05/31/2022 11:36:53 - INFO - __main__ - Step 340 Global step 340 Train loss 7.970252 on epoch=42
05/31/2022 11:36:58 - INFO - __main__ - Step 350 Global step 350 Train loss 7.559815 on epoch=43
05/31/2022 11:37:00 - INFO - __main__ - Global step 350 Train loss 8.553273 Classification-F1 0.0 on epoch=43
05/31/2022 11:37:05 - INFO - __main__ - Step 360 Global step 360 Train loss 6.903345 on epoch=44
05/31/2022 11:37:10 - INFO - __main__ - Step 370 Global step 370 Train loss 5.474428 on epoch=46
05/31/2022 11:37:15 - INFO - __main__ - Step 380 Global step 380 Train loss 6.102799 on epoch=47
05/31/2022 11:37:21 - INFO - __main__ - Step 390 Global step 390 Train loss 4.722120 on epoch=48
05/31/2022 11:37:26 - INFO - __main__ - Step 400 Global step 400 Train loss 4.487794 on epoch=49
05/31/2022 11:37:27 - INFO - __main__ - Global step 400 Train loss 5.538097 Classification-F1 0.009523809523809523 on epoch=49
05/31/2022 11:37:33 - INFO - __main__ - Step 410 Global step 410 Train loss 3.833703 on epoch=51
05/31/2022 11:37:38 - INFO - __main__ - Step 420 Global step 420 Train loss 4.018860 on epoch=52
05/31/2022 11:37:43 - INFO - __main__ - Step 430 Global step 430 Train loss 3.906170 on epoch=53
05/31/2022 11:37:48 - INFO - __main__ - Step 440 Global step 440 Train loss 3.639036 on epoch=54
05/31/2022 11:37:54 - INFO - __main__ - Step 450 Global step 450 Train loss 3.305863 on epoch=56
05/31/2022 11:37:55 - INFO - __main__ - Global step 450 Train loss 3.740726 Classification-F1 0.1159717051377513 on epoch=56
05/31/2022 11:38:00 - INFO - __main__ - Step 460 Global step 460 Train loss 3.095244 on epoch=57
05/31/2022 11:38:06 - INFO - __main__ - Step 470 Global step 470 Train loss 3.101121 on epoch=58
05/31/2022 11:38:11 - INFO - __main__ - Step 480 Global step 480 Train loss 3.094831 on epoch=59
05/31/2022 11:38:16 - INFO - __main__ - Step 490 Global step 490 Train loss 2.792736 on epoch=61
05/31/2022 11:38:21 - INFO - __main__ - Step 500 Global step 500 Train loss 2.595557 on epoch=62
05/31/2022 11:38:22 - INFO - __main__ - Global step 500 Train loss 2.935898 Classification-F1 0.285338185691461 on epoch=62
05/31/2022 11:38:28 - INFO - __main__ - Step 510 Global step 510 Train loss 3.135514 on epoch=63
05/31/2022 11:38:33 - INFO - __main__ - Step 520 Global step 520 Train loss 3.167488 on epoch=64
05/31/2022 11:38:38 - INFO - __main__ - Step 530 Global step 530 Train loss 3.528849 on epoch=66
05/31/2022 11:38:43 - INFO - __main__ - Step 540 Global step 540 Train loss 2.375120 on epoch=67
05/31/2022 11:38:48 - INFO - __main__ - Step 550 Global step 550 Train loss 2.924580 on epoch=68
05/31/2022 11:38:49 - INFO - __main__ - Global step 550 Train loss 3.026310 Classification-F1 0.3497872017023864 on epoch=68
05/31/2022 11:38:55 - INFO - __main__ - Step 560 Global step 560 Train loss 3.460784 on epoch=69
05/31/2022 11:39:00 - INFO - __main__ - Step 570 Global step 570 Train loss 3.273819 on epoch=71
05/31/2022 11:39:05 - INFO - __main__ - Step 580 Global step 580 Train loss 2.633376 on epoch=72
05/31/2022 11:39:10 - INFO - __main__ - Step 590 Global step 590 Train loss 3.297522 on epoch=73
05/31/2022 11:39:15 - INFO - __main__ - Step 600 Global step 600 Train loss 2.270111 on epoch=74
05/31/2022 11:39:16 - INFO - __main__ - Global step 600 Train loss 2.987123 Classification-F1 0.1 on epoch=74
05/31/2022 11:39:21 - INFO - __main__ - Step 610 Global step 610 Train loss 2.896324 on epoch=76
05/31/2022 11:39:26 - INFO - __main__ - Step 620 Global step 620 Train loss 2.827063 on epoch=77
05/31/2022 11:39:32 - INFO - __main__ - Step 630 Global step 630 Train loss 3.353946 on epoch=78
05/31/2022 11:39:37 - INFO - __main__ - Step 640 Global step 640 Train loss 3.289397 on epoch=79
05/31/2022 11:39:42 - INFO - __main__ - Step 650 Global step 650 Train loss 2.896651 on epoch=81
05/31/2022 11:39:43 - INFO - __main__ - Global step 650 Train loss 3.052676 Classification-F1 0.16103896103896104 on epoch=81
05/31/2022 11:39:48 - INFO - __main__ - Step 660 Global step 660 Train loss 2.549363 on epoch=82
05/31/2022 11:39:53 - INFO - __main__ - Step 670 Global step 670 Train loss 1.977654 on epoch=83
05/31/2022 11:39:58 - INFO - __main__ - Step 680 Global step 680 Train loss 2.699554 on epoch=84
05/31/2022 11:40:03 - INFO - __main__ - Step 690 Global step 690 Train loss 2.430849 on epoch=86
05/31/2022 11:40:08 - INFO - __main__ - Step 700 Global step 700 Train loss 2.504666 on epoch=87
05/31/2022 11:40:09 - INFO - __main__ - Global step 700 Train loss 2.432417 Classification-F1 0.1 on epoch=87
05/31/2022 11:40:14 - INFO - __main__ - Step 710 Global step 710 Train loss 2.915028 on epoch=88
05/31/2022 11:40:19 - INFO - __main__ - Step 720 Global step 720 Train loss 2.471048 on epoch=89
05/31/2022 11:40:24 - INFO - __main__ - Step 730 Global step 730 Train loss 2.074062 on epoch=91
05/31/2022 11:40:29 - INFO - __main__ - Step 740 Global step 740 Train loss 2.021868 on epoch=92
05/31/2022 11:40:34 - INFO - __main__ - Step 750 Global step 750 Train loss 2.452961 on epoch=93
05/31/2022 11:40:35 - INFO - __main__ - Global step 750 Train loss 2.386993 Classification-F1 0.16057276057276054 on epoch=93
05/31/2022 11:40:40 - INFO - __main__ - Step 760 Global step 760 Train loss 2.892168 on epoch=94
05/31/2022 11:40:46 - INFO - __main__ - Step 770 Global step 770 Train loss 2.715934 on epoch=96
05/31/2022 11:40:51 - INFO - __main__ - Step 780 Global step 780 Train loss 1.995785 on epoch=97
05/31/2022 11:40:56 - INFO - __main__ - Step 790 Global step 790 Train loss 1.822416 on epoch=98
05/31/2022 11:41:01 - INFO - __main__ - Step 800 Global step 800 Train loss 1.823848 on epoch=99
05/31/2022 11:41:02 - INFO - __main__ - Global step 800 Train loss 2.250030 Classification-F1 0.19417293233082708 on epoch=99
05/31/2022 11:41:07 - INFO - __main__ - Step 810 Global step 810 Train loss 1.999809 on epoch=101
05/31/2022 11:41:12 - INFO - __main__ - Step 820 Global step 820 Train loss 1.917163 on epoch=102
05/31/2022 11:41:17 - INFO - __main__ - Step 830 Global step 830 Train loss 2.366310 on epoch=103
05/31/2022 11:41:23 - INFO - __main__ - Step 840 Global step 840 Train loss 1.700866 on epoch=104
05/31/2022 11:41:28 - INFO - __main__ - Step 850 Global step 850 Train loss 2.099648 on epoch=106
05/31/2022 11:41:29 - INFO - __main__ - Global step 850 Train loss 2.016759 Classification-F1 0.1 on epoch=106
05/31/2022 11:41:34 - INFO - __main__ - Step 860 Global step 860 Train loss 1.991363 on epoch=107
05/31/2022 11:41:39 - INFO - __main__ - Step 870 Global step 870 Train loss 1.866671 on epoch=108
05/31/2022 11:41:44 - INFO - __main__ - Step 880 Global step 880 Train loss 1.680086 on epoch=109
05/31/2022 11:41:49 - INFO - __main__ - Step 890 Global step 890 Train loss 1.980107 on epoch=111
05/31/2022 11:41:54 - INFO - __main__ - Step 900 Global step 900 Train loss 1.466028 on epoch=112
05/31/2022 11:41:55 - INFO - __main__ - Global step 900 Train loss 1.796851 Classification-F1 0.13067758749069247 on epoch=112
05/31/2022 11:42:00 - INFO - __main__ - Step 910 Global step 910 Train loss 1.633065 on epoch=113
05/31/2022 11:42:05 - INFO - __main__ - Step 920 Global step 920 Train loss 2.114905 on epoch=114
05/31/2022 11:42:11 - INFO - __main__ - Step 930 Global step 930 Train loss 1.580123 on epoch=116
05/31/2022 11:42:16 - INFO - __main__ - Step 940 Global step 940 Train loss 1.536056 on epoch=117
05/31/2022 11:42:21 - INFO - __main__ - Step 950 Global step 950 Train loss 1.640054 on epoch=118
05/31/2022 11:42:22 - INFO - __main__ - Global step 950 Train loss 1.700841 Classification-F1 0.21380952380952384 on epoch=118
05/31/2022 11:42:27 - INFO - __main__ - Step 960 Global step 960 Train loss 1.455293 on epoch=119
05/31/2022 11:42:32 - INFO - __main__ - Step 970 Global step 970 Train loss 1.388565 on epoch=121
05/31/2022 11:42:37 - INFO - __main__ - Step 980 Global step 980 Train loss 1.478181 on epoch=122
05/31/2022 11:42:42 - INFO - __main__ - Step 990 Global step 990 Train loss 1.513942 on epoch=123
05/31/2022 11:42:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.277550 on epoch=124
05/31/2022 11:42:48 - INFO - __main__ - Global step 1000 Train loss 1.422706 Classification-F1 0.21937915742793793 on epoch=124
05/31/2022 11:42:48 - INFO - __main__ - save last model!
05/31/2022 11:42:48 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:42:48 - INFO - __main__ - Printing 3 examples
05/31/2022 11:42:48 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/31/2022 11:42:48 - INFO - __main__ - ['happy']
05/31/2022 11:42:48 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/31/2022 11:42:48 - INFO - __main__ - ['happy']
05/31/2022 11:42:48 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/31/2022 11:42:48 - INFO - __main__ - ['happy']
05/31/2022 11:42:48 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:42:49 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:42:49 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:42:49 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:42:49 - INFO - __main__ - Printing 3 examples
05/31/2022 11:42:49 - INFO - __main__ -  [emo] can i get yes if i'm allowed to get a tiger so funny
05/31/2022 11:42:49 - INFO - __main__ - ['happy']
05/31/2022 11:42:49 - INFO - __main__ -  [emo] haha facewithtearsofjoy of course i wont leave you alone what do you think of me facewithtearsofjoy are u a grl
05/31/2022 11:42:49 - INFO - __main__ - ['happy']
05/31/2022 11:42:49 - INFO - __main__ -  [emo] awesome thank you but for what todays my best friend's birthday and we are enjoying the precious  day
05/31/2022 11:42:49 - INFO - __main__ - ['happy']
05/31/2022 11:42:49 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:42:49 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:42:49 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:42:56 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 11:42:56 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 11:42:56 - INFO - __main__ - Printing 3 examples
05/31/2022 11:42:56 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 11:42:56 - INFO - __main__ - ['others']
05/31/2022 11:42:56 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 11:42:56 - INFO - __main__ - ['others']
05/31/2022 11:42:56 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 11:42:56 - INFO - __main__ - ['others']
05/31/2022 11:42:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:42:59 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:43:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:43:02 - INFO - __main__ - Starting training!
05/31/2022 11:43:04 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 11:43:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_21_0.0001_8_predictions.txt
05/31/2022 11:43:47 - INFO - __main__ - Classification-F1 on test data: 0.1108
05/31/2022 11:43:47 - INFO - __main__ - prefix=emo_32_21, lr=0.0001, bsz=8, dev_performance=0.3497872017023864, test_performance=0.1107939547344658
05/31/2022 11:43:47 - INFO - __main__ - Running ... prefix=emo_32_42, lr=0.0005, bsz=8 ...
05/31/2022 11:43:48 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:43:48 - INFO - __main__ - Printing 3 examples
05/31/2022 11:43:48 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/31/2022 11:43:48 - INFO - __main__ - ['happy']
05/31/2022 11:43:48 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/31/2022 11:43:48 - INFO - __main__ - ['happy']
05/31/2022 11:43:48 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/31/2022 11:43:48 - INFO - __main__ - ['happy']
05/31/2022 11:43:48 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:43:48 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:43:48 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:43:48 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:43:48 - INFO - __main__ - Printing 3 examples
05/31/2022 11:43:48 - INFO - __main__ -  [emo] can i get yes if i'm allowed to get a tiger so funny
05/31/2022 11:43:48 - INFO - __main__ - ['happy']
05/31/2022 11:43:48 - INFO - __main__ -  [emo] haha facewithtearsofjoy of course i wont leave you alone what do you think of me facewithtearsofjoy are u a grl
05/31/2022 11:43:48 - INFO - __main__ - ['happy']
05/31/2022 11:43:48 - INFO - __main__ -  [emo] awesome thank you but for what todays my best friend's birthday and we are enjoying the precious  day
05/31/2022 11:43:48 - INFO - __main__ - ['happy']
05/31/2022 11:43:48 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:43:48 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:43:49 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:43:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:43:59 - INFO - __main__ - Starting training!
05/31/2022 11:44:04 - INFO - __main__ - Step 10 Global step 10 Train loss 24.019865 on epoch=1
05/31/2022 11:44:09 - INFO - __main__ - Step 20 Global step 20 Train loss 18.965214 on epoch=2
05/31/2022 11:44:14 - INFO - __main__ - Step 30 Global step 30 Train loss 16.372240 on epoch=3
05/31/2022 11:44:19 - INFO - __main__ - Step 40 Global step 40 Train loss 15.260000 on epoch=4
05/31/2022 11:44:24 - INFO - __main__ - Step 50 Global step 50 Train loss 13.297615 on epoch=6
05/31/2022 11:44:25 - INFO - __main__ - Global step 50 Train loss 17.582987 Classification-F1 0.0 on epoch=6
05/31/2022 11:44:31 - INFO - __main__ - Step 60 Global step 60 Train loss 11.867006 on epoch=7
05/31/2022 11:44:36 - INFO - __main__ - Step 70 Global step 70 Train loss 10.033297 on epoch=8
05/31/2022 11:44:41 - INFO - __main__ - Step 80 Global step 80 Train loss 7.076464 on epoch=9
05/31/2022 11:44:46 - INFO - __main__ - Step 90 Global step 90 Train loss 4.785867 on epoch=11
05/31/2022 11:44:52 - INFO - __main__ - Step 100 Global step 100 Train loss 3.448493 on epoch=12
05/31/2022 11:44:53 - INFO - __main__ - Global step 100 Train loss 7.442225 Classification-F1 0.11578044596912522 on epoch=12
05/31/2022 11:44:58 - INFO - __main__ - Step 110 Global step 110 Train loss 4.134362 on epoch=13
05/31/2022 11:45:03 - INFO - __main__ - Step 120 Global step 120 Train loss 3.427102 on epoch=14
05/31/2022 11:45:08 - INFO - __main__ - Step 130 Global step 130 Train loss 3.081521 on epoch=16
05/31/2022 11:45:14 - INFO - __main__ - Step 140 Global step 140 Train loss 2.706334 on epoch=17
05/31/2022 11:45:19 - INFO - __main__ - Step 150 Global step 150 Train loss 2.760258 on epoch=18
05/31/2022 11:45:20 - INFO - __main__ - Global step 150 Train loss 3.221915 Classification-F1 0.13067758749069247 on epoch=18
05/31/2022 11:45:26 - INFO - __main__ - Step 160 Global step 160 Train loss 2.249660 on epoch=19
05/31/2022 11:45:31 - INFO - __main__ - Step 170 Global step 170 Train loss 2.365413 on epoch=21
05/31/2022 11:45:36 - INFO - __main__ - Step 180 Global step 180 Train loss 2.086656 on epoch=22
05/31/2022 11:45:41 - INFO - __main__ - Step 190 Global step 190 Train loss 2.854196 on epoch=23
05/31/2022 11:45:46 - INFO - __main__ - Step 200 Global step 200 Train loss 2.001676 on epoch=24
05/31/2022 11:45:47 - INFO - __main__ - Global step 200 Train loss 2.311520 Classification-F1 0.1 on epoch=24
05/31/2022 11:45:52 - INFO - __main__ - Step 210 Global step 210 Train loss 1.505376 on epoch=26
05/31/2022 11:45:57 - INFO - __main__ - Step 220 Global step 220 Train loss 1.465015 on epoch=27
05/31/2022 11:46:03 - INFO - __main__ - Step 230 Global step 230 Train loss 1.225852 on epoch=28
05/31/2022 11:46:08 - INFO - __main__ - Step 240 Global step 240 Train loss 1.282272 on epoch=29
05/31/2022 11:46:13 - INFO - __main__ - Step 250 Global step 250 Train loss 1.082798 on epoch=31
05/31/2022 11:46:14 - INFO - __main__ - Global step 250 Train loss 1.312263 Classification-F1 0.09748427672955975 on epoch=31
05/31/2022 11:46:19 - INFO - __main__ - Step 260 Global step 260 Train loss 1.249763 on epoch=32
05/31/2022 11:46:24 - INFO - __main__ - Step 270 Global step 270 Train loss 1.144436 on epoch=33
05/31/2022 11:46:29 - INFO - __main__ - Step 280 Global step 280 Train loss 1.148111 on epoch=34
05/31/2022 11:46:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.967748 on epoch=36
05/31/2022 11:46:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.793635 on epoch=37
05/31/2022 11:46:40 - INFO - __main__ - Global step 300 Train loss 1.060739 Classification-F1 0.1 on epoch=37
05/31/2022 11:46:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.817950 on epoch=38
05/31/2022 11:46:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.875572 on epoch=39
05/31/2022 11:46:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.981283 on epoch=41
05/31/2022 11:47:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.902251 on epoch=42
05/31/2022 11:47:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.919858 on epoch=43
05/31/2022 11:47:07 - INFO - __main__ - Global step 350 Train loss 0.899383 Classification-F1 0.1609203203474956 on epoch=43
05/31/2022 11:47:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.963158 on epoch=44
05/31/2022 11:47:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.836235 on epoch=46
05/31/2022 11:47:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.847192 on epoch=47
05/31/2022 11:47:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.783011 on epoch=48
05/31/2022 11:47:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.788895 on epoch=49
05/31/2022 11:47:34 - INFO - __main__ - Global step 400 Train loss 0.843698 Classification-F1 0.1 on epoch=49
05/31/2022 11:47:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.801523 on epoch=51
05/31/2022 11:47:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.785030 on epoch=52
05/31/2022 11:47:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.748830 on epoch=53
05/31/2022 11:47:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.775992 on epoch=54
05/31/2022 11:47:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.769617 on epoch=56
05/31/2022 11:48:00 - INFO - __main__ - Global step 450 Train loss 0.776198 Classification-F1 0.1 on epoch=56
05/31/2022 11:48:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.742251 on epoch=57
05/31/2022 11:48:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.742823 on epoch=58
05/31/2022 11:48:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.754200 on epoch=59
05/31/2022 11:48:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.763601 on epoch=61
05/31/2022 11:48:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.793414 on epoch=62
05/31/2022 11:48:26 - INFO - __main__ - Global step 500 Train loss 0.759258 Classification-F1 0.24141733793632525 on epoch=62
05/31/2022 11:48:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.694885 on epoch=63
05/31/2022 11:48:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.711712 on epoch=64
05/31/2022 11:48:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.724189 on epoch=66
05/31/2022 11:48:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.734354 on epoch=67
05/31/2022 11:48:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.669080 on epoch=68
05/31/2022 11:48:53 - INFO - __main__ - Global step 550 Train loss 0.706844 Classification-F1 0.3704262966105925 on epoch=68
05/31/2022 11:48:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.628527 on epoch=69
05/31/2022 11:49:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.690884 on epoch=71
05/31/2022 11:49:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.649706 on epoch=72
05/31/2022 11:49:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.622521 on epoch=73
05/31/2022 11:49:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.692461 on epoch=74
05/31/2022 11:49:20 - INFO - __main__ - Global step 600 Train loss 0.656820 Classification-F1 0.21260621580724015 on epoch=74
05/31/2022 11:49:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.567515 on epoch=76
05/31/2022 11:49:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.569094 on epoch=77
05/31/2022 11:49:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.510498 on epoch=78
05/31/2022 11:49:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.487335 on epoch=79
05/31/2022 11:49:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.458052 on epoch=81
05/31/2022 11:49:46 - INFO - __main__ - Global step 650 Train loss 0.518499 Classification-F1 0.4386874041224438 on epoch=81
05/31/2022 11:49:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.415469 on epoch=82
05/31/2022 11:49:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.303051 on epoch=83
05/31/2022 11:50:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.316044 on epoch=84
05/31/2022 11:50:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.256411 on epoch=86
05/31/2022 11:50:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.226828 on epoch=87
05/31/2022 11:50:13 - INFO - __main__ - Global step 700 Train loss 0.303561 Classification-F1 0.5829146902877766 on epoch=87
05/31/2022 11:50:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.175271 on epoch=88
05/31/2022 11:50:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.087257 on epoch=89
05/31/2022 11:50:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.055398 on epoch=91
05/31/2022 11:50:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.169350 on epoch=92
05/31/2022 11:50:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.119608 on epoch=93
05/31/2022 11:50:41 - INFO - __main__ - Global step 750 Train loss 0.121377 Classification-F1 0.6274102740511409 on epoch=93
05/31/2022 11:50:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.090146 on epoch=94
05/31/2022 11:50:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.039805 on epoch=96
05/31/2022 11:50:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.009132 on epoch=97
05/31/2022 11:51:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.047742 on epoch=98
05/31/2022 11:51:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.022552 on epoch=99
05/31/2022 11:51:08 - INFO - __main__ - Global step 800 Train loss 0.041875 Classification-F1 0.5104877259470672 on epoch=99
05/31/2022 11:51:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.033225 on epoch=101
05/31/2022 11:51:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.065881 on epoch=102
05/31/2022 11:51:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.011934 on epoch=103
05/31/2022 11:51:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.004754 on epoch=104
05/31/2022 11:51:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.030979 on epoch=106
05/31/2022 11:51:35 - INFO - __main__ - Global step 850 Train loss 0.029355 Classification-F1 0.5221274129782494 on epoch=106
05/31/2022 11:51:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.012034 on epoch=107
05/31/2022 11:51:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.006830 on epoch=108
05/31/2022 11:51:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001177 on epoch=109
05/31/2022 11:51:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.014442 on epoch=111
05/31/2022 11:52:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.007220 on epoch=112
05/31/2022 11:52:01 - INFO - __main__ - Global step 900 Train loss 0.008341 Classification-F1 0.6432461622949124 on epoch=112
05/31/2022 11:52:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000543 on epoch=113
05/31/2022 11:52:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.007863 on epoch=114
05/31/2022 11:52:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.004220 on epoch=116
05/31/2022 11:52:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000364 on epoch=117
05/31/2022 11:52:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000424 on epoch=118
05/31/2022 11:52:29 - INFO - __main__ - Global step 950 Train loss 0.002683 Classification-F1 0.6188724538796826 on epoch=118
05/31/2022 11:52:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.002074 on epoch=119
05/31/2022 11:52:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.002774 on epoch=121
05/31/2022 11:52:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000574 on epoch=122
05/31/2022 11:52:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.004701 on epoch=123
05/31/2022 11:52:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.008183 on epoch=124
05/31/2022 11:52:55 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:52:55 - INFO - __main__ - Printing 3 examples
05/31/2022 11:52:55 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/31/2022 11:52:55 - INFO - __main__ - ['happy']
05/31/2022 11:52:55 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/31/2022 11:52:55 - INFO - __main__ - ['happy']
05/31/2022 11:52:55 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/31/2022 11:52:55 - INFO - __main__ - ['happy']
05/31/2022 11:52:55 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:52:55 - INFO - __main__ - Global step 1000 Train loss 0.003661 Classification-F1 0.512118355199912 on epoch=124
05/31/2022 11:52:55 - INFO - __main__ - save last model!
05/31/2022 11:52:55 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:52:55 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:52:55 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:52:55 - INFO - __main__ - Printing 3 examples
05/31/2022 11:52:55 - INFO - __main__ -  [emo] can i get yes if i'm allowed to get a tiger so funny
05/31/2022 11:52:55 - INFO - __main__ - ['happy']
05/31/2022 11:52:55 - INFO - __main__ -  [emo] haha facewithtearsofjoy of course i wont leave you alone what do you think of me facewithtearsofjoy are u a grl
05/31/2022 11:52:55 - INFO - __main__ - ['happy']
05/31/2022 11:52:55 - INFO - __main__ -  [emo] awesome thank you but for what todays my best friend's birthday and we are enjoying the precious  day
05/31/2022 11:52:55 - INFO - __main__ - ['happy']
05/31/2022 11:52:55 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:52:55 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:52:56 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:53:02 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 11:53:03 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 11:53:03 - INFO - __main__ - Printing 3 examples
05/31/2022 11:53:03 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 11:53:03 - INFO - __main__ - ['others']
05/31/2022 11:53:03 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 11:53:03 - INFO - __main__ - ['others']
05/31/2022 11:53:03 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 11:53:03 - INFO - __main__ - ['others']
05/31/2022 11:53:03 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:53:05 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:53:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:53:07 - INFO - __main__ - Starting training!
05/31/2022 11:53:10 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 11:54:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_42_0.0005_8_predictions.txt
05/31/2022 11:54:07 - INFO - __main__ - Classification-F1 on test data: 0.1199
05/31/2022 11:54:07 - INFO - __main__ - prefix=emo_32_42, lr=0.0005, bsz=8, dev_performance=0.6432461622949124, test_performance=0.11985183238399831
05/31/2022 11:54:07 - INFO - __main__ - Running ... prefix=emo_32_42, lr=0.0003, bsz=8 ...
05/31/2022 11:54:08 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:54:08 - INFO - __main__ - Printing 3 examples
05/31/2022 11:54:08 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/31/2022 11:54:08 - INFO - __main__ - ['happy']
05/31/2022 11:54:08 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/31/2022 11:54:08 - INFO - __main__ - ['happy']
05/31/2022 11:54:08 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/31/2022 11:54:08 - INFO - __main__ - ['happy']
05/31/2022 11:54:08 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:54:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:54:08 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 11:54:08 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 11:54:08 - INFO - __main__ - Printing 3 examples
05/31/2022 11:54:08 - INFO - __main__ -  [emo] can i get yes if i'm allowed to get a tiger so funny
05/31/2022 11:54:08 - INFO - __main__ - ['happy']
05/31/2022 11:54:08 - INFO - __main__ -  [emo] haha facewithtearsofjoy of course i wont leave you alone what do you think of me facewithtearsofjoy are u a grl
05/31/2022 11:54:08 - INFO - __main__ - ['happy']
05/31/2022 11:54:08 - INFO - __main__ -  [emo] awesome thank you but for what todays my best friend's birthday and we are enjoying the precious  day
05/31/2022 11:54:08 - INFO - __main__ - ['happy']
05/31/2022 11:54:08 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:54:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:54:08 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 11:54:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 11:54:21 - INFO - __main__ - Starting training!
05/31/2022 11:54:26 - INFO - __main__ - Step 10 Global step 10 Train loss 25.532747 on epoch=1
05/31/2022 11:54:31 - INFO - __main__ - Step 20 Global step 20 Train loss 20.295942 on epoch=2
05/31/2022 11:54:36 - INFO - __main__ - Step 30 Global step 30 Train loss 17.131413 on epoch=3
05/31/2022 11:54:41 - INFO - __main__ - Step 40 Global step 40 Train loss 15.875661 on epoch=4
05/31/2022 11:54:46 - INFO - __main__ - Step 50 Global step 50 Train loss 14.894094 on epoch=6
05/31/2022 11:55:06 - INFO - __main__ - Global step 50 Train loss 18.745970 Classification-F1 0.0 on epoch=6
05/31/2022 11:55:12 - INFO - __main__ - Step 60 Global step 60 Train loss 14.786255 on epoch=7
05/31/2022 11:55:17 - INFO - __main__ - Step 70 Global step 70 Train loss 13.730764 on epoch=8
05/31/2022 11:55:22 - INFO - __main__ - Step 80 Global step 80 Train loss 12.511750 on epoch=9
05/31/2022 11:55:27 - INFO - __main__ - Step 90 Global step 90 Train loss 11.273078 on epoch=11
05/31/2022 11:55:32 - INFO - __main__ - Step 100 Global step 100 Train loss 10.624994 on epoch=12
05/31/2022 11:55:44 - INFO - __main__ - Global step 100 Train loss 12.585370 Classification-F1 0.0 on epoch=12
05/31/2022 11:55:50 - INFO - __main__ - Step 110 Global step 110 Train loss 7.803393 on epoch=13
05/31/2022 11:55:55 - INFO - __main__ - Step 120 Global step 120 Train loss 4.209895 on epoch=14
05/31/2022 11:56:00 - INFO - __main__ - Step 130 Global step 130 Train loss 2.338849 on epoch=16
05/31/2022 11:56:04 - INFO - __main__ - Step 140 Global step 140 Train loss 1.891994 on epoch=17
05/31/2022 11:56:09 - INFO - __main__ - Step 150 Global step 150 Train loss 1.833685 on epoch=18
05/31/2022 11:56:10 - INFO - __main__ - Global step 150 Train loss 3.615564 Classification-F1 0.2874670404066798 on epoch=18
05/31/2022 11:56:16 - INFO - __main__ - Step 160 Global step 160 Train loss 1.737310 on epoch=19
05/31/2022 11:56:22 - INFO - __main__ - Step 170 Global step 170 Train loss 1.052414 on epoch=21
05/31/2022 11:56:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.965418 on epoch=22
05/31/2022 11:56:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.869645 on epoch=23
05/31/2022 11:56:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.695527 on epoch=24
05/31/2022 11:56:38 - INFO - __main__ - Global step 200 Train loss 1.064063 Classification-F1 0.6153793574846207 on epoch=24
05/31/2022 11:56:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.954261 on epoch=26
05/31/2022 11:56:49 - INFO - __main__ - Step 220 Global step 220 Train loss 1.298569 on epoch=27
05/31/2022 11:56:54 - INFO - __main__ - Step 230 Global step 230 Train loss 1.158337 on epoch=28
05/31/2022 11:56:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.747084 on epoch=29
05/31/2022 11:57:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.616712 on epoch=31
05/31/2022 11:57:05 - INFO - __main__ - Global step 250 Train loss 0.954992 Classification-F1 0.5903361344537815 on epoch=31
05/31/2022 11:57:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.527792 on epoch=32
05/31/2022 11:57:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.470109 on epoch=33
05/31/2022 11:57:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.455501 on epoch=34
05/31/2022 11:57:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.342335 on epoch=36
05/31/2022 11:57:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.356566 on epoch=37
05/31/2022 11:57:32 - INFO - __main__ - Global step 300 Train loss 0.430461 Classification-F1 0.6233968042206119 on epoch=37
05/31/2022 11:57:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.466755 on epoch=38
05/31/2022 11:57:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.423734 on epoch=39
05/31/2022 11:57:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.486936 on epoch=41
05/31/2022 11:57:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.441589 on epoch=42
05/31/2022 11:57:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.403077 on epoch=43
05/31/2022 11:58:00 - INFO - __main__ - Global step 350 Train loss 0.444418 Classification-F1 0.41213352007469656 on epoch=43
05/31/2022 11:58:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.404160 on epoch=44
05/31/2022 11:58:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.428145 on epoch=46
05/31/2022 11:58:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.349146 on epoch=47
05/31/2022 11:58:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.525449 on epoch=48
05/31/2022 11:58:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.316112 on epoch=49
05/31/2022 11:58:27 - INFO - __main__ - Global step 400 Train loss 0.404602 Classification-F1 0.5244913084437891 on epoch=49
05/31/2022 11:58:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.410997 on epoch=51
05/31/2022 11:58:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.260085 on epoch=52
05/31/2022 11:58:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.417443 on epoch=53
05/31/2022 11:58:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.248674 on epoch=54
05/31/2022 11:58:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.239026 on epoch=56
05/31/2022 11:58:55 - INFO - __main__ - Global step 450 Train loss 0.315245 Classification-F1 0.4655354449472096 on epoch=56
05/31/2022 11:59:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.310401 on epoch=57
05/31/2022 11:59:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.288069 on epoch=58
05/31/2022 11:59:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.293033 on epoch=59
05/31/2022 11:59:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.118983 on epoch=61
05/31/2022 11:59:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.175077 on epoch=62
05/31/2022 11:59:22 - INFO - __main__ - Global step 500 Train loss 0.237113 Classification-F1 0.5407574417919245 on epoch=62
05/31/2022 11:59:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.196218 on epoch=63
05/31/2022 11:59:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.121894 on epoch=64
05/31/2022 11:59:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.124507 on epoch=66
05/31/2022 11:59:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.120002 on epoch=67
05/31/2022 11:59:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.128176 on epoch=68
05/31/2022 11:59:49 - INFO - __main__ - Global step 550 Train loss 0.138159 Classification-F1 0.3954280081040644 on epoch=68
05/31/2022 11:59:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.048161 on epoch=69
05/31/2022 12:00:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.125119 on epoch=71
05/31/2022 12:00:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.058828 on epoch=72
05/31/2022 12:00:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.047021 on epoch=73
05/31/2022 12:00:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.128725 on epoch=74
05/31/2022 12:00:17 - INFO - __main__ - Global step 600 Train loss 0.081571 Classification-F1 0.38564570778932783 on epoch=74
05/31/2022 12:00:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.026633 on epoch=76
05/31/2022 12:00:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.046291 on epoch=77
05/31/2022 12:00:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.120879 on epoch=78
05/31/2022 12:00:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.068859 on epoch=79
05/31/2022 12:00:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.074986 on epoch=81
05/31/2022 12:00:44 - INFO - __main__ - Global step 650 Train loss 0.067530 Classification-F1 0.4235262888036501 on epoch=81
05/31/2022 12:00:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.108860 on epoch=82
05/31/2022 12:00:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.104284 on epoch=83
05/31/2022 12:01:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.035517 on epoch=84
05/31/2022 12:01:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.082986 on epoch=86
05/31/2022 12:01:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.109447 on epoch=87
05/31/2022 12:01:11 - INFO - __main__ - Global step 700 Train loss 0.088219 Classification-F1 0.5141799327447114 on epoch=87
05/31/2022 12:01:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.038977 on epoch=88
05/31/2022 12:01:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.092100 on epoch=89
05/31/2022 12:01:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.039465 on epoch=91
05/31/2022 12:01:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.027985 on epoch=92
05/31/2022 12:01:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.031798 on epoch=93
05/31/2022 12:01:39 - INFO - __main__ - Global step 750 Train loss 0.046065 Classification-F1 0.5320060014433385 on epoch=93
05/31/2022 12:01:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.008733 on epoch=94
05/31/2022 12:01:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.006405 on epoch=96
05/31/2022 12:01:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.041305 on epoch=97
05/31/2022 12:02:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.047184 on epoch=98
05/31/2022 12:02:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.027280 on epoch=99
05/31/2022 12:02:06 - INFO - __main__ - Global step 800 Train loss 0.026181 Classification-F1 0.6338577502387243 on epoch=99
05/31/2022 12:02:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.056286 on epoch=101
05/31/2022 12:02:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.022419 on epoch=102
05/31/2022 12:02:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.053002 on epoch=103
05/31/2022 12:02:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.017020 on epoch=104
05/31/2022 12:02:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.048159 on epoch=106
05/31/2022 12:02:34 - INFO - __main__ - Global step 850 Train loss 0.039377 Classification-F1 0.39418378390430814 on epoch=106
05/31/2022 12:02:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.005368 on epoch=107
05/31/2022 12:02:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.019954 on epoch=108
05/31/2022 12:02:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.011186 on epoch=109
05/31/2022 12:02:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.017003 on epoch=111
05/31/2022 12:03:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.025510 on epoch=112
05/31/2022 12:03:01 - INFO - __main__ - Global step 900 Train loss 0.015804 Classification-F1 0.6246081656346749 on epoch=112
05/31/2022 12:03:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.016512 on epoch=113
05/31/2022 12:03:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.043907 on epoch=114
05/31/2022 12:03:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.003480 on epoch=116
05/31/2022 12:03:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.015728 on epoch=117
05/31/2022 12:03:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000980 on epoch=118
05/31/2022 12:03:29 - INFO - __main__ - Global step 950 Train loss 0.016121 Classification-F1 0.5510010490292181 on epoch=118
05/31/2022 12:03:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.007842 on epoch=119
05/31/2022 12:03:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.005718 on epoch=121
05/31/2022 12:03:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.012554 on epoch=122
05/31/2022 12:03:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.013151 on epoch=123
05/31/2022 12:03:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.003296 on epoch=124
05/31/2022 12:03:56 - INFO - __main__ - Global step 1000 Train loss 0.008512 Classification-F1 0.6117288343315741 on epoch=124
05/31/2022 12:03:56 - INFO - __main__ - save last model!
05/31/2022 12:03:56 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:03:56 - INFO - __main__ - Printing 3 examples
05/31/2022 12:03:56 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/31/2022 12:03:56 - INFO - __main__ - ['happy']
05/31/2022 12:03:56 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/31/2022 12:03:56 - INFO - __main__ - ['happy']
05/31/2022 12:03:56 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/31/2022 12:03:56 - INFO - __main__ - ['happy']
05/31/2022 12:03:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:03:56 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:03:56 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:03:56 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:03:56 - INFO - __main__ - Printing 3 examples
05/31/2022 12:03:56 - INFO - __main__ -  [emo] can i get yes if i'm allowed to get a tiger so funny
05/31/2022 12:03:56 - INFO - __main__ - ['happy']
05/31/2022 12:03:56 - INFO - __main__ -  [emo] haha facewithtearsofjoy of course i wont leave you alone what do you think of me facewithtearsofjoy are u a grl
05/31/2022 12:03:56 - INFO - __main__ - ['happy']
05/31/2022 12:03:56 - INFO - __main__ -  [emo] awesome thank you but for what todays my best friend's birthday and we are enjoying the precious  day
05/31/2022 12:03:56 - INFO - __main__ - ['happy']
05/31/2022 12:03:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:03:56 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:03:56 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:04:03 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 12:04:04 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 12:04:04 - INFO - __main__ - Printing 3 examples
05/31/2022 12:04:04 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 12:04:04 - INFO - __main__ - ['others']
05/31/2022 12:04:04 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 12:04:04 - INFO - __main__ - ['others']
05/31/2022 12:04:04 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 12:04:04 - INFO - __main__ - ['others']
05/31/2022 12:04:04 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:04:06 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:04:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:04:07 - INFO - __main__ - Starting training!
05/31/2022 12:04:11 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 12:04:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_42_0.0003_8_predictions.txt
05/31/2022 12:04:54 - INFO - __main__ - Classification-F1 on test data: 0.0222
05/31/2022 12:04:54 - INFO - __main__ - prefix=emo_32_42, lr=0.0003, bsz=8, dev_performance=0.6338577502387243, test_performance=0.022223930268982627
05/31/2022 12:04:54 - INFO - __main__ - Running ... prefix=emo_32_42, lr=0.0002, bsz=8 ...
05/31/2022 12:04:55 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:04:55 - INFO - __main__ - Printing 3 examples
05/31/2022 12:04:55 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/31/2022 12:04:55 - INFO - __main__ - ['happy']
05/31/2022 12:04:55 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/31/2022 12:04:55 - INFO - __main__ - ['happy']
05/31/2022 12:04:55 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/31/2022 12:04:55 - INFO - __main__ - ['happy']
05/31/2022 12:04:55 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:04:55 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:04:55 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:04:55 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:04:55 - INFO - __main__ - Printing 3 examples
05/31/2022 12:04:55 - INFO - __main__ -  [emo] can i get yes if i'm allowed to get a tiger so funny
05/31/2022 12:04:55 - INFO - __main__ - ['happy']
05/31/2022 12:04:55 - INFO - __main__ -  [emo] haha facewithtearsofjoy of course i wont leave you alone what do you think of me facewithtearsofjoy are u a grl
05/31/2022 12:04:55 - INFO - __main__ - ['happy']
05/31/2022 12:04:55 - INFO - __main__ -  [emo] awesome thank you but for what todays my best friend's birthday and we are enjoying the precious  day
05/31/2022 12:04:55 - INFO - __main__ - ['happy']
05/31/2022 12:04:55 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:04:55 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:04:55 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:05:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:05:06 - INFO - __main__ - Starting training!
05/31/2022 12:05:10 - INFO - __main__ - Step 10 Global step 10 Train loss 25.213753 on epoch=1
05/31/2022 12:05:15 - INFO - __main__ - Step 20 Global step 20 Train loss 21.199730 on epoch=2
05/31/2022 12:05:20 - INFO - __main__ - Step 30 Global step 30 Train loss 18.898430 on epoch=3
05/31/2022 12:05:25 - INFO - __main__ - Step 40 Global step 40 Train loss 17.389729 on epoch=4
05/31/2022 12:05:30 - INFO - __main__ - Step 50 Global step 50 Train loss 16.155525 on epoch=6
05/31/2022 12:06:07 - INFO - __main__ - Global step 50 Train loss 19.771433 Classification-F1 0.0005560189046427579 on epoch=6
05/31/2022 12:06:13 - INFO - __main__ - Step 60 Global step 60 Train loss 15.543889 on epoch=7
05/31/2022 12:06:18 - INFO - __main__ - Step 70 Global step 70 Train loss 15.137575 on epoch=8
05/31/2022 12:06:23 - INFO - __main__ - Step 80 Global step 80 Train loss 15.292745 on epoch=9
05/31/2022 12:06:28 - INFO - __main__ - Step 90 Global step 90 Train loss 13.316714 on epoch=11
05/31/2022 12:06:33 - INFO - __main__ - Step 100 Global step 100 Train loss 13.430422 on epoch=12
05/31/2022 12:07:03 - INFO - __main__ - Global step 100 Train loss 14.544269 Classification-F1 0.0018963097811658513 on epoch=12
05/31/2022 12:07:09 - INFO - __main__ - Step 110 Global step 110 Train loss 12.477415 on epoch=13
05/31/2022 12:07:14 - INFO - __main__ - Step 120 Global step 120 Train loss 12.189988 on epoch=14
05/31/2022 12:07:19 - INFO - __main__ - Step 130 Global step 130 Train loss 11.121094 on epoch=16
05/31/2022 12:07:24 - INFO - __main__ - Step 140 Global step 140 Train loss 11.061655 on epoch=17
05/31/2022 12:07:29 - INFO - __main__ - Step 150 Global step 150 Train loss 9.705708 on epoch=18
05/31/2022 12:07:38 - INFO - __main__ - Global step 150 Train loss 11.311172 Classification-F1 0.0 on epoch=18
05/31/2022 12:07:43 - INFO - __main__ - Step 160 Global step 160 Train loss 7.037585 on epoch=19
05/31/2022 12:07:48 - INFO - __main__ - Step 170 Global step 170 Train loss 4.778471 on epoch=21
05/31/2022 12:07:53 - INFO - __main__ - Step 180 Global step 180 Train loss 1.770400 on epoch=22
05/31/2022 12:07:58 - INFO - __main__ - Step 190 Global step 190 Train loss 2.600537 on epoch=23
05/31/2022 12:08:03 - INFO - __main__ - Step 200 Global step 200 Train loss 1.098287 on epoch=24
05/31/2022 12:08:04 - INFO - __main__ - Global step 200 Train loss 3.457057 Classification-F1 0.31351124365830246 on epoch=24
05/31/2022 12:08:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.838930 on epoch=26
05/31/2022 12:08:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.601383 on epoch=27
05/31/2022 12:08:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.589501 on epoch=28
05/31/2022 12:08:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.529384 on epoch=29
05/31/2022 12:08:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.538128 on epoch=31
05/31/2022 12:08:31 - INFO - __main__ - Global step 250 Train loss 0.619465 Classification-F1 0.4821445221445222 on epoch=31
05/31/2022 12:08:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.610507 on epoch=32
05/31/2022 12:08:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.552615 on epoch=33
05/31/2022 12:08:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.505834 on epoch=34
05/31/2022 12:08:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.580349 on epoch=36
05/31/2022 12:08:57 - INFO - __main__ - Step 300 Global step 300 Train loss 1.028614 on epoch=37
05/31/2022 12:08:58 - INFO - __main__ - Global step 300 Train loss 0.655584 Classification-F1 0.513498440377316 on epoch=37
05/31/2022 12:09:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.742458 on epoch=38
05/31/2022 12:09:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.659650 on epoch=39
05/31/2022 12:09:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.577865 on epoch=41
05/31/2022 12:09:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.382041 on epoch=42
05/31/2022 12:09:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.393524 on epoch=43
05/31/2022 12:09:25 - INFO - __main__ - Global step 350 Train loss 0.551108 Classification-F1 0.6435283063190039 on epoch=43
05/31/2022 12:09:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.426573 on epoch=44
05/31/2022 12:09:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.367949 on epoch=46
05/31/2022 12:09:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.436455 on epoch=47
05/31/2022 12:09:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.585371 on epoch=48
05/31/2022 12:09:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.808530 on epoch=49
05/31/2022 12:09:52 - INFO - __main__ - Global step 400 Train loss 0.524976 Classification-F1 0.6481474195029182 on epoch=49
05/31/2022 12:09:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.507491 on epoch=51
05/31/2022 12:10:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.550823 on epoch=52
05/31/2022 12:10:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.312903 on epoch=53
05/31/2022 12:10:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.313223 on epoch=54
05/31/2022 12:10:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.284174 on epoch=56
05/31/2022 12:10:19 - INFO - __main__ - Global step 450 Train loss 0.393723 Classification-F1 0.6745504275653924 on epoch=56
05/31/2022 12:10:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.223704 on epoch=57
05/31/2022 12:10:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.271653 on epoch=58
05/31/2022 12:10:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.235045 on epoch=59
05/31/2022 12:10:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.135311 on epoch=61
05/31/2022 12:10:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.159841 on epoch=62
05/31/2022 12:10:46 - INFO - __main__ - Global step 500 Train loss 0.205111 Classification-F1 0.6187779892521272 on epoch=62
05/31/2022 12:10:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.236056 on epoch=63
05/31/2022 12:10:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.265763 on epoch=64
05/31/2022 12:11:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.137439 on epoch=66
05/31/2022 12:11:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.172726 on epoch=67
05/31/2022 12:11:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.389323 on epoch=68
05/31/2022 12:11:12 - INFO - __main__ - Global step 550 Train loss 0.240262 Classification-F1 0.6162749376838534 on epoch=68
05/31/2022 12:11:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.156557 on epoch=69
05/31/2022 12:11:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.216348 on epoch=71
05/31/2022 12:11:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.205443 on epoch=72
05/31/2022 12:11:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.262368 on epoch=73
05/31/2022 12:11:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.431166 on epoch=74
05/31/2022 12:11:38 - INFO - __main__ - Global step 600 Train loss 0.254377 Classification-F1 0.5109829016315117 on epoch=74
05/31/2022 12:11:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.385501 on epoch=76
05/31/2022 12:11:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.300769 on epoch=77
05/31/2022 12:11:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.301056 on epoch=78
05/31/2022 12:11:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.415647 on epoch=79
05/31/2022 12:12:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.513941 on epoch=81
05/31/2022 12:12:04 - INFO - __main__ - Global step 650 Train loss 0.383383 Classification-F1 0.5328661324817531 on epoch=81
05/31/2022 12:12:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.452799 on epoch=82
05/31/2022 12:12:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.437239 on epoch=83
05/31/2022 12:12:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.274949 on epoch=84
05/31/2022 12:12:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.770407 on epoch=86
05/31/2022 12:12:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.278370 on epoch=87
05/31/2022 12:12:31 - INFO - __main__ - Global step 700 Train loss 0.442753 Classification-F1 0.6303433074260932 on epoch=87
05/31/2022 12:12:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.880361 on epoch=88
05/31/2022 12:12:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.945495 on epoch=89
05/31/2022 12:12:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.803372 on epoch=91
05/31/2022 12:12:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.662127 on epoch=92
05/31/2022 12:12:56 - INFO - __main__ - Step 750 Global step 750 Train loss 1.016174 on epoch=93
05/31/2022 12:12:57 - INFO - __main__ - Global step 750 Train loss 0.861506 Classification-F1 0.37389748882008944 on epoch=93
05/31/2022 12:13:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.719582 on epoch=94
05/31/2022 12:13:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.802065 on epoch=96
05/31/2022 12:13:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.984138 on epoch=97
05/31/2022 12:13:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.742328 on epoch=98
05/31/2022 12:13:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.657524 on epoch=99
05/31/2022 12:13:24 - INFO - __main__ - Global step 800 Train loss 0.781127 Classification-F1 0.5084368272892863 on epoch=99
05/31/2022 12:13:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.580859 on epoch=101
05/31/2022 12:13:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.654190 on epoch=102
05/31/2022 12:13:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.709783 on epoch=103
05/31/2022 12:13:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.701687 on epoch=104
05/31/2022 12:13:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.755905 on epoch=106
05/31/2022 12:13:50 - INFO - __main__ - Global step 850 Train loss 0.680485 Classification-F1 0.49537490093821096 on epoch=106
05/31/2022 12:13:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.642134 on epoch=107
05/31/2022 12:14:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.797335 on epoch=108
05/31/2022 12:14:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.556980 on epoch=109
05/31/2022 12:14:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.585978 on epoch=111
05/31/2022 12:14:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.537984 on epoch=112
05/31/2022 12:14:17 - INFO - __main__ - Global step 900 Train loss 0.624082 Classification-F1 0.3694509758941715 on epoch=112
05/31/2022 12:14:22 - INFO - __main__ - Step 910 Global step 910 Train loss 1.001227 on epoch=113
05/31/2022 12:14:27 - INFO - __main__ - Step 920 Global step 920 Train loss 1.016896 on epoch=114
05/31/2022 12:14:32 - INFO - __main__ - Step 930 Global step 930 Train loss 1.076355 on epoch=116
05/31/2022 12:14:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.876201 on epoch=117
05/31/2022 12:14:42 - INFO - __main__ - Step 950 Global step 950 Train loss 1.450506 on epoch=118
05/31/2022 12:14:44 - INFO - __main__ - Global step 950 Train loss 1.084237 Classification-F1 0.3722888567157307 on epoch=118
05/31/2022 12:14:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.567405 on epoch=119
05/31/2022 12:14:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.498621 on epoch=121
05/31/2022 12:14:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.979962 on epoch=122
05/31/2022 12:15:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.928030 on epoch=123
05/31/2022 12:15:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.731653 on epoch=124
05/31/2022 12:15:10 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:15:10 - INFO - __main__ - Printing 3 examples
05/31/2022 12:15:10 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/31/2022 12:15:10 - INFO - __main__ - ['happy']
05/31/2022 12:15:10 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/31/2022 12:15:10 - INFO - __main__ - ['happy']
05/31/2022 12:15:10 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/31/2022 12:15:10 - INFO - __main__ - ['happy']
05/31/2022 12:15:10 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:15:10 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:15:10 - INFO - __main__ - Global step 1000 Train loss 0.741134 Classification-F1 0.32979316257635843 on epoch=124
05/31/2022 12:15:10 - INFO - __main__ - save last model!
05/31/2022 12:15:10 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:15:10 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:15:10 - INFO - __main__ - Printing 3 examples
05/31/2022 12:15:10 - INFO - __main__ -  [emo] can i get yes if i'm allowed to get a tiger so funny
05/31/2022 12:15:10 - INFO - __main__ - ['happy']
05/31/2022 12:15:10 - INFO - __main__ -  [emo] haha facewithtearsofjoy of course i wont leave you alone what do you think of me facewithtearsofjoy are u a grl
05/31/2022 12:15:10 - INFO - __main__ - ['happy']
05/31/2022 12:15:10 - INFO - __main__ -  [emo] awesome thank you but for what todays my best friend's birthday and we are enjoying the precious  day
05/31/2022 12:15:10 - INFO - __main__ - ['happy']
05/31/2022 12:15:10 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:15:10 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:15:10 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:15:17 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 12:15:18 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 12:15:18 - INFO - __main__ - Printing 3 examples
05/31/2022 12:15:18 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 12:15:18 - INFO - __main__ - ['others']
05/31/2022 12:15:18 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 12:15:18 - INFO - __main__ - ['others']
05/31/2022 12:15:18 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 12:15:18 - INFO - __main__ - ['others']
05/31/2022 12:15:18 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:15:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:15:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:15:21 - INFO - __main__ - Starting training!
05/31/2022 12:15:25 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 12:16:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_42_0.0002_8_predictions.txt
05/31/2022 12:16:07 - INFO - __main__ - Classification-F1 on test data: 0.2375
05/31/2022 12:16:08 - INFO - __main__ - prefix=emo_32_42, lr=0.0002, bsz=8, dev_performance=0.6745504275653924, test_performance=0.23748865187547985
05/31/2022 12:16:08 - INFO - __main__ - Running ... prefix=emo_32_42, lr=0.0001, bsz=8 ...
05/31/2022 12:16:09 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:16:09 - INFO - __main__ - Printing 3 examples
05/31/2022 12:16:09 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/31/2022 12:16:09 - INFO - __main__ - ['happy']
05/31/2022 12:16:09 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/31/2022 12:16:09 - INFO - __main__ - ['happy']
05/31/2022 12:16:09 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/31/2022 12:16:09 - INFO - __main__ - ['happy']
05/31/2022 12:16:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:16:09 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:16:09 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:16:09 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:16:09 - INFO - __main__ - Printing 3 examples
05/31/2022 12:16:09 - INFO - __main__ -  [emo] can i get yes if i'm allowed to get a tiger so funny
05/31/2022 12:16:09 - INFO - __main__ - ['happy']
05/31/2022 12:16:09 - INFO - __main__ -  [emo] haha facewithtearsofjoy of course i wont leave you alone what do you think of me facewithtearsofjoy are u a grl
05/31/2022 12:16:09 - INFO - __main__ - ['happy']
05/31/2022 12:16:09 - INFO - __main__ -  [emo] awesome thank you but for what todays my best friend's birthday and we are enjoying the precious  day
05/31/2022 12:16:09 - INFO - __main__ - ['happy']
05/31/2022 12:16:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:16:09 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:16:09 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:16:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:16:22 - INFO - __main__ - Starting training!
05/31/2022 12:16:27 - INFO - __main__ - Step 10 Global step 10 Train loss 25.152792 on epoch=1
05/31/2022 12:16:32 - INFO - __main__ - Step 20 Global step 20 Train loss 22.079586 on epoch=2
05/31/2022 12:16:37 - INFO - __main__ - Step 30 Global step 30 Train loss 19.646206 on epoch=3
05/31/2022 12:16:42 - INFO - __main__ - Step 40 Global step 40 Train loss 18.262794 on epoch=4
05/31/2022 12:16:47 - INFO - __main__ - Step 50 Global step 50 Train loss 17.599377 on epoch=6
05/31/2022 12:17:26 - INFO - __main__ - Global step 50 Train loss 20.548149 Classification-F1 0.0 on epoch=6
05/31/2022 12:17:31 - INFO - __main__ - Step 60 Global step 60 Train loss 17.958624 on epoch=7
05/31/2022 12:17:37 - INFO - __main__ - Step 70 Global step 70 Train loss 17.509148 on epoch=8
05/31/2022 12:17:42 - INFO - __main__ - Step 80 Global step 80 Train loss 17.414608 on epoch=9
05/31/2022 12:17:47 - INFO - __main__ - Step 90 Global step 90 Train loss 16.159235 on epoch=11
05/31/2022 12:17:52 - INFO - __main__ - Step 100 Global step 100 Train loss 16.080303 on epoch=12
05/31/2022 12:18:26 - INFO - __main__ - Global step 100 Train loss 17.024384 Classification-F1 0.0 on epoch=12
05/31/2022 12:18:31 - INFO - __main__ - Step 110 Global step 110 Train loss 16.457840 on epoch=13
05/31/2022 12:18:36 - INFO - __main__ - Step 120 Global step 120 Train loss 15.793653 on epoch=14
05/31/2022 12:18:41 - INFO - __main__ - Step 130 Global step 130 Train loss 15.188711 on epoch=16
05/31/2022 12:18:46 - INFO - __main__ - Step 140 Global step 140 Train loss 14.865529 on epoch=17
05/31/2022 12:18:51 - INFO - __main__ - Step 150 Global step 150 Train loss 14.718234 on epoch=18
05/31/2022 12:19:22 - INFO - __main__ - Global step 150 Train loss 15.404794 Classification-F1 0.0 on epoch=18
05/31/2022 12:19:27 - INFO - __main__ - Step 160 Global step 160 Train loss 14.276213 on epoch=19
05/31/2022 12:19:33 - INFO - __main__ - Step 170 Global step 170 Train loss 13.613317 on epoch=21
05/31/2022 12:19:38 - INFO - __main__ - Step 180 Global step 180 Train loss 14.154388 on epoch=22
05/31/2022 12:19:43 - INFO - __main__ - Step 190 Global step 190 Train loss 13.560881 on epoch=23
05/31/2022 12:19:48 - INFO - __main__ - Step 200 Global step 200 Train loss 13.680931 on epoch=24
05/31/2022 12:20:09 - INFO - __main__ - Global step 200 Train loss 13.857146 Classification-F1 0.0007974481658692185 on epoch=24
05/31/2022 12:20:14 - INFO - __main__ - Step 210 Global step 210 Train loss 12.448004 on epoch=26
05/31/2022 12:20:19 - INFO - __main__ - Step 220 Global step 220 Train loss 12.098721 on epoch=27
05/31/2022 12:20:25 - INFO - __main__ - Step 230 Global step 230 Train loss 11.717493 on epoch=28
05/31/2022 12:20:30 - INFO - __main__ - Step 240 Global step 240 Train loss 11.711598 on epoch=29
05/31/2022 12:20:35 - INFO - __main__ - Step 250 Global step 250 Train loss 11.915350 on epoch=31
05/31/2022 12:20:47 - INFO - __main__ - Global step 250 Train loss 11.978232 Classification-F1 0.0015151515151515152 on epoch=31
05/31/2022 12:20:53 - INFO - __main__ - Step 260 Global step 260 Train loss 10.617218 on epoch=32
05/31/2022 12:20:58 - INFO - __main__ - Step 270 Global step 270 Train loss 10.601683 on epoch=33
05/31/2022 12:21:03 - INFO - __main__ - Step 280 Global step 280 Train loss 9.841245 on epoch=34
05/31/2022 12:21:08 - INFO - __main__ - Step 290 Global step 290 Train loss 10.152028 on epoch=36
05/31/2022 12:21:13 - INFO - __main__ - Step 300 Global step 300 Train loss 9.267008 on epoch=37
05/31/2022 12:21:22 - INFO - __main__ - Global step 300 Train loss 10.095837 Classification-F1 0.0 on epoch=37
05/31/2022 12:21:27 - INFO - __main__ - Step 310 Global step 310 Train loss 9.067552 on epoch=38
05/31/2022 12:21:32 - INFO - __main__ - Step 320 Global step 320 Train loss 7.436832 on epoch=39
05/31/2022 12:21:37 - INFO - __main__ - Step 330 Global step 330 Train loss 7.490220 on epoch=41
05/31/2022 12:21:42 - INFO - __main__ - Step 340 Global step 340 Train loss 5.253050 on epoch=42
05/31/2022 12:21:47 - INFO - __main__ - Step 350 Global step 350 Train loss 4.627671 on epoch=43
05/31/2022 12:21:48 - INFO - __main__ - Global step 350 Train loss 6.775065 Classification-F1 0.08050314465408806 on epoch=43
05/31/2022 12:21:54 - INFO - __main__ - Step 360 Global step 360 Train loss 4.005634 on epoch=44
05/31/2022 12:21:59 - INFO - __main__ - Step 370 Global step 370 Train loss 4.413724 on epoch=46
05/31/2022 12:22:04 - INFO - __main__ - Step 380 Global step 380 Train loss 4.680455 on epoch=47
05/31/2022 12:22:09 - INFO - __main__ - Step 390 Global step 390 Train loss 4.566554 on epoch=48
05/31/2022 12:22:14 - INFO - __main__ - Step 400 Global step 400 Train loss 3.772263 on epoch=49
05/31/2022 12:22:15 - INFO - __main__ - Global step 400 Train loss 4.287726 Classification-F1 0.1392343724761751 on epoch=49
05/31/2022 12:22:20 - INFO - __main__ - Step 410 Global step 410 Train loss 4.125127 on epoch=51
05/31/2022 12:22:26 - INFO - __main__ - Step 420 Global step 420 Train loss 3.697786 on epoch=52
05/31/2022 12:22:31 - INFO - __main__ - Step 430 Global step 430 Train loss 4.265944 on epoch=53
05/31/2022 12:22:36 - INFO - __main__ - Step 440 Global step 440 Train loss 3.547749 on epoch=54
05/31/2022 12:22:41 - INFO - __main__ - Step 450 Global step 450 Train loss 3.586068 on epoch=56
05/31/2022 12:22:42 - INFO - __main__ - Global step 450 Train loss 3.844535 Classification-F1 0.252205105483794 on epoch=56
05/31/2022 12:22:48 - INFO - __main__ - Step 460 Global step 460 Train loss 3.517383 on epoch=57
05/31/2022 12:22:53 - INFO - __main__ - Step 470 Global step 470 Train loss 3.006947 on epoch=58
05/31/2022 12:22:58 - INFO - __main__ - Step 480 Global step 480 Train loss 3.844960 on epoch=59
05/31/2022 12:23:03 - INFO - __main__ - Step 490 Global step 490 Train loss 3.140765 on epoch=61
05/31/2022 12:23:08 - INFO - __main__ - Step 500 Global step 500 Train loss 3.242155 on epoch=62
05/31/2022 12:23:09 - INFO - __main__ - Global step 500 Train loss 3.350442 Classification-F1 0.13592311419802938 on epoch=62
05/31/2022 12:23:14 - INFO - __main__ - Step 510 Global step 510 Train loss 3.515865 on epoch=63
05/31/2022 12:23:19 - INFO - __main__ - Step 520 Global step 520 Train loss 2.752117 on epoch=64
05/31/2022 12:23:24 - INFO - __main__ - Step 530 Global step 530 Train loss 3.150268 on epoch=66
05/31/2022 12:23:29 - INFO - __main__ - Step 540 Global step 540 Train loss 3.229798 on epoch=67
05/31/2022 12:23:34 - INFO - __main__ - Step 550 Global step 550 Train loss 2.626902 on epoch=68
05/31/2022 12:23:35 - INFO - __main__ - Global step 550 Train loss 3.054990 Classification-F1 0.2051341084378957 on epoch=68
05/31/2022 12:23:40 - INFO - __main__ - Step 560 Global step 560 Train loss 2.882096 on epoch=69
05/31/2022 12:23:45 - INFO - __main__ - Step 570 Global step 570 Train loss 2.465076 on epoch=71
05/31/2022 12:23:51 - INFO - __main__ - Step 580 Global step 580 Train loss 2.880870 on epoch=72
05/31/2022 12:23:56 - INFO - __main__ - Step 590 Global step 590 Train loss 2.902512 on epoch=73
05/31/2022 12:24:01 - INFO - __main__ - Step 600 Global step 600 Train loss 2.509444 on epoch=74
05/31/2022 12:24:02 - INFO - __main__ - Global step 600 Train loss 2.728000 Classification-F1 0.11578044596912522 on epoch=74
05/31/2022 12:24:07 - INFO - __main__ - Step 610 Global step 610 Train loss 2.830097 on epoch=76
05/31/2022 12:24:12 - INFO - __main__ - Step 620 Global step 620 Train loss 2.330625 on epoch=77
05/31/2022 12:24:17 - INFO - __main__ - Step 630 Global step 630 Train loss 2.199954 on epoch=78
05/31/2022 12:24:22 - INFO - __main__ - Step 640 Global step 640 Train loss 2.547436 on epoch=79
05/31/2022 12:24:27 - INFO - __main__ - Step 650 Global step 650 Train loss 2.461959 on epoch=81
05/31/2022 12:24:28 - INFO - __main__ - Global step 650 Train loss 2.474015 Classification-F1 0.17347440571124784 on epoch=81
05/31/2022 12:24:34 - INFO - __main__ - Step 660 Global step 660 Train loss 2.092338 on epoch=82
05/31/2022 12:24:39 - INFO - __main__ - Step 670 Global step 670 Train loss 2.053674 on epoch=83
05/31/2022 12:24:44 - INFO - __main__ - Step 680 Global step 680 Train loss 2.943330 on epoch=84
05/31/2022 12:24:49 - INFO - __main__ - Step 690 Global step 690 Train loss 2.094918 on epoch=86
05/31/2022 12:24:54 - INFO - __main__ - Step 700 Global step 700 Train loss 1.907484 on epoch=87
05/31/2022 12:24:55 - INFO - __main__ - Global step 700 Train loss 2.218349 Classification-F1 0.32307973016807423 on epoch=87
05/31/2022 12:25:01 - INFO - __main__ - Step 710 Global step 710 Train loss 1.660692 on epoch=88
05/31/2022 12:25:06 - INFO - __main__ - Step 720 Global step 720 Train loss 2.128402 on epoch=89
05/31/2022 12:25:11 - INFO - __main__ - Step 730 Global step 730 Train loss 1.781657 on epoch=91
05/31/2022 12:25:16 - INFO - __main__ - Step 740 Global step 740 Train loss 1.561401 on epoch=92
05/31/2022 12:25:21 - INFO - __main__ - Step 750 Global step 750 Train loss 1.410788 on epoch=93
05/31/2022 12:25:50 - INFO - __main__ - Global step 750 Train loss 1.708588 Classification-F1 0.18329157331668458 on epoch=93
05/31/2022 12:25:55 - INFO - __main__ - Step 760 Global step 760 Train loss 1.389706 on epoch=94
05/31/2022 12:26:00 - INFO - __main__ - Step 770 Global step 770 Train loss 1.062707 on epoch=96
05/31/2022 12:26:05 - INFO - __main__ - Step 780 Global step 780 Train loss 1.049762 on epoch=97
05/31/2022 12:26:10 - INFO - __main__ - Step 790 Global step 790 Train loss 1.158041 on epoch=98
05/31/2022 12:26:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.860160 on epoch=99
05/31/2022 12:26:16 - INFO - __main__ - Global step 800 Train loss 1.104075 Classification-F1 0.6214507487048471 on epoch=99
05/31/2022 12:26:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.818566 on epoch=101
05/31/2022 12:26:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.653025 on epoch=102
05/31/2022 12:26:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.647645 on epoch=103
05/31/2022 12:26:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.572088 on epoch=104
05/31/2022 12:26:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.597186 on epoch=106
05/31/2022 12:26:44 - INFO - __main__ - Global step 850 Train loss 0.657702 Classification-F1 0.6698462057837057 on epoch=106
05/31/2022 12:26:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.623914 on epoch=107
05/31/2022 12:26:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.504699 on epoch=108
05/31/2022 12:27:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.539649 on epoch=109
05/31/2022 12:27:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.497027 on epoch=111
05/31/2022 12:27:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.375186 on epoch=112
05/31/2022 12:27:11 - INFO - __main__ - Global step 900 Train loss 0.508095 Classification-F1 0.6835414716385826 on epoch=112
05/31/2022 12:27:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.493466 on epoch=113
05/31/2022 12:27:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.398990 on epoch=114
05/31/2022 12:27:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.339782 on epoch=116
05/31/2022 12:27:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.430004 on epoch=117
05/31/2022 12:27:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.408068 on epoch=118
05/31/2022 12:27:38 - INFO - __main__ - Global step 950 Train loss 0.414062 Classification-F1 0.6898821702726626 on epoch=118
05/31/2022 12:27:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.330492 on epoch=119
05/31/2022 12:27:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.399827 on epoch=121
05/31/2022 12:27:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.379671 on epoch=122
05/31/2022 12:28:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.445332 on epoch=123
05/31/2022 12:28:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.695882 on epoch=124
05/31/2022 12:28:06 - INFO - __main__ - Global step 1000 Train loss 0.450241 Classification-F1 0.5543318567737172 on epoch=124
05/31/2022 12:28:06 - INFO - __main__ - save last model!
05/31/2022 12:28:06 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:28:06 - INFO - __main__ - Printing 3 examples
05/31/2022 12:28:06 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/31/2022 12:28:06 - INFO - __main__ - ['others']
05/31/2022 12:28:06 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/31/2022 12:28:06 - INFO - __main__ - ['others']
05/31/2022 12:28:06 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/31/2022 12:28:06 - INFO - __main__ - ['others']
05/31/2022 12:28:06 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:28:06 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:28:06 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:28:06 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:28:06 - INFO - __main__ - Printing 3 examples
05/31/2022 12:28:06 - INFO - __main__ -  [emo] yes i can see clearly now i am sleepy
05/31/2022 12:28:06 - INFO - __main__ - ['others']
05/31/2022 12:28:06 - INFO - __main__ -  [emo] oh what the timr don't forget the one behind the knee time
05/31/2022 12:28:06 - INFO - __main__ - ['others']
05/31/2022 12:28:06 - INFO - __main__ -  [emo] almost robots are dangerous and inhuman how exactly would robots take over the planet and you are one of them
05/31/2022 12:28:06 - INFO - __main__ - ['others']
05/31/2022 12:28:06 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:28:06 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:28:06 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:28:13 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 12:28:14 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 12:28:14 - INFO - __main__ - Printing 3 examples
05/31/2022 12:28:14 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 12:28:14 - INFO - __main__ - ['others']
05/31/2022 12:28:14 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 12:28:14 - INFO - __main__ - ['others']
05/31/2022 12:28:14 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 12:28:14 - INFO - __main__ - ['others']
05/31/2022 12:28:14 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:28:16 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:28:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:28:17 - INFO - __main__ - Starting training!
05/31/2022 12:28:21 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 12:29:04 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_42_0.0001_8_predictions.txt
05/31/2022 12:29:04 - INFO - __main__ - Classification-F1 on test data: 0.4568
05/31/2022 12:29:04 - INFO - __main__ - prefix=emo_32_42, lr=0.0001, bsz=8, dev_performance=0.6898821702726626, test_performance=0.4567793124304934
05/31/2022 12:29:04 - INFO - __main__ - Running ... prefix=emo_32_87, lr=0.0005, bsz=8 ...
05/31/2022 12:29:05 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:29:05 - INFO - __main__ - Printing 3 examples
05/31/2022 12:29:05 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/31/2022 12:29:05 - INFO - __main__ - ['others']
05/31/2022 12:29:05 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/31/2022 12:29:05 - INFO - __main__ - ['others']
05/31/2022 12:29:05 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/31/2022 12:29:05 - INFO - __main__ - ['others']
05/31/2022 12:29:05 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:29:05 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:29:05 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:29:05 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:29:05 - INFO - __main__ - Printing 3 examples
05/31/2022 12:29:05 - INFO - __main__ -  [emo] yes i can see clearly now i am sleepy
05/31/2022 12:29:05 - INFO - __main__ - ['others']
05/31/2022 12:29:05 - INFO - __main__ -  [emo] oh what the timr don't forget the one behind the knee time
05/31/2022 12:29:05 - INFO - __main__ - ['others']
05/31/2022 12:29:05 - INFO - __main__ -  [emo] almost robots are dangerous and inhuman how exactly would robots take over the planet and you are one of them
05/31/2022 12:29:05 - INFO - __main__ - ['others']
05/31/2022 12:29:05 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:29:05 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:29:06 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:29:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:29:18 - INFO - __main__ - Starting training!
05/31/2022 12:29:23 - INFO - __main__ - Step 10 Global step 10 Train loss 24.638224 on epoch=1
05/31/2022 12:29:28 - INFO - __main__ - Step 20 Global step 20 Train loss 19.089058 on epoch=2
05/31/2022 12:29:33 - INFO - __main__ - Step 30 Global step 30 Train loss 16.590443 on epoch=3
05/31/2022 12:29:38 - INFO - __main__ - Step 40 Global step 40 Train loss 14.500183 on epoch=4
05/31/2022 12:29:43 - INFO - __main__ - Step 50 Global step 50 Train loss 13.361593 on epoch=6
05/31/2022 12:29:45 - INFO - __main__ - Global step 50 Train loss 17.635900 Classification-F1 0.0 on epoch=6
05/31/2022 12:29:51 - INFO - __main__ - Step 60 Global step 60 Train loss 11.975817 on epoch=7
05/31/2022 12:29:56 - INFO - __main__ - Step 70 Global step 70 Train loss 11.468238 on epoch=8
05/31/2022 12:30:01 - INFO - __main__ - Step 80 Global step 80 Train loss 7.661471 on epoch=9
05/31/2022 12:30:06 - INFO - __main__ - Step 90 Global step 90 Train loss 5.947341 on epoch=11
05/31/2022 12:30:11 - INFO - __main__ - Step 100 Global step 100 Train loss 3.294340 on epoch=12
05/31/2022 12:30:12 - INFO - __main__ - Global step 100 Train loss 8.069442 Classification-F1 0.18218623481781374 on epoch=12
05/31/2022 12:30:18 - INFO - __main__ - Step 110 Global step 110 Train loss 1.450935 on epoch=13
05/31/2022 12:30:23 - INFO - __main__ - Step 120 Global step 120 Train loss 1.483212 on epoch=14
05/31/2022 12:30:28 - INFO - __main__ - Step 130 Global step 130 Train loss 1.862560 on epoch=16
05/31/2022 12:30:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.068702 on epoch=17
05/31/2022 12:30:38 - INFO - __main__ - Step 150 Global step 150 Train loss 1.008225 on epoch=18
05/31/2022 12:30:39 - INFO - __main__ - Global step 150 Train loss 1.374727 Classification-F1 0.2208126868318387 on epoch=18
05/31/2022 12:30:45 - INFO - __main__ - Step 160 Global step 160 Train loss 1.231255 on epoch=19
05/31/2022 12:30:50 - INFO - __main__ - Step 170 Global step 170 Train loss 1.017839 on epoch=21
05/31/2022 12:30:55 - INFO - __main__ - Step 180 Global step 180 Train loss 1.115891 on epoch=22
05/31/2022 12:31:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.984255 on epoch=23
05/31/2022 12:31:05 - INFO - __main__ - Step 200 Global step 200 Train loss 1.082635 on epoch=24
05/31/2022 12:31:06 - INFO - __main__ - Global step 200 Train loss 1.086375 Classification-F1 0.18868856273879492 on epoch=24
05/31/2022 12:31:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.845648 on epoch=26
05/31/2022 12:31:16 - INFO - __main__ - Step 220 Global step 220 Train loss 1.018579 on epoch=27
05/31/2022 12:31:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.727600 on epoch=28
05/31/2022 12:31:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.920368 on epoch=29
05/31/2022 12:31:32 - INFO - __main__ - Step 250 Global step 250 Train loss 1.056555 on epoch=31
05/31/2022 12:31:33 - INFO - __main__ - Global step 250 Train loss 0.913750 Classification-F1 0.24032976827094474 on epoch=31
05/31/2022 12:31:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.758802 on epoch=32
05/31/2022 12:31:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.788107 on epoch=33
05/31/2022 12:31:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.752537 on epoch=34
05/31/2022 12:31:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.816353 on epoch=36
05/31/2022 12:31:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.710930 on epoch=37
05/31/2022 12:32:00 - INFO - __main__ - Global step 300 Train loss 0.765346 Classification-F1 0.33692008879023305 on epoch=37
05/31/2022 12:32:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.797657 on epoch=38
05/31/2022 12:32:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.681740 on epoch=39
05/31/2022 12:32:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.719536 on epoch=41
05/31/2022 12:32:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.620983 on epoch=42
05/31/2022 12:32:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.683612 on epoch=43
05/31/2022 12:32:27 - INFO - __main__ - Global step 350 Train loss 0.700706 Classification-F1 0.272274322169059 on epoch=43
05/31/2022 12:32:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.736974 on epoch=44
05/31/2022 12:32:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.703688 on epoch=46
05/31/2022 12:32:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.663300 on epoch=47
05/31/2022 12:32:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.658382 on epoch=48
05/31/2022 12:32:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.713588 on epoch=49
05/31/2022 12:32:53 - INFO - __main__ - Global step 400 Train loss 0.695186 Classification-F1 0.24921090387374462 on epoch=49
05/31/2022 12:32:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.679302 on epoch=51
05/31/2022 12:33:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.677524 on epoch=52
05/31/2022 12:33:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.608090 on epoch=53
05/31/2022 12:33:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.666677 on epoch=54
05/31/2022 12:33:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.671483 on epoch=56
05/31/2022 12:33:20 - INFO - __main__ - Global step 450 Train loss 0.660615 Classification-F1 0.26114872125510424 on epoch=56
05/31/2022 12:33:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.618299 on epoch=57
05/31/2022 12:33:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.697677 on epoch=58
05/31/2022 12:33:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.570442 on epoch=59
05/31/2022 12:33:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.556104 on epoch=61
05/31/2022 12:33:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.545610 on epoch=62
05/31/2022 12:33:46 - INFO - __main__ - Global step 500 Train loss 0.597626 Classification-F1 0.24569141550273627 on epoch=62
05/31/2022 12:33:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.595503 on epoch=63
05/31/2022 12:33:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.570624 on epoch=64
05/31/2022 12:34:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.603527 on epoch=66
05/31/2022 12:34:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.513258 on epoch=67
05/31/2022 12:34:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.546227 on epoch=68
05/31/2022 12:34:13 - INFO - __main__ - Global step 550 Train loss 0.565828 Classification-F1 0.3424573001511699 on epoch=68
05/31/2022 12:34:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.505267 on epoch=69
05/31/2022 12:34:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.535377 on epoch=71
05/31/2022 12:34:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.431116 on epoch=72
05/31/2022 12:34:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.423243 on epoch=73
05/31/2022 12:34:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.473814 on epoch=74
05/31/2022 12:34:40 - INFO - __main__ - Global step 600 Train loss 0.473763 Classification-F1 0.42555860805860807 on epoch=74
05/31/2022 12:34:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.445161 on epoch=76
05/31/2022 12:34:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.468774 on epoch=77
05/31/2022 12:34:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.433600 on epoch=78
05/31/2022 12:35:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.354906 on epoch=79
05/31/2022 12:35:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.264799 on epoch=81
05/31/2022 12:35:08 - INFO - __main__ - Global step 650 Train loss 0.393448 Classification-F1 0.5488882164162792 on epoch=81
05/31/2022 12:35:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.287364 on epoch=82
05/31/2022 12:35:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.306663 on epoch=83
05/31/2022 12:35:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.255394 on epoch=84
05/31/2022 12:35:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.290407 on epoch=86
05/31/2022 12:35:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.180926 on epoch=87
05/31/2022 12:35:35 - INFO - __main__ - Global step 700 Train loss 0.264151 Classification-F1 0.6436687021861025 on epoch=87
05/31/2022 12:35:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.194799 on epoch=88
05/31/2022 12:35:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.137985 on epoch=89
05/31/2022 12:35:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.146304 on epoch=91
05/31/2022 12:35:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.164686 on epoch=92
05/31/2022 12:36:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.092694 on epoch=93
05/31/2022 12:36:02 - INFO - __main__ - Global step 750 Train loss 0.147293 Classification-F1 0.6487262550881954 on epoch=93
05/31/2022 12:36:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.086966 on epoch=94
05/31/2022 12:36:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.108174 on epoch=96
05/31/2022 12:36:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.161066 on epoch=97
05/31/2022 12:36:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.129893 on epoch=98
05/31/2022 12:36:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.194535 on epoch=99
05/31/2022 12:36:30 - INFO - __main__ - Global step 800 Train loss 0.136127 Classification-F1 0.7183056248684725 on epoch=99
05/31/2022 12:36:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.071842 on epoch=101
05/31/2022 12:36:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.034594 on epoch=102
05/31/2022 12:36:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.037086 on epoch=103
05/31/2022 12:36:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.018571 on epoch=104
05/31/2022 12:36:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.045301 on epoch=106
05/31/2022 12:36:57 - INFO - __main__ - Global step 850 Train loss 0.041479 Classification-F1 0.6375204874355977 on epoch=106
05/31/2022 12:37:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.061922 on epoch=107
05/31/2022 12:37:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.086006 on epoch=108
05/31/2022 12:37:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.132208 on epoch=109
05/31/2022 12:37:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.029433 on epoch=111
05/31/2022 12:37:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.072025 on epoch=112
05/31/2022 12:37:24 - INFO - __main__ - Global step 900 Train loss 0.076319 Classification-F1 0.6441199517781795 on epoch=112
05/31/2022 12:37:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.011559 on epoch=113
05/31/2022 12:37:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.032734 on epoch=114
05/31/2022 12:37:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.076409 on epoch=116
05/31/2022 12:37:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.025847 on epoch=117
05/31/2022 12:37:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.004426 on epoch=118
05/31/2022 12:37:51 - INFO - __main__ - Global step 950 Train loss 0.030195 Classification-F1 0.6613078595837216 on epoch=118
05/31/2022 12:37:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.034418 on epoch=119
05/31/2022 12:38:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.068217 on epoch=121
05/31/2022 12:38:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.006900 on epoch=122
05/31/2022 12:38:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.007235 on epoch=123
05/31/2022 12:38:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.008749 on epoch=124
05/31/2022 12:38:17 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:38:17 - INFO - __main__ - Printing 3 examples
05/31/2022 12:38:17 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/31/2022 12:38:17 - INFO - __main__ - ['others']
05/31/2022 12:38:17 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/31/2022 12:38:17 - INFO - __main__ - ['others']
05/31/2022 12:38:17 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/31/2022 12:38:17 - INFO - __main__ - ['others']
05/31/2022 12:38:17 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:38:17 - INFO - __main__ - Global step 1000 Train loss 0.025104 Classification-F1 0.6066188649977979 on epoch=124
05/31/2022 12:38:17 - INFO - __main__ - save last model!
05/31/2022 12:38:17 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:38:18 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:38:18 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:38:18 - INFO - __main__ - Printing 3 examples
05/31/2022 12:38:18 - INFO - __main__ -  [emo] yes i can see clearly now i am sleepy
05/31/2022 12:38:18 - INFO - __main__ - ['others']
05/31/2022 12:38:18 - INFO - __main__ -  [emo] oh what the timr don't forget the one behind the knee time
05/31/2022 12:38:18 - INFO - __main__ - ['others']
05/31/2022 12:38:18 - INFO - __main__ -  [emo] almost robots are dangerous and inhuman how exactly would robots take over the planet and you are one of them
05/31/2022 12:38:18 - INFO - __main__ - ['others']
05/31/2022 12:38:18 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:38:18 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:38:18 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:38:24 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 12:38:25 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 12:38:25 - INFO - __main__ - Printing 3 examples
05/31/2022 12:38:25 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 12:38:25 - INFO - __main__ - ['others']
05/31/2022 12:38:25 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 12:38:25 - INFO - __main__ - ['others']
05/31/2022 12:38:25 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 12:38:25 - INFO - __main__ - ['others']
05/31/2022 12:38:25 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:38:27 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:38:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:38:29 - INFO - __main__ - Starting training!
05/31/2022 12:38:33 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 12:39:18 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_87_0.0005_8_predictions.txt
05/31/2022 12:39:18 - INFO - __main__ - Classification-F1 on test data: 0.1542
05/31/2022 12:39:19 - INFO - __main__ - prefix=emo_32_87, lr=0.0005, bsz=8, dev_performance=0.7183056248684725, test_performance=0.15422572923619146
05/31/2022 12:39:19 - INFO - __main__ - Running ... prefix=emo_32_87, lr=0.0003, bsz=8 ...
05/31/2022 12:39:20 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:39:20 - INFO - __main__ - Printing 3 examples
05/31/2022 12:39:20 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/31/2022 12:39:20 - INFO - __main__ - ['others']
05/31/2022 12:39:20 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/31/2022 12:39:20 - INFO - __main__ - ['others']
05/31/2022 12:39:20 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/31/2022 12:39:20 - INFO - __main__ - ['others']
05/31/2022 12:39:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:39:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:39:20 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:39:20 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:39:20 - INFO - __main__ - Printing 3 examples
05/31/2022 12:39:20 - INFO - __main__ -  [emo] yes i can see clearly now i am sleepy
05/31/2022 12:39:20 - INFO - __main__ - ['others']
05/31/2022 12:39:20 - INFO - __main__ -  [emo] oh what the timr don't forget the one behind the knee time
05/31/2022 12:39:20 - INFO - __main__ - ['others']
05/31/2022 12:39:20 - INFO - __main__ -  [emo] almost robots are dangerous and inhuman how exactly would robots take over the planet and you are one of them
05/31/2022 12:39:20 - INFO - __main__ - ['others']
05/31/2022 12:39:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:39:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:39:20 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:39:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:39:33 - INFO - __main__ - Starting training!
05/31/2022 12:39:37 - INFO - __main__ - Step 10 Global step 10 Train loss 23.792194 on epoch=1
05/31/2022 12:39:42 - INFO - __main__ - Step 20 Global step 20 Train loss 20.016567 on epoch=2
05/31/2022 12:39:47 - INFO - __main__ - Step 30 Global step 30 Train loss 17.752274 on epoch=3
05/31/2022 12:39:52 - INFO - __main__ - Step 40 Global step 40 Train loss 15.852652 on epoch=4
05/31/2022 12:39:57 - INFO - __main__ - Step 50 Global step 50 Train loss 14.995567 on epoch=6
05/31/2022 12:39:59 - INFO - __main__ - Global step 50 Train loss 18.481850 Classification-F1 0.0 on epoch=6
05/31/2022 12:40:04 - INFO - __main__ - Step 60 Global step 60 Train loss 14.552748 on epoch=7
05/31/2022 12:40:09 - INFO - __main__ - Step 70 Global step 70 Train loss 13.337778 on epoch=8
05/31/2022 12:40:15 - INFO - __main__ - Step 80 Global step 80 Train loss 12.677724 on epoch=9
05/31/2022 12:40:20 - INFO - __main__ - Step 90 Global step 90 Train loss 12.006783 on epoch=11
05/31/2022 12:40:25 - INFO - __main__ - Step 100 Global step 100 Train loss 11.131166 on epoch=12
05/31/2022 12:40:26 - INFO - __main__ - Global step 100 Train loss 12.741239 Classification-F1 0.0 on epoch=12
05/31/2022 12:40:31 - INFO - __main__ - Step 110 Global step 110 Train loss 10.140127 on epoch=13
05/31/2022 12:40:36 - INFO - __main__ - Step 120 Global step 120 Train loss 6.447823 on epoch=14
05/31/2022 12:40:41 - INFO - __main__ - Step 130 Global step 130 Train loss 4.765490 on epoch=16
05/31/2022 12:40:46 - INFO - __main__ - Step 140 Global step 140 Train loss 4.745814 on epoch=17
05/31/2022 12:40:51 - INFO - __main__ - Step 150 Global step 150 Train loss 4.220494 on epoch=18
05/31/2022 12:40:52 - INFO - __main__ - Global step 150 Train loss 6.063950 Classification-F1 0.1 on epoch=18
05/31/2022 12:40:57 - INFO - __main__ - Step 160 Global step 160 Train loss 3.450570 on epoch=19
05/31/2022 12:41:02 - INFO - __main__ - Step 170 Global step 170 Train loss 4.469640 on epoch=21
05/31/2022 12:41:08 - INFO - __main__ - Step 180 Global step 180 Train loss 3.339054 on epoch=22
05/31/2022 12:41:13 - INFO - __main__ - Step 190 Global step 190 Train loss 3.398497 on epoch=23
05/31/2022 12:41:18 - INFO - __main__ - Step 200 Global step 200 Train loss 3.415029 on epoch=24
05/31/2022 12:41:19 - INFO - __main__ - Global step 200 Train loss 3.614558 Classification-F1 0.1 on epoch=24
05/31/2022 12:41:24 - INFO - __main__ - Step 210 Global step 210 Train loss 3.240299 on epoch=26
05/31/2022 12:41:29 - INFO - __main__ - Step 220 Global step 220 Train loss 3.226035 on epoch=27
05/31/2022 12:41:34 - INFO - __main__ - Step 230 Global step 230 Train loss 2.965346 on epoch=28
05/31/2022 12:41:39 - INFO - __main__ - Step 240 Global step 240 Train loss 3.186327 on epoch=29
05/31/2022 12:41:44 - INFO - __main__ - Step 250 Global step 250 Train loss 3.401513 on epoch=31
05/31/2022 12:41:45 - INFO - __main__ - Global step 250 Train loss 3.203904 Classification-F1 0.1 on epoch=31
05/31/2022 12:41:50 - INFO - __main__ - Step 260 Global step 260 Train loss 2.682036 on epoch=32
05/31/2022 12:41:55 - INFO - __main__ - Step 270 Global step 270 Train loss 2.427494 on epoch=33
05/31/2022 12:42:00 - INFO - __main__ - Step 280 Global step 280 Train loss 3.017461 on epoch=34
05/31/2022 12:42:05 - INFO - __main__ - Step 290 Global step 290 Train loss 2.831304 on epoch=36
05/31/2022 12:42:11 - INFO - __main__ - Step 300 Global step 300 Train loss 2.948669 on epoch=37
05/31/2022 12:42:12 - INFO - __main__ - Global step 300 Train loss 2.781393 Classification-F1 0.1507936507936508 on epoch=37
05/31/2022 12:42:17 - INFO - __main__ - Step 310 Global step 310 Train loss 2.093273 on epoch=38
05/31/2022 12:42:22 - INFO - __main__ - Step 320 Global step 320 Train loss 2.093064 on epoch=39
05/31/2022 12:42:27 - INFO - __main__ - Step 330 Global step 330 Train loss 2.167923 on epoch=41
05/31/2022 12:42:32 - INFO - __main__ - Step 340 Global step 340 Train loss 2.221652 on epoch=42
05/31/2022 12:42:38 - INFO - __main__ - Step 350 Global step 350 Train loss 1.897107 on epoch=43
05/31/2022 12:42:39 - INFO - __main__ - Global step 350 Train loss 2.094604 Classification-F1 0.1 on epoch=43
05/31/2022 12:42:44 - INFO - __main__ - Step 360 Global step 360 Train loss 1.626979 on epoch=44
05/31/2022 12:42:49 - INFO - __main__ - Step 370 Global step 370 Train loss 1.446131 on epoch=46
05/31/2022 12:42:54 - INFO - __main__ - Step 380 Global step 380 Train loss 1.420987 on epoch=47
05/31/2022 12:42:59 - INFO - __main__ - Step 390 Global step 390 Train loss 1.342553 on epoch=48
05/31/2022 12:43:04 - INFO - __main__ - Step 400 Global step 400 Train loss 1.200052 on epoch=49
05/31/2022 12:43:05 - INFO - __main__ - Global step 400 Train loss 1.407340 Classification-F1 0.19733044733044736 on epoch=49
05/31/2022 12:43:11 - INFO - __main__ - Step 410 Global step 410 Train loss 1.347208 on epoch=51
05/31/2022 12:43:16 - INFO - __main__ - Step 420 Global step 420 Train loss 1.177390 on epoch=52
05/31/2022 12:43:21 - INFO - __main__ - Step 430 Global step 430 Train loss 1.083460 on epoch=53
05/31/2022 12:43:26 - INFO - __main__ - Step 440 Global step 440 Train loss 1.063507 on epoch=54
05/31/2022 12:43:31 - INFO - __main__ - Step 450 Global step 450 Train loss 1.165694 on epoch=56
05/31/2022 12:43:32 - INFO - __main__ - Global step 450 Train loss 1.167452 Classification-F1 0.1 on epoch=56
05/31/2022 12:43:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.877711 on epoch=57
05/31/2022 12:43:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.946776 on epoch=58
05/31/2022 12:43:47 - INFO - __main__ - Step 480 Global step 480 Train loss 1.022433 on epoch=59
05/31/2022 12:43:52 - INFO - __main__ - Step 490 Global step 490 Train loss 1.091218 on epoch=61
05/31/2022 12:43:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.976598 on epoch=62
05/31/2022 12:43:58 - INFO - __main__ - Global step 500 Train loss 0.982947 Classification-F1 0.1304822565969063 on epoch=62
05/31/2022 12:44:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.988794 on epoch=63
05/31/2022 12:44:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.810788 on epoch=64
05/31/2022 12:44:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.913316 on epoch=66
05/31/2022 12:44:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.996991 on epoch=67
05/31/2022 12:44:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.862106 on epoch=68
05/31/2022 12:44:25 - INFO - __main__ - Global step 550 Train loss 0.914399 Classification-F1 0.2631451940662467 on epoch=68
05/31/2022 12:44:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.924589 on epoch=69
05/31/2022 12:44:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.886450 on epoch=71
05/31/2022 12:44:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.995793 on epoch=72
05/31/2022 12:44:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.861872 on epoch=73
05/31/2022 12:44:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.812209 on epoch=74
05/31/2022 12:44:52 - INFO - __main__ - Global step 600 Train loss 0.896183 Classification-F1 0.12170329670329669 on epoch=74
05/31/2022 12:44:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.978831 on epoch=76
05/31/2022 12:45:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.877555 on epoch=77
05/31/2022 12:45:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.869469 on epoch=78
05/31/2022 12:45:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.792965 on epoch=79
05/31/2022 12:45:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.861423 on epoch=81
05/31/2022 12:45:19 - INFO - __main__ - Global step 650 Train loss 0.876049 Classification-F1 0.13293650793650794 on epoch=81
05/31/2022 12:45:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.816079 on epoch=82
05/31/2022 12:45:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.805209 on epoch=83
05/31/2022 12:45:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.945422 on epoch=84
05/31/2022 12:45:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.822264 on epoch=86
05/31/2022 12:45:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.924244 on epoch=87
05/31/2022 12:45:45 - INFO - __main__ - Global step 700 Train loss 0.862644 Classification-F1 0.11888159693738037 on epoch=87
05/31/2022 12:45:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.889198 on epoch=88
05/31/2022 12:45:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.822425 on epoch=89
05/31/2022 12:46:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.854491 on epoch=91
05/31/2022 12:46:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.816164 on epoch=92
05/31/2022 12:46:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.805399 on epoch=93
05/31/2022 12:46:11 - INFO - __main__ - Global step 750 Train loss 0.837536 Classification-F1 0.26437847866419295 on epoch=93
05/31/2022 12:46:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.881573 on epoch=94
05/31/2022 12:46:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.830186 on epoch=96
05/31/2022 12:46:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.802614 on epoch=97
05/31/2022 12:46:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.814587 on epoch=98
05/31/2022 12:46:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.928816 on epoch=99
05/31/2022 12:46:38 - INFO - __main__ - Global step 800 Train loss 0.851555 Classification-F1 0.2414388161141408 on epoch=99
05/31/2022 12:46:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.810558 on epoch=101
05/31/2022 12:46:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.738653 on epoch=102
05/31/2022 12:46:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.768094 on epoch=103
05/31/2022 12:46:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.786300 on epoch=104
05/31/2022 12:47:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.830023 on epoch=106
05/31/2022 12:47:05 - INFO - __main__ - Global step 850 Train loss 0.786726 Classification-F1 0.22649667405764967 on epoch=106
05/31/2022 12:47:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.792815 on epoch=107
05/31/2022 12:47:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.742922 on epoch=108
05/31/2022 12:47:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.723612 on epoch=109
05/31/2022 12:47:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.762975 on epoch=111
05/31/2022 12:47:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.778331 on epoch=112
05/31/2022 12:47:32 - INFO - __main__ - Global step 900 Train loss 0.760131 Classification-F1 0.353027786167321 on epoch=112
05/31/2022 12:47:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.791864 on epoch=113
05/31/2022 12:47:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.783716 on epoch=114
05/31/2022 12:47:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.783657 on epoch=116
05/31/2022 12:47:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.756592 on epoch=117
05/31/2022 12:47:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.727973 on epoch=118
05/31/2022 12:47:59 - INFO - __main__ - Global step 950 Train loss 0.768761 Classification-F1 0.3092532467532468 on epoch=118
05/31/2022 12:48:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.786142 on epoch=119
05/31/2022 12:48:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.733996 on epoch=121
05/31/2022 12:48:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.786782 on epoch=122
05/31/2022 12:48:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.764375 on epoch=123
05/31/2022 12:48:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.739029 on epoch=124
05/31/2022 12:48:26 - INFO - __main__ - Global step 1000 Train loss 0.762065 Classification-F1 0.25583369931196015 on epoch=124
05/31/2022 12:48:26 - INFO - __main__ - save last model!
05/31/2022 12:48:26 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:48:26 - INFO - __main__ - Printing 3 examples
05/31/2022 12:48:26 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/31/2022 12:48:26 - INFO - __main__ - ['others']
05/31/2022 12:48:26 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/31/2022 12:48:26 - INFO - __main__ - ['others']
05/31/2022 12:48:26 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/31/2022 12:48:26 - INFO - __main__ - ['others']
05/31/2022 12:48:26 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:48:26 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:48:26 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:48:26 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:48:26 - INFO - __main__ - Printing 3 examples
05/31/2022 12:48:26 - INFO - __main__ -  [emo] yes i can see clearly now i am sleepy
05/31/2022 12:48:26 - INFO - __main__ - ['others']
05/31/2022 12:48:26 - INFO - __main__ -  [emo] oh what the timr don't forget the one behind the knee time
05/31/2022 12:48:26 - INFO - __main__ - ['others']
05/31/2022 12:48:26 - INFO - __main__ -  [emo] almost robots are dangerous and inhuman how exactly would robots take over the planet and you are one of them
05/31/2022 12:48:26 - INFO - __main__ - ['others']
05/31/2022 12:48:26 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:48:26 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:48:27 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:48:33 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 12:48:33 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 12:48:33 - INFO - __main__ - Printing 3 examples
05/31/2022 12:48:33 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 12:48:33 - INFO - __main__ - ['others']
05/31/2022 12:48:33 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 12:48:33 - INFO - __main__ - ['others']
05/31/2022 12:48:33 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 12:48:33 - INFO - __main__ - ['others']
05/31/2022 12:48:33 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:48:35 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:48:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:48:37 - INFO - __main__ - Starting training!
05/31/2022 12:48:41 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 12:49:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_87_0.0003_8_predictions.txt
05/31/2022 12:49:23 - INFO - __main__ - Classification-F1 on test data: 0.1602
05/31/2022 12:49:23 - INFO - __main__ - prefix=emo_32_87, lr=0.0003, bsz=8, dev_performance=0.353027786167321, test_performance=0.16023722670733037
05/31/2022 12:49:23 - INFO - __main__ - Running ... prefix=emo_32_87, lr=0.0002, bsz=8 ...
05/31/2022 12:49:24 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:49:24 - INFO - __main__ - Printing 3 examples
05/31/2022 12:49:24 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/31/2022 12:49:24 - INFO - __main__ - ['others']
05/31/2022 12:49:24 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/31/2022 12:49:24 - INFO - __main__ - ['others']
05/31/2022 12:49:24 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/31/2022 12:49:24 - INFO - __main__ - ['others']
05/31/2022 12:49:24 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:49:24 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:49:24 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:49:24 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:49:24 - INFO - __main__ - Printing 3 examples
05/31/2022 12:49:24 - INFO - __main__ -  [emo] yes i can see clearly now i am sleepy
05/31/2022 12:49:24 - INFO - __main__ - ['others']
05/31/2022 12:49:24 - INFO - __main__ -  [emo] oh what the timr don't forget the one behind the knee time
05/31/2022 12:49:24 - INFO - __main__ - ['others']
05/31/2022 12:49:24 - INFO - __main__ -  [emo] almost robots are dangerous and inhuman how exactly would robots take over the planet and you are one of them
05/31/2022 12:49:24 - INFO - __main__ - ['others']
05/31/2022 12:49:24 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:49:24 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:49:25 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:49:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:49:37 - INFO - __main__ - Starting training!
05/31/2022 12:49:42 - INFO - __main__ - Step 10 Global step 10 Train loss 25.111423 on epoch=1
05/31/2022 12:49:47 - INFO - __main__ - Step 20 Global step 20 Train loss 21.536053 on epoch=2
05/31/2022 12:49:52 - INFO - __main__ - Step 30 Global step 30 Train loss 18.657579 on epoch=3
05/31/2022 12:49:57 - INFO - __main__ - Step 40 Global step 40 Train loss 17.644308 on epoch=4
05/31/2022 12:50:02 - INFO - __main__ - Step 50 Global step 50 Train loss 16.928782 on epoch=6
05/31/2022 12:50:04 - INFO - __main__ - Global step 50 Train loss 19.975630 Classification-F1 0.0 on epoch=6
05/31/2022 12:50:09 - INFO - __main__ - Step 60 Global step 60 Train loss 15.915445 on epoch=7
05/31/2022 12:50:14 - INFO - __main__ - Step 70 Global step 70 Train loss 15.946871 on epoch=8
05/31/2022 12:50:19 - INFO - __main__ - Step 80 Global step 80 Train loss 14.900045 on epoch=9
05/31/2022 12:50:25 - INFO - __main__ - Step 90 Global step 90 Train loss 15.154866 on epoch=11
05/31/2022 12:50:30 - INFO - __main__ - Step 100 Global step 100 Train loss 14.093791 on epoch=12
05/31/2022 12:50:32 - INFO - __main__ - Global step 100 Train loss 15.202204 Classification-F1 0.0 on epoch=12
05/31/2022 12:50:37 - INFO - __main__ - Step 110 Global step 110 Train loss 12.675776 on epoch=13
05/31/2022 12:50:42 - INFO - __main__ - Step 120 Global step 120 Train loss 12.057786 on epoch=14
05/31/2022 12:50:47 - INFO - __main__ - Step 130 Global step 130 Train loss 11.722259 on epoch=16
05/31/2022 12:50:52 - INFO - __main__ - Step 140 Global step 140 Train loss 10.897008 on epoch=17
05/31/2022 12:50:57 - INFO - __main__ - Step 150 Global step 150 Train loss 11.170027 on epoch=18
05/31/2022 12:50:59 - INFO - __main__ - Global step 150 Train loss 11.704572 Classification-F1 0.0 on epoch=18
05/31/2022 12:51:04 - INFO - __main__ - Step 160 Global step 160 Train loss 9.811267 on epoch=19
05/31/2022 12:51:09 - INFO - __main__ - Step 170 Global step 170 Train loss 8.498184 on epoch=21
05/31/2022 12:51:14 - INFO - __main__ - Step 180 Global step 180 Train loss 7.777289 on epoch=22
05/31/2022 12:51:19 - INFO - __main__ - Step 190 Global step 190 Train loss 6.396338 on epoch=23
05/31/2022 12:51:24 - INFO - __main__ - Step 200 Global step 200 Train loss 4.546523 on epoch=24
05/31/2022 12:51:25 - INFO - __main__ - Global step 200 Train loss 7.405920 Classification-F1 0.1 on epoch=24
05/31/2022 12:51:31 - INFO - __main__ - Step 210 Global step 210 Train loss 3.417636 on epoch=26
05/31/2022 12:51:36 - INFO - __main__ - Step 220 Global step 220 Train loss 4.440972 on epoch=27
05/31/2022 12:51:41 - INFO - __main__ - Step 230 Global step 230 Train loss 2.717138 on epoch=28
05/31/2022 12:51:46 - INFO - __main__ - Step 240 Global step 240 Train loss 4.083063 on epoch=29
05/31/2022 12:51:51 - INFO - __main__ - Step 250 Global step 250 Train loss 2.948042 on epoch=31
05/31/2022 12:51:52 - INFO - __main__ - Global step 250 Train loss 3.521371 Classification-F1 0.1 on epoch=31
05/31/2022 12:51:57 - INFO - __main__ - Step 260 Global step 260 Train loss 3.117951 on epoch=32
05/31/2022 12:52:02 - INFO - __main__ - Step 270 Global step 270 Train loss 3.176749 on epoch=33
05/31/2022 12:52:07 - INFO - __main__ - Step 280 Global step 280 Train loss 3.316728 on epoch=34
05/31/2022 12:52:12 - INFO - __main__ - Step 290 Global step 290 Train loss 3.255412 on epoch=36
05/31/2022 12:52:17 - INFO - __main__ - Step 300 Global step 300 Train loss 3.248413 on epoch=37
05/31/2022 12:52:18 - INFO - __main__ - Global step 300 Train loss 3.223051 Classification-F1 0.23851417399804498 on epoch=37
05/31/2022 12:52:24 - INFO - __main__ - Step 310 Global step 310 Train loss 2.354644 on epoch=38
05/31/2022 12:52:29 - INFO - __main__ - Step 320 Global step 320 Train loss 2.822461 on epoch=39
05/31/2022 12:52:34 - INFO - __main__ - Step 330 Global step 330 Train loss 2.134877 on epoch=41
05/31/2022 12:52:39 - INFO - __main__ - Step 340 Global step 340 Train loss 2.668613 on epoch=42
05/31/2022 12:52:44 - INFO - __main__ - Step 350 Global step 350 Train loss 2.601620 on epoch=43
05/31/2022 12:52:45 - INFO - __main__ - Global step 350 Train loss 2.516443 Classification-F1 0.21245286447685247 on epoch=43
05/31/2022 12:52:50 - INFO - __main__ - Step 360 Global step 360 Train loss 2.351958 on epoch=44
05/31/2022 12:52:55 - INFO - __main__ - Step 370 Global step 370 Train loss 1.834127 on epoch=46
05/31/2022 12:53:01 - INFO - __main__ - Step 380 Global step 380 Train loss 1.903757 on epoch=47
05/31/2022 12:53:06 - INFO - __main__ - Step 390 Global step 390 Train loss 1.832332 on epoch=48
05/31/2022 12:53:11 - INFO - __main__ - Step 400 Global step 400 Train loss 1.551013 on epoch=49
05/31/2022 12:53:12 - INFO - __main__ - Global step 400 Train loss 1.894637 Classification-F1 0.14476797088262056 on epoch=49
05/31/2022 12:53:17 - INFO - __main__ - Step 410 Global step 410 Train loss 1.358104 on epoch=51
05/31/2022 12:53:22 - INFO - __main__ - Step 420 Global step 420 Train loss 1.275742 on epoch=52
05/31/2022 12:53:27 - INFO - __main__ - Step 430 Global step 430 Train loss 1.232921 on epoch=53
05/31/2022 12:53:32 - INFO - __main__ - Step 440 Global step 440 Train loss 1.094162 on epoch=54
05/31/2022 12:53:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.873769 on epoch=56
05/31/2022 12:53:38 - INFO - __main__ - Global step 450 Train loss 1.166940 Classification-F1 0.35465587044534413 on epoch=56
05/31/2022 12:53:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.857108 on epoch=57
05/31/2022 12:53:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.668651 on epoch=58
05/31/2022 12:53:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.709064 on epoch=59
05/31/2022 12:53:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.521147 on epoch=61
05/31/2022 12:54:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.663713 on epoch=62
05/31/2022 12:54:05 - INFO - __main__ - Global step 500 Train loss 0.683937 Classification-F1 0.5439996432239171 on epoch=62
05/31/2022 12:54:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.677733 on epoch=63
05/31/2022 12:54:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.614604 on epoch=64
05/31/2022 12:54:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.509068 on epoch=66
05/31/2022 12:54:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.362634 on epoch=67
05/31/2022 12:54:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.405980 on epoch=68
05/31/2022 12:54:32 - INFO - __main__ - Global step 550 Train loss 0.514004 Classification-F1 0.6980260045477437 on epoch=68
05/31/2022 12:54:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.529755 on epoch=69
05/31/2022 12:54:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.448323 on epoch=71
05/31/2022 12:54:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.264938 on epoch=72
05/31/2022 12:54:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.262398 on epoch=73
05/31/2022 12:54:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.154937 on epoch=74
05/31/2022 12:54:59 - INFO - __main__ - Global step 600 Train loss 0.332070 Classification-F1 0.708157215673 on epoch=74
05/31/2022 12:55:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.212720 on epoch=76
05/31/2022 12:55:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.165227 on epoch=77
05/31/2022 12:55:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.167325 on epoch=78
05/31/2022 12:55:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.130272 on epoch=79
05/31/2022 12:55:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.101576 on epoch=81
05/31/2022 12:55:26 - INFO - __main__ - Global step 650 Train loss 0.155424 Classification-F1 0.7327750568754337 on epoch=81
05/31/2022 12:55:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.148287 on epoch=82
05/31/2022 12:55:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.096094 on epoch=83
05/31/2022 12:55:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.085850 on epoch=84
05/31/2022 12:55:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.093582 on epoch=86
05/31/2022 12:55:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.107889 on epoch=87
05/31/2022 12:55:53 - INFO - __main__ - Global step 700 Train loss 0.106340 Classification-F1 0.6859470776137443 on epoch=87
05/31/2022 12:55:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.054143 on epoch=88
05/31/2022 12:56:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.109418 on epoch=89
05/31/2022 12:56:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.037464 on epoch=91
05/31/2022 12:56:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.033141 on epoch=92
05/31/2022 12:56:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.029104 on epoch=93
05/31/2022 12:56:20 - INFO - __main__ - Global step 750 Train loss 0.052654 Classification-F1 0.7163646842692986 on epoch=93
05/31/2022 12:56:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.197009 on epoch=94
05/31/2022 12:56:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.032430 on epoch=96
05/31/2022 12:56:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.030310 on epoch=97
05/31/2022 12:56:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.079213 on epoch=98
05/31/2022 12:56:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.009443 on epoch=99
05/31/2022 12:56:47 - INFO - __main__ - Global step 800 Train loss 0.069681 Classification-F1 0.7254766717807233 on epoch=99
05/31/2022 12:56:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.049621 on epoch=101
05/31/2022 12:56:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.029015 on epoch=102
05/31/2022 12:57:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.014569 on epoch=103
05/31/2022 12:57:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.085894 on epoch=104
05/31/2022 12:57:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.017903 on epoch=106
05/31/2022 12:57:13 - INFO - __main__ - Global step 850 Train loss 0.039400 Classification-F1 0.7152361748971058 on epoch=106
05/31/2022 12:57:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.022477 on epoch=107
05/31/2022 12:57:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.015449 on epoch=108
05/31/2022 12:57:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.008252 on epoch=109
05/31/2022 12:57:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.011678 on epoch=111
05/31/2022 12:57:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.006923 on epoch=112
05/31/2022 12:57:40 - INFO - __main__ - Global step 900 Train loss 0.012956 Classification-F1 0.7269607007979101 on epoch=112
05/31/2022 12:57:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.027911 on epoch=113
05/31/2022 12:57:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.003522 on epoch=114
05/31/2022 12:57:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.011967 on epoch=116
05/31/2022 12:58:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.003116 on epoch=117
05/31/2022 12:58:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.012581 on epoch=118
05/31/2022 12:58:06 - INFO - __main__ - Global step 950 Train loss 0.011819 Classification-F1 0.3774177106362073 on epoch=118
05/31/2022 12:58:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.002611 on epoch=119
05/31/2022 12:58:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.009614 on epoch=121
05/31/2022 12:58:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.021897 on epoch=122
05/31/2022 12:58:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.004719 on epoch=123
05/31/2022 12:58:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.008828 on epoch=124
05/31/2022 12:58:33 - INFO - __main__ - Global step 1000 Train loss 0.009534 Classification-F1 0.7555200341005968 on epoch=124
05/31/2022 12:58:33 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:58:33 - INFO - __main__ - Printing 3 examples
05/31/2022 12:58:33 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/31/2022 12:58:33 - INFO - __main__ - ['others']
05/31/2022 12:58:33 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/31/2022 12:58:33 - INFO - __main__ - ['others']
05/31/2022 12:58:33 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/31/2022 12:58:33 - INFO - __main__ - ['others']
05/31/2022 12:58:33 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:58:33 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:58:33 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:58:33 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:58:33 - INFO - __main__ - Printing 3 examples
05/31/2022 12:58:33 - INFO - __main__ -  [emo] yes i can see clearly now i am sleepy
05/31/2022 12:58:33 - INFO - __main__ - ['others']
05/31/2022 12:58:33 - INFO - __main__ -  [emo] oh what the timr don't forget the one behind the knee time
05/31/2022 12:58:33 - INFO - __main__ - ['others']
05/31/2022 12:58:33 - INFO - __main__ -  [emo] almost robots are dangerous and inhuman how exactly would robots take over the planet and you are one of them
05/31/2022 12:58:33 - INFO - __main__ - ['others']
05/31/2022 12:58:33 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:58:33 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:58:33 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:58:33 - INFO - __main__ - save last model!
05/31/2022 12:58:40 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 12:58:41 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 12:58:41 - INFO - __main__ - Printing 3 examples
05/31/2022 12:58:41 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 12:58:41 - INFO - __main__ - ['others']
05/31/2022 12:58:41 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 12:58:41 - INFO - __main__ - ['others']
05/31/2022 12:58:41 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 12:58:41 - INFO - __main__ - ['others']
05/31/2022 12:58:41 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:58:43 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:58:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:58:45 - INFO - __main__ - Starting training!
05/31/2022 12:58:48 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 12:59:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_87_0.0002_8_predictions.txt
05/31/2022 12:59:31 - INFO - __main__ - Classification-F1 on test data: 0.3563
05/31/2022 12:59:31 - INFO - __main__ - prefix=emo_32_87, lr=0.0002, bsz=8, dev_performance=0.7555200341005968, test_performance=0.3562543760649908
05/31/2022 12:59:31 - INFO - __main__ - Running ... prefix=emo_32_87, lr=0.0001, bsz=8 ...
05/31/2022 12:59:32 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:59:32 - INFO - __main__ - Printing 3 examples
05/31/2022 12:59:32 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/31/2022 12:59:32 - INFO - __main__ - ['others']
05/31/2022 12:59:32 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/31/2022 12:59:32 - INFO - __main__ - ['others']
05/31/2022 12:59:32 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/31/2022 12:59:32 - INFO - __main__ - ['others']
05/31/2022 12:59:32 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:59:32 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:59:32 - INFO - __main__ - Loaded 128 examples from train data
05/31/2022 12:59:32 - INFO - __main__ - Start tokenizing ... 128 instances
05/31/2022 12:59:32 - INFO - __main__ - Printing 3 examples
05/31/2022 12:59:32 - INFO - __main__ -  [emo] yes i can see clearly now i am sleepy
05/31/2022 12:59:32 - INFO - __main__ - ['others']
05/31/2022 12:59:32 - INFO - __main__ -  [emo] oh what the timr don't forget the one behind the knee time
05/31/2022 12:59:32 - INFO - __main__ - ['others']
05/31/2022 12:59:32 - INFO - __main__ -  [emo] almost robots are dangerous and inhuman how exactly would robots take over the planet and you are one of them
05/31/2022 12:59:32 - INFO - __main__ - ['others']
05/31/2022 12:59:32 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:59:32 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:59:32 - INFO - __main__ - Loaded 128 examples from dev data
05/31/2022 12:59:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/31/2022 12:59:45 - INFO - __main__ - Starting training!
05/31/2022 12:59:49 - INFO - __main__ - Step 10 Global step 10 Train loss 26.525385 on epoch=1
05/31/2022 12:59:54 - INFO - __main__ - Step 20 Global step 20 Train loss 21.527294 on epoch=2
05/31/2022 12:59:59 - INFO - __main__ - Step 30 Global step 30 Train loss 19.822416 on epoch=3
05/31/2022 13:00:05 - INFO - __main__ - Step 40 Global step 40 Train loss 18.513248 on epoch=4
05/31/2022 13:00:10 - INFO - __main__ - Step 50 Global step 50 Train loss 17.441153 on epoch=6
05/31/2022 13:00:48 - INFO - __main__ - Global step 50 Train loss 20.765898 Classification-F1 0.0 on epoch=6
05/31/2022 13:00:54 - INFO - __main__ - Step 60 Global step 60 Train loss 17.414740 on epoch=7
05/31/2022 13:00:59 - INFO - __main__ - Step 70 Global step 70 Train loss 16.680958 on epoch=8
05/31/2022 13:01:04 - INFO - __main__ - Step 80 Global step 80 Train loss 16.362375 on epoch=9
05/31/2022 13:01:09 - INFO - __main__ - Step 90 Global step 90 Train loss 16.009243 on epoch=11
05/31/2022 13:01:14 - INFO - __main__ - Step 100 Global step 100 Train loss 15.714043 on epoch=12
05/31/2022 13:01:44 - INFO - __main__ - Global step 100 Train loss 16.436274 Classification-F1 0.0 on epoch=12
05/31/2022 13:01:49 - INFO - __main__ - Step 110 Global step 110 Train loss 16.013947 on epoch=13
05/31/2022 13:01:54 - INFO - __main__ - Step 120 Global step 120 Train loss 14.631833 on epoch=14
05/31/2022 13:01:59 - INFO - __main__ - Step 130 Global step 130 Train loss 15.254272 on epoch=16
05/31/2022 13:02:04 - INFO - __main__ - Step 140 Global step 140 Train loss 15.086362 on epoch=17
05/31/2022 13:02:09 - INFO - __main__ - Step 150 Global step 150 Train loss 14.709317 on epoch=18
05/31/2022 13:02:35 - INFO - __main__ - Global step 150 Train loss 15.139146 Classification-F1 0.0 on epoch=18
05/31/2022 13:02:40 - INFO - __main__ - Step 160 Global step 160 Train loss 13.870583 on epoch=19
05/31/2022 13:02:45 - INFO - __main__ - Step 170 Global step 170 Train loss 14.065761 on epoch=21
05/31/2022 13:02:50 - INFO - __main__ - Step 180 Global step 180 Train loss 13.345589 on epoch=22
05/31/2022 13:02:56 - INFO - __main__ - Step 190 Global step 190 Train loss 13.368937 on epoch=23
05/31/2022 13:03:01 - INFO - __main__ - Step 200 Global step 200 Train loss 13.300346 on epoch=24
05/31/2022 13:03:18 - INFO - __main__ - Global step 200 Train loss 13.590241 Classification-F1 0.0 on epoch=24
05/31/2022 13:03:23 - INFO - __main__ - Step 210 Global step 210 Train loss 12.326760 on epoch=26
05/31/2022 13:03:28 - INFO - __main__ - Step 220 Global step 220 Train loss 12.034960 on epoch=27
05/31/2022 13:03:34 - INFO - __main__ - Step 230 Global step 230 Train loss 12.541408 on epoch=28
05/31/2022 13:03:39 - INFO - __main__ - Step 240 Global step 240 Train loss 11.425168 on epoch=29
05/31/2022 13:03:44 - INFO - __main__ - Step 250 Global step 250 Train loss 11.150477 on epoch=31
05/31/2022 13:03:58 - INFO - __main__ - Global step 250 Train loss 11.895755 Classification-F1 0.0 on epoch=31
05/31/2022 13:04:04 - INFO - __main__ - Step 260 Global step 260 Train loss 11.053787 on epoch=32
05/31/2022 13:04:09 - INFO - __main__ - Step 270 Global step 270 Train loss 10.405667 on epoch=33
05/31/2022 13:04:14 - INFO - __main__ - Step 280 Global step 280 Train loss 9.607779 on epoch=34
05/31/2022 13:04:19 - INFO - __main__ - Step 290 Global step 290 Train loss 8.897459 on epoch=36
05/31/2022 13:04:24 - INFO - __main__ - Step 300 Global step 300 Train loss 7.072745 on epoch=37
05/31/2022 13:04:25 - INFO - __main__ - Global step 300 Train loss 9.407487 Classification-F1 0.0 on epoch=37
05/31/2022 13:04:30 - INFO - __main__ - Step 310 Global step 310 Train loss 5.822598 on epoch=38
05/31/2022 13:04:35 - INFO - __main__ - Step 320 Global step 320 Train loss 5.495576 on epoch=39
05/31/2022 13:04:40 - INFO - __main__ - Step 330 Global step 330 Train loss 4.908814 on epoch=41
05/31/2022 13:04:45 - INFO - __main__ - Step 340 Global step 340 Train loss 5.244533 on epoch=42
05/31/2022 13:04:51 - INFO - __main__ - Step 350 Global step 350 Train loss 4.739144 on epoch=43
05/31/2022 13:04:52 - INFO - __main__ - Global step 350 Train loss 5.242133 Classification-F1 0.07848101265822785 on epoch=43
05/31/2022 13:04:57 - INFO - __main__ - Step 360 Global step 360 Train loss 4.560802 on epoch=44
05/31/2022 13:05:02 - INFO - __main__ - Step 370 Global step 370 Train loss 3.901421 on epoch=46
05/31/2022 13:05:07 - INFO - __main__ - Step 380 Global step 380 Train loss 3.998106 on epoch=47
05/31/2022 13:05:12 - INFO - __main__ - Step 390 Global step 390 Train loss 3.859021 on epoch=48
05/31/2022 13:05:17 - INFO - __main__ - Step 400 Global step 400 Train loss 4.380956 on epoch=49
05/31/2022 13:05:18 - INFO - __main__ - Global step 400 Train loss 4.140061 Classification-F1 0.1731016731016731 on epoch=49
05/31/2022 13:05:24 - INFO - __main__ - Step 410 Global step 410 Train loss 3.774127 on epoch=51
05/31/2022 13:05:29 - INFO - __main__ - Step 420 Global step 420 Train loss 3.376592 on epoch=52
05/31/2022 13:05:34 - INFO - __main__ - Step 430 Global step 430 Train loss 3.690369 on epoch=53
05/31/2022 13:05:39 - INFO - __main__ - Step 440 Global step 440 Train loss 3.901918 on epoch=54
05/31/2022 13:05:44 - INFO - __main__ - Step 450 Global step 450 Train loss 3.889405 on epoch=56
05/31/2022 13:05:45 - INFO - __main__ - Global step 450 Train loss 3.726482 Classification-F1 0.13067758749069247 on epoch=56
05/31/2022 13:05:51 - INFO - __main__ - Step 460 Global step 460 Train loss 3.463588 on epoch=57
05/31/2022 13:05:56 - INFO - __main__ - Step 470 Global step 470 Train loss 3.780549 on epoch=58
05/31/2022 13:06:01 - INFO - __main__ - Step 480 Global step 480 Train loss 3.404866 on epoch=59
05/31/2022 13:06:06 - INFO - __main__ - Step 490 Global step 490 Train loss 4.426595 on epoch=61
05/31/2022 13:06:11 - INFO - __main__ - Step 500 Global step 500 Train loss 2.889684 on epoch=62
05/31/2022 13:06:12 - INFO - __main__ - Global step 500 Train loss 3.593056 Classification-F1 0.41115814479998725 on epoch=62
05/31/2022 13:06:17 - INFO - __main__ - Step 510 Global step 510 Train loss 3.463254 on epoch=63
05/31/2022 13:06:23 - INFO - __main__ - Step 520 Global step 520 Train loss 3.410903 on epoch=64
05/31/2022 13:06:28 - INFO - __main__ - Step 530 Global step 530 Train loss 3.355548 on epoch=66
05/31/2022 13:06:33 - INFO - __main__ - Step 540 Global step 540 Train loss 3.273966 on epoch=67
05/31/2022 13:06:38 - INFO - __main__ - Step 550 Global step 550 Train loss 3.496030 on epoch=68
05/31/2022 13:06:39 - INFO - __main__ - Global step 550 Train loss 3.399940 Classification-F1 0.2860204835814592 on epoch=68
05/31/2022 13:06:44 - INFO - __main__ - Step 560 Global step 560 Train loss 3.066590 on epoch=69
05/31/2022 13:06:49 - INFO - __main__ - Step 570 Global step 570 Train loss 2.593335 on epoch=71
05/31/2022 13:06:54 - INFO - __main__ - Step 580 Global step 580 Train loss 3.012953 on epoch=72
05/31/2022 13:06:59 - INFO - __main__ - Step 590 Global step 590 Train loss 2.786577 on epoch=73
05/31/2022 13:07:04 - INFO - __main__ - Step 600 Global step 600 Train loss 2.548523 on epoch=74
05/31/2022 13:07:05 - INFO - __main__ - Global step 600 Train loss 2.801595 Classification-F1 0.14476797088262056 on epoch=74
05/31/2022 13:07:10 - INFO - __main__ - Step 610 Global step 610 Train loss 3.676499 on epoch=76
05/31/2022 13:07:16 - INFO - __main__ - Step 620 Global step 620 Train loss 2.832562 on epoch=77
05/31/2022 13:07:21 - INFO - __main__ - Step 630 Global step 630 Train loss 2.466355 on epoch=78
05/31/2022 13:07:26 - INFO - __main__ - Step 640 Global step 640 Train loss 2.726347 on epoch=79
05/31/2022 13:07:31 - INFO - __main__ - Step 650 Global step 650 Train loss 2.592370 on epoch=81
05/31/2022 13:07:32 - INFO - __main__ - Global step 650 Train loss 2.858826 Classification-F1 0.11578044596912522 on epoch=81
05/31/2022 13:07:37 - INFO - __main__ - Step 660 Global step 660 Train loss 2.411907 on epoch=82
05/31/2022 13:07:42 - INFO - __main__ - Step 670 Global step 670 Train loss 2.360740 on epoch=83
05/31/2022 13:07:47 - INFO - __main__ - Step 680 Global step 680 Train loss 2.058705 on epoch=84
05/31/2022 13:07:52 - INFO - __main__ - Step 690 Global step 690 Train loss 1.504018 on epoch=86
05/31/2022 13:07:57 - INFO - __main__ - Step 700 Global step 700 Train loss 1.440959 on epoch=87
05/31/2022 13:07:58 - INFO - __main__ - Global step 700 Train loss 1.955266 Classification-F1 0.5700711931321395 on epoch=87
05/31/2022 13:08:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.821602 on epoch=88
05/31/2022 13:08:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.692441 on epoch=89
05/31/2022 13:08:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.629993 on epoch=91
05/31/2022 13:08:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.628413 on epoch=92
05/31/2022 13:08:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.506511 on epoch=93
05/31/2022 13:08:25 - INFO - __main__ - Global step 750 Train loss 0.655792 Classification-F1 0.6146411642266183 on epoch=93
05/31/2022 13:08:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.456080 on epoch=94
05/31/2022 13:08:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.456526 on epoch=96
05/31/2022 13:08:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.375336 on epoch=97
05/31/2022 13:08:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.420653 on epoch=98
05/31/2022 13:08:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.361006 on epoch=99
05/31/2022 13:08:52 - INFO - __main__ - Global step 800 Train loss 0.413920 Classification-F1 0.6578334387656422 on epoch=99
05/31/2022 13:08:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.413649 on epoch=101
05/31/2022 13:09:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.403350 on epoch=102
05/31/2022 13:09:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.378011 on epoch=103
05/31/2022 13:09:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.300559 on epoch=104
05/31/2022 13:09:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.275498 on epoch=106
05/31/2022 13:09:19 - INFO - __main__ - Global step 850 Train loss 0.354214 Classification-F1 0.660167456952711 on epoch=106
05/31/2022 13:09:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.330937 on epoch=107
05/31/2022 13:09:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.391089 on epoch=108
05/31/2022 13:09:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.322366 on epoch=109
05/31/2022 13:09:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.267002 on epoch=111
05/31/2022 13:09:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.170091 on epoch=112
05/31/2022 13:09:46 - INFO - __main__ - Global step 900 Train loss 0.296297 Classification-F1 0.6694365459686296 on epoch=112
05/31/2022 13:09:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.246217 on epoch=113
05/31/2022 13:09:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.205306 on epoch=114
05/31/2022 13:10:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.282383 on epoch=116
05/31/2022 13:10:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.152118 on epoch=117
05/31/2022 13:10:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.169118 on epoch=118
05/31/2022 13:10:13 - INFO - __main__ - Global step 950 Train loss 0.211029 Classification-F1 0.6746815138119486 on epoch=118
05/31/2022 13:10:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.118667 on epoch=119
05/31/2022 13:10:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.165332 on epoch=121
05/31/2022 13:10:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.133162 on epoch=122
05/31/2022 13:10:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.142740 on epoch=123
05/31/2022 13:10:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.091955 on epoch=124
05/31/2022 13:10:41 - INFO - __main__ - Global step 1000 Train loss 0.130371 Classification-F1 0.7041406533209811 on epoch=124
05/31/2022 13:10:41 - INFO - __main__ - save last model!
05/31/2022 13:10:48 - INFO - __main__ - Loading checkpoint on the fly
05/31/2022 13:10:49 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 13:10:49 - INFO - __main__ - Printing 3 examples
05/31/2022 13:10:49 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 13:10:49 - INFO - __main__ - ['others']
05/31/2022 13:10:49 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 13:10:49 - INFO - __main__ - ['others']
05/31/2022 13:10:49 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 13:10:49 - INFO - __main__ - ['others']
05/31/2022 13:10:49 - INFO - __main__ - Tokenizing Input ...
05/31/2022 13:10:51 - INFO - __main__ - Tokenizing Output ...
05/31/2022 13:10:57 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 13:11:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down32shot/singletask-emo/emo_32_87_0.0001_8_predictions.txt
05/31/2022 13:11:42 - INFO - __main__ - Classification-F1 on test data: 0.3795
05/31/2022 13:11:43 - INFO - __main__ - prefix=emo_32_87, lr=0.0001, bsz=8, dev_performance=0.7041406533209811, test_performance=0.37950227421389515
