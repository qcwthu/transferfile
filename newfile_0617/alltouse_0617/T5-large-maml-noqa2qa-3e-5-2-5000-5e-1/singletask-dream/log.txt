04/06/2022 14:10:25 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=False, bsz_list=[4], cache_dir='/data/qin/cache/', checkpoint='None', cuda='4', dataset='nlp_forest_single', debug=False, dev_file='data', do_lowercase=False, do_predict=True, do_train=True, eval_period=50, freeze_embeds=False, gradient_accumulation_steps=2, identifier='T5-large-maml-noqa2qa-3e-5-2-5000-5e-1', learning_rate=0.5, learning_rate_list=[0.5], lm_adapted_path='/data/qin/lm_adapted_t5model/torch_ckpt/large/pytorch_model.bin', local_rank=0, log_step=10, max_grad_norm=1.0, max_input_length=512, max_output_length=128, model='google/t5-v1_1-large', num_beams=4, num_train_epochs=1000.0, output_dir='models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream', predict_batch_size=16, predict_checkpoint='best-model.pt', prefix='', prompt_number=100, quiet=False, seed=42, task_dir='data/dream/', task_name='dream', test_file='data', total_steps=3000, train_batch_size=4, train_file='data', wait_step=10000000000, warmup_steps=50, weight_decay=1e-05)
04/06/2022 14:10:25 - INFO - __main__ - models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream
04/25/2022 03:04:00 - INFO - __main__ - Namespace(task_dir='data/dream/', task_name='dream', identifier='T5-large-maml-noqa2qa-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-noqa2qa-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
04/25/2022 03:04:00 - INFO - __main__ - models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream
04/25/2022 03:04:00 - INFO - __main__ - Namespace(task_dir='data/dream/', task_name='dream', identifier='T5-large-maml-noqa2qa-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-noqa2qa-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
04/25/2022 03:04:00 - INFO - __main__ - models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream
04/25/2022 03:04:02 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
04/25/2022 03:04:02 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
04/25/2022 03:04:02 - INFO - __main__ - args.device: cuda:0
04/25/2022 03:04:02 - INFO - __main__ - Using 2 gpus
04/25/2022 03:04:02 - INFO - __main__ - Fine-tuning the following samples: ['dream_32_100', 'dream_32_13', 'dream_32_21', 'dream_32_42', 'dream_32_87']
04/25/2022 03:04:02 - INFO - __main__ - args.device: cuda:1
04/25/2022 03:04:02 - INFO - __main__ - Using 2 gpus
04/25/2022 03:04:02 - INFO - __main__ - Fine-tuning the following samples: ['dream_32_100', 'dream_32_13', 'dream_32_21', 'dream_32_42', 'dream_32_87']
04/25/2022 03:04:06 - INFO - __main__ - Running ... prefix=dream_32_100, lr=0.5, bsz=8 ...
04/25/2022 03:04:07 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:04:07 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:04:07 - INFO - __main__ - Printing 3 examples
04/25/2022 03:04:07 - INFO - __main__ - Printing 3 examples
04/25/2022 03:04:07 - INFO - __main__ -  [dream] What does she have to take before she begins practical training? [SEP] W: Dr. Steven, I am thinking about changing my major before the end of the freshman year. M: What are you studying now? W: I am taking three genera] requirements and American history and American literature this. semester. Last semester I took four requirements and freshman French. M: I believe it's not too late to change your major because you've mainly taken general requirements which all freshmen have to take. Also we have twelve electives so the mo lit courses will be included in them, so you can change your major without losing any credit hours. W: I am very happy to know I am still able to change my major. I am interested in writing newspaper articles, and after finishing my degree I would like to work for some newspaper firm. M: Oh, I think you will be a good writer. W: Dr. Steven, when do students start practical training? M: They don't begin practical training in reporting until the sophomore year. Journalism 121 normally is taken in the freshman year as a general background course. W: I see. I will take the course next semester. Thank you very much for your help. M: You're welcome. I look forward to seeing you in my department. [SEP]  (A) English sociology. (B) Journalism 121. (C) Freshman French.
04/25/2022 03:04:07 - INFO - __main__ -  [dream] What does she have to take before she begins practical training? [SEP] W: Dr. Steven, I am thinking about changing my major before the end of the freshman year. M: What are you studying now? W: I am taking three genera] requirements and American history and American literature this. semester. Last semester I took four requirements and freshman French. M: I believe it's not too late to change your major because you've mainly taken general requirements which all freshmen have to take. Also we have twelve electives so the mo lit courses will be included in them, so you can change your major without losing any credit hours. W: I am very happy to know I am still able to change my major. I am interested in writing newspaper articles, and after finishing my degree I would like to work for some newspaper firm. M: Oh, I think you will be a good writer. W: Dr. Steven, when do students start practical training? M: They don't begin practical training in reporting until the sophomore year. Journalism 121 normally is taken in the freshman year as a general background course. W: I see. I will take the course next semester. Thank you very much for your help. M: You're welcome. I look forward to seeing you in my department. [SEP]  (A) English sociology. (B) Journalism 121. (C) Freshman French.
04/25/2022 03:04:07 - INFO - __main__ - ['Journalism 121.']
04/25/2022 03:04:07 - INFO - __main__ -  [dream] Which skirt does the man want to buy? [SEP] W: The red skirt is fifty-five yuan, the brown one fifty yuan, and the green one thirty-eight yuan. Which one do you want? M: Hm, hm... I think I'd like the cheapest one. [SEP]  (A) The red one. (B) The green one. (C) The brown one.
04/25/2022 03:04:07 - INFO - __main__ - ['Journalism 121.']
04/25/2022 03:04:07 - INFO - __main__ - ['The green one.']
04/25/2022 03:04:07 - INFO - __main__ -  [dream] Which skirt does the man want to buy? [SEP] W: The red skirt is fifty-five yuan, the brown one fifty yuan, and the green one thirty-eight yuan. Which one do you want? M: Hm, hm... I think I'd like the cheapest one. [SEP]  (A) The red one. (B) The green one. (C) The brown one.
04/25/2022 03:04:07 - INFO - __main__ -  [dream] How much should the man pay in total? [SEP] M: Excuse me, how much does an ice cream cost? W: Two yuan. And how many do you want? M: Two, please. W: Anything else you want to buy? M: Oh, yes, I still want some bread. A piece of bread, please. W: Here you are. 1.5 yuan a piece. M: By the way, do you have any fruit here, such as apples or oranges? W: I'm sorry we haven't got any. M: All right. Here's the money for you. W: Thank you. [SEP]  (A) 1.5 yuan. (B) 2 yuan (C) 5.5 yuan
04/25/2022 03:04:07 - INFO - __main__ - ['The green one.']
04/25/2022 03:04:07 - INFO - __main__ - ['5.5 yuan']
04/25/2022 03:04:07 - INFO - __main__ -  [dream] How much should the man pay in total? [SEP] M: Excuse me, how much does an ice cream cost? W: Two yuan. And how many do you want? M: Two, please. W: Anything else you want to buy? M: Oh, yes, I still want some bread. A piece of bread, please. W: Here you are. 1.5 yuan a piece. M: By the way, do you have any fruit here, such as apples or oranges? W: I'm sorry we haven't got any. M: All right. Here's the money for you. W: Thank you. [SEP]  (A) 1.5 yuan. (B) 2 yuan (C) 5.5 yuan
04/25/2022 03:04:07 - INFO - __main__ - ['5.5 yuan']
04/25/2022 03:04:07 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:04:07 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:04:07 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:04:07 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:04:07 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 03:04:07 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:04:07 - INFO - __main__ - Printing 3 examples
04/25/2022 03:04:07 - INFO - __main__ -  [dream] Where will the woman go first after work? [SEP] Man: Are you and Sarah going straight to the restaurant from work tonight? Woman: Actually, I'm leaving work early because I need to do some shopping in the market, and after that we're going to play tennis at the sports centre before we go to the restaurant. [SEP]  (A) The restaurant. (B) The market. (C) The cinema.
04/25/2022 03:04:07 - INFO - __main__ - ['The market.']
04/25/2022 03:04:07 - INFO - __main__ -  [dream] What can be inferred about Larry and Bill? [SEP] W: I can't make up my mind whether to ask Larry or Bill to paint the house. M: What difference does it make? They're both excellent painters. And I think we just need to check who is available the day after tomorrow. [SEP]  (A) They are quite different in painting skills. (B) Neither of them is good at house-painting. (C) They are equally good at house-painting.
04/25/2022 03:04:07 - INFO - __main__ - ['They are equally good at house-painting.']
04/25/2022 03:04:07 - INFO - __main__ -  [dream] What was wrong with the alarm clock? [SEP] W: Come to my office, Billy. Look at your messy hair. This is the third time you have been late for class within this week. And every time you went in, you disturbed the teacher's class. M: Sorry, Miss. I didn't mean to do that, but it was my alarm clock that didn't wake me up. W: That is not a proper excuse. You could set it ahead of the exact time so that you could have got up earlier and not have been late. M: I did that, but it seemed useless when I found the batteries had run out after I woke up this morning. I rushed to school and luckily, my neighbour Mr. Green gave me a hand and took me to school with his car. W: So you could have made it. But you were still late for class. M: The thing is, when I got off, I found I had left my bag in his car. And it was too late to get it back when I realized that. I went straight to the nearest shop and gave him a call. I waited for five minutes before Mr. Green drove back. W: Finally, he gave you your bag and you came to the classroom and you were already 15 minutes late for class at that time. Is that all? M: Miss, you seem to know the entire story. May I go back to class now? Otherwise I will be late for the following class. [SEP]  (A) The batteries ran out. (B) The alarm clock was broken. (C) The alarm clock was ahead of the exact time.
04/25/2022 03:04:07 - INFO - __main__ - ['The batteries ran out.']
04/25/2022 03:04:07 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:04:07 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 03:04:07 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:04:07 - INFO - __main__ - Printing 3 examples
04/25/2022 03:04:07 - INFO - __main__ -  [dream] Where will the woman go first after work? [SEP] Man: Are you and Sarah going straight to the restaurant from work tonight? Woman: Actually, I'm leaving work early because I need to do some shopping in the market, and after that we're going to play tennis at the sports centre before we go to the restaurant. [SEP]  (A) The restaurant. (B) The market. (C) The cinema.
04/25/2022 03:04:07 - INFO - __main__ - ['The market.']
04/25/2022 03:04:07 - INFO - __main__ -  [dream] What can be inferred about Larry and Bill? [SEP] W: I can't make up my mind whether to ask Larry or Bill to paint the house. M: What difference does it make? They're both excellent painters. And I think we just need to check who is available the day after tomorrow. [SEP]  (A) They are quite different in painting skills. (B) Neither of them is good at house-painting. (C) They are equally good at house-painting.
04/25/2022 03:04:07 - INFO - __main__ - ['They are equally good at house-painting.']
04/25/2022 03:04:07 - INFO - __main__ -  [dream] What was wrong with the alarm clock? [SEP] W: Come to my office, Billy. Look at your messy hair. This is the third time you have been late for class within this week. And every time you went in, you disturbed the teacher's class. M: Sorry, Miss. I didn't mean to do that, but it was my alarm clock that didn't wake me up. W: That is not a proper excuse. You could set it ahead of the exact time so that you could have got up earlier and not have been late. M: I did that, but it seemed useless when I found the batteries had run out after I woke up this morning. I rushed to school and luckily, my neighbour Mr. Green gave me a hand and took me to school with his car. W: So you could have made it. But you were still late for class. M: The thing is, when I got off, I found I had left my bag in his car. And it was too late to get it back when I realized that. I went straight to the nearest shop and gave him a call. I waited for five minutes before Mr. Green drove back. W: Finally, he gave you your bag and you came to the classroom and you were already 15 minutes late for class at that time. Is that all? M: Miss, you seem to know the entire story. May I go back to class now? Otherwise I will be late for the following class. [SEP]  (A) The batteries ran out. (B) The alarm clock was broken. (C) The alarm clock was ahead of the exact time.
04/25/2022 03:04:07 - INFO - __main__ - ['The batteries ran out.']
04/25/2022 03:04:07 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:04:07 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:04:07 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:04:07 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 03:04:07 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 03:04:25 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 03:04:25 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 03:04:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 03:04:26 - INFO - __main__ - Starting training!
04/25/2022 03:04:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 03:04:31 - INFO - __main__ - Starting training!
04/25/2022 03:04:36 - INFO - __main__ - Step 10 Global step 10 Train loss 1.63 on epoch=4
04/25/2022 03:04:41 - INFO - __main__ - Step 20 Global step 20 Train loss 1.34 on epoch=9
04/25/2022 03:04:45 - INFO - __main__ - Step 30 Global step 30 Train loss 1.01 on epoch=14
04/25/2022 03:04:49 - INFO - __main__ - Step 40 Global step 40 Train loss 0.75 on epoch=19
04/25/2022 03:04:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.63 on epoch=24
04/25/2022 03:04:56 - INFO - __main__ - Global step 50 Train loss 1.07 ACC 0.34375 on epoch=24
04/25/2022 03:04:56 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.34375 on epoch=24, global_step=50
04/25/2022 03:05:01 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=29
04/25/2022 03:05:05 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=34
04/25/2022 03:05:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=39
04/25/2022 03:05:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=44
04/25/2022 03:05:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=49
04/25/2022 03:05:21 - INFO - __main__ - Global step 100 Train loss 0.46 ACC 0.21875 on epoch=49
04/25/2022 03:05:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.36 on epoch=54
04/25/2022 03:05:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
04/25/2022 03:05:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
04/25/2022 03:05:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=69
04/25/2022 03:05:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.23 on epoch=74
04/25/2022 03:05:45 - INFO - __main__ - Global step 150 Train loss 0.29 ACC 0.375 on epoch=74
04/25/2022 03:05:45 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=74, global_step=150
04/25/2022 03:05:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.21 on epoch=79
04/25/2022 03:05:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.22 on epoch=84
04/25/2022 03:05:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=89
04/25/2022 03:06:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=94
04/25/2022 03:06:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.16 on epoch=99
04/25/2022 03:06:10 - INFO - __main__ - Global step 200 Train loss 0.23 ACC 0.28125 on epoch=99
04/25/2022 03:06:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.14 on epoch=104
04/25/2022 03:06:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.14 on epoch=109
04/25/2022 03:06:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.10 on epoch=114
04/25/2022 03:06:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.19 on epoch=119
04/25/2022 03:06:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.16 on epoch=124
04/25/2022 03:06:34 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.34375 on epoch=124
04/25/2022 03:06:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.10 on epoch=129
04/25/2022 03:06:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.11 on epoch=134
04/25/2022 03:06:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.16 on epoch=139
04/25/2022 03:06:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.11 on epoch=144
04/25/2022 03:06:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.14 on epoch=149
04/25/2022 03:06:59 - INFO - __main__ - Global step 300 Train loss 0.12 ACC 0.28125 on epoch=149
04/25/2022 03:07:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.11 on epoch=154
04/25/2022 03:07:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.13 on epoch=159
04/25/2022 03:07:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.06 on epoch=164
04/25/2022 03:07:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.08 on epoch=169
04/25/2022 03:07:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.10 on epoch=174
04/25/2022 03:07:23 - INFO - __main__ - Global step 350 Train loss 0.10 ACC 0.34375 on epoch=174
04/25/2022 03:07:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.07 on epoch=179
04/25/2022 03:07:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.09 on epoch=184
04/25/2022 03:07:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.10 on epoch=189
04/25/2022 03:07:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.06 on epoch=194
04/25/2022 03:07:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.09 on epoch=199
04/25/2022 03:07:47 - INFO - __main__ - Global step 400 Train loss 0.08 ACC 0.3125 on epoch=199
04/25/2022 03:07:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.09 on epoch=204
04/25/2022 03:07:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
04/25/2022 03:08:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.07 on epoch=214
04/25/2022 03:08:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=219
04/25/2022 03:08:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.05 on epoch=224
04/25/2022 03:08:12 - INFO - __main__ - Global step 450 Train loss 0.07 ACC 0.3125 on epoch=224
04/25/2022 03:08:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
04/25/2022 03:08:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
04/25/2022 03:08:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
04/25/2022 03:08:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.04 on epoch=244
04/25/2022 03:08:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
04/25/2022 03:08:36 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.34375 on epoch=249
04/25/2022 03:08:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
04/25/2022 03:08:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
04/25/2022 03:08:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.04 on epoch=264
04/25/2022 03:08:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
04/25/2022 03:08:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
04/25/2022 03:09:00 - INFO - __main__ - Global step 550 Train loss 0.05 ACC 0.34375 on epoch=274
04/25/2022 03:09:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
04/25/2022 03:09:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
04/25/2022 03:09:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
04/25/2022 03:09:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
04/25/2022 03:09:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.06 on epoch=299
04/25/2022 03:09:25 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.28125 on epoch=299
04/25/2022 03:09:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
04/25/2022 03:09:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=309
04/25/2022 03:09:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
04/25/2022 03:09:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
04/25/2022 03:09:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
04/25/2022 03:09:49 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.34375 on epoch=324
04/25/2022 03:09:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
04/25/2022 03:09:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
04/25/2022 03:10:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
04/25/2022 03:10:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
04/25/2022 03:10:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
04/25/2022 03:10:14 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.34375 on epoch=349
04/25/2022 03:10:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
04/25/2022 03:10:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
04/25/2022 03:10:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
04/25/2022 03:10:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
04/25/2022 03:10:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
04/25/2022 03:10:39 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.34375 on epoch=374
04/25/2022 03:10:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
04/25/2022 03:10:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
04/25/2022 03:10:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
04/25/2022 03:10:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
04/25/2022 03:11:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
04/25/2022 03:11:04 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.3125 on epoch=399
04/25/2022 03:11:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
04/25/2022 03:11:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
04/25/2022 03:11:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
04/25/2022 03:11:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=419
04/25/2022 03:11:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
04/25/2022 03:11:29 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.25 on epoch=424
04/25/2022 03:11:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
04/25/2022 03:11:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
04/25/2022 03:11:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
04/25/2022 03:11:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
04/25/2022 03:11:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
04/25/2022 03:11:54 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.1875 on epoch=449
04/25/2022 03:11:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
04/25/2022 03:12:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
04/25/2022 03:12:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
04/25/2022 03:12:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
04/25/2022 03:12:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
04/25/2022 03:12:19 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.21875 on epoch=474
04/25/2022 03:12:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/25/2022 03:12:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
04/25/2022 03:12:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
04/25/2022 03:12:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
04/25/2022 03:12:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/25/2022 03:12:43 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.3125 on epoch=499
04/25/2022 03:12:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
04/25/2022 03:12:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
04/25/2022 03:12:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
04/25/2022 03:13:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
04/25/2022 03:13:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/25/2022 03:13:08 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.3125 on epoch=524
04/25/2022 03:13:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
04/25/2022 03:13:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
04/25/2022 03:13:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/25/2022 03:13:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
04/25/2022 03:13:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
04/25/2022 03:13:32 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.375 on epoch=549
04/25/2022 03:13:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/25/2022 03:13:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
04/25/2022 03:13:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/25/2022 03:13:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/25/2022 03:13:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
04/25/2022 03:13:56 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.34375 on epoch=574
04/25/2022 03:14:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 03:14:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
04/25/2022 03:14:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/25/2022 03:14:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 03:14:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
04/25/2022 03:14:21 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.28125 on epoch=599
04/25/2022 03:14:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
04/25/2022 03:14:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
04/25/2022 03:14:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
04/25/2022 03:14:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
04/25/2022 03:14:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
04/25/2022 03:14:46 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.34375 on epoch=624
04/25/2022 03:14:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 03:14:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 03:14:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
04/25/2022 03:15:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/25/2022 03:15:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/25/2022 03:15:10 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.34375 on epoch=649
04/25/2022 03:15:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
04/25/2022 03:15:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 03:15:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/25/2022 03:15:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/25/2022 03:15:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 03:15:34 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.375 on epoch=674
04/25/2022 03:15:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 03:15:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
04/25/2022 03:15:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
04/25/2022 03:15:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
04/25/2022 03:15:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 03:15:59 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.34375 on epoch=699
04/25/2022 03:16:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/25/2022 03:16:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
04/25/2022 03:16:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 03:16:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/25/2022 03:16:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
04/25/2022 03:16:23 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.3125 on epoch=724
04/25/2022 03:16:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
04/25/2022 03:16:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/25/2022 03:16:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 03:16:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 03:16:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 03:16:48 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.3125 on epoch=749
04/25/2022 03:16:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 03:16:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 03:17:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 03:17:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
04/25/2022 03:17:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/25/2022 03:17:13 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.28125 on epoch=774
04/25/2022 03:17:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/25/2022 03:17:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
04/25/2022 03:17:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 03:17:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
04/25/2022 03:17:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
04/25/2022 03:17:38 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.34375 on epoch=799
04/25/2022 03:17:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
04/25/2022 03:17:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/25/2022 03:17:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 03:17:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 03:17:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 03:18:02 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.375 on epoch=824
04/25/2022 03:18:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/25/2022 03:18:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 03:18:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 03:18:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 03:18:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
04/25/2022 03:18:27 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.40625 on epoch=849
04/25/2022 03:18:27 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.40625 on epoch=849, global_step=1700
04/25/2022 03:18:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
04/25/2022 03:18:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
04/25/2022 03:18:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 03:18:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 03:18:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 03:18:52 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.40625 on epoch=874
04/25/2022 03:18:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
04/25/2022 03:19:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
04/25/2022 03:19:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 03:19:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 03:19:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 03:19:16 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.375 on epoch=899
04/25/2022 03:19:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
04/25/2022 03:19:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/25/2022 03:19:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
04/25/2022 03:19:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 03:19:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 03:19:41 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.34375 on epoch=924
04/25/2022 03:19:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
04/25/2022 03:19:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
04/25/2022 03:19:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 03:19:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
04/25/2022 03:20:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
04/25/2022 03:20:05 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.28125 on epoch=949
04/25/2022 03:20:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
04/25/2022 03:20:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
04/25/2022 03:20:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 03:20:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 03:20:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
04/25/2022 03:20:30 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.375 on epoch=974
04/25/2022 03:20:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
04/25/2022 03:20:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
04/25/2022 03:20:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/25/2022 03:20:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 03:20:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 03:20:53 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:20:53 - INFO - __main__ - Printing 3 examples
04/25/2022 03:20:53 - INFO - __main__ -  [dream] What does she have to take before she begins practical training? [SEP] W: Dr. Steven, I am thinking about changing my major before the end of the freshman year. M: What are you studying now? W: I am taking three genera] requirements and American history and American literature this. semester. Last semester I took four requirements and freshman French. M: I believe it's not too late to change your major because you've mainly taken general requirements which all freshmen have to take. Also we have twelve electives so the mo lit courses will be included in them, so you can change your major without losing any credit hours. W: I am very happy to know I am still able to change my major. I am interested in writing newspaper articles, and after finishing my degree I would like to work for some newspaper firm. M: Oh, I think you will be a good writer. W: Dr. Steven, when do students start practical training? M: They don't begin practical training in reporting until the sophomore year. Journalism 121 normally is taken in the freshman year as a general background course. W: I see. I will take the course next semester. Thank you very much for your help. M: You're welcome. I look forward to seeing you in my department. [SEP]  (A) English sociology. (B) Journalism 121. (C) Freshman French.
04/25/2022 03:20:53 - INFO - __main__ - ['Journalism 121.']
04/25/2022 03:20:53 - INFO - __main__ -  [dream] Which skirt does the man want to buy? [SEP] W: The red skirt is fifty-five yuan, the brown one fifty yuan, and the green one thirty-eight yuan. Which one do you want? M: Hm, hm... I think I'd like the cheapest one. [SEP]  (A) The red one. (B) The green one. (C) The brown one.
04/25/2022 03:20:53 - INFO - __main__ - ['The green one.']
04/25/2022 03:20:53 - INFO - __main__ -  [dream] How much should the man pay in total? [SEP] M: Excuse me, how much does an ice cream cost? W: Two yuan. And how many do you want? M: Two, please. W: Anything else you want to buy? M: Oh, yes, I still want some bread. A piece of bread, please. W: Here you are. 1.5 yuan a piece. M: By the way, do you have any fruit here, such as apples or oranges? W: I'm sorry we haven't got any. M: All right. Here's the money for you. W: Thank you. [SEP]  (A) 1.5 yuan. (B) 2 yuan (C) 5.5 yuan
04/25/2022 03:20:53 - INFO - __main__ - ['5.5 yuan']
04/25/2022 03:20:53 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:20:53 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:20:53 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 03:20:53 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:20:53 - INFO - __main__ - Printing 3 examples
04/25/2022 03:20:53 - INFO - __main__ -  [dream] Where will the woman go first after work? [SEP] Man: Are you and Sarah going straight to the restaurant from work tonight? Woman: Actually, I'm leaving work early because I need to do some shopping in the market, and after that we're going to play tennis at the sports centre before we go to the restaurant. [SEP]  (A) The restaurant. (B) The market. (C) The cinema.
04/25/2022 03:20:53 - INFO - __main__ - ['The market.']
04/25/2022 03:20:53 - INFO - __main__ -  [dream] What can be inferred about Larry and Bill? [SEP] W: I can't make up my mind whether to ask Larry or Bill to paint the house. M: What difference does it make? They're both excellent painters. And I think we just need to check who is available the day after tomorrow. [SEP]  (A) They are quite different in painting skills. (B) Neither of them is good at house-painting. (C) They are equally good at house-painting.
04/25/2022 03:20:53 - INFO - __main__ - ['They are equally good at house-painting.']
04/25/2022 03:20:53 - INFO - __main__ -  [dream] What was wrong with the alarm clock? [SEP] W: Come to my office, Billy. Look at your messy hair. This is the third time you have been late for class within this week. And every time you went in, you disturbed the teacher's class. M: Sorry, Miss. I didn't mean to do that, but it was my alarm clock that didn't wake me up. W: That is not a proper excuse. You could set it ahead of the exact time so that you could have got up earlier and not have been late. M: I did that, but it seemed useless when I found the batteries had run out after I woke up this morning. I rushed to school and luckily, my neighbour Mr. Green gave me a hand and took me to school with his car. W: So you could have made it. But you were still late for class. M: The thing is, when I got off, I found I had left my bag in his car. And it was too late to get it back when I realized that. I went straight to the nearest shop and gave him a call. I waited for five minutes before Mr. Green drove back. W: Finally, he gave you your bag and you came to the classroom and you were already 15 minutes late for class at that time. Is that all? M: Miss, you seem to know the entire story. May I go back to class now? Otherwise I will be late for the following class. [SEP]  (A) The batteries ran out. (B) The alarm clock was broken. (C) The alarm clock was ahead of the exact time.
04/25/2022 03:20:53 - INFO - __main__ - ['The batteries ran out.']
04/25/2022 03:20:53 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:20:53 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:20:53 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 03:20:54 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.34375 on epoch=999
04/25/2022 03:20:54 - INFO - __main__ - save last model!
04/25/2022 03:20:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 03:20:55 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 03:20:55 - INFO - __main__ - Printing 3 examples
04/25/2022 03:20:55 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 03:20:55 - INFO - __main__ - ['To change her job.']
04/25/2022 03:20:55 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 03:20:55 - INFO - __main__ - ['Watch TV.']
04/25/2022 03:20:55 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 03:20:55 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 03:20:55 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:20:55 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:20:56 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 03:21:08 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 03:21:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 03:21:09 - INFO - __main__ - Starting training!
04/25/2022 03:22:11 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_100_0.5_8_predictions.txt
04/25/2022 03:22:11 - INFO - __main__ - ACC on test data: 0.3090
04/25/2022 03:22:11 - INFO - __main__ - prefix=dream_32_100, lr=0.5, bsz=8, dev_performance=0.40625, test_performance=0.309
04/25/2022 03:22:11 - INFO - __main__ - Running ... prefix=dream_32_100, lr=0.4, bsz=8 ...
04/25/2022 03:22:12 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:22:12 - INFO - __main__ - Printing 3 examples
04/25/2022 03:22:12 - INFO - __main__ -  [dream] What does she have to take before she begins practical training? [SEP] W: Dr. Steven, I am thinking about changing my major before the end of the freshman year. M: What are you studying now? W: I am taking three genera] requirements and American history and American literature this. semester. Last semester I took four requirements and freshman French. M: I believe it's not too late to change your major because you've mainly taken general requirements which all freshmen have to take. Also we have twelve electives so the mo lit courses will be included in them, so you can change your major without losing any credit hours. W: I am very happy to know I am still able to change my major. I am interested in writing newspaper articles, and after finishing my degree I would like to work for some newspaper firm. M: Oh, I think you will be a good writer. W: Dr. Steven, when do students start practical training? M: They don't begin practical training in reporting until the sophomore year. Journalism 121 normally is taken in the freshman year as a general background course. W: I see. I will take the course next semester. Thank you very much for your help. M: You're welcome. I look forward to seeing you in my department. [SEP]  (A) English sociology. (B) Journalism 121. (C) Freshman French.
04/25/2022 03:22:12 - INFO - __main__ - ['Journalism 121.']
04/25/2022 03:22:12 - INFO - __main__ -  [dream] Which skirt does the man want to buy? [SEP] W: The red skirt is fifty-five yuan, the brown one fifty yuan, and the green one thirty-eight yuan. Which one do you want? M: Hm, hm... I think I'd like the cheapest one. [SEP]  (A) The red one. (B) The green one. (C) The brown one.
04/25/2022 03:22:12 - INFO - __main__ - ['The green one.']
04/25/2022 03:22:12 - INFO - __main__ -  [dream] How much should the man pay in total? [SEP] M: Excuse me, how much does an ice cream cost? W: Two yuan. And how many do you want? M: Two, please. W: Anything else you want to buy? M: Oh, yes, I still want some bread. A piece of bread, please. W: Here you are. 1.5 yuan a piece. M: By the way, do you have any fruit here, such as apples or oranges? W: I'm sorry we haven't got any. M: All right. Here's the money for you. W: Thank you. [SEP]  (A) 1.5 yuan. (B) 2 yuan (C) 5.5 yuan
04/25/2022 03:22:12 - INFO - __main__ - ['5.5 yuan']
04/25/2022 03:22:12 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:22:12 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:22:12 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 03:22:12 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:22:12 - INFO - __main__ - Printing 3 examples
04/25/2022 03:22:12 - INFO - __main__ -  [dream] Where will the woman go first after work? [SEP] Man: Are you and Sarah going straight to the restaurant from work tonight? Woman: Actually, I'm leaving work early because I need to do some shopping in the market, and after that we're going to play tennis at the sports centre before we go to the restaurant. [SEP]  (A) The restaurant. (B) The market. (C) The cinema.
04/25/2022 03:22:12 - INFO - __main__ - ['The market.']
04/25/2022 03:22:12 - INFO - __main__ -  [dream] What can be inferred about Larry and Bill? [SEP] W: I can't make up my mind whether to ask Larry or Bill to paint the house. M: What difference does it make? They're both excellent painters. And I think we just need to check who is available the day after tomorrow. [SEP]  (A) They are quite different in painting skills. (B) Neither of them is good at house-painting. (C) They are equally good at house-painting.
04/25/2022 03:22:12 - INFO - __main__ - ['They are equally good at house-painting.']
04/25/2022 03:22:12 - INFO - __main__ -  [dream] What was wrong with the alarm clock? [SEP] W: Come to my office, Billy. Look at your messy hair. This is the third time you have been late for class within this week. And every time you went in, you disturbed the teacher's class. M: Sorry, Miss. I didn't mean to do that, but it was my alarm clock that didn't wake me up. W: That is not a proper excuse. You could set it ahead of the exact time so that you could have got up earlier and not have been late. M: I did that, but it seemed useless when I found the batteries had run out after I woke up this morning. I rushed to school and luckily, my neighbour Mr. Green gave me a hand and took me to school with his car. W: So you could have made it. But you were still late for class. M: The thing is, when I got off, I found I had left my bag in his car. And it was too late to get it back when I realized that. I went straight to the nearest shop and gave him a call. I waited for five minutes before Mr. Green drove back. W: Finally, he gave you your bag and you came to the classroom and you were already 15 minutes late for class at that time. Is that all? M: Miss, you seem to know the entire story. May I go back to class now? Otherwise I will be late for the following class. [SEP]  (A) The batteries ran out. (B) The alarm clock was broken. (C) The alarm clock was ahead of the exact time.
04/25/2022 03:22:12 - INFO - __main__ - ['The batteries ran out.']
04/25/2022 03:22:12 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:22:12 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:22:12 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 03:22:27 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 03:22:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 03:22:28 - INFO - __main__ - Starting training!
04/25/2022 03:22:33 - INFO - __main__ - Step 10 Global step 10 Train loss 1.69 on epoch=4
04/25/2022 03:22:37 - INFO - __main__ - Step 20 Global step 20 Train loss 1.34 on epoch=9
04/25/2022 03:22:41 - INFO - __main__ - Step 30 Global step 30 Train loss 1.07 on epoch=14
04/25/2022 03:22:46 - INFO - __main__ - Step 40 Global step 40 Train loss 0.88 on epoch=19
04/25/2022 03:22:50 - INFO - __main__ - Step 50 Global step 50 Train loss 0.71 on epoch=24
04/25/2022 03:22:53 - INFO - __main__ - Global step 50 Train loss 1.14 ACC 0.28125 on epoch=24
04/25/2022 03:22:53 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.28125 on epoch=24, global_step=50
04/25/2022 03:22:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=29
04/25/2022 03:23:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=34
04/25/2022 03:23:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=39
04/25/2022 03:23:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=44
04/25/2022 03:23:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=49
04/25/2022 03:23:18 - INFO - __main__ - Global step 100 Train loss 0.49 ACC 0.3125 on epoch=49
04/25/2022 03:23:18 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=49, global_step=100
04/25/2022 03:23:22 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=54
04/25/2022 03:23:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.30 on epoch=59
04/25/2022 03:23:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.33 on epoch=64
04/25/2022 03:23:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
04/25/2022 03:23:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=74
04/25/2022 03:23:42 - INFO - __main__ - Global step 150 Train loss 0.31 ACC 0.34375 on epoch=74
04/25/2022 03:23:42 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.34375 on epoch=74, global_step=150
04/25/2022 03:23:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=79
04/25/2022 03:23:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=84
04/25/2022 03:23:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.32 on epoch=89
04/25/2022 03:23:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=94
04/25/2022 03:24:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.19 on epoch=99
04/25/2022 03:24:07 - INFO - __main__ - Global step 200 Train loss 0.25 ACC 0.34375 on epoch=99
04/25/2022 03:24:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=104
04/25/2022 03:24:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
04/25/2022 03:24:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
04/25/2022 03:24:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.19 on epoch=119
04/25/2022 03:24:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=124
04/25/2022 03:24:31 - INFO - __main__ - Global step 250 Train loss 0.22 ACC 0.28125 on epoch=124
04/25/2022 03:24:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.15 on epoch=129
04/25/2022 03:24:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.16 on epoch=134
04/25/2022 03:24:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.19 on epoch=139
04/25/2022 03:24:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
04/25/2022 03:24:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.12 on epoch=149
04/25/2022 03:24:56 - INFO - __main__ - Global step 300 Train loss 0.16 ACC 0.3125 on epoch=149
04/25/2022 03:25:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
04/25/2022 03:25:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.12 on epoch=159
04/25/2022 03:25:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.12 on epoch=164
04/25/2022 03:25:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.10 on epoch=169
04/25/2022 03:25:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
04/25/2022 03:25:20 - INFO - __main__ - Global step 350 Train loss 0.12 ACC 0.375 on epoch=174
04/25/2022 03:25:20 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=174, global_step=350
04/25/2022 03:25:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.11 on epoch=179
04/25/2022 03:25:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
04/25/2022 03:25:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.10 on epoch=189
04/25/2022 03:25:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.10 on epoch=194
04/25/2022 03:25:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=199
04/25/2022 03:25:44 - INFO - __main__ - Global step 400 Train loss 0.11 ACC 0.3125 on epoch=199
04/25/2022 03:25:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.07 on epoch=204
04/25/2022 03:25:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.12 on epoch=209
04/25/2022 03:25:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.06 on epoch=214
04/25/2022 03:26:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.10 on epoch=219
04/25/2022 03:26:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
04/25/2022 03:26:09 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.375 on epoch=224
04/25/2022 03:26:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.08 on epoch=229
04/25/2022 03:26:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=234
04/25/2022 03:26:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.10 on epoch=239
04/25/2022 03:26:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
04/25/2022 03:26:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
04/25/2022 03:26:33 - INFO - __main__ - Global step 500 Train loss 0.08 ACC 0.40625 on epoch=249
04/25/2022 03:26:33 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.40625 on epoch=249, global_step=500
04/25/2022 03:26:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
04/25/2022 03:26:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=259
04/25/2022 03:26:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
04/25/2022 03:26:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
04/25/2022 03:26:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
04/25/2022 03:26:58 - INFO - __main__ - Global step 550 Train loss 0.07 ACC 0.375 on epoch=274
04/25/2022 03:27:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
04/25/2022 03:27:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
04/25/2022 03:27:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
04/25/2022 03:27:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
04/25/2022 03:27:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
04/25/2022 03:27:22 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.40625 on epoch=299
04/25/2022 03:27:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
04/25/2022 03:27:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
04/25/2022 03:27:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
04/25/2022 03:27:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
04/25/2022 03:27:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=324
04/25/2022 03:27:47 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.28125 on epoch=324
04/25/2022 03:27:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
04/25/2022 03:27:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=334
04/25/2022 03:28:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
04/25/2022 03:28:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
04/25/2022 03:28:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
04/25/2022 03:28:11 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.3125 on epoch=349
04/25/2022 03:28:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
04/25/2022 03:28:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
04/25/2022 03:28:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
04/25/2022 03:28:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
04/25/2022 03:28:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
04/25/2022 03:28:36 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.21875 on epoch=374
04/25/2022 03:28:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
04/25/2022 03:28:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
04/25/2022 03:28:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
04/25/2022 03:28:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
04/25/2022 03:28:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
04/25/2022 03:29:00 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.3125 on epoch=399
04/25/2022 03:29:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
04/25/2022 03:29:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
04/25/2022 03:29:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=414
04/25/2022 03:29:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
04/25/2022 03:29:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
04/25/2022 03:29:25 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.3125 on epoch=424
04/25/2022 03:29:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
04/25/2022 03:29:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
04/25/2022 03:29:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
04/25/2022 03:29:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
04/25/2022 03:29:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
04/25/2022 03:29:49 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.28125 on epoch=449
04/25/2022 03:29:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
04/25/2022 03:29:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
04/25/2022 03:30:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
04/25/2022 03:30:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
04/25/2022 03:30:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
04/25/2022 03:30:14 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.375 on epoch=474
04/25/2022 03:30:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/25/2022 03:30:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
04/25/2022 03:30:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
04/25/2022 03:30:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
04/25/2022 03:30:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
04/25/2022 03:30:38 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.34375 on epoch=499
04/25/2022 03:30:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
04/25/2022 03:30:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
04/25/2022 03:30:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
04/25/2022 03:30:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
04/25/2022 03:31:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/25/2022 03:31:03 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.46875 on epoch=524
04/25/2022 03:31:03 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=524, global_step=1050
04/25/2022 03:31:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
04/25/2022 03:31:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
04/25/2022 03:31:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
04/25/2022 03:31:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
04/25/2022 03:31:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
04/25/2022 03:31:27 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.375 on epoch=549
04/25/2022 03:31:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
04/25/2022 03:31:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/25/2022 03:31:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
04/25/2022 03:31:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
04/25/2022 03:31:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
04/25/2022 03:31:52 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.375 on epoch=574
04/25/2022 03:31:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 03:32:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
04/25/2022 03:32:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
04/25/2022 03:32:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
04/25/2022 03:32:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
04/25/2022 03:32:16 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.46875 on epoch=599
04/25/2022 03:32:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
04/25/2022 03:32:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 03:32:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
04/25/2022 03:32:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
04/25/2022 03:32:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
04/25/2022 03:32:41 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.40625 on epoch=624
04/25/2022 03:32:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 03:32:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 03:32:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
04/25/2022 03:32:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
04/25/2022 03:33:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/25/2022 03:33:06 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.46875 on epoch=649
04/25/2022 03:33:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
04/25/2022 03:33:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 03:33:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/25/2022 03:33:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/25/2022 03:33:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
04/25/2022 03:33:30 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.375 on epoch=674
04/25/2022 03:33:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 03:33:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 03:33:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/25/2022 03:33:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 03:33:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
04/25/2022 03:33:55 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.3125 on epoch=699
04/25/2022 03:33:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 03:34:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
04/25/2022 03:34:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/25/2022 03:34:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 03:34:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/25/2022 03:34:19 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.34375 on epoch=724
04/25/2022 03:34:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
04/25/2022 03:34:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
04/25/2022 03:34:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
04/25/2022 03:34:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 03:34:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 03:34:44 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.34375 on epoch=749
04/25/2022 03:34:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 03:34:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
04/25/2022 03:34:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 03:35:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
04/25/2022 03:35:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
04/25/2022 03:35:08 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.375 on epoch=774
04/25/2022 03:35:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/25/2022 03:35:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 03:35:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
04/25/2022 03:35:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 03:35:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
04/25/2022 03:35:33 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.40625 on epoch=799
04/25/2022 03:35:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 03:35:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 03:35:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 03:35:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
04/25/2022 03:35:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 03:35:57 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.375 on epoch=824
04/25/2022 03:36:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/25/2022 03:36:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/25/2022 03:36:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 03:36:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 03:36:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 03:36:21 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.4375 on epoch=849
04/25/2022 03:36:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
04/25/2022 03:36:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
04/25/2022 03:36:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 03:36:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 03:36:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 03:36:46 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.375 on epoch=874
04/25/2022 03:36:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
04/25/2022 03:36:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 03:36:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 03:37:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
04/25/2022 03:37:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 03:37:10 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.34375 on epoch=899
04/25/2022 03:37:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 03:37:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/25/2022 03:37:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=914
04/25/2022 03:37:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
04/25/2022 03:37:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 03:37:35 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.4375 on epoch=924
04/25/2022 03:37:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 03:37:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 03:37:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 03:37:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
04/25/2022 03:37:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 03:37:59 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.4375 on epoch=949
04/25/2022 03:38:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 03:38:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 03:38:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 03:38:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 03:38:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
04/25/2022 03:38:24 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.34375 on epoch=974
04/25/2022 03:38:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 03:38:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 03:38:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/25/2022 03:38:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
04/25/2022 03:38:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 03:38:48 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:38:48 - INFO - __main__ - Printing 3 examples
04/25/2022 03:38:48 - INFO - __main__ -  [dream] What does she have to take before she begins practical training? [SEP] W: Dr. Steven, I am thinking about changing my major before the end of the freshman year. M: What are you studying now? W: I am taking three genera] requirements and American history and American literature this. semester. Last semester I took four requirements and freshman French. M: I believe it's not too late to change your major because you've mainly taken general requirements which all freshmen have to take. Also we have twelve electives so the mo lit courses will be included in them, so you can change your major without losing any credit hours. W: I am very happy to know I am still able to change my major. I am interested in writing newspaper articles, and after finishing my degree I would like to work for some newspaper firm. M: Oh, I think you will be a good writer. W: Dr. Steven, when do students start practical training? M: They don't begin practical training in reporting until the sophomore year. Journalism 121 normally is taken in the freshman year as a general background course. W: I see. I will take the course next semester. Thank you very much for your help. M: You're welcome. I look forward to seeing you in my department. [SEP]  (A) English sociology. (B) Journalism 121. (C) Freshman French.
04/25/2022 03:38:48 - INFO - __main__ - ['Journalism 121.']
04/25/2022 03:38:48 - INFO - __main__ -  [dream] Which skirt does the man want to buy? [SEP] W: The red skirt is fifty-five yuan, the brown one fifty yuan, and the green one thirty-eight yuan. Which one do you want? M: Hm, hm... I think I'd like the cheapest one. [SEP]  (A) The red one. (B) The green one. (C) The brown one.
04/25/2022 03:38:48 - INFO - __main__ - ['The green one.']
04/25/2022 03:38:48 - INFO - __main__ -  [dream] How much should the man pay in total? [SEP] M: Excuse me, how much does an ice cream cost? W: Two yuan. And how many do you want? M: Two, please. W: Anything else you want to buy? M: Oh, yes, I still want some bread. A piece of bread, please. W: Here you are. 1.5 yuan a piece. M: By the way, do you have any fruit here, such as apples or oranges? W: I'm sorry we haven't got any. M: All right. Here's the money for you. W: Thank you. [SEP]  (A) 1.5 yuan. (B) 2 yuan (C) 5.5 yuan
04/25/2022 03:38:48 - INFO - __main__ - ['5.5 yuan']
04/25/2022 03:38:48 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:38:48 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:38:48 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 03:38:48 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:38:48 - INFO - __main__ - Printing 3 examples
04/25/2022 03:38:48 - INFO - __main__ -  [dream] Where will the woman go first after work? [SEP] Man: Are you and Sarah going straight to the restaurant from work tonight? Woman: Actually, I'm leaving work early because I need to do some shopping in the market, and after that we're going to play tennis at the sports centre before we go to the restaurant. [SEP]  (A) The restaurant. (B) The market. (C) The cinema.
04/25/2022 03:38:48 - INFO - __main__ - ['The market.']
04/25/2022 03:38:48 - INFO - __main__ -  [dream] What can be inferred about Larry and Bill? [SEP] W: I can't make up my mind whether to ask Larry or Bill to paint the house. M: What difference does it make? They're both excellent painters. And I think we just need to check who is available the day after tomorrow. [SEP]  (A) They are quite different in painting skills. (B) Neither of them is good at house-painting. (C) They are equally good at house-painting.
04/25/2022 03:38:48 - INFO - __main__ - ['They are equally good at house-painting.']
04/25/2022 03:38:48 - INFO - __main__ -  [dream] What was wrong with the alarm clock? [SEP] W: Come to my office, Billy. Look at your messy hair. This is the third time you have been late for class within this week. And every time you went in, you disturbed the teacher's class. M: Sorry, Miss. I didn't mean to do that, but it was my alarm clock that didn't wake me up. W: That is not a proper excuse. You could set it ahead of the exact time so that you could have got up earlier and not have been late. M: I did that, but it seemed useless when I found the batteries had run out after I woke up this morning. I rushed to school and luckily, my neighbour Mr. Green gave me a hand and took me to school with his car. W: So you could have made it. But you were still late for class. M: The thing is, when I got off, I found I had left my bag in his car. And it was too late to get it back when I realized that. I went straight to the nearest shop and gave him a call. I waited for five minutes before Mr. Green drove back. W: Finally, he gave you your bag and you came to the classroom and you were already 15 minutes late for class at that time. Is that all? M: Miss, you seem to know the entire story. May I go back to class now? Otherwise I will be late for the following class. [SEP]  (A) The batteries ran out. (B) The alarm clock was broken. (C) The alarm clock was ahead of the exact time.
04/25/2022 03:38:48 - INFO - __main__ - ['The batteries ran out.']
04/25/2022 03:38:48 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:38:48 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:38:48 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 03:38:48 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.375 on epoch=999
04/25/2022 03:38:48 - INFO - __main__ - save last model!
04/25/2022 03:38:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 03:38:48 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 03:38:48 - INFO - __main__ - Printing 3 examples
04/25/2022 03:38:48 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 03:38:48 - INFO - __main__ - ['To change her job.']
04/25/2022 03:38:48 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 03:38:48 - INFO - __main__ - ['Watch TV.']
04/25/2022 03:38:48 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 03:38:48 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 03:38:48 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:38:49 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:38:50 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 03:39:03 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 03:39:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 03:39:03 - INFO - __main__ - Starting training!
04/25/2022 03:40:05 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_100_0.4_8_predictions.txt
04/25/2022 03:40:05 - INFO - __main__ - ACC on test data: 0.2990
04/25/2022 03:40:05 - INFO - __main__ - prefix=dream_32_100, lr=0.4, bsz=8, dev_performance=0.46875, test_performance=0.299
04/25/2022 03:40:05 - INFO - __main__ - Running ... prefix=dream_32_100, lr=0.3, bsz=8 ...
04/25/2022 03:40:06 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:40:06 - INFO - __main__ - Printing 3 examples
04/25/2022 03:40:06 - INFO - __main__ -  [dream] What does she have to take before she begins practical training? [SEP] W: Dr. Steven, I am thinking about changing my major before the end of the freshman year. M: What are you studying now? W: I am taking three genera] requirements and American history and American literature this. semester. Last semester I took four requirements and freshman French. M: I believe it's not too late to change your major because you've mainly taken general requirements which all freshmen have to take. Also we have twelve electives so the mo lit courses will be included in them, so you can change your major without losing any credit hours. W: I am very happy to know I am still able to change my major. I am interested in writing newspaper articles, and after finishing my degree I would like to work for some newspaper firm. M: Oh, I think you will be a good writer. W: Dr. Steven, when do students start practical training? M: They don't begin practical training in reporting until the sophomore year. Journalism 121 normally is taken in the freshman year as a general background course. W: I see. I will take the course next semester. Thank you very much for your help. M: You're welcome. I look forward to seeing you in my department. [SEP]  (A) English sociology. (B) Journalism 121. (C) Freshman French.
04/25/2022 03:40:06 - INFO - __main__ - ['Journalism 121.']
04/25/2022 03:40:06 - INFO - __main__ -  [dream] Which skirt does the man want to buy? [SEP] W: The red skirt is fifty-five yuan, the brown one fifty yuan, and the green one thirty-eight yuan. Which one do you want? M: Hm, hm... I think I'd like the cheapest one. [SEP]  (A) The red one. (B) The green one. (C) The brown one.
04/25/2022 03:40:06 - INFO - __main__ - ['The green one.']
04/25/2022 03:40:06 - INFO - __main__ -  [dream] How much should the man pay in total? [SEP] M: Excuse me, how much does an ice cream cost? W: Two yuan. And how many do you want? M: Two, please. W: Anything else you want to buy? M: Oh, yes, I still want some bread. A piece of bread, please. W: Here you are. 1.5 yuan a piece. M: By the way, do you have any fruit here, such as apples or oranges? W: I'm sorry we haven't got any. M: All right. Here's the money for you. W: Thank you. [SEP]  (A) 1.5 yuan. (B) 2 yuan (C) 5.5 yuan
04/25/2022 03:40:06 - INFO - __main__ - ['5.5 yuan']
04/25/2022 03:40:06 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:40:06 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:40:06 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 03:40:06 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:40:06 - INFO - __main__ - Printing 3 examples
04/25/2022 03:40:06 - INFO - __main__ -  [dream] Where will the woman go first after work? [SEP] Man: Are you and Sarah going straight to the restaurant from work tonight? Woman: Actually, I'm leaving work early because I need to do some shopping in the market, and after that we're going to play tennis at the sports centre before we go to the restaurant. [SEP]  (A) The restaurant. (B) The market. (C) The cinema.
04/25/2022 03:40:06 - INFO - __main__ - ['The market.']
04/25/2022 03:40:06 - INFO - __main__ -  [dream] What can be inferred about Larry and Bill? [SEP] W: I can't make up my mind whether to ask Larry or Bill to paint the house. M: What difference does it make? They're both excellent painters. And I think we just need to check who is available the day after tomorrow. [SEP]  (A) They are quite different in painting skills. (B) Neither of them is good at house-painting. (C) They are equally good at house-painting.
04/25/2022 03:40:06 - INFO - __main__ - ['They are equally good at house-painting.']
04/25/2022 03:40:06 - INFO - __main__ -  [dream] What was wrong with the alarm clock? [SEP] W: Come to my office, Billy. Look at your messy hair. This is the third time you have been late for class within this week. And every time you went in, you disturbed the teacher's class. M: Sorry, Miss. I didn't mean to do that, but it was my alarm clock that didn't wake me up. W: That is not a proper excuse. You could set it ahead of the exact time so that you could have got up earlier and not have been late. M: I did that, but it seemed useless when I found the batteries had run out after I woke up this morning. I rushed to school and luckily, my neighbour Mr. Green gave me a hand and took me to school with his car. W: So you could have made it. But you were still late for class. M: The thing is, when I got off, I found I had left my bag in his car. And it was too late to get it back when I realized that. I went straight to the nearest shop and gave him a call. I waited for five minutes before Mr. Green drove back. W: Finally, he gave you your bag and you came to the classroom and you were already 15 minutes late for class at that time. Is that all? M: Miss, you seem to know the entire story. May I go back to class now? Otherwise I will be late for the following class. [SEP]  (A) The batteries ran out. (B) The alarm clock was broken. (C) The alarm clock was ahead of the exact time.
04/25/2022 03:40:06 - INFO - __main__ - ['The batteries ran out.']
04/25/2022 03:40:06 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:40:06 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:40:06 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 03:40:25 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 03:40:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 03:40:26 - INFO - __main__ - Starting training!
04/25/2022 03:40:31 - INFO - __main__ - Step 10 Global step 10 Train loss 1.67 on epoch=4
04/25/2022 03:40:35 - INFO - __main__ - Step 20 Global step 20 Train loss 1.33 on epoch=9
04/25/2022 03:40:40 - INFO - __main__ - Step 30 Global step 30 Train loss 1.15 on epoch=14
04/25/2022 03:40:44 - INFO - __main__ - Step 40 Global step 40 Train loss 0.97 on epoch=19
04/25/2022 03:40:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.82 on epoch=24
04/25/2022 03:40:52 - INFO - __main__ - Global step 50 Train loss 1.19 ACC 0.21875 on epoch=24
04/25/2022 03:40:52 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.21875 on epoch=24, global_step=50
04/25/2022 03:40:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.72 on epoch=29
04/25/2022 03:41:01 - INFO - __main__ - Step 70 Global step 70 Train loss 1.25 on epoch=34
04/25/2022 03:41:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.79 on epoch=39
04/25/2022 03:41:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=44
04/25/2022 03:41:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.64 on epoch=49
04/25/2022 03:41:17 - INFO - __main__ - Global step 100 Train loss 0.80 ACC 0.34375 on epoch=49
04/25/2022 03:41:17 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.34375 on epoch=49, global_step=100
04/25/2022 03:41:22 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=54
04/25/2022 03:41:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=59
04/25/2022 03:41:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=64
04/25/2022 03:41:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=69
04/25/2022 03:41:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=74
04/25/2022 03:41:42 - INFO - __main__ - Global step 150 Train loss 0.53 ACC 0.34375 on epoch=74
04/25/2022 03:41:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=79
04/25/2022 03:41:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=84
04/25/2022 03:41:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=89
04/25/2022 03:42:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=94
04/25/2022 03:42:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=99
04/25/2022 03:42:08 - INFO - __main__ - Global step 200 Train loss 0.46 ACC 0.4375 on epoch=99
04/25/2022 03:42:08 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.4375 on epoch=99, global_step=200
04/25/2022 03:42:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=104
04/25/2022 03:42:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
04/25/2022 03:42:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
04/25/2022 03:42:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
04/25/2022 03:42:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=124
04/25/2022 03:42:33 - INFO - __main__ - Global step 250 Train loss 0.37 ACC 0.34375 on epoch=124
04/25/2022 03:42:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.35 on epoch=129
04/25/2022 03:42:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
04/25/2022 03:42:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=139
04/25/2022 03:42:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
04/25/2022 03:42:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=149
04/25/2022 03:42:58 - INFO - __main__ - Global step 300 Train loss 0.33 ACC 0.40625 on epoch=149
04/25/2022 03:43:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=154
04/25/2022 03:43:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=159
04/25/2022 03:43:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
04/25/2022 03:43:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
04/25/2022 03:43:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.32 on epoch=174
04/25/2022 03:43:23 - INFO - __main__ - Global step 350 Train loss 0.29 ACC 0.3125 on epoch=174
04/25/2022 03:43:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=179
04/25/2022 03:43:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=184
04/25/2022 03:43:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.18 on epoch=189
04/25/2022 03:43:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.16 on epoch=194
04/25/2022 03:43:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
04/25/2022 03:43:48 - INFO - __main__ - Global step 400 Train loss 0.20 ACC 0.4375 on epoch=199
04/25/2022 03:43:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=204
04/25/2022 03:43:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
04/25/2022 03:44:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.19 on epoch=214
04/25/2022 03:44:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
04/25/2022 03:44:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=224
04/25/2022 03:44:13 - INFO - __main__ - Global step 450 Train loss 0.19 ACC 0.28125 on epoch=224
04/25/2022 03:44:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=229
04/25/2022 03:44:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=234
04/25/2022 03:44:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=239
04/25/2022 03:44:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.19 on epoch=244
04/25/2022 03:44:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
04/25/2022 03:44:37 - INFO - __main__ - Global step 500 Train loss 0.18 ACC 0.3125 on epoch=249
04/25/2022 03:44:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=254
04/25/2022 03:44:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.17 on epoch=259
04/25/2022 03:44:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=264
04/25/2022 03:44:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=269
04/25/2022 03:45:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=274
04/25/2022 03:45:02 - INFO - __main__ - Global step 550 Train loss 0.27 ACC 0.34375 on epoch=274
04/25/2022 03:45:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=279
04/25/2022 03:45:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=284
04/25/2022 03:45:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=289
04/25/2022 03:45:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=294
04/25/2022 03:45:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=299
04/25/2022 03:45:27 - INFO - __main__ - Global step 600 Train loss 0.21 ACC 0.40625 on epoch=299
04/25/2022 03:45:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=304
04/25/2022 03:45:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=309
04/25/2022 03:45:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=314
04/25/2022 03:45:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
04/25/2022 03:45:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=324
04/25/2022 03:45:52 - INFO - __main__ - Global step 650 Train loss 0.19 ACC 0.34375 on epoch=324
04/25/2022 03:45:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=329
04/25/2022 03:46:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
04/25/2022 03:46:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=339
04/25/2022 03:46:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=344
04/25/2022 03:46:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=349
04/25/2022 03:46:17 - INFO - __main__ - Global step 700 Train loss 0.14 ACC 0.34375 on epoch=349
04/25/2022 03:46:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=354
04/25/2022 03:46:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=359
04/25/2022 03:46:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
04/25/2022 03:46:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=369
04/25/2022 03:46:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=374
04/25/2022 03:46:42 - INFO - __main__ - Global step 750 Train loss 0.15 ACC 0.28125 on epoch=374
04/25/2022 03:46:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=379
04/25/2022 03:46:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=384
04/25/2022 03:46:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=389
04/25/2022 03:46:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=394
04/25/2022 03:47:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=399
04/25/2022 03:47:07 - INFO - __main__ - Global step 800 Train loss 0.14 ACC 0.34375 on epoch=399
04/25/2022 03:47:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=404
04/25/2022 03:47:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.13 on epoch=409
04/25/2022 03:47:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=414
04/25/2022 03:47:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=419
04/25/2022 03:47:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=424
04/25/2022 03:47:31 - INFO - __main__ - Global step 850 Train loss 0.12 ACC 0.375 on epoch=424
04/25/2022 03:47:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=429
04/25/2022 03:47:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=434
04/25/2022 03:47:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=439
04/25/2022 03:47:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=444
04/25/2022 03:47:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=449
04/25/2022 03:47:56 - INFO - __main__ - Global step 900 Train loss 0.12 ACC 0.3125 on epoch=449
04/25/2022 03:48:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=454
04/25/2022 03:48:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=459
04/25/2022 03:48:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.16 on epoch=464
04/25/2022 03:48:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=469
04/25/2022 03:48:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=474
04/25/2022 03:48:21 - INFO - __main__ - Global step 950 Train loss 0.14 ACC 0.375 on epoch=474
04/25/2022 03:48:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=479
04/25/2022 03:48:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=484
04/25/2022 03:48:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=489
04/25/2022 03:48:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=494
04/25/2022 03:48:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=499
04/25/2022 03:48:46 - INFO - __main__ - Global step 1000 Train loss 0.11 ACC 0.28125 on epoch=499
04/25/2022 03:48:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=504
04/25/2022 03:48:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=509
04/25/2022 03:48:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=514
04/25/2022 03:49:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=519
04/25/2022 03:49:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=524
04/25/2022 03:49:11 - INFO - __main__ - Global step 1050 Train loss 0.11 ACC 0.25 on epoch=524
04/25/2022 03:49:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=529
04/25/2022 03:49:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=534
04/25/2022 03:49:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=539
04/25/2022 03:49:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=544
04/25/2022 03:49:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=549
04/25/2022 03:49:36 - INFO - __main__ - Global step 1100 Train loss 0.09 ACC 0.25 on epoch=549
04/25/2022 03:49:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=554
04/25/2022 03:49:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=559
04/25/2022 03:49:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=564
04/25/2022 03:49:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
04/25/2022 03:49:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=574
04/25/2022 03:50:01 - INFO - __main__ - Global step 1150 Train loss 0.08 ACC 0.28125 on epoch=574
04/25/2022 03:50:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=579
04/25/2022 03:50:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
04/25/2022 03:50:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=589
04/25/2022 03:50:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=594
04/25/2022 03:50:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=599
04/25/2022 03:50:25 - INFO - __main__ - Global step 1200 Train loss 0.09 ACC 0.28125 on epoch=599
04/25/2022 03:50:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=604
04/25/2022 03:50:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=609
04/25/2022 03:50:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
04/25/2022 03:50:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=619
04/25/2022 03:50:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=624
04/25/2022 03:50:51 - INFO - __main__ - Global step 1250 Train loss 0.09 ACC 0.28125 on epoch=624
04/25/2022 03:50:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.09 on epoch=629
04/25/2022 03:50:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=634
04/25/2022 03:51:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=639
04/25/2022 03:51:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=644
04/25/2022 03:51:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=649
04/25/2022 03:51:15 - INFO - __main__ - Global step 1300 Train loss 0.08 ACC 0.25 on epoch=649
04/25/2022 03:51:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=654
04/25/2022 03:51:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=659
04/25/2022 03:51:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=664
04/25/2022 03:51:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
04/25/2022 03:51:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=674
04/25/2022 03:51:40 - INFO - __main__ - Global step 1350 Train loss 0.07 ACC 0.28125 on epoch=674
04/25/2022 03:51:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=679
04/25/2022 03:51:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=684
04/25/2022 03:51:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=689
04/25/2022 03:51:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=694
04/25/2022 03:52:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
04/25/2022 03:52:05 - INFO - __main__ - Global step 1400 Train loss 0.07 ACC 0.28125 on epoch=699
04/25/2022 03:52:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=704
04/25/2022 03:52:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=709
04/25/2022 03:52:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=714
04/25/2022 03:52:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=719
04/25/2022 03:52:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=724
04/25/2022 03:52:30 - INFO - __main__ - Global step 1450 Train loss 0.07 ACC 0.25 on epoch=724
04/25/2022 03:52:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=729
04/25/2022 03:52:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=734
04/25/2022 03:52:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=739
04/25/2022 03:52:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=744
04/25/2022 03:52:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
04/25/2022 03:52:55 - INFO - __main__ - Global step 1500 Train loss 0.07 ACC 0.21875 on epoch=749
04/25/2022 03:52:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=754
04/25/2022 03:53:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=759
04/25/2022 03:53:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=764
04/25/2022 03:53:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=769
04/25/2022 03:53:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=774
04/25/2022 03:53:20 - INFO - __main__ - Global step 1550 Train loss 0.06 ACC 0.21875 on epoch=774
04/25/2022 03:53:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=779
04/25/2022 03:53:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=784
04/25/2022 03:53:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=789
04/25/2022 03:53:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=794
04/25/2022 03:53:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=799
04/25/2022 03:53:45 - INFO - __main__ - Global step 1600 Train loss 0.06 ACC 0.3125 on epoch=799
04/25/2022 03:53:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=804
04/25/2022 03:53:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
04/25/2022 03:53:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=814
04/25/2022 03:54:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=819
04/25/2022 03:54:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=824
04/25/2022 03:54:10 - INFO - __main__ - Global step 1650 Train loss 0.06 ACC 0.21875 on epoch=824
04/25/2022 03:54:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=829
04/25/2022 03:54:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=834
04/25/2022 03:54:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
04/25/2022 03:54:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=844
04/25/2022 03:54:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=849
04/25/2022 03:54:35 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.21875 on epoch=849
04/25/2022 03:54:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=854
04/25/2022 03:54:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=859
04/25/2022 03:54:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
04/25/2022 03:54:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=869
04/25/2022 03:54:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=874
04/25/2022 03:55:00 - INFO - __main__ - Global step 1750 Train loss 0.04 ACC 0.3125 on epoch=874
04/25/2022 03:55:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
04/25/2022 03:55:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=884
04/25/2022 03:55:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
04/25/2022 03:55:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=894
04/25/2022 03:55:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=899
04/25/2022 03:55:25 - INFO - __main__ - Global step 1800 Train loss 0.04 ACC 0.1875 on epoch=899
04/25/2022 03:55:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=904
04/25/2022 03:55:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=909
04/25/2022 03:55:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=914
04/25/2022 03:55:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
04/25/2022 03:55:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
04/25/2022 03:55:50 - INFO - __main__ - Global step 1850 Train loss 0.05 ACC 0.21875 on epoch=924
04/25/2022 03:55:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=929
04/25/2022 03:55:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
04/25/2022 03:56:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=939
04/25/2022 03:56:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=944
04/25/2022 03:56:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=949
04/25/2022 03:56:16 - INFO - __main__ - Global step 1900 Train loss 0.04 ACC 0.1875 on epoch=949
04/25/2022 03:56:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=954
04/25/2022 03:56:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=959
04/25/2022 03:56:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=964
04/25/2022 03:56:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
04/25/2022 03:56:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
04/25/2022 03:56:41 - INFO - __main__ - Global step 1950 Train loss 0.04 ACC 0.1875 on epoch=974
04/25/2022 03:56:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=979
04/25/2022 03:56:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=984
04/25/2022 03:56:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
04/25/2022 03:56:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=994
04/25/2022 03:57:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
04/25/2022 03:57:04 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:57:04 - INFO - __main__ - Printing 3 examples
04/25/2022 03:57:04 - INFO - __main__ -  [dream] What does she have to take before she begins practical training? [SEP] W: Dr. Steven, I am thinking about changing my major before the end of the freshman year. M: What are you studying now? W: I am taking three genera] requirements and American history and American literature this. semester. Last semester I took four requirements and freshman French. M: I believe it's not too late to change your major because you've mainly taken general requirements which all freshmen have to take. Also we have twelve electives so the mo lit courses will be included in them, so you can change your major without losing any credit hours. W: I am very happy to know I am still able to change my major. I am interested in writing newspaper articles, and after finishing my degree I would like to work for some newspaper firm. M: Oh, I think you will be a good writer. W: Dr. Steven, when do students start practical training? M: They don't begin practical training in reporting until the sophomore year. Journalism 121 normally is taken in the freshman year as a general background course. W: I see. I will take the course next semester. Thank you very much for your help. M: You're welcome. I look forward to seeing you in my department. [SEP]  (A) English sociology. (B) Journalism 121. (C) Freshman French.
04/25/2022 03:57:04 - INFO - __main__ - ['Journalism 121.']
04/25/2022 03:57:04 - INFO - __main__ -  [dream] Which skirt does the man want to buy? [SEP] W: The red skirt is fifty-five yuan, the brown one fifty yuan, and the green one thirty-eight yuan. Which one do you want? M: Hm, hm... I think I'd like the cheapest one. [SEP]  (A) The red one. (B) The green one. (C) The brown one.
04/25/2022 03:57:04 - INFO - __main__ - ['The green one.']
04/25/2022 03:57:04 - INFO - __main__ -  [dream] How much should the man pay in total? [SEP] M: Excuse me, how much does an ice cream cost? W: Two yuan. And how many do you want? M: Two, please. W: Anything else you want to buy? M: Oh, yes, I still want some bread. A piece of bread, please. W: Here you are. 1.5 yuan a piece. M: By the way, do you have any fruit here, such as apples or oranges? W: I'm sorry we haven't got any. M: All right. Here's the money for you. W: Thank you. [SEP]  (A) 1.5 yuan. (B) 2 yuan (C) 5.5 yuan
04/25/2022 03:57:04 - INFO - __main__ - ['5.5 yuan']
04/25/2022 03:57:04 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:57:04 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:57:04 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 03:57:04 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:57:04 - INFO - __main__ - Printing 3 examples
04/25/2022 03:57:04 - INFO - __main__ -  [dream] Where will the woman go first after work? [SEP] Man: Are you and Sarah going straight to the restaurant from work tonight? Woman: Actually, I'm leaving work early because I need to do some shopping in the market, and after that we're going to play tennis at the sports centre before we go to the restaurant. [SEP]  (A) The restaurant. (B) The market. (C) The cinema.
04/25/2022 03:57:04 - INFO - __main__ - ['The market.']
04/25/2022 03:57:04 - INFO - __main__ -  [dream] What can be inferred about Larry and Bill? [SEP] W: I can't make up my mind whether to ask Larry or Bill to paint the house. M: What difference does it make? They're both excellent painters. And I think we just need to check who is available the day after tomorrow. [SEP]  (A) They are quite different in painting skills. (B) Neither of them is good at house-painting. (C) They are equally good at house-painting.
04/25/2022 03:57:04 - INFO - __main__ - ['They are equally good at house-painting.']
04/25/2022 03:57:04 - INFO - __main__ -  [dream] What was wrong with the alarm clock? [SEP] W: Come to my office, Billy. Look at your messy hair. This is the third time you have been late for class within this week. And every time you went in, you disturbed the teacher's class. M: Sorry, Miss. I didn't mean to do that, but it was my alarm clock that didn't wake me up. W: That is not a proper excuse. You could set it ahead of the exact time so that you could have got up earlier and not have been late. M: I did that, but it seemed useless when I found the batteries had run out after I woke up this morning. I rushed to school and luckily, my neighbour Mr. Green gave me a hand and took me to school with his car. W: So you could have made it. But you were still late for class. M: The thing is, when I got off, I found I had left my bag in his car. And it was too late to get it back when I realized that. I went straight to the nearest shop and gave him a call. I waited for five minutes before Mr. Green drove back. W: Finally, he gave you your bag and you came to the classroom and you were already 15 minutes late for class at that time. Is that all? M: Miss, you seem to know the entire story. May I go back to class now? Otherwise I will be late for the following class. [SEP]  (A) The batteries ran out. (B) The alarm clock was broken. (C) The alarm clock was ahead of the exact time.
04/25/2022 03:57:04 - INFO - __main__ - ['The batteries ran out.']
04/25/2022 03:57:04 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:57:04 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:57:04 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 03:57:06 - INFO - __main__ - Global step 2000 Train loss 0.04 ACC 0.21875 on epoch=999
04/25/2022 03:57:06 - INFO - __main__ - save last model!
04/25/2022 03:57:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 03:57:06 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 03:57:06 - INFO - __main__ - Printing 3 examples
04/25/2022 03:57:06 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 03:57:06 - INFO - __main__ - ['To change her job.']
04/25/2022 03:57:06 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 03:57:06 - INFO - __main__ - ['Watch TV.']
04/25/2022 03:57:06 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 03:57:06 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 03:57:06 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:57:07 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:57:08 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 03:57:19 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 03:57:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 03:57:20 - INFO - __main__ - Starting training!
04/25/2022 03:58:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_100_0.3_8_predictions.txt
04/25/2022 03:58:28 - INFO - __main__ - ACC on test data: 0.3280
04/25/2022 03:58:28 - INFO - __main__ - prefix=dream_32_100, lr=0.3, bsz=8, dev_performance=0.4375, test_performance=0.328
04/25/2022 03:58:28 - INFO - __main__ - Running ... prefix=dream_32_100, lr=0.2, bsz=8 ...
04/25/2022 03:58:29 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:58:29 - INFO - __main__ - Printing 3 examples
04/25/2022 03:58:29 - INFO - __main__ -  [dream] What does she have to take before she begins practical training? [SEP] W: Dr. Steven, I am thinking about changing my major before the end of the freshman year. M: What are you studying now? W: I am taking three genera] requirements and American history and American literature this. semester. Last semester I took four requirements and freshman French. M: I believe it's not too late to change your major because you've mainly taken general requirements which all freshmen have to take. Also we have twelve electives so the mo lit courses will be included in them, so you can change your major without losing any credit hours. W: I am very happy to know I am still able to change my major. I am interested in writing newspaper articles, and after finishing my degree I would like to work for some newspaper firm. M: Oh, I think you will be a good writer. W: Dr. Steven, when do students start practical training? M: They don't begin practical training in reporting until the sophomore year. Journalism 121 normally is taken in the freshman year as a general background course. W: I see. I will take the course next semester. Thank you very much for your help. M: You're welcome. I look forward to seeing you in my department. [SEP]  (A) English sociology. (B) Journalism 121. (C) Freshman French.
04/25/2022 03:58:29 - INFO - __main__ - ['Journalism 121.']
04/25/2022 03:58:29 - INFO - __main__ -  [dream] Which skirt does the man want to buy? [SEP] W: The red skirt is fifty-five yuan, the brown one fifty yuan, and the green one thirty-eight yuan. Which one do you want? M: Hm, hm... I think I'd like the cheapest one. [SEP]  (A) The red one. (B) The green one. (C) The brown one.
04/25/2022 03:58:29 - INFO - __main__ - ['The green one.']
04/25/2022 03:58:29 - INFO - __main__ -  [dream] How much should the man pay in total? [SEP] M: Excuse me, how much does an ice cream cost? W: Two yuan. And how many do you want? M: Two, please. W: Anything else you want to buy? M: Oh, yes, I still want some bread. A piece of bread, please. W: Here you are. 1.5 yuan a piece. M: By the way, do you have any fruit here, such as apples or oranges? W: I'm sorry we haven't got any. M: All right. Here's the money for you. W: Thank you. [SEP]  (A) 1.5 yuan. (B) 2 yuan (C) 5.5 yuan
04/25/2022 03:58:29 - INFO - __main__ - ['5.5 yuan']
04/25/2022 03:58:29 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:58:29 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:58:29 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 03:58:29 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 03:58:29 - INFO - __main__ - Printing 3 examples
04/25/2022 03:58:29 - INFO - __main__ -  [dream] Where will the woman go first after work? [SEP] Man: Are you and Sarah going straight to the restaurant from work tonight? Woman: Actually, I'm leaving work early because I need to do some shopping in the market, and after that we're going to play tennis at the sports centre before we go to the restaurant. [SEP]  (A) The restaurant. (B) The market. (C) The cinema.
04/25/2022 03:58:29 - INFO - __main__ - ['The market.']
04/25/2022 03:58:29 - INFO - __main__ -  [dream] What can be inferred about Larry and Bill? [SEP] W: I can't make up my mind whether to ask Larry or Bill to paint the house. M: What difference does it make? They're both excellent painters. And I think we just need to check who is available the day after tomorrow. [SEP]  (A) They are quite different in painting skills. (B) Neither of them is good at house-painting. (C) They are equally good at house-painting.
04/25/2022 03:58:29 - INFO - __main__ - ['They are equally good at house-painting.']
04/25/2022 03:58:29 - INFO - __main__ -  [dream] What was wrong with the alarm clock? [SEP] W: Come to my office, Billy. Look at your messy hair. This is the third time you have been late for class within this week. And every time you went in, you disturbed the teacher's class. M: Sorry, Miss. I didn't mean to do that, but it was my alarm clock that didn't wake me up. W: That is not a proper excuse. You could set it ahead of the exact time so that you could have got up earlier and not have been late. M: I did that, but it seemed useless when I found the batteries had run out after I woke up this morning. I rushed to school and luckily, my neighbour Mr. Green gave me a hand and took me to school with his car. W: So you could have made it. But you were still late for class. M: The thing is, when I got off, I found I had left my bag in his car. And it was too late to get it back when I realized that. I went straight to the nearest shop and gave him a call. I waited for five minutes before Mr. Green drove back. W: Finally, he gave you your bag and you came to the classroom and you were already 15 minutes late for class at that time. Is that all? M: Miss, you seem to know the entire story. May I go back to class now? Otherwise I will be late for the following class. [SEP]  (A) The batteries ran out. (B) The alarm clock was broken. (C) The alarm clock was ahead of the exact time.
04/25/2022 03:58:29 - INFO - __main__ - ['The batteries ran out.']
04/25/2022 03:58:29 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:58:29 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:58:29 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 03:58:44 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 03:58:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 03:58:45 - INFO - __main__ - Starting training!
04/25/2022 03:58:53 - INFO - __main__ - Step 10 Global step 10 Train loss 1.83 on epoch=4
04/25/2022 03:58:57 - INFO - __main__ - Step 20 Global step 20 Train loss 1.48 on epoch=9
04/25/2022 03:59:02 - INFO - __main__ - Step 30 Global step 30 Train loss 1.31 on epoch=14
04/25/2022 03:59:06 - INFO - __main__ - Step 40 Global step 40 Train loss 1.18 on epoch=19
04/25/2022 03:59:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.03 on epoch=24
04/25/2022 03:59:13 - INFO - __main__ - Global step 50 Train loss 1.36 ACC 0.3125 on epoch=24
04/25/2022 03:59:13 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.3125 on epoch=24, global_step=50
04/25/2022 03:59:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.94 on epoch=29
04/25/2022 03:59:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.81 on epoch=34
04/25/2022 03:59:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.72 on epoch=39
04/25/2022 03:59:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.64 on epoch=44
04/25/2022 03:59:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=49
04/25/2022 03:59:37 - INFO - __main__ - Global step 100 Train loss 0.74 ACC 0.4375 on epoch=49
04/25/2022 03:59:38 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.4375 on epoch=49, global_step=100
04/25/2022 03:59:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=54
04/25/2022 03:59:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=59
04/25/2022 03:59:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=64
04/25/2022 03:59:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=69
04/25/2022 03:59:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=74
04/25/2022 04:00:02 - INFO - __main__ - Global step 150 Train loss 0.51 ACC 0.4375 on epoch=74
04/25/2022 04:00:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=79
04/25/2022 04:00:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=84
04/25/2022 04:00:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
04/25/2022 04:00:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.33 on epoch=94
04/25/2022 04:00:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=99
04/25/2022 04:00:27 - INFO - __main__ - Global step 200 Train loss 0.40 ACC 0.34375 on epoch=99
04/25/2022 04:00:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=104
04/25/2022 04:00:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.31 on epoch=109
04/25/2022 04:00:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=114
04/25/2022 04:00:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
04/25/2022 04:00:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=124
04/25/2022 04:00:52 - INFO - __main__ - Global step 250 Train loss 0.33 ACC 0.40625 on epoch=124
04/25/2022 04:00:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=129
04/25/2022 04:01:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
04/25/2022 04:01:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
04/25/2022 04:01:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
04/25/2022 04:01:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=149
04/25/2022 04:01:17 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.3125 on epoch=149
04/25/2022 04:01:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=154
04/25/2022 04:01:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=159
04/25/2022 04:01:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
04/25/2022 04:01:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
04/25/2022 04:01:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
04/25/2022 04:01:41 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.28125 on epoch=174
04/25/2022 04:01:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.20 on epoch=179
04/25/2022 04:01:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.16 on epoch=184
04/25/2022 04:01:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=189
04/25/2022 04:01:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=194
04/25/2022 04:02:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=199
04/25/2022 04:02:06 - INFO - __main__ - Global step 400 Train loss 0.19 ACC 0.40625 on epoch=199
04/25/2022 04:02:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=204
04/25/2022 04:02:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=209
04/25/2022 04:02:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=214
04/25/2022 04:02:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.19 on epoch=219
04/25/2022 04:02:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=224
04/25/2022 04:02:31 - INFO - __main__ - Global step 450 Train loss 0.18 ACC 0.40625 on epoch=224
04/25/2022 04:02:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=229
04/25/2022 04:02:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.11 on epoch=234
04/25/2022 04:02:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=239
04/25/2022 04:02:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
04/25/2022 04:02:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=249
04/25/2022 04:02:56 - INFO - __main__ - Global step 500 Train loss 0.13 ACC 0.34375 on epoch=249
04/25/2022 04:03:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=254
04/25/2022 04:03:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=259
04/25/2022 04:03:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
04/25/2022 04:03:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.12 on epoch=269
04/25/2022 04:03:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=274
04/25/2022 04:03:20 - INFO - __main__ - Global step 550 Train loss 0.15 ACC 0.21875 on epoch=274
04/25/2022 04:03:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=279
04/25/2022 04:03:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=284
04/25/2022 04:03:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=289
04/25/2022 04:03:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
04/25/2022 04:03:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.10 on epoch=299
04/25/2022 04:03:45 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.25 on epoch=299
04/25/2022 04:03:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.08 on epoch=304
04/25/2022 04:03:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=309
04/25/2022 04:03:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=314
04/25/2022 04:04:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=319
04/25/2022 04:04:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
04/25/2022 04:04:10 - INFO - __main__ - Global step 650 Train loss 0.11 ACC 0.25 on epoch=324
04/25/2022 04:04:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.10 on epoch=329
04/25/2022 04:04:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=334
04/25/2022 04:04:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=339
04/25/2022 04:04:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=344
04/25/2022 04:04:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=349
04/25/2022 04:04:34 - INFO - __main__ - Global step 700 Train loss 0.10 ACC 0.28125 on epoch=349
04/25/2022 04:04:39 - INFO - __main__ - Step 710 Global step 710 Train loss 1.24 on epoch=354
04/25/2022 04:04:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=359
04/25/2022 04:04:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=364
04/25/2022 04:04:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
04/25/2022 04:04:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=374
04/25/2022 04:04:59 - INFO - __main__ - Global step 750 Train loss 0.36 ACC 0.21875 on epoch=374
04/25/2022 04:05:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=379
04/25/2022 04:05:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=384
04/25/2022 04:05:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=389
04/25/2022 04:05:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=394
04/25/2022 04:05:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=399
04/25/2022 04:05:24 - INFO - __main__ - Global step 800 Train loss 0.11 ACC 0.21875 on epoch=399
04/25/2022 04:05:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=404
04/25/2022 04:05:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=409
04/25/2022 04:05:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=414
04/25/2022 04:05:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=419
04/25/2022 04:05:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=424
04/25/2022 04:05:49 - INFO - __main__ - Global step 850 Train loss 0.13 ACC 0.25 on epoch=424
04/25/2022 04:05:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=429
04/25/2022 04:05:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=434
04/25/2022 04:06:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=439
04/25/2022 04:06:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=444
04/25/2022 04:06:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=449
04/25/2022 04:06:14 - INFO - __main__ - Global step 900 Train loss 0.10 ACC 0.25 on epoch=449
04/25/2022 04:06:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=454
04/25/2022 04:06:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=459
04/25/2022 04:06:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
04/25/2022 04:06:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=469
04/25/2022 04:06:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=474
04/25/2022 04:06:39 - INFO - __main__ - Global step 950 Train loss 0.14 ACC 0.21875 on epoch=474
04/25/2022 04:06:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=479
04/25/2022 04:06:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=484
04/25/2022 04:06:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=489
04/25/2022 04:06:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.13 on epoch=494
04/25/2022 04:07:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=499
04/25/2022 04:07:04 - INFO - __main__ - Global step 1000 Train loss 0.10 ACC 0.1875 on epoch=499
04/25/2022 04:07:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=504
04/25/2022 04:07:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=509
04/25/2022 04:07:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=514
04/25/2022 04:07:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
04/25/2022 04:07:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=524
04/25/2022 04:07:28 - INFO - __main__ - Global step 1050 Train loss 0.09 ACC 0.1875 on epoch=524
04/25/2022 04:07:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=529
04/25/2022 04:07:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=534
04/25/2022 04:07:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=539
04/25/2022 04:07:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
04/25/2022 04:07:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=549
04/25/2022 04:07:53 - INFO - __main__ - Global step 1100 Train loss 0.09 ACC 0.21875 on epoch=549
04/25/2022 04:07:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=554
04/25/2022 04:08:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=559
04/25/2022 04:08:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=564
04/25/2022 04:08:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=569
04/25/2022 04:08:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=574
04/25/2022 04:08:18 - INFO - __main__ - Global step 1150 Train loss 0.09 ACC 0.21875 on epoch=574
04/25/2022 04:08:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=579
04/25/2022 04:08:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
04/25/2022 04:08:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=589
04/25/2022 04:08:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=594
04/25/2022 04:08:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=599
04/25/2022 04:08:42 - INFO - __main__ - Global step 1200 Train loss 0.07 ACC 0.28125 on epoch=599
04/25/2022 04:08:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.09 on epoch=604
04/25/2022 04:08:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
04/25/2022 04:08:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=614
04/25/2022 04:09:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=619
04/25/2022 04:09:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=624
04/25/2022 04:09:07 - INFO - __main__ - Global step 1250 Train loss 0.08 ACC 0.25 on epoch=624
04/25/2022 04:09:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=629
04/25/2022 04:09:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=634
04/25/2022 04:09:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
04/25/2022 04:09:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
04/25/2022 04:09:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=649
04/25/2022 04:09:31 - INFO - __main__ - Global step 1300 Train loss 0.06 ACC 0.25 on epoch=649
04/25/2022 04:09:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=654
04/25/2022 04:09:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=659
04/25/2022 04:09:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=664
04/25/2022 04:09:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=669
04/25/2022 04:09:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=674
04/25/2022 04:09:57 - INFO - __main__ - Global step 1350 Train loss 0.08 ACC 0.25 on epoch=674
04/25/2022 04:10:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=679
04/25/2022 04:10:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=684
04/25/2022 04:10:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=689
04/25/2022 04:10:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=694
04/25/2022 04:10:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
04/25/2022 04:10:22 - INFO - __main__ - Global step 1400 Train loss 0.07 ACC 0.25 on epoch=699
04/25/2022 04:10:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=704
04/25/2022 04:10:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=709
04/25/2022 04:10:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=714
04/25/2022 04:10:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=719
04/25/2022 04:10:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=724
04/25/2022 04:10:47 - INFO - __main__ - Global step 1450 Train loss 0.06 ACC 0.25 on epoch=724
04/25/2022 04:10:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=729
04/25/2022 04:10:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=734
04/25/2022 04:11:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=739
04/25/2022 04:11:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=744
04/25/2022 04:11:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=749
04/25/2022 04:11:12 - INFO - __main__ - Global step 1500 Train loss 0.06 ACC 0.25 on epoch=749
04/25/2022 04:11:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
04/25/2022 04:11:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=759
04/25/2022 04:11:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=764
04/25/2022 04:11:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=769
04/25/2022 04:11:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=774
04/25/2022 04:11:37 - INFO - __main__ - Global step 1550 Train loss 0.06 ACC 0.25 on epoch=774
04/25/2022 04:11:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=779
04/25/2022 04:11:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=784
04/25/2022 04:11:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=789
04/25/2022 04:11:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=794
04/25/2022 04:11:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=799
04/25/2022 04:12:02 - INFO - __main__ - Global step 1600 Train loss 0.06 ACC 0.25 on epoch=799
04/25/2022 04:12:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=804
04/25/2022 04:12:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=809
04/25/2022 04:12:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=814
04/25/2022 04:12:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=819
04/25/2022 04:12:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=824
04/25/2022 04:12:28 - INFO - __main__ - Global step 1650 Train loss 0.06 ACC 0.25 on epoch=824
04/25/2022 04:12:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=829
04/25/2022 04:12:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=834
04/25/2022 04:12:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=839
04/25/2022 04:12:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=844
04/25/2022 04:12:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=849
04/25/2022 04:12:53 - INFO - __main__ - Global step 1700 Train loss 0.06 ACC 0.21875 on epoch=849
04/25/2022 04:12:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=854
04/25/2022 04:13:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=859
04/25/2022 04:13:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=864
04/25/2022 04:13:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=869
04/25/2022 04:13:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=874
04/25/2022 04:13:18 - INFO - __main__ - Global step 1750 Train loss 0.06 ACC 0.25 on epoch=874
04/25/2022 04:13:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=879
04/25/2022 04:13:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=884
04/25/2022 04:13:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=889
04/25/2022 04:13:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=894
04/25/2022 04:13:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=899
04/25/2022 04:13:43 - INFO - __main__ - Global step 1800 Train loss 0.06 ACC 0.25 on epoch=899
04/25/2022 04:13:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=904
04/25/2022 04:13:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=909
04/25/2022 04:13:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=914
04/25/2022 04:14:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=919
04/25/2022 04:14:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
04/25/2022 04:14:08 - INFO - __main__ - Global step 1850 Train loss 0.06 ACC 0.25 on epoch=924
04/25/2022 04:14:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=929
04/25/2022 04:14:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=934
04/25/2022 04:14:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=939
04/25/2022 04:14:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=944
04/25/2022 04:14:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=949
04/25/2022 04:14:34 - INFO - __main__ - Global step 1900 Train loss 0.06 ACC 0.25 on epoch=949
04/25/2022 04:14:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=954
04/25/2022 04:14:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=959
04/25/2022 04:14:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=964
04/25/2022 04:14:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=969
04/25/2022 04:14:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
04/25/2022 04:14:59 - INFO - __main__ - Global step 1950 Train loss 0.06 ACC 0.25 on epoch=974
04/25/2022 04:15:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=979
04/25/2022 04:15:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=984
04/25/2022 04:15:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=989
04/25/2022 04:15:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=994
04/25/2022 04:15:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=999
04/25/2022 04:15:22 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:15:22 - INFO - __main__ - Printing 3 examples
04/25/2022 04:15:22 - INFO - __main__ -  [dream] What will too many cars lead to according to the woman? [SEP] M: The traffic is becoming worse and worse. I think there'll be huge changes in the ways people use cars. New laws will be made about what kind of car you can own and when you can drive. W: Maybe there'll just be too many of them on the roads. The air will be so seriously polluted that nobody will be able to breathe. M: Exactly. People will have to rely on trains. W: Why do you say that? M: Well, we won't be able to use cars, and airports take up too much space. That leaves trains. W: Huh. So do you think there'll be more efficient train systems between cities? M: Sure. They'll enable people to travel between cities in a matter of hours. There may even be trains going under the oceans to connect the main continents. W: Under the oceans? Oh, no! I get nervous enough flying on a plane. [SEP]  (A) Air pollution. (B) Traffic jam. (C) Oil shortage.
04/25/2022 04:15:22 - INFO - __main__ - ['Air pollution.']
04/25/2022 04:15:22 - INFO - __main__ -  [dream] Where was the woman on July 4th? [SEP] M: Hi, Jane. It's nice to see you again. I heard that you went to the US during the vacation. W: Yes. I went to New York to attend a summer course in English. M: Wow. You were lucky. How long did you stay there? W: About 50 days. I went there on July 5th and came back on August 25th. M: How about the course? W: The course was very good. The teachers were nice. They taught us to listen, speak, read and write in English, but it was mostly speaking. One interesting thing I found was that the American classes are different from our classes here because the students have a lot more freedom. You can sit anywhere you like in the classroom. You can ask the teachers questions at any time during the class, and you are welcome to share your ideas with the class. I really like this kind of class. M: How interesting! Maybe our teacher should try that. [SEP]  (A) In an American university. (B) In New York. (C) At home.
04/25/2022 04:15:22 - INFO - __main__ - ['At home.']
04/25/2022 04:15:22 - INFO - __main__ -  [dream] How does the woman go to John's home? [SEP] M: Would you like to have lunch with us? W: I'd love to, but I have to help John with his math problem right now. M: Would you like me to give you a lift? W: No,thanks. His home is not far and I can walk there. [SEP]  (A) By bus. (B) On foot. (C) Take the man's car.
04/25/2022 04:15:22 - INFO - __main__ - ['On foot.']
04/25/2022 04:15:22 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:15:22 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:15:22 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 04:15:22 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:15:22 - INFO - __main__ - Printing 3 examples
04/25/2022 04:15:22 - INFO - __main__ -  [dream] What can we learn from the conversation? [SEP] W: Why haven't we received any newspapers yet? M: Well, sometimes it takes a while for the post office to deliver it. [SEP]  (A) The man will go to the post office. (B) The post office is closed for the day. (C) The woman is expecting the newspaper.
04/25/2022 04:15:22 - INFO - __main__ - ['The woman is expecting the newspaper.']
04/25/2022 04:15:22 - INFO - __main__ -  [dream] What can be inferred from the conversation? [SEP] W: What are the things in our our suitcase? There aren't any toys at all. Where have you put them? M: Oh, no. This is not our suitcase. The old lady must have taken ours bymistake. She was sitting next to us at the restaurant. [SEP]  (A) The old lady sitting next to the couple likes toys very much. (B) An old lady took the couple's suitcasefor her own. (C) The couple's suitcase was stolen inthe restaurant.
04/25/2022 04:15:22 - INFO - __main__ - ["An old lady took the couple's suitcasefor her own."]
04/25/2022 04:15:22 - INFO - __main__ -  [dream] When did the woman go to see Kate? [SEP] M: Have you seen Kate recently, Vicky? W: Yes, I have. I saw her a couple of days ago. She hasn't been very well in the last couple of weeks. M: Has she seen a doctor since she's been ill? W: Yes, she has. The doctor told her to take it easy for a while, but she hasn't been taking his advice. She's as busy as usual. M: Do you think it useful for me to ask her to have a rest when I go to see her? Or shall we go together? W: I think you can go yourself and show your concern to her since she sometimes would take your advice. So it's unnecessary for me to go with you. What's more, I've got some other things to do at the moment. [SEP]  (A) Two days ago. (B) Two weeks ago. (C) A week ago.
04/25/2022 04:15:22 - INFO - __main__ - ['Two days ago.']
04/25/2022 04:15:22 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:15:22 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:15:22 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 04:15:24 - INFO - __main__ - Global step 2000 Train loss 0.05 ACC 0.25 on epoch=999
04/25/2022 04:15:24 - INFO - __main__ - save last model!
04/25/2022 04:15:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 04:15:24 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 04:15:24 - INFO - __main__ - Printing 3 examples
04/25/2022 04:15:24 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 04:15:24 - INFO - __main__ - ['To change her job.']
04/25/2022 04:15:24 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 04:15:24 - INFO - __main__ - ['Watch TV.']
04/25/2022 04:15:24 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 04:15:24 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 04:15:24 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:15:25 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:15:26 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 04:15:37 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 04:15:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 04:15:38 - INFO - __main__ - Starting training!
04/25/2022 04:16:43 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_100_0.2_8_predictions.txt
04/25/2022 04:16:43 - INFO - __main__ - ACC on test data: 0.3060
04/25/2022 04:16:43 - INFO - __main__ - prefix=dream_32_100, lr=0.2, bsz=8, dev_performance=0.4375, test_performance=0.306
04/25/2022 04:16:43 - INFO - __main__ - Running ... prefix=dream_32_13, lr=0.5, bsz=8 ...
04/25/2022 04:16:44 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:16:44 - INFO - __main__ - Printing 3 examples
04/25/2022 04:16:44 - INFO - __main__ -  [dream] What will too many cars lead to according to the woman? [SEP] M: The traffic is becoming worse and worse. I think there'll be huge changes in the ways people use cars. New laws will be made about what kind of car you can own and when you can drive. W: Maybe there'll just be too many of them on the roads. The air will be so seriously polluted that nobody will be able to breathe. M: Exactly. People will have to rely on trains. W: Why do you say that? M: Well, we won't be able to use cars, and airports take up too much space. That leaves trains. W: Huh. So do you think there'll be more efficient train systems between cities? M: Sure. They'll enable people to travel between cities in a matter of hours. There may even be trains going under the oceans to connect the main continents. W: Under the oceans? Oh, no! I get nervous enough flying on a plane. [SEP]  (A) Air pollution. (B) Traffic jam. (C) Oil shortage.
04/25/2022 04:16:44 - INFO - __main__ - ['Air pollution.']
04/25/2022 04:16:44 - INFO - __main__ -  [dream] Where was the woman on July 4th? [SEP] M: Hi, Jane. It's nice to see you again. I heard that you went to the US during the vacation. W: Yes. I went to New York to attend a summer course in English. M: Wow. You were lucky. How long did you stay there? W: About 50 days. I went there on July 5th and came back on August 25th. M: How about the course? W: The course was very good. The teachers were nice. They taught us to listen, speak, read and write in English, but it was mostly speaking. One interesting thing I found was that the American classes are different from our classes here because the students have a lot more freedom. You can sit anywhere you like in the classroom. You can ask the teachers questions at any time during the class, and you are welcome to share your ideas with the class. I really like this kind of class. M: How interesting! Maybe our teacher should try that. [SEP]  (A) In an American university. (B) In New York. (C) At home.
04/25/2022 04:16:44 - INFO - __main__ - ['At home.']
04/25/2022 04:16:44 - INFO - __main__ -  [dream] How does the woman go to John's home? [SEP] M: Would you like to have lunch with us? W: I'd love to, but I have to help John with his math problem right now. M: Would you like me to give you a lift? W: No,thanks. His home is not far and I can walk there. [SEP]  (A) By bus. (B) On foot. (C) Take the man's car.
04/25/2022 04:16:44 - INFO - __main__ - ['On foot.']
04/25/2022 04:16:44 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:16:44 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:16:44 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 04:16:44 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:16:44 - INFO - __main__ - Printing 3 examples
04/25/2022 04:16:44 - INFO - __main__ -  [dream] What can we learn from the conversation? [SEP] W: Why haven't we received any newspapers yet? M: Well, sometimes it takes a while for the post office to deliver it. [SEP]  (A) The man will go to the post office. (B) The post office is closed for the day. (C) The woman is expecting the newspaper.
04/25/2022 04:16:44 - INFO - __main__ - ['The woman is expecting the newspaper.']
04/25/2022 04:16:44 - INFO - __main__ -  [dream] What can be inferred from the conversation? [SEP] W: What are the things in our our suitcase? There aren't any toys at all. Where have you put them? M: Oh, no. This is not our suitcase. The old lady must have taken ours bymistake. She was sitting next to us at the restaurant. [SEP]  (A) The old lady sitting next to the couple likes toys very much. (B) An old lady took the couple's suitcasefor her own. (C) The couple's suitcase was stolen inthe restaurant.
04/25/2022 04:16:44 - INFO - __main__ - ["An old lady took the couple's suitcasefor her own."]
04/25/2022 04:16:44 - INFO - __main__ -  [dream] When did the woman go to see Kate? [SEP] M: Have you seen Kate recently, Vicky? W: Yes, I have. I saw her a couple of days ago. She hasn't been very well in the last couple of weeks. M: Has she seen a doctor since she's been ill? W: Yes, she has. The doctor told her to take it easy for a while, but she hasn't been taking his advice. She's as busy as usual. M: Do you think it useful for me to ask her to have a rest when I go to see her? Or shall we go together? W: I think you can go yourself and show your concern to her since she sometimes would take your advice. So it's unnecessary for me to go with you. What's more, I've got some other things to do at the moment. [SEP]  (A) Two days ago. (B) Two weeks ago. (C) A week ago.
04/25/2022 04:16:44 - INFO - __main__ - ['Two days ago.']
04/25/2022 04:16:44 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:16:45 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:16:45 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 04:17:00 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 04:17:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 04:17:00 - INFO - __main__ - Starting training!
04/25/2022 04:17:05 - INFO - __main__ - Step 10 Global step 10 Train loss 1.53 on epoch=4
04/25/2022 04:17:10 - INFO - __main__ - Step 20 Global step 20 Train loss 1.09 on epoch=9
04/25/2022 04:17:14 - INFO - __main__ - Step 30 Global step 30 Train loss 0.72 on epoch=14
04/25/2022 04:17:19 - INFO - __main__ - Step 40 Global step 40 Train loss 0.63 on epoch=19
04/25/2022 04:17:23 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=24
04/25/2022 04:17:26 - INFO - __main__ - Global step 50 Train loss 0.90 ACC 0.25 on epoch=24
04/25/2022 04:17:26 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.25 on epoch=24, global_step=50
04/25/2022 04:17:30 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=29
04/25/2022 04:17:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
04/25/2022 04:17:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.36 on epoch=39
04/25/2022 04:17:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=44
04/25/2022 04:17:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.34 on epoch=49
04/25/2022 04:17:51 - INFO - __main__ - Global step 100 Train loss 0.40 ACC 0.28125 on epoch=49
04/25/2022 04:17:51 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=49, global_step=100
04/25/2022 04:17:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=54
04/25/2022 04:17:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.25 on epoch=59
04/25/2022 04:18:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.25 on epoch=64
04/25/2022 04:18:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.20 on epoch=69
04/25/2022 04:18:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.22 on epoch=74
04/25/2022 04:18:15 - INFO - __main__ - Global step 150 Train loss 0.24 ACC 0.25 on epoch=74
04/25/2022 04:18:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.20 on epoch=79
04/25/2022 04:18:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.17 on epoch=84
04/25/2022 04:18:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
04/25/2022 04:18:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.20 on epoch=94
04/25/2022 04:18:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.17 on epoch=99
04/25/2022 04:18:40 - INFO - __main__ - Global step 200 Train loss 0.19 ACC 0.25 on epoch=99
04/25/2022 04:18:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.12 on epoch=104
04/25/2022 04:18:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.14 on epoch=109
04/25/2022 04:18:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.09 on epoch=114
04/25/2022 04:18:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.15 on epoch=119
04/25/2022 04:19:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.11 on epoch=124
04/25/2022 04:19:04 - INFO - __main__ - Global step 250 Train loss 0.12 ACC 0.25 on epoch=124
04/25/2022 04:19:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.09 on epoch=129
04/25/2022 04:19:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.09 on epoch=134
04/25/2022 04:19:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.08 on epoch=139
04/25/2022 04:19:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.09 on epoch=144
04/25/2022 04:19:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.07 on epoch=149
04/25/2022 04:19:29 - INFO - __main__ - Global step 300 Train loss 0.08 ACC 0.21875 on epoch=149
04/25/2022 04:19:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.11 on epoch=154
04/25/2022 04:19:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.07 on epoch=159
04/25/2022 04:19:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.07 on epoch=164
04/25/2022 04:19:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.07 on epoch=169
04/25/2022 04:19:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.05 on epoch=174
04/25/2022 04:19:53 - INFO - __main__ - Global step 350 Train loss 0.07 ACC 0.3125 on epoch=174
04/25/2022 04:19:53 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=174, global_step=350
04/25/2022 04:19:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.10 on epoch=179
04/25/2022 04:20:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.08 on epoch=184
04/25/2022 04:20:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.07 on epoch=189
04/25/2022 04:20:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.10 on epoch=194
04/25/2022 04:20:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.05 on epoch=199
04/25/2022 04:20:18 - INFO - __main__ - Global step 400 Train loss 0.08 ACC 0.25 on epoch=199
04/25/2022 04:20:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.08 on epoch=204
04/25/2022 04:20:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
04/25/2022 04:20:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.03 on epoch=214
04/25/2022 04:20:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.08 on epoch=219
04/25/2022 04:20:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.04 on epoch=224
04/25/2022 04:20:42 - INFO - __main__ - Global step 450 Train loss 0.06 ACC 0.34375 on epoch=224
04/25/2022 04:20:42 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.34375 on epoch=224, global_step=450
04/25/2022 04:20:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
04/25/2022 04:20:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
04/25/2022 04:20:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.05 on epoch=239
04/25/2022 04:21:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
04/25/2022 04:21:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=249
04/25/2022 04:21:07 - INFO - __main__ - Global step 500 Train loss 0.05 ACC 0.28125 on epoch=249
04/25/2022 04:21:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
04/25/2022 04:21:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
04/25/2022 04:21:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.03 on epoch=264
04/25/2022 04:21:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
04/25/2022 04:21:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
04/25/2022 04:21:31 - INFO - __main__ - Global step 550 Train loss 0.04 ACC 0.34375 on epoch=274
04/25/2022 04:21:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
04/25/2022 04:21:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
04/25/2022 04:21:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.03 on epoch=289
04/25/2022 04:21:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
04/25/2022 04:21:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
04/25/2022 04:21:55 - INFO - __main__ - Global step 600 Train loss 0.03 ACC 0.3125 on epoch=299
04/25/2022 04:22:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
04/25/2022 04:22:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
04/25/2022 04:22:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
04/25/2022 04:22:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
04/25/2022 04:22:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
04/25/2022 04:22:20 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.25 on epoch=324
04/25/2022 04:22:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
04/25/2022 04:22:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
04/25/2022 04:22:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
04/25/2022 04:22:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
04/25/2022 04:22:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
04/25/2022 04:22:44 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.28125 on epoch=349
04/25/2022 04:22:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
04/25/2022 04:22:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
04/25/2022 04:22:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
04/25/2022 04:23:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
04/25/2022 04:23:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
04/25/2022 04:23:08 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.34375 on epoch=374
04/25/2022 04:23:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
04/25/2022 04:23:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
04/25/2022 04:23:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
04/25/2022 04:23:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
04/25/2022 04:23:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
04/25/2022 04:23:33 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.28125 on epoch=399
04/25/2022 04:23:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
04/25/2022 04:23:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
04/25/2022 04:23:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
04/25/2022 04:23:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
04/25/2022 04:23:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
04/25/2022 04:23:57 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.3125 on epoch=424
04/25/2022 04:24:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
04/25/2022 04:24:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
04/25/2022 04:24:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
04/25/2022 04:24:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
04/25/2022 04:24:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
04/25/2022 04:24:21 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.3125 on epoch=449
04/25/2022 04:24:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
04/25/2022 04:24:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
04/25/2022 04:24:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
04/25/2022 04:24:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
04/25/2022 04:24:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
04/25/2022 04:24:46 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.3125 on epoch=474
04/25/2022 04:24:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/25/2022 04:24:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
04/25/2022 04:24:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
04/25/2022 04:25:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
04/25/2022 04:25:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
04/25/2022 04:25:10 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.28125 on epoch=499
04/25/2022 04:25:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
04/25/2022 04:25:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
04/25/2022 04:25:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
04/25/2022 04:25:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
04/25/2022 04:25:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/25/2022 04:25:34 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.3125 on epoch=524
04/25/2022 04:25:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
04/25/2022 04:25:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
04/25/2022 04:25:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/25/2022 04:25:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
04/25/2022 04:25:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/25/2022 04:25:59 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.375 on epoch=549
04/25/2022 04:25:59 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=549, global_step=1100
04/25/2022 04:26:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/25/2022 04:26:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
04/25/2022 04:26:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
04/25/2022 04:26:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=569
04/25/2022 04:26:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 04:26:23 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.34375 on epoch=574
04/25/2022 04:26:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 04:26:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
04/25/2022 04:26:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
04/25/2022 04:26:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 04:26:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
04/25/2022 04:26:47 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.25 on epoch=599
04/25/2022 04:26:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
04/25/2022 04:26:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 04:27:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
04/25/2022 04:27:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
04/25/2022 04:27:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
04/25/2022 04:27:12 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.25 on epoch=624
04/25/2022 04:27:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
04/25/2022 04:27:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 04:27:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
04/25/2022 04:27:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/25/2022 04:27:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/25/2022 04:27:36 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.28125 on epoch=649
04/25/2022 04:27:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 04:27:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 04:27:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/25/2022 04:27:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/25/2022 04:27:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 04:28:01 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.3125 on epoch=674
04/25/2022 04:28:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/25/2022 04:28:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 04:28:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
04/25/2022 04:28:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 04:28:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
04/25/2022 04:28:25 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.28125 on epoch=699
04/25/2022 04:28:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 04:28:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 04:28:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/25/2022 04:28:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
04/25/2022 04:28:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
04/25/2022 04:28:50 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.28125 on epoch=724
04/25/2022 04:28:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
04/25/2022 04:28:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/25/2022 04:29:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 04:29:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
04/25/2022 04:29:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
04/25/2022 04:29:14 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.28125 on epoch=749
04/25/2022 04:29:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 04:29:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 04:29:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 04:29:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 04:29:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/25/2022 04:29:39 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.28125 on epoch=774
04/25/2022 04:29:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 04:29:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
04/25/2022 04:29:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
04/25/2022 04:29:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/25/2022 04:30:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
04/25/2022 04:30:04 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.34375 on epoch=799
04/25/2022 04:30:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
04/25/2022 04:30:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/25/2022 04:30:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 04:30:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
04/25/2022 04:30:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
04/25/2022 04:30:29 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.28125 on epoch=824
04/25/2022 04:30:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
04/25/2022 04:30:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 04:30:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
04/25/2022 04:30:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
04/25/2022 04:30:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
04/25/2022 04:30:54 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.28125 on epoch=849
04/25/2022 04:30:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 04:31:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
04/25/2022 04:31:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 04:31:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 04:31:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 04:31:18 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.28125 on epoch=874
04/25/2022 04:31:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/25/2022 04:31:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 04:31:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 04:31:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 04:31:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 04:31:43 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.3125 on epoch=899
04/25/2022 04:31:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 04:31:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/25/2022 04:31:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
04/25/2022 04:32:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 04:32:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 04:32:08 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.3125 on epoch=924
04/25/2022 04:32:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
04/25/2022 04:32:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 04:32:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 04:32:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
04/25/2022 04:32:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
04/25/2022 04:32:33 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.3125 on epoch=949
04/25/2022 04:32:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
04/25/2022 04:32:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
04/25/2022 04:32:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 04:32:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 04:32:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 04:32:58 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.375 on epoch=974
04/25/2022 04:33:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
04/25/2022 04:33:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 04:33:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
04/25/2022 04:33:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
04/25/2022 04:33:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 04:33:22 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:33:22 - INFO - __main__ - Printing 3 examples
04/25/2022 04:33:22 - INFO - __main__ -  [dream] What will too many cars lead to according to the woman? [SEP] M: The traffic is becoming worse and worse. I think there'll be huge changes in the ways people use cars. New laws will be made about what kind of car you can own and when you can drive. W: Maybe there'll just be too many of them on the roads. The air will be so seriously polluted that nobody will be able to breathe. M: Exactly. People will have to rely on trains. W: Why do you say that? M: Well, we won't be able to use cars, and airports take up too much space. That leaves trains. W: Huh. So do you think there'll be more efficient train systems between cities? M: Sure. They'll enable people to travel between cities in a matter of hours. There may even be trains going under the oceans to connect the main continents. W: Under the oceans? Oh, no! I get nervous enough flying on a plane. [SEP]  (A) Air pollution. (B) Traffic jam. (C) Oil shortage.
04/25/2022 04:33:22 - INFO - __main__ - ['Air pollution.']
04/25/2022 04:33:22 - INFO - __main__ -  [dream] Where was the woman on July 4th? [SEP] M: Hi, Jane. It's nice to see you again. I heard that you went to the US during the vacation. W: Yes. I went to New York to attend a summer course in English. M: Wow. You were lucky. How long did you stay there? W: About 50 days. I went there on July 5th and came back on August 25th. M: How about the course? W: The course was very good. The teachers were nice. They taught us to listen, speak, read and write in English, but it was mostly speaking. One interesting thing I found was that the American classes are different from our classes here because the students have a lot more freedom. You can sit anywhere you like in the classroom. You can ask the teachers questions at any time during the class, and you are welcome to share your ideas with the class. I really like this kind of class. M: How interesting! Maybe our teacher should try that. [SEP]  (A) In an American university. (B) In New York. (C) At home.
04/25/2022 04:33:22 - INFO - __main__ - ['At home.']
04/25/2022 04:33:22 - INFO - __main__ -  [dream] How does the woman go to John's home? [SEP] M: Would you like to have lunch with us? W: I'd love to, but I have to help John with his math problem right now. M: Would you like me to give you a lift? W: No,thanks. His home is not far and I can walk there. [SEP]  (A) By bus. (B) On foot. (C) Take the man's car.
04/25/2022 04:33:22 - INFO - __main__ - ['On foot.']
04/25/2022 04:33:22 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:33:22 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:33:22 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 04:33:22 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:33:22 - INFO - __main__ - Printing 3 examples
04/25/2022 04:33:22 - INFO - __main__ -  [dream] What can we learn from the conversation? [SEP] W: Why haven't we received any newspapers yet? M: Well, sometimes it takes a while for the post office to deliver it. [SEP]  (A) The man will go to the post office. (B) The post office is closed for the day. (C) The woman is expecting the newspaper.
04/25/2022 04:33:22 - INFO - __main__ - ['The woman is expecting the newspaper.']
04/25/2022 04:33:22 - INFO - __main__ -  [dream] What can be inferred from the conversation? [SEP] W: What are the things in our our suitcase? There aren't any toys at all. Where have you put them? M: Oh, no. This is not our suitcase. The old lady must have taken ours bymistake. She was sitting next to us at the restaurant. [SEP]  (A) The old lady sitting next to the couple likes toys very much. (B) An old lady took the couple's suitcasefor her own. (C) The couple's suitcase was stolen inthe restaurant.
04/25/2022 04:33:22 - INFO - __main__ - ["An old lady took the couple's suitcasefor her own."]
04/25/2022 04:33:22 - INFO - __main__ -  [dream] When did the woman go to see Kate? [SEP] M: Have you seen Kate recently, Vicky? W: Yes, I have. I saw her a couple of days ago. She hasn't been very well in the last couple of weeks. M: Has she seen a doctor since she's been ill? W: Yes, she has. The doctor told her to take it easy for a while, but she hasn't been taking his advice. She's as busy as usual. M: Do you think it useful for me to ask her to have a rest when I go to see her? Or shall we go together? W: I think you can go yourself and show your concern to her since she sometimes would take your advice. So it's unnecessary for me to go with you. What's more, I've got some other things to do at the moment. [SEP]  (A) Two days ago. (B) Two weeks ago. (C) A week ago.
04/25/2022 04:33:22 - INFO - __main__ - ['Two days ago.']
04/25/2022 04:33:22 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:33:22 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:33:22 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 04:33:23 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.375 on epoch=999
04/25/2022 04:33:23 - INFO - __main__ - save last model!
04/25/2022 04:33:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 04:33:23 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 04:33:23 - INFO - __main__ - Printing 3 examples
04/25/2022 04:33:23 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 04:33:23 - INFO - __main__ - ['To change her job.']
04/25/2022 04:33:23 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 04:33:23 - INFO - __main__ - ['Watch TV.']
04/25/2022 04:33:23 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 04:33:23 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 04:33:23 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:33:24 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:33:25 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 04:33:40 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 04:33:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 04:33:41 - INFO - __main__ - Starting training!
04/25/2022 04:34:46 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_13_0.5_8_predictions.txt
04/25/2022 04:34:46 - INFO - __main__ - ACC on test data: 0.3070
04/25/2022 04:34:47 - INFO - __main__ - prefix=dream_32_13, lr=0.5, bsz=8, dev_performance=0.375, test_performance=0.307
04/25/2022 04:34:47 - INFO - __main__ - Running ... prefix=dream_32_13, lr=0.4, bsz=8 ...
04/25/2022 04:34:47 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:34:47 - INFO - __main__ - Printing 3 examples
04/25/2022 04:34:47 - INFO - __main__ -  [dream] What will too many cars lead to according to the woman? [SEP] M: The traffic is becoming worse and worse. I think there'll be huge changes in the ways people use cars. New laws will be made about what kind of car you can own and when you can drive. W: Maybe there'll just be too many of them on the roads. The air will be so seriously polluted that nobody will be able to breathe. M: Exactly. People will have to rely on trains. W: Why do you say that? M: Well, we won't be able to use cars, and airports take up too much space. That leaves trains. W: Huh. So do you think there'll be more efficient train systems between cities? M: Sure. They'll enable people to travel between cities in a matter of hours. There may even be trains going under the oceans to connect the main continents. W: Under the oceans? Oh, no! I get nervous enough flying on a plane. [SEP]  (A) Air pollution. (B) Traffic jam. (C) Oil shortage.
04/25/2022 04:34:47 - INFO - __main__ - ['Air pollution.']
04/25/2022 04:34:47 - INFO - __main__ -  [dream] Where was the woman on July 4th? [SEP] M: Hi, Jane. It's nice to see you again. I heard that you went to the US during the vacation. W: Yes. I went to New York to attend a summer course in English. M: Wow. You were lucky. How long did you stay there? W: About 50 days. I went there on July 5th and came back on August 25th. M: How about the course? W: The course was very good. The teachers were nice. They taught us to listen, speak, read and write in English, but it was mostly speaking. One interesting thing I found was that the American classes are different from our classes here because the students have a lot more freedom. You can sit anywhere you like in the classroom. You can ask the teachers questions at any time during the class, and you are welcome to share your ideas with the class. I really like this kind of class. M: How interesting! Maybe our teacher should try that. [SEP]  (A) In an American university. (B) In New York. (C) At home.
04/25/2022 04:34:47 - INFO - __main__ - ['At home.']
04/25/2022 04:34:47 - INFO - __main__ -  [dream] How does the woman go to John's home? [SEP] M: Would you like to have lunch with us? W: I'd love to, but I have to help John with his math problem right now. M: Would you like me to give you a lift? W: No,thanks. His home is not far and I can walk there. [SEP]  (A) By bus. (B) On foot. (C) Take the man's car.
04/25/2022 04:34:47 - INFO - __main__ - ['On foot.']
04/25/2022 04:34:47 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:34:48 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:34:48 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 04:34:48 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:34:48 - INFO - __main__ - Printing 3 examples
04/25/2022 04:34:48 - INFO - __main__ -  [dream] What can we learn from the conversation? [SEP] W: Why haven't we received any newspapers yet? M: Well, sometimes it takes a while for the post office to deliver it. [SEP]  (A) The man will go to the post office. (B) The post office is closed for the day. (C) The woman is expecting the newspaper.
04/25/2022 04:34:48 - INFO - __main__ - ['The woman is expecting the newspaper.']
04/25/2022 04:34:48 - INFO - __main__ -  [dream] What can be inferred from the conversation? [SEP] W: What are the things in our our suitcase? There aren't any toys at all. Where have you put them? M: Oh, no. This is not our suitcase. The old lady must have taken ours bymistake. She was sitting next to us at the restaurant. [SEP]  (A) The old lady sitting next to the couple likes toys very much. (B) An old lady took the couple's suitcasefor her own. (C) The couple's suitcase was stolen inthe restaurant.
04/25/2022 04:34:48 - INFO - __main__ - ["An old lady took the couple's suitcasefor her own."]
04/25/2022 04:34:48 - INFO - __main__ -  [dream] When did the woman go to see Kate? [SEP] M: Have you seen Kate recently, Vicky? W: Yes, I have. I saw her a couple of days ago. She hasn't been very well in the last couple of weeks. M: Has she seen a doctor since she's been ill? W: Yes, she has. The doctor told her to take it easy for a while, but she hasn't been taking his advice. She's as busy as usual. M: Do you think it useful for me to ask her to have a rest when I go to see her? Or shall we go together? W: I think you can go yourself and show your concern to her since she sometimes would take your advice. So it's unnecessary for me to go with you. What's more, I've got some other things to do at the moment. [SEP]  (A) Two days ago. (B) Two weeks ago. (C) A week ago.
04/25/2022 04:34:48 - INFO - __main__ - ['Two days ago.']
04/25/2022 04:34:48 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:34:48 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:34:48 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 04:35:02 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 04:35:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 04:35:03 - INFO - __main__ - Starting training!
04/25/2022 04:35:08 - INFO - __main__ - Step 10 Global step 10 Train loss 1.47 on epoch=4
04/25/2022 04:35:13 - INFO - __main__ - Step 20 Global step 20 Train loss 1.09 on epoch=9
04/25/2022 04:35:17 - INFO - __main__ - Step 30 Global step 30 Train loss 0.79 on epoch=14
04/25/2022 04:35:21 - INFO - __main__ - Step 40 Global step 40 Train loss 0.60 on epoch=19
04/25/2022 04:35:26 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=24
04/25/2022 04:35:29 - INFO - __main__ - Global step 50 Train loss 0.90 ACC 0.28125 on epoch=24
04/25/2022 04:35:29 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.28125 on epoch=24, global_step=50
04/25/2022 04:35:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.47 on epoch=29
04/25/2022 04:35:38 - INFO - __main__ - Step 70 Global step 70 Train loss 0.85 on epoch=34
04/25/2022 04:35:42 - INFO - __main__ - Step 80 Global step 80 Train loss 1.10 on epoch=39
04/25/2022 04:35:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=44
04/25/2022 04:35:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=49
04/25/2022 04:35:55 - INFO - __main__ - Global step 100 Train loss 0.67 ACC 0.25 on epoch=49
04/25/2022 04:35:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=54
04/25/2022 04:36:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=59
04/25/2022 04:36:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=64
04/25/2022 04:36:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=69
04/25/2022 04:36:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=74
04/25/2022 04:36:20 - INFO - __main__ - Global step 150 Train loss 0.43 ACC 0.28125 on epoch=74
04/25/2022 04:36:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=79
04/25/2022 04:36:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=84
04/25/2022 04:36:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=89
04/25/2022 04:36:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=94
04/25/2022 04:36:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=99
04/25/2022 04:36:44 - INFO - __main__ - Global step 200 Train loss 0.42 ACC 0.28125 on epoch=99
04/25/2022 04:36:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
04/25/2022 04:36:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=109
04/25/2022 04:36:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
04/25/2022 04:37:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=119
04/25/2022 04:37:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=124
04/25/2022 04:37:08 - INFO - __main__ - Global step 250 Train loss 0.34 ACC 0.3125 on epoch=124
04/25/2022 04:37:08 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=124, global_step=250
04/25/2022 04:37:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.34 on epoch=129
04/25/2022 04:37:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
04/25/2022 04:37:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
04/25/2022 04:37:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.34 on epoch=144
04/25/2022 04:37:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.32 on epoch=149
04/25/2022 04:37:32 - INFO - __main__ - Global step 300 Train loss 0.33 ACC 0.21875 on epoch=149
04/25/2022 04:37:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.30 on epoch=154
04/25/2022 04:37:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
04/25/2022 04:37:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=164
04/25/2022 04:37:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
04/25/2022 04:37:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
04/25/2022 04:37:56 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.25 on epoch=174
04/25/2022 04:38:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=179
04/25/2022 04:38:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
04/25/2022 04:38:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
04/25/2022 04:38:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=194
04/25/2022 04:38:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=199
04/25/2022 04:38:21 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.25 on epoch=199
04/25/2022 04:38:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
04/25/2022 04:38:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=209
04/25/2022 04:38:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
04/25/2022 04:38:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.15 on epoch=219
04/25/2022 04:38:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=224
04/25/2022 04:38:45 - INFO - __main__ - Global step 450 Train loss 0.20 ACC 0.25 on epoch=224
04/25/2022 04:38:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=229
04/25/2022 04:38:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.18 on epoch=234
04/25/2022 04:38:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=239
04/25/2022 04:39:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.18 on epoch=244
04/25/2022 04:39:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=249
04/25/2022 04:39:10 - INFO - __main__ - Global step 500 Train loss 0.19 ACC 0.3125 on epoch=249
04/25/2022 04:39:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=254
04/25/2022 04:39:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=259
04/25/2022 04:39:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=264
04/25/2022 04:39:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=269
04/25/2022 04:39:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.13 on epoch=274
04/25/2022 04:39:34 - INFO - __main__ - Global step 550 Train loss 0.16 ACC 0.25 on epoch=274
04/25/2022 04:39:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=279
04/25/2022 04:39:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=284
04/25/2022 04:39:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=289
04/25/2022 04:39:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=294
04/25/2022 04:39:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=299
04/25/2022 04:39:59 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.34375 on epoch=299
04/25/2022 04:39:59 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.34375 on epoch=299, global_step=600
04/25/2022 04:40:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=304
04/25/2022 04:40:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=309
04/25/2022 04:40:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=314
04/25/2022 04:40:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=319
04/25/2022 04:40:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=324
04/25/2022 04:40:23 - INFO - __main__ - Global step 650 Train loss 0.12 ACC 0.25 on epoch=324
04/25/2022 04:40:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=329
04/25/2022 04:40:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=334
04/25/2022 04:40:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=339
04/25/2022 04:40:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=344
04/25/2022 04:40:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=349
04/25/2022 04:40:48 - INFO - __main__ - Global step 700 Train loss 0.10 ACC 0.25 on epoch=349
04/25/2022 04:40:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=354
04/25/2022 04:40:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=359
04/25/2022 04:41:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=364
04/25/2022 04:41:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=369
04/25/2022 04:41:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=374
04/25/2022 04:41:13 - INFO - __main__ - Global step 750 Train loss 0.09 ACC 0.34375 on epoch=374
04/25/2022 04:41:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
04/25/2022 04:41:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
04/25/2022 04:41:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
04/25/2022 04:41:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=394
04/25/2022 04:41:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=399
04/25/2022 04:41:37 - INFO - __main__ - Global step 800 Train loss 0.08 ACC 0.25 on epoch=399
04/25/2022 04:41:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
04/25/2022 04:41:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=409
04/25/2022 04:41:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=414
04/25/2022 04:41:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
04/25/2022 04:41:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=424
04/25/2022 04:42:01 - INFO - __main__ - Global step 850 Train loss 0.06 ACC 0.25 on epoch=424
04/25/2022 04:42:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
04/25/2022 04:42:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=434
04/25/2022 04:42:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=439
04/25/2022 04:42:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=444
04/25/2022 04:42:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
04/25/2022 04:42:26 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.28125 on epoch=449
04/25/2022 04:42:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
04/25/2022 04:42:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
04/25/2022 04:42:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
04/25/2022 04:42:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
04/25/2022 04:42:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
04/25/2022 04:42:50 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.25 on epoch=474
04/25/2022 04:42:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
04/25/2022 04:42:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
04/25/2022 04:43:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
04/25/2022 04:43:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
04/25/2022 04:43:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/25/2022 04:43:15 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.28125 on epoch=499
04/25/2022 04:43:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
04/25/2022 04:43:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=509
04/25/2022 04:43:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
04/25/2022 04:43:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
04/25/2022 04:43:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
04/25/2022 04:43:39 - INFO - __main__ - Global step 1050 Train loss 0.05 ACC 0.25 on epoch=524
04/25/2022 04:43:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=529
04/25/2022 04:43:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
04/25/2022 04:43:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
04/25/2022 04:43:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=544
04/25/2022 04:44:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
04/25/2022 04:44:04 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.34375 on epoch=549
04/25/2022 04:44:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
04/25/2022 04:44:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
04/25/2022 04:44:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
04/25/2022 04:44:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
04/25/2022 04:44:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
04/25/2022 04:44:29 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.28125 on epoch=574
04/25/2022 04:44:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
04/25/2022 04:44:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/25/2022 04:44:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
04/25/2022 04:44:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
04/25/2022 04:44:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
04/25/2022 04:44:53 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.28125 on epoch=599
04/25/2022 04:44:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/25/2022 04:45:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
04/25/2022 04:45:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 04:45:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
04/25/2022 04:45:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
04/25/2022 04:45:17 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.3125 on epoch=624
04/25/2022 04:45:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 04:45:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
04/25/2022 04:45:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
04/25/2022 04:45:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/25/2022 04:45:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/25/2022 04:45:42 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.28125 on epoch=649
04/25/2022 04:45:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
04/25/2022 04:45:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 04:45:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/25/2022 04:45:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
04/25/2022 04:46:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
04/25/2022 04:46:06 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.21875 on epoch=674
04/25/2022 04:46:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 04:46:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 04:46:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/25/2022 04:46:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
04/25/2022 04:46:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
04/25/2022 04:46:30 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.28125 on epoch=699
04/25/2022 04:46:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/25/2022 04:46:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
04/25/2022 04:46:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
04/25/2022 04:46:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
04/25/2022 04:46:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
04/25/2022 04:46:55 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.3125 on epoch=724
04/25/2022 04:46:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
04/25/2022 04:47:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/25/2022 04:47:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 04:47:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
04/25/2022 04:47:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
04/25/2022 04:47:19 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.28125 on epoch=749
04/25/2022 04:47:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 04:47:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
04/25/2022 04:47:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 04:47:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
04/25/2022 04:47:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/25/2022 04:47:44 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.25 on epoch=774
04/25/2022 04:47:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 04:47:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/25/2022 04:47:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=789
04/25/2022 04:48:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/25/2022 04:48:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
04/25/2022 04:48:08 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.28125 on epoch=799
04/25/2022 04:48:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/25/2022 04:48:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
04/25/2022 04:48:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/25/2022 04:48:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 04:48:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 04:48:33 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.375 on epoch=824
04/25/2022 04:48:33 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=824, global_step=1650
04/25/2022 04:48:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=829
04/25/2022 04:48:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 04:48:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/25/2022 04:48:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 04:48:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 04:48:57 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.21875 on epoch=849
04/25/2022 04:49:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 04:49:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 04:49:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 04:49:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 04:49:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/25/2022 04:49:22 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.34375 on epoch=874
04/25/2022 04:49:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/25/2022 04:49:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
04/25/2022 04:49:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
04/25/2022 04:49:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 04:49:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 04:49:46 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.34375 on epoch=899
04/25/2022 04:49:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 04:49:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/25/2022 04:49:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 04:50:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 04:50:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
04/25/2022 04:50:11 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.3125 on epoch=924
04/25/2022 04:50:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=929
04/25/2022 04:50:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
04/25/2022 04:50:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
04/25/2022 04:50:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 04:50:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 04:50:35 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.28125 on epoch=949
04/25/2022 04:50:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 04:50:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 04:50:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 04:50:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
04/25/2022 04:50:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 04:51:00 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.15625 on epoch=974
04/25/2022 04:51:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 04:51:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
04/25/2022 04:51:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
04/25/2022 04:51:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 04:51:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 04:51:23 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:51:23 - INFO - __main__ - Printing 3 examples
04/25/2022 04:51:23 - INFO - __main__ -  [dream] What will too many cars lead to according to the woman? [SEP] M: The traffic is becoming worse and worse. I think there'll be huge changes in the ways people use cars. New laws will be made about what kind of car you can own and when you can drive. W: Maybe there'll just be too many of them on the roads. The air will be so seriously polluted that nobody will be able to breathe. M: Exactly. People will have to rely on trains. W: Why do you say that? M: Well, we won't be able to use cars, and airports take up too much space. That leaves trains. W: Huh. So do you think there'll be more efficient train systems between cities? M: Sure. They'll enable people to travel between cities in a matter of hours. There may even be trains going under the oceans to connect the main continents. W: Under the oceans? Oh, no! I get nervous enough flying on a plane. [SEP]  (A) Air pollution. (B) Traffic jam. (C) Oil shortage.
04/25/2022 04:51:23 - INFO - __main__ - ['Air pollution.']
04/25/2022 04:51:23 - INFO - __main__ -  [dream] Where was the woman on July 4th? [SEP] M: Hi, Jane. It's nice to see you again. I heard that you went to the US during the vacation. W: Yes. I went to New York to attend a summer course in English. M: Wow. You were lucky. How long did you stay there? W: About 50 days. I went there on July 5th and came back on August 25th. M: How about the course? W: The course was very good. The teachers were nice. They taught us to listen, speak, read and write in English, but it was mostly speaking. One interesting thing I found was that the American classes are different from our classes here because the students have a lot more freedom. You can sit anywhere you like in the classroom. You can ask the teachers questions at any time during the class, and you are welcome to share your ideas with the class. I really like this kind of class. M: How interesting! Maybe our teacher should try that. [SEP]  (A) In an American university. (B) In New York. (C) At home.
04/25/2022 04:51:23 - INFO - __main__ - ['At home.']
04/25/2022 04:51:23 - INFO - __main__ -  [dream] How does the woman go to John's home? [SEP] M: Would you like to have lunch with us? W: I'd love to, but I have to help John with his math problem right now. M: Would you like me to give you a lift? W: No,thanks. His home is not far and I can walk there. [SEP]  (A) By bus. (B) On foot. (C) Take the man's car.
04/25/2022 04:51:23 - INFO - __main__ - ['On foot.']
04/25/2022 04:51:23 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:51:23 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:51:23 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 04:51:23 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:51:23 - INFO - __main__ - Printing 3 examples
04/25/2022 04:51:23 - INFO - __main__ -  [dream] What can we learn from the conversation? [SEP] W: Why haven't we received any newspapers yet? M: Well, sometimes it takes a while for the post office to deliver it. [SEP]  (A) The man will go to the post office. (B) The post office is closed for the day. (C) The woman is expecting the newspaper.
04/25/2022 04:51:23 - INFO - __main__ - ['The woman is expecting the newspaper.']
04/25/2022 04:51:23 - INFO - __main__ -  [dream] What can be inferred from the conversation? [SEP] W: What are the things in our our suitcase? There aren't any toys at all. Where have you put them? M: Oh, no. This is not our suitcase. The old lady must have taken ours bymistake. She was sitting next to us at the restaurant. [SEP]  (A) The old lady sitting next to the couple likes toys very much. (B) An old lady took the couple's suitcasefor her own. (C) The couple's suitcase was stolen inthe restaurant.
04/25/2022 04:51:23 - INFO - __main__ - ["An old lady took the couple's suitcasefor her own."]
04/25/2022 04:51:23 - INFO - __main__ -  [dream] When did the woman go to see Kate? [SEP] M: Have you seen Kate recently, Vicky? W: Yes, I have. I saw her a couple of days ago. She hasn't been very well in the last couple of weeks. M: Has she seen a doctor since she's been ill? W: Yes, she has. The doctor told her to take it easy for a while, but she hasn't been taking his advice. She's as busy as usual. M: Do you think it useful for me to ask her to have a rest when I go to see her? Or shall we go together? W: I think you can go yourself and show your concern to her since she sometimes would take your advice. So it's unnecessary for me to go with you. What's more, I've got some other things to do at the moment. [SEP]  (A) Two days ago. (B) Two weeks ago. (C) A week ago.
04/25/2022 04:51:23 - INFO - __main__ - ['Two days ago.']
04/25/2022 04:51:23 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:51:23 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:51:23 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 04:51:24 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.25 on epoch=999
04/25/2022 04:51:24 - INFO - __main__ - save last model!
04/25/2022 04:51:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 04:51:24 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 04:51:24 - INFO - __main__ - Printing 3 examples
04/25/2022 04:51:24 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 04:51:24 - INFO - __main__ - ['To change her job.']
04/25/2022 04:51:24 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 04:51:24 - INFO - __main__ - ['Watch TV.']
04/25/2022 04:51:24 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 04:51:24 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 04:51:24 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:51:25 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:51:26 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 04:51:42 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 04:51:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 04:51:43 - INFO - __main__ - Starting training!
04/25/2022 04:52:43 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_13_0.4_8_predictions.txt
04/25/2022 04:52:43 - INFO - __main__ - ACC on test data: 0.3250
04/25/2022 04:52:43 - INFO - __main__ - prefix=dream_32_13, lr=0.4, bsz=8, dev_performance=0.375, test_performance=0.325
04/25/2022 04:52:43 - INFO - __main__ - Running ... prefix=dream_32_13, lr=0.3, bsz=8 ...
04/25/2022 04:52:44 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:52:44 - INFO - __main__ - Printing 3 examples
04/25/2022 04:52:44 - INFO - __main__ -  [dream] What will too many cars lead to according to the woman? [SEP] M: The traffic is becoming worse and worse. I think there'll be huge changes in the ways people use cars. New laws will be made about what kind of car you can own and when you can drive. W: Maybe there'll just be too many of them on the roads. The air will be so seriously polluted that nobody will be able to breathe. M: Exactly. People will have to rely on trains. W: Why do you say that? M: Well, we won't be able to use cars, and airports take up too much space. That leaves trains. W: Huh. So do you think there'll be more efficient train systems between cities? M: Sure. They'll enable people to travel between cities in a matter of hours. There may even be trains going under the oceans to connect the main continents. W: Under the oceans? Oh, no! I get nervous enough flying on a plane. [SEP]  (A) Air pollution. (B) Traffic jam. (C) Oil shortage.
04/25/2022 04:52:44 - INFO - __main__ - ['Air pollution.']
04/25/2022 04:52:44 - INFO - __main__ -  [dream] Where was the woman on July 4th? [SEP] M: Hi, Jane. It's nice to see you again. I heard that you went to the US during the vacation. W: Yes. I went to New York to attend a summer course in English. M: Wow. You were lucky. How long did you stay there? W: About 50 days. I went there on July 5th and came back on August 25th. M: How about the course? W: The course was very good. The teachers were nice. They taught us to listen, speak, read and write in English, but it was mostly speaking. One interesting thing I found was that the American classes are different from our classes here because the students have a lot more freedom. You can sit anywhere you like in the classroom. You can ask the teachers questions at any time during the class, and you are welcome to share your ideas with the class. I really like this kind of class. M: How interesting! Maybe our teacher should try that. [SEP]  (A) In an American university. (B) In New York. (C) At home.
04/25/2022 04:52:44 - INFO - __main__ - ['At home.']
04/25/2022 04:52:44 - INFO - __main__ -  [dream] How does the woman go to John's home? [SEP] M: Would you like to have lunch with us? W: I'd love to, but I have to help John with his math problem right now. M: Would you like me to give you a lift? W: No,thanks. His home is not far and I can walk there. [SEP]  (A) By bus. (B) On foot. (C) Take the man's car.
04/25/2022 04:52:44 - INFO - __main__ - ['On foot.']
04/25/2022 04:52:44 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:52:44 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:52:44 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 04:52:44 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 04:52:44 - INFO - __main__ - Printing 3 examples
04/25/2022 04:52:44 - INFO - __main__ -  [dream] What can we learn from the conversation? [SEP] W: Why haven't we received any newspapers yet? M: Well, sometimes it takes a while for the post office to deliver it. [SEP]  (A) The man will go to the post office. (B) The post office is closed for the day. (C) The woman is expecting the newspaper.
04/25/2022 04:52:44 - INFO - __main__ - ['The woman is expecting the newspaper.']
04/25/2022 04:52:44 - INFO - __main__ -  [dream] What can be inferred from the conversation? [SEP] W: What are the things in our our suitcase? There aren't any toys at all. Where have you put them? M: Oh, no. This is not our suitcase. The old lady must have taken ours bymistake. She was sitting next to us at the restaurant. [SEP]  (A) The old lady sitting next to the couple likes toys very much. (B) An old lady took the couple's suitcasefor her own. (C) The couple's suitcase was stolen inthe restaurant.
04/25/2022 04:52:44 - INFO - __main__ - ["An old lady took the couple's suitcasefor her own."]
04/25/2022 04:52:44 - INFO - __main__ -  [dream] When did the woman go to see Kate? [SEP] M: Have you seen Kate recently, Vicky? W: Yes, I have. I saw her a couple of days ago. She hasn't been very well in the last couple of weeks. M: Has she seen a doctor since she's been ill? W: Yes, she has. The doctor told her to take it easy for a while, but she hasn't been taking his advice. She's as busy as usual. M: Do you think it useful for me to ask her to have a rest when I go to see her? Or shall we go together? W: I think you can go yourself and show your concern to her since she sometimes would take your advice. So it's unnecessary for me to go with you. What's more, I've got some other things to do at the moment. [SEP]  (A) Two days ago. (B) Two weeks ago. (C) A week ago.
04/25/2022 04:52:44 - INFO - __main__ - ['Two days ago.']
04/25/2022 04:52:44 - INFO - __main__ - Tokenizing Input ...
04/25/2022 04:52:44 - INFO - __main__ - Tokenizing Output ...
04/25/2022 04:52:44 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 04:52:59 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 04:53:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 04:53:00 - INFO - __main__ - Starting training!
04/25/2022 04:53:05 - INFO - __main__ - Step 10 Global step 10 Train loss 1.63 on epoch=4
04/25/2022 04:53:09 - INFO - __main__ - Step 20 Global step 20 Train loss 1.88 on epoch=9
04/25/2022 04:53:13 - INFO - __main__ - Step 30 Global step 30 Train loss 2.06 on epoch=14
04/25/2022 04:53:18 - INFO - __main__ - Step 40 Global step 40 Train loss 1.55 on epoch=19
04/25/2022 04:53:22 - INFO - __main__ - Step 50 Global step 50 Train loss 1.42 on epoch=24
04/25/2022 04:53:25 - INFO - __main__ - Global step 50 Train loss 1.71 ACC 0.0 on epoch=24
04/25/2022 04:53:25 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
04/25/2022 04:53:29 - INFO - __main__ - Step 60 Global step 60 Train loss 1.38 on epoch=29
04/25/2022 04:53:34 - INFO - __main__ - Step 70 Global step 70 Train loss 1.36 on epoch=34
04/25/2022 04:53:38 - INFO - __main__ - Step 80 Global step 80 Train loss 1.35 on epoch=39
04/25/2022 04:53:42 - INFO - __main__ - Step 90 Global step 90 Train loss 1.35 on epoch=44
04/25/2022 04:53:47 - INFO - __main__ - Step 100 Global step 100 Train loss 1.25 on epoch=49
04/25/2022 04:53:50 - INFO - __main__ - Global step 100 Train loss 1.34 ACC 0.0 on epoch=49
04/25/2022 04:53:54 - INFO - __main__ - Step 110 Global step 110 Train loss 1.20 on epoch=54
04/25/2022 04:53:58 - INFO - __main__ - Step 120 Global step 120 Train loss 1.17 on epoch=59
04/25/2022 04:54:03 - INFO - __main__ - Step 130 Global step 130 Train loss 1.13 on epoch=64
04/25/2022 04:54:07 - INFO - __main__ - Step 140 Global step 140 Train loss 1.05 on epoch=69
04/25/2022 04:54:11 - INFO - __main__ - Step 150 Global step 150 Train loss 1.00 on epoch=74
04/25/2022 04:54:16 - INFO - __main__ - Global step 150 Train loss 1.11 ACC 0.09375 on epoch=74
04/25/2022 04:54:16 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=74, global_step=150
04/25/2022 04:54:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=79
04/25/2022 04:54:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.88 on epoch=84
04/25/2022 04:54:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.83 on epoch=89
04/25/2022 04:54:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.79 on epoch=94
04/25/2022 04:54:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.68 on epoch=99
04/25/2022 04:54:41 - INFO - __main__ - Global step 200 Train loss 0.83 ACC 0.25 on epoch=99
04/25/2022 04:54:41 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.25 on epoch=99, global_step=200
04/25/2022 04:54:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.71 on epoch=104
04/25/2022 04:54:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.71 on epoch=109
04/25/2022 04:54:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.63 on epoch=114
04/25/2022 04:54:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.60 on epoch=119
04/25/2022 04:55:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.59 on epoch=124
04/25/2022 04:55:06 - INFO - __main__ - Global step 250 Train loss 0.65 ACC 0.25 on epoch=124
04/25/2022 04:55:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.56 on epoch=129
04/25/2022 04:55:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.56 on epoch=134
04/25/2022 04:55:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=139
04/25/2022 04:55:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=144
04/25/2022 04:55:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=149
04/25/2022 04:55:30 - INFO - __main__ - Global step 300 Train loss 0.52 ACC 0.28125 on epoch=149
04/25/2022 04:55:30 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=149, global_step=300
04/25/2022 04:55:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=154
04/25/2022 04:55:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=159
04/25/2022 04:55:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=164
04/25/2022 04:55:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=169
04/25/2022 04:55:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=174
04/25/2022 04:55:54 - INFO - __main__ - Global step 350 Train loss 0.42 ACC 0.3125 on epoch=174
04/25/2022 04:55:54 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=174, global_step=350
04/25/2022 04:55:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=179
04/25/2022 04:56:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=184
04/25/2022 04:56:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=189
04/25/2022 04:56:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=194
04/25/2022 04:56:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=199
04/25/2022 04:56:18 - INFO - __main__ - Global step 400 Train loss 0.34 ACC 0.28125 on epoch=199
04/25/2022 04:56:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.33 on epoch=204
04/25/2022 04:56:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=209
04/25/2022 04:56:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=214
04/25/2022 04:56:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=219
04/25/2022 04:56:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
04/25/2022 04:56:42 - INFO - __main__ - Global step 450 Train loss 0.29 ACC 0.25 on epoch=224
04/25/2022 04:56:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=229
04/25/2022 04:56:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=234
04/25/2022 04:56:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=239
04/25/2022 04:57:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=244
04/25/2022 04:57:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
04/25/2022 04:57:06 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.3125 on epoch=249
04/25/2022 04:57:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=254
04/25/2022 04:57:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=259
04/25/2022 04:57:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
04/25/2022 04:57:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
04/25/2022 04:57:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=274
04/25/2022 04:57:30 - INFO - __main__ - Global step 550 Train loss 0.21 ACC 0.3125 on epoch=274
04/25/2022 04:57:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.16 on epoch=279
04/25/2022 04:57:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=284
04/25/2022 04:57:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=289
04/25/2022 04:57:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=294
04/25/2022 04:57:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=299
04/25/2022 04:57:54 - INFO - __main__ - Global step 600 Train loss 0.17 ACC 0.34375 on epoch=299
04/25/2022 04:57:54 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.34375 on epoch=299, global_step=600
04/25/2022 04:57:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=304
04/25/2022 04:58:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=309
04/25/2022 04:58:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=314
04/25/2022 04:58:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
04/25/2022 04:58:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=324
04/25/2022 04:58:19 - INFO - __main__ - Global step 650 Train loss 0.14 ACC 0.3125 on epoch=324
04/25/2022 04:58:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=329
04/25/2022 04:58:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=334
04/25/2022 04:58:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=339
04/25/2022 04:58:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=344
04/25/2022 04:58:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=349
04/25/2022 04:58:43 - INFO - __main__ - Global step 700 Train loss 0.15 ACC 0.28125 on epoch=349
04/25/2022 04:58:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=354
04/25/2022 04:58:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=359
04/25/2022 04:58:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
04/25/2022 04:59:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=369
04/25/2022 04:59:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=374
04/25/2022 04:59:07 - INFO - __main__ - Global step 750 Train loss 0.12 ACC 0.28125 on epoch=374
04/25/2022 04:59:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=379
04/25/2022 04:59:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=384
04/25/2022 04:59:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=389
04/25/2022 04:59:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=394
04/25/2022 04:59:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=399
04/25/2022 04:59:31 - INFO - __main__ - Global step 800 Train loss 0.11 ACC 0.3125 on epoch=399
04/25/2022 04:59:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.13 on epoch=404
04/25/2022 04:59:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=409
04/25/2022 04:59:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=414
04/25/2022 04:59:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=419
04/25/2022 04:59:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=424
04/25/2022 04:59:55 - INFO - __main__ - Global step 850 Train loss 0.11 ACC 0.3125 on epoch=424
04/25/2022 04:59:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=429
04/25/2022 05:00:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=434
04/25/2022 05:00:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=439
04/25/2022 05:00:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=444
04/25/2022 05:00:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=449
04/25/2022 05:00:19 - INFO - __main__ - Global step 900 Train loss 0.09 ACC 0.28125 on epoch=449
04/25/2022 05:00:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
04/25/2022 05:00:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
04/25/2022 05:00:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=464
04/25/2022 05:00:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
04/25/2022 05:00:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=474
04/25/2022 05:00:44 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.25 on epoch=474
04/25/2022 05:00:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
04/25/2022 05:00:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=484
04/25/2022 05:00:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=489
04/25/2022 05:01:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=494
04/25/2022 05:01:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
04/25/2022 05:01:08 - INFO - __main__ - Global step 1000 Train loss 0.07 ACC 0.28125 on epoch=499
04/25/2022 05:01:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
04/25/2022 05:01:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=509
04/25/2022 05:01:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
04/25/2022 05:01:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
04/25/2022 05:01:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
04/25/2022 05:01:32 - INFO - __main__ - Global step 1050 Train loss 0.05 ACC 0.25 on epoch=524
04/25/2022 05:01:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
04/25/2022 05:01:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
04/25/2022 05:01:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=539
04/25/2022 05:01:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=544
04/25/2022 05:01:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
04/25/2022 05:01:56 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.28125 on epoch=549
04/25/2022 05:02:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=554
04/25/2022 05:02:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=559
04/25/2022 05:02:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=564
04/25/2022 05:02:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=569
04/25/2022 05:02:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
04/25/2022 05:02:20 - INFO - __main__ - Global step 1150 Train loss 0.06 ACC 0.25 on epoch=574
04/25/2022 05:02:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
04/25/2022 05:02:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
04/25/2022 05:02:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
04/25/2022 05:02:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
04/25/2022 05:02:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=599
04/25/2022 05:02:45 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.28125 on epoch=599
04/25/2022 05:02:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
04/25/2022 05:02:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
04/25/2022 05:02:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 05:03:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
04/25/2022 05:03:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
04/25/2022 05:03:09 - INFO - __main__ - Global step 1250 Train loss 0.04 ACC 0.28125 on epoch=624
04/25/2022 05:03:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
04/25/2022 05:03:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=634
04/25/2022 05:03:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=639
04/25/2022 05:03:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=644
04/25/2022 05:03:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
04/25/2022 05:03:33 - INFO - __main__ - Global step 1300 Train loss 0.05 ACC 0.25 on epoch=649
04/25/2022 05:03:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
04/25/2022 05:03:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
04/25/2022 05:03:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
04/25/2022 05:03:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
04/25/2022 05:03:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
04/25/2022 05:03:57 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.25 on epoch=674
04/25/2022 05:04:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
04/25/2022 05:04:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=684
04/25/2022 05:04:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=689
04/25/2022 05:04:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
04/25/2022 05:04:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
04/25/2022 05:04:21 - INFO - __main__ - Global step 1400 Train loss 0.04 ACC 0.28125 on epoch=699
04/25/2022 05:04:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/25/2022 05:04:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
04/25/2022 05:04:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
04/25/2022 05:04:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=719
04/25/2022 05:04:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
04/25/2022 05:04:45 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.3125 on epoch=724
04/25/2022 05:04:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
04/25/2022 05:04:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=734
04/25/2022 05:04:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
04/25/2022 05:05:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 05:05:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=749
04/25/2022 05:05:09 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.3125 on epoch=749
04/25/2022 05:05:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
04/25/2022 05:05:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
04/25/2022 05:05:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
04/25/2022 05:05:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
04/25/2022 05:05:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/25/2022 05:05:34 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.3125 on epoch=774
04/25/2022 05:05:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/25/2022 05:05:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
04/25/2022 05:05:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
04/25/2022 05:05:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
04/25/2022 05:05:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=799
04/25/2022 05:05:58 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.3125 on epoch=799
04/25/2022 05:06:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/25/2022 05:06:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
04/25/2022 05:06:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=814
04/25/2022 05:06:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
04/25/2022 05:06:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
04/25/2022 05:06:22 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.28125 on epoch=824
04/25/2022 05:06:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
04/25/2022 05:06:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/25/2022 05:06:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/25/2022 05:06:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/25/2022 05:06:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
04/25/2022 05:06:46 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.34375 on epoch=849
04/25/2022 05:06:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
04/25/2022 05:06:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
04/25/2022 05:06:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
04/25/2022 05:07:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
04/25/2022 05:07:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/25/2022 05:07:10 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.28125 on epoch=874
04/25/2022 05:07:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/25/2022 05:07:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
04/25/2022 05:07:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
04/25/2022 05:07:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
04/25/2022 05:07:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
04/25/2022 05:07:34 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.25 on epoch=899
04/25/2022 05:07:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
04/25/2022 05:07:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/25/2022 05:07:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
04/25/2022 05:07:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
04/25/2022 05:07:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
04/25/2022 05:07:58 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.3125 on epoch=924
04/25/2022 05:08:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
04/25/2022 05:08:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
04/25/2022 05:08:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
04/25/2022 05:08:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
04/25/2022 05:08:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
04/25/2022 05:08:22 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.25 on epoch=949
04/25/2022 05:08:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 05:08:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
04/25/2022 05:08:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 05:08:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
04/25/2022 05:08:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 05:08:46 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.21875 on epoch=974
04/25/2022 05:08:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=979
04/25/2022 05:08:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=984
04/25/2022 05:08:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=989
04/25/2022 05:09:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
04/25/2022 05:09:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 05:09:10 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:09:10 - INFO - __main__ - Printing 3 examples
04/25/2022 05:09:10 - INFO - __main__ -  [dream] What will too many cars lead to according to the woman? [SEP] M: The traffic is becoming worse and worse. I think there'll be huge changes in the ways people use cars. New laws will be made about what kind of car you can own and when you can drive. W: Maybe there'll just be too many of them on the roads. The air will be so seriously polluted that nobody will be able to breathe. M: Exactly. People will have to rely on trains. W: Why do you say that? M: Well, we won't be able to use cars, and airports take up too much space. That leaves trains. W: Huh. So do you think there'll be more efficient train systems between cities? M: Sure. They'll enable people to travel between cities in a matter of hours. There may even be trains going under the oceans to connect the main continents. W: Under the oceans? Oh, no! I get nervous enough flying on a plane. [SEP]  (A) Air pollution. (B) Traffic jam. (C) Oil shortage.
04/25/2022 05:09:10 - INFO - __main__ - ['Air pollution.']
04/25/2022 05:09:10 - INFO - __main__ -  [dream] Where was the woman on July 4th? [SEP] M: Hi, Jane. It's nice to see you again. I heard that you went to the US during the vacation. W: Yes. I went to New York to attend a summer course in English. M: Wow. You were lucky. How long did you stay there? W: About 50 days. I went there on July 5th and came back on August 25th. M: How about the course? W: The course was very good. The teachers were nice. They taught us to listen, speak, read and write in English, but it was mostly speaking. One interesting thing I found was that the American classes are different from our classes here because the students have a lot more freedom. You can sit anywhere you like in the classroom. You can ask the teachers questions at any time during the class, and you are welcome to share your ideas with the class. I really like this kind of class. M: How interesting! Maybe our teacher should try that. [SEP]  (A) In an American university. (B) In New York. (C) At home.
04/25/2022 05:09:10 - INFO - __main__ - ['At home.']
04/25/2022 05:09:10 - INFO - __main__ -  [dream] How does the woman go to John's home? [SEP] M: Would you like to have lunch with us? W: I'd love to, but I have to help John with his math problem right now. M: Would you like me to give you a lift? W: No,thanks. His home is not far and I can walk there. [SEP]  (A) By bus. (B) On foot. (C) Take the man's car.
04/25/2022 05:09:10 - INFO - __main__ - ['On foot.']
04/25/2022 05:09:10 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:09:10 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:09:10 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 05:09:10 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:09:10 - INFO - __main__ - Printing 3 examples
04/25/2022 05:09:10 - INFO - __main__ -  [dream] What can we learn from the conversation? [SEP] W: Why haven't we received any newspapers yet? M: Well, sometimes it takes a while for the post office to deliver it. [SEP]  (A) The man will go to the post office. (B) The post office is closed for the day. (C) The woman is expecting the newspaper.
04/25/2022 05:09:10 - INFO - __main__ - ['The woman is expecting the newspaper.']
04/25/2022 05:09:10 - INFO - __main__ -  [dream] What can be inferred from the conversation? [SEP] W: What are the things in our our suitcase? There aren't any toys at all. Where have you put them? M: Oh, no. This is not our suitcase. The old lady must have taken ours bymistake. She was sitting next to us at the restaurant. [SEP]  (A) The old lady sitting next to the couple likes toys very much. (B) An old lady took the couple's suitcasefor her own. (C) The couple's suitcase was stolen inthe restaurant.
04/25/2022 05:09:10 - INFO - __main__ - ["An old lady took the couple's suitcasefor her own."]
04/25/2022 05:09:10 - INFO - __main__ -  [dream] When did the woman go to see Kate? [SEP] M: Have you seen Kate recently, Vicky? W: Yes, I have. I saw her a couple of days ago. She hasn't been very well in the last couple of weeks. M: Has she seen a doctor since she's been ill? W: Yes, she has. The doctor told her to take it easy for a while, but she hasn't been taking his advice. She's as busy as usual. M: Do you think it useful for me to ask her to have a rest when I go to see her? Or shall we go together? W: I think you can go yourself and show your concern to her since she sometimes would take your advice. So it's unnecessary for me to go with you. What's more, I've got some other things to do at the moment. [SEP]  (A) Two days ago. (B) Two weeks ago. (C) A week ago.
04/25/2022 05:09:10 - INFO - __main__ - ['Two days ago.']
04/25/2022 05:09:10 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:09:10 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:09:10 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 05:09:10 - INFO - __main__ - Global step 2000 Train loss 0.03 ACC 0.21875 on epoch=999
04/25/2022 05:09:10 - INFO - __main__ - save last model!
04/25/2022 05:09:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 05:09:10 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 05:09:10 - INFO - __main__ - Printing 3 examples
04/25/2022 05:09:10 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 05:09:10 - INFO - __main__ - ['To change her job.']
04/25/2022 05:09:10 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 05:09:10 - INFO - __main__ - ['Watch TV.']
04/25/2022 05:09:10 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 05:09:10 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 05:09:10 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:09:11 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:09:12 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 05:09:28 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 05:09:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 05:09:29 - INFO - __main__ - Starting training!
04/25/2022 05:10:29 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_13_0.3_8_predictions.txt
04/25/2022 05:10:29 - INFO - __main__ - ACC on test data: 0.3090
04/25/2022 05:10:29 - INFO - __main__ - prefix=dream_32_13, lr=0.3, bsz=8, dev_performance=0.34375, test_performance=0.309
04/25/2022 05:10:29 - INFO - __main__ - Running ... prefix=dream_32_13, lr=0.2, bsz=8 ...
04/25/2022 05:10:30 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:10:30 - INFO - __main__ - Printing 3 examples
04/25/2022 05:10:30 - INFO - __main__ -  [dream] What will too many cars lead to according to the woman? [SEP] M: The traffic is becoming worse and worse. I think there'll be huge changes in the ways people use cars. New laws will be made about what kind of car you can own and when you can drive. W: Maybe there'll just be too many of them on the roads. The air will be so seriously polluted that nobody will be able to breathe. M: Exactly. People will have to rely on trains. W: Why do you say that? M: Well, we won't be able to use cars, and airports take up too much space. That leaves trains. W: Huh. So do you think there'll be more efficient train systems between cities? M: Sure. They'll enable people to travel between cities in a matter of hours. There may even be trains going under the oceans to connect the main continents. W: Under the oceans? Oh, no! I get nervous enough flying on a plane. [SEP]  (A) Air pollution. (B) Traffic jam. (C) Oil shortage.
04/25/2022 05:10:30 - INFO - __main__ - ['Air pollution.']
04/25/2022 05:10:30 - INFO - __main__ -  [dream] Where was the woman on July 4th? [SEP] M: Hi, Jane. It's nice to see you again. I heard that you went to the US during the vacation. W: Yes. I went to New York to attend a summer course in English. M: Wow. You were lucky. How long did you stay there? W: About 50 days. I went there on July 5th and came back on August 25th. M: How about the course? W: The course was very good. The teachers were nice. They taught us to listen, speak, read and write in English, but it was mostly speaking. One interesting thing I found was that the American classes are different from our classes here because the students have a lot more freedom. You can sit anywhere you like in the classroom. You can ask the teachers questions at any time during the class, and you are welcome to share your ideas with the class. I really like this kind of class. M: How interesting! Maybe our teacher should try that. [SEP]  (A) In an American university. (B) In New York. (C) At home.
04/25/2022 05:10:30 - INFO - __main__ - ['At home.']
04/25/2022 05:10:30 - INFO - __main__ -  [dream] How does the woman go to John's home? [SEP] M: Would you like to have lunch with us? W: I'd love to, but I have to help John with his math problem right now. M: Would you like me to give you a lift? W: No,thanks. His home is not far and I can walk there. [SEP]  (A) By bus. (B) On foot. (C) Take the man's car.
04/25/2022 05:10:30 - INFO - __main__ - ['On foot.']
04/25/2022 05:10:30 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:10:30 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:10:30 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 05:10:30 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:10:30 - INFO - __main__ - Printing 3 examples
04/25/2022 05:10:30 - INFO - __main__ -  [dream] What can we learn from the conversation? [SEP] W: Why haven't we received any newspapers yet? M: Well, sometimes it takes a while for the post office to deliver it. [SEP]  (A) The man will go to the post office. (B) The post office is closed for the day. (C) The woman is expecting the newspaper.
04/25/2022 05:10:30 - INFO - __main__ - ['The woman is expecting the newspaper.']
04/25/2022 05:10:30 - INFO - __main__ -  [dream] What can be inferred from the conversation? [SEP] W: What are the things in our our suitcase? There aren't any toys at all. Where have you put them? M: Oh, no. This is not our suitcase. The old lady must have taken ours bymistake. She was sitting next to us at the restaurant. [SEP]  (A) The old lady sitting next to the couple likes toys very much. (B) An old lady took the couple's suitcasefor her own. (C) The couple's suitcase was stolen inthe restaurant.
04/25/2022 05:10:30 - INFO - __main__ - ["An old lady took the couple's suitcasefor her own."]
04/25/2022 05:10:30 - INFO - __main__ -  [dream] When did the woman go to see Kate? [SEP] M: Have you seen Kate recently, Vicky? W: Yes, I have. I saw her a couple of days ago. She hasn't been very well in the last couple of weeks. M: Has she seen a doctor since she's been ill? W: Yes, she has. The doctor told her to take it easy for a while, but she hasn't been taking his advice. She's as busy as usual. M: Do you think it useful for me to ask her to have a rest when I go to see her? Or shall we go together? W: I think you can go yourself and show your concern to her since she sometimes would take your advice. So it's unnecessary for me to go with you. What's more, I've got some other things to do at the moment. [SEP]  (A) Two days ago. (B) Two weeks ago. (C) A week ago.
04/25/2022 05:10:30 - INFO - __main__ - ['Two days ago.']
04/25/2022 05:10:30 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:10:30 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:10:30 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 05:10:49 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 05:10:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 05:10:49 - INFO - __main__ - Starting training!
04/25/2022 05:10:55 - INFO - __main__ - Step 10 Global step 10 Train loss 1.58 on epoch=4
04/25/2022 05:10:59 - INFO - __main__ - Step 20 Global step 20 Train loss 1.32 on epoch=9
04/25/2022 05:11:03 - INFO - __main__ - Step 30 Global step 30 Train loss 1.07 on epoch=14
04/25/2022 05:11:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.84 on epoch=19
04/25/2022 05:11:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.79 on epoch=24
04/25/2022 05:11:18 - INFO - __main__ - Global step 50 Train loss 1.12 ACC 0.21875 on epoch=24
04/25/2022 05:11:18 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.21875 on epoch=24, global_step=50
04/25/2022 05:11:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=29
04/25/2022 05:11:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.62 on epoch=34
04/25/2022 05:11:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=39
04/25/2022 05:11:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=44
04/25/2022 05:11:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=49
04/25/2022 05:11:45 - INFO - __main__ - Global step 100 Train loss 0.55 ACC 0.28125 on epoch=49
04/25/2022 05:11:45 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.28125 on epoch=49, global_step=100
04/25/2022 05:11:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=54
04/25/2022 05:11:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.39 on epoch=59
04/25/2022 05:11:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=64
04/25/2022 05:12:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.34 on epoch=69
04/25/2022 05:12:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=74
04/25/2022 05:12:09 - INFO - __main__ - Global step 150 Train loss 0.39 ACC 0.28125 on epoch=74
04/25/2022 05:12:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
04/25/2022 05:12:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.26 on epoch=84
04/25/2022 05:12:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.30 on epoch=89
04/25/2022 05:12:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.19 on epoch=94
04/25/2022 05:12:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
04/25/2022 05:12:34 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.28125 on epoch=99
04/25/2022 05:12:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
04/25/2022 05:12:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
04/25/2022 05:12:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=114
04/25/2022 05:12:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.19 on epoch=119
04/25/2022 05:12:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=124
04/25/2022 05:12:59 - INFO - __main__ - Global step 250 Train loss 0.23 ACC 0.3125 on epoch=124
04/25/2022 05:12:59 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=124, global_step=250
04/25/2022 05:13:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
04/25/2022 05:13:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.18 on epoch=134
04/25/2022 05:13:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=139
04/25/2022 05:13:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=144
04/25/2022 05:13:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=149
04/25/2022 05:13:25 - INFO - __main__ - Global step 300 Train loss 0.22 ACC 0.34375 on epoch=149
04/25/2022 05:13:25 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.34375 on epoch=149, global_step=300
04/25/2022 05:13:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=154
04/25/2022 05:13:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.13 on epoch=159
04/25/2022 05:13:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=164
04/25/2022 05:13:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=169
04/25/2022 05:13:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
04/25/2022 05:13:50 - INFO - __main__ - Global step 350 Train loss 0.16 ACC 0.34375 on epoch=174
04/25/2022 05:13:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
04/25/2022 05:13:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.13 on epoch=184
04/25/2022 05:14:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.13 on epoch=189
04/25/2022 05:14:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.14 on epoch=194
04/25/2022 05:14:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=199
04/25/2022 05:14:15 - INFO - __main__ - Global step 400 Train loss 0.13 ACC 0.28125 on epoch=199
04/25/2022 05:14:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
04/25/2022 05:14:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.11 on epoch=209
04/25/2022 05:14:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=214
04/25/2022 05:14:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.15 on epoch=219
04/25/2022 05:14:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
04/25/2022 05:14:40 - INFO - __main__ - Global step 450 Train loss 0.12 ACC 0.3125 on epoch=224
04/25/2022 05:14:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
04/25/2022 05:14:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.11 on epoch=234
04/25/2022 05:14:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.08 on epoch=239
04/25/2022 05:14:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
04/25/2022 05:15:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.11 on epoch=249
04/25/2022 05:15:05 - INFO - __main__ - Global step 500 Train loss 0.09 ACC 0.3125 on epoch=249
04/25/2022 05:15:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
04/25/2022 05:15:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.07 on epoch=259
04/25/2022 05:15:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=264
04/25/2022 05:15:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.07 on epoch=269
04/25/2022 05:15:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=274
04/25/2022 05:15:30 - INFO - __main__ - Global step 550 Train loss 0.08 ACC 0.28125 on epoch=274
04/25/2022 05:15:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=279
04/25/2022 05:15:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
04/25/2022 05:15:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
04/25/2022 05:15:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
04/25/2022 05:15:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=299
04/25/2022 05:15:55 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.34375 on epoch=299
04/25/2022 05:15:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.08 on epoch=304
04/25/2022 05:16:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=309
04/25/2022 05:16:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=314
04/25/2022 05:16:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=319
04/25/2022 05:16:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=324
04/25/2022 05:16:20 - INFO - __main__ - Global step 650 Train loss 0.07 ACC 0.3125 on epoch=324
04/25/2022 05:16:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
04/25/2022 05:16:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
04/25/2022 05:16:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=339
04/25/2022 05:16:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
04/25/2022 05:16:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=349
04/25/2022 05:16:45 - INFO - __main__ - Global step 700 Train loss 0.07 ACC 0.34375 on epoch=349
04/25/2022 05:16:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
04/25/2022 05:16:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
04/25/2022 05:16:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=364
04/25/2022 05:17:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=369
04/25/2022 05:17:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
04/25/2022 05:17:09 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.3125 on epoch=374
04/25/2022 05:17:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=379
04/25/2022 05:17:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
04/25/2022 05:17:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=389
04/25/2022 05:17:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
04/25/2022 05:17:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
04/25/2022 05:17:33 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.28125 on epoch=399
04/25/2022 05:17:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
04/25/2022 05:17:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
04/25/2022 05:17:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
04/25/2022 05:17:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
04/25/2022 05:17:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
04/25/2022 05:17:58 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.3125 on epoch=424
04/25/2022 05:18:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=429
04/25/2022 05:18:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=434
04/25/2022 05:18:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
04/25/2022 05:18:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
04/25/2022 05:18:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
04/25/2022 05:18:22 - INFO - __main__ - Global step 900 Train loss 0.05 ACC 0.34375 on epoch=449
04/25/2022 05:18:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
04/25/2022 05:18:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=459
04/25/2022 05:18:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
04/25/2022 05:18:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
04/25/2022 05:18:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
04/25/2022 05:18:47 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.34375 on epoch=474
04/25/2022 05:18:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=479
04/25/2022 05:18:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
04/25/2022 05:19:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
04/25/2022 05:19:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
04/25/2022 05:19:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/25/2022 05:19:11 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.34375 on epoch=499
04/25/2022 05:19:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
04/25/2022 05:19:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
04/25/2022 05:19:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
04/25/2022 05:19:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
04/25/2022 05:19:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
04/25/2022 05:19:36 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.34375 on epoch=524
04/25/2022 05:19:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=529
04/25/2022 05:19:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=534
04/25/2022 05:19:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/25/2022 05:19:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
04/25/2022 05:19:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
04/25/2022 05:20:00 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.3125 on epoch=549
04/25/2022 05:20:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
04/25/2022 05:20:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
04/25/2022 05:20:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/25/2022 05:20:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
04/25/2022 05:20:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 05:20:24 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.28125 on epoch=574
04/25/2022 05:20:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 05:20:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
04/25/2022 05:20:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
04/25/2022 05:20:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
04/25/2022 05:20:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
04/25/2022 05:20:49 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.3125 on epoch=599
04/25/2022 05:20:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/25/2022 05:20:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=609
04/25/2022 05:21:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=614
04/25/2022 05:21:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
04/25/2022 05:21:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
04/25/2022 05:21:13 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.34375 on epoch=624
04/25/2022 05:21:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 05:21:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
04/25/2022 05:21:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/25/2022 05:21:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/25/2022 05:21:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/25/2022 05:21:37 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.3125 on epoch=649
04/25/2022 05:21:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
04/25/2022 05:21:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 05:21:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/25/2022 05:21:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
04/25/2022 05:21:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 05:22:02 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.375 on epoch=674
04/25/2022 05:22:02 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=674, global_step=1350
04/25/2022 05:22:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/25/2022 05:22:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 05:22:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
04/25/2022 05:22:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
04/25/2022 05:22:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 05:22:26 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.34375 on epoch=699
04/25/2022 05:22:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 05:22:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
04/25/2022 05:22:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
04/25/2022 05:22:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 05:22:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
04/25/2022 05:22:50 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.375 on epoch=724
04/25/2022 05:22:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/25/2022 05:22:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/25/2022 05:23:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
04/25/2022 05:23:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
04/25/2022 05:23:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 05:23:15 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.3125 on epoch=749
04/25/2022 05:23:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
04/25/2022 05:23:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
04/25/2022 05:23:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
04/25/2022 05:23:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 05:23:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/25/2022 05:23:39 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.34375 on epoch=774
04/25/2022 05:23:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
04/25/2022 05:23:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/25/2022 05:23:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
04/25/2022 05:23:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/25/2022 05:24:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/25/2022 05:24:03 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.3125 on epoch=799
04/25/2022 05:24:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 05:24:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 05:24:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 05:24:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 05:24:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 05:24:27 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.3125 on epoch=824
04/25/2022 05:24:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/25/2022 05:24:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/25/2022 05:24:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=839
04/25/2022 05:24:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 05:24:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
04/25/2022 05:24:52 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.34375 on epoch=849
04/25/2022 05:24:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
04/25/2022 05:25:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 05:25:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
04/25/2022 05:25:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
04/25/2022 05:25:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/25/2022 05:25:16 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.28125 on epoch=874
04/25/2022 05:25:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/25/2022 05:25:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 05:25:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 05:25:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 05:25:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 05:25:40 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.28125 on epoch=899
04/25/2022 05:25:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
04/25/2022 05:25:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
04/25/2022 05:25:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 05:25:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 05:26:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
04/25/2022 05:26:04 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.28125 on epoch=924
04/25/2022 05:26:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 05:26:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 05:26:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
04/25/2022 05:26:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
04/25/2022 05:26:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 05:26:28 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.28125 on epoch=949
04/25/2022 05:26:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
04/25/2022 05:26:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 05:26:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 05:26:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
04/25/2022 05:26:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
04/25/2022 05:26:53 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.34375 on epoch=974
04/25/2022 05:26:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
04/25/2022 05:27:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
04/25/2022 05:27:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/25/2022 05:27:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 05:27:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
04/25/2022 05:27:17 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.375 on epoch=999
04/25/2022 05:27:17 - INFO - __main__ - save last model!
04/25/2022 05:27:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 05:27:17 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 05:27:17 - INFO - __main__ - Printing 3 examples
04/25/2022 05:27:17 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 05:27:17 - INFO - __main__ - ['To change her job.']
04/25/2022 05:27:17 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 05:27:17 - INFO - __main__ - ['Watch TV.']
04/25/2022 05:27:17 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 05:27:17 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 05:27:17 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:27:17 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:27:17 - INFO - __main__ - Printing 3 examples
04/25/2022 05:27:17 - INFO - __main__ -  [dream] What are some of the benefits that the company provides? [SEP] Kelly: So, have you found a job yet? Josh: No, but, I have a few leads, so things are looking up. Kelly: But isn't that what you always say? Josh: Well ... uh ... this time is different. Kelly: What are you looking for this time, then? Josh: Actually, I want to work for a Web hosting company. Kelly: What would you do there? Josh: Well, in a nut shell, Web hosting companies provide space for people to store and run their Websites. Does it sound like I know what I'm talking about? Kelly: Oh, yeah, sort of. Josh: Well, And then, sort of? Well, they allow people to run their Web sites without having to buy and maintain their own servers, and I'd like to work in technical support, you know, helping customers resolve computer-related problems with their sites. And you know I'm a good communicator. Kelly: So, how's the pay for that kind of job? Josh: Well, most people I know start out with a very reasonable salary; you can earn pay increases depending on your performance. Kelly: So, what about benefits? Josh: Oh, the benefits are pretty good. They provide health insurance, two weeks (of) paid vacation a year, and opportunities for advancement. And in the end, I'd like to work in a management position. You know, sitting back, enjoying the view out of the twentieth-story window of the office building. Something like that. Kelly: Well, is there any long-term job security in a job like that? Josh: Uhh. That's hard to tell. I mean, the Internet is booming, and these kinds of companies are sprouting up everywhere, which is a good thing, but just like the dot-com era, you never know how long things will last. Kelly: Well, have you ever thought about going back to school to improve your job skills? Josh: Wait, wait. What are you suggesting? Kelly: Well, you know, more training might help you land a better job. Josh: Wh ... wh ... Are you trying to say something about my current job? I mean, is there something going on here? I mean, what are you saying? Kelly: You know, you did drop out of college. Josh: I know, I know, but I don't know. I'm just seeing my current job at McDonalds as a step up. [McDonalds!]. Yeah, but, you know, I don't have the resources to go back to school at the moment; however, the job I am looking at will pay for some classes after I have been with the company for six months. Kelly: Well, it looks like you have things planned out this time. Josh: If I last that long. [SEP]  (A) insurance, paid vacation, and a company vehicle (B) paid vacation, opportunities for promotion, and insurance (C) opportunities for advancement, insurance, and a free bus pass
04/25/2022 05:27:17 - INFO - __main__ - ['paid vacation, opportunities for promotion, and insurance']
04/25/2022 05:27:17 - INFO - __main__ -  [dream] What is the probable result for her exam? [SEP] M: What did you think of the mid-term exam? W: I was expecting it to be easy, but at the end of the first hour, I was still on the first page. I hardly had time to get to the last question. [SEP]  (A) It's OK. (B) Poor. (C) Excellent.
04/25/2022 05:27:17 - INFO - __main__ - ['Poor.']
04/25/2022 05:27:17 - INFO - __main__ -  [dream] What kind of coffee would the woman like? [SEP] M: How would you like your coffee? W: Milky one without sugar, please. [SEP]  (A) Black coffee. (B) With milk and sugar. (C) With milk and no sugar
04/25/2022 05:27:17 - INFO - __main__ - ['With milk and no sugar']
04/25/2022 05:27:17 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:27:17 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:27:17 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 05:27:17 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:27:17 - INFO - __main__ - Printing 3 examples
04/25/2022 05:27:17 - INFO - __main__ -  [dream] Whose wedding dress do the speakers mention? [SEP] M: Catherine, you look great in these photos. W: Oh, thanks. I got into that wedding dress with great difficulty. It was too small. M: Why didn't you get one your size? W: Oh, it was my grandmother's - my mother wore it on her wedding day, and I really wanted to wear it on mine. I suffered for six months so I could wear it. M: How did you do it? W: Do you know this diet that many actors are doing? I don't think it's very healthy if you do it for a long time, but it really works. M: I think meat is not allowed in your diet. W: No, actually, you can eat things like, um, roast chicken, steak, and bacon, but you can't eat any bread and butter. M: Is it the diet that you love but have to avoid? W: That's right. Dieting is so boring. Hopefully, I won't need to wear the wedding dress again anyway. [SEP]  (A) The woman's. (B) The woman's mother's. (C) The woman's grandmother's.
04/25/2022 05:27:17 - INFO - __main__ - ["The woman's grandmother's."]
04/25/2022 05:27:17 - INFO - __main__ -  [dream] What did the woman do in the caf today? [SEP] M: Is that a new computer? W: Yeah. My grandpa gave it to me as a graduation gift. Isn't it nice? M: It is. I can't believe it's so thin! What kind of computer is it? W: It's an Apple MacBook Air. It's very lightweight and has a bunch of built-in apps. I couldn't be happier with it. M: Do they have any other colors besides that one? W: I don't know. I like this color. I've ordered a pink cover for it, but it won't arrive for a couple of weeks. M: Does the computer have a disk drive? W: No, it doesn't, but I don't need one. All of my documents are online. M: Well, you're very lucky. It will be great to have this when you start looking for jobs. W: I already have. Today, I spent the day at the caf working on my rsum. This computer has great battery life. M: Well, that's good. But the caf near here is always so crowded. W: I know. But I like to work in busy cafs. The noise reminds me that everyone else is working hard, so I work hard, too. [SEP]  (A) She talked with a friend. (B) She worked on her rsum. (C) She looked for a job there.
04/25/2022 05:27:17 - INFO - __main__ - ['She worked on her rsum.']
04/25/2022 05:27:17 - INFO - __main__ -  [dream] Why is William Shakespeare mentioned in the conversation? [SEP] W: I enjoy going through a secondhand bookstore, don't you? It's interesting to see what people used to enjoy reading. Did you see this old book of children's stories? M: Some of these books aren't so old, though... See? This mystery was published only six years ago. It cost seventy-five cents. You can't beat that. W: Hey! Look at this! M: What? Are you getting interested in nineteenth century poetry all of a sudden? W: No. Look at the inscription! Someone gave this book as a present, and wrote a note on the inside of the front cover. It's dated 1893. Maybe it's worth something. M: Everything on that shelf is worth fifty cents. W: But if this is a signature of someone who is well known, it might bring a lot more. I hear William Shakespeare's signature is worth about a million dollars. [SEP]  (A) He gave gifts to millions of people. (B) He was a very wealthy man in his times. (C) His signature is worth a lot of money.
04/25/2022 05:27:17 - INFO - __main__ - ['His signature is worth a lot of money.']
04/25/2022 05:27:17 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:27:17 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:27:17 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 05:27:18 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:27:19 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 05:27:36 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 05:27:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 05:27:37 - INFO - __main__ - Starting training!
04/25/2022 05:28:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_13_0.2_8_predictions.txt
04/25/2022 05:28:35 - INFO - __main__ - ACC on test data: 0.2990
04/25/2022 05:28:35 - INFO - __main__ - prefix=dream_32_13, lr=0.2, bsz=8, dev_performance=0.375, test_performance=0.299
04/25/2022 05:28:35 - INFO - __main__ - Running ... prefix=dream_32_21, lr=0.5, bsz=8 ...
04/25/2022 05:28:36 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:28:36 - INFO - __main__ - Printing 3 examples
04/25/2022 05:28:36 - INFO - __main__ -  [dream] What are some of the benefits that the company provides? [SEP] Kelly: So, have you found a job yet? Josh: No, but, I have a few leads, so things are looking up. Kelly: But isn't that what you always say? Josh: Well ... uh ... this time is different. Kelly: What are you looking for this time, then? Josh: Actually, I want to work for a Web hosting company. Kelly: What would you do there? Josh: Well, in a nut shell, Web hosting companies provide space for people to store and run their Websites. Does it sound like I know what I'm talking about? Kelly: Oh, yeah, sort of. Josh: Well, And then, sort of? Well, they allow people to run their Web sites without having to buy and maintain their own servers, and I'd like to work in technical support, you know, helping customers resolve computer-related problems with their sites. And you know I'm a good communicator. Kelly: So, how's the pay for that kind of job? Josh: Well, most people I know start out with a very reasonable salary; you can earn pay increases depending on your performance. Kelly: So, what about benefits? Josh: Oh, the benefits are pretty good. They provide health insurance, two weeks (of) paid vacation a year, and opportunities for advancement. And in the end, I'd like to work in a management position. You know, sitting back, enjoying the view out of the twentieth-story window of the office building. Something like that. Kelly: Well, is there any long-term job security in a job like that? Josh: Uhh. That's hard to tell. I mean, the Internet is booming, and these kinds of companies are sprouting up everywhere, which is a good thing, but just like the dot-com era, you never know how long things will last. Kelly: Well, have you ever thought about going back to school to improve your job skills? Josh: Wait, wait. What are you suggesting? Kelly: Well, you know, more training might help you land a better job. Josh: Wh ... wh ... Are you trying to say something about my current job? I mean, is there something going on here? I mean, what are you saying? Kelly: You know, you did drop out of college. Josh: I know, I know, but I don't know. I'm just seeing my current job at McDonalds as a step up. [McDonalds!]. Yeah, but, you know, I don't have the resources to go back to school at the moment; however, the job I am looking at will pay for some classes after I have been with the company for six months. Kelly: Well, it looks like you have things planned out this time. Josh: If I last that long. [SEP]  (A) insurance, paid vacation, and a company vehicle (B) paid vacation, opportunities for promotion, and insurance (C) opportunities for advancement, insurance, and a free bus pass
04/25/2022 05:28:36 - INFO - __main__ - ['paid vacation, opportunities for promotion, and insurance']
04/25/2022 05:28:36 - INFO - __main__ -  [dream] What is the probable result for her exam? [SEP] M: What did you think of the mid-term exam? W: I was expecting it to be easy, but at the end of the first hour, I was still on the first page. I hardly had time to get to the last question. [SEP]  (A) It's OK. (B) Poor. (C) Excellent.
04/25/2022 05:28:36 - INFO - __main__ - ['Poor.']
04/25/2022 05:28:36 - INFO - __main__ -  [dream] What kind of coffee would the woman like? [SEP] M: How would you like your coffee? W: Milky one without sugar, please. [SEP]  (A) Black coffee. (B) With milk and sugar. (C) With milk and no sugar
04/25/2022 05:28:36 - INFO - __main__ - ['With milk and no sugar']
04/25/2022 05:28:36 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:28:36 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:28:36 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 05:28:36 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:28:36 - INFO - __main__ - Printing 3 examples
04/25/2022 05:28:36 - INFO - __main__ -  [dream] Whose wedding dress do the speakers mention? [SEP] M: Catherine, you look great in these photos. W: Oh, thanks. I got into that wedding dress with great difficulty. It was too small. M: Why didn't you get one your size? W: Oh, it was my grandmother's - my mother wore it on her wedding day, and I really wanted to wear it on mine. I suffered for six months so I could wear it. M: How did you do it? W: Do you know this diet that many actors are doing? I don't think it's very healthy if you do it for a long time, but it really works. M: I think meat is not allowed in your diet. W: No, actually, you can eat things like, um, roast chicken, steak, and bacon, but you can't eat any bread and butter. M: Is it the diet that you love but have to avoid? W: That's right. Dieting is so boring. Hopefully, I won't need to wear the wedding dress again anyway. [SEP]  (A) The woman's. (B) The woman's mother's. (C) The woman's grandmother's.
04/25/2022 05:28:36 - INFO - __main__ - ["The woman's grandmother's."]
04/25/2022 05:28:36 - INFO - __main__ -  [dream] What did the woman do in the caf today? [SEP] M: Is that a new computer? W: Yeah. My grandpa gave it to me as a graduation gift. Isn't it nice? M: It is. I can't believe it's so thin! What kind of computer is it? W: It's an Apple MacBook Air. It's very lightweight and has a bunch of built-in apps. I couldn't be happier with it. M: Do they have any other colors besides that one? W: I don't know. I like this color. I've ordered a pink cover for it, but it won't arrive for a couple of weeks. M: Does the computer have a disk drive? W: No, it doesn't, but I don't need one. All of my documents are online. M: Well, you're very lucky. It will be great to have this when you start looking for jobs. W: I already have. Today, I spent the day at the caf working on my rsum. This computer has great battery life. M: Well, that's good. But the caf near here is always so crowded. W: I know. But I like to work in busy cafs. The noise reminds me that everyone else is working hard, so I work hard, too. [SEP]  (A) She talked with a friend. (B) She worked on her rsum. (C) She looked for a job there.
04/25/2022 05:28:36 - INFO - __main__ - ['She worked on her rsum.']
04/25/2022 05:28:36 - INFO - __main__ -  [dream] Why is William Shakespeare mentioned in the conversation? [SEP] W: I enjoy going through a secondhand bookstore, don't you? It's interesting to see what people used to enjoy reading. Did you see this old book of children's stories? M: Some of these books aren't so old, though... See? This mystery was published only six years ago. It cost seventy-five cents. You can't beat that. W: Hey! Look at this! M: What? Are you getting interested in nineteenth century poetry all of a sudden? W: No. Look at the inscription! Someone gave this book as a present, and wrote a note on the inside of the front cover. It's dated 1893. Maybe it's worth something. M: Everything on that shelf is worth fifty cents. W: But if this is a signature of someone who is well known, it might bring a lot more. I hear William Shakespeare's signature is worth about a million dollars. [SEP]  (A) He gave gifts to millions of people. (B) He was a very wealthy man in his times. (C) His signature is worth a lot of money.
04/25/2022 05:28:36 - INFO - __main__ - ['His signature is worth a lot of money.']
04/25/2022 05:28:36 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:28:36 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:28:36 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 05:28:55 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 05:28:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 05:28:55 - INFO - __main__ - Starting training!
04/25/2022 05:29:01 - INFO - __main__ - Step 10 Global step 10 Train loss 1.28 on epoch=4
04/25/2022 05:29:05 - INFO - __main__ - Step 20 Global step 20 Train loss 0.98 on epoch=9
04/25/2022 05:29:10 - INFO - __main__ - Step 30 Global step 30 Train loss 0.82 on epoch=14
04/25/2022 05:29:14 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=19
04/25/2022 05:29:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=24
04/25/2022 05:29:21 - INFO - __main__ - Global step 50 Train loss 0.81 ACC 0.34375 on epoch=24
04/25/2022 05:29:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.34375 on epoch=24, global_step=50
04/25/2022 05:29:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.31 on epoch=29
04/25/2022 05:29:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.23 on epoch=34
04/25/2022 05:29:35 - INFO - __main__ - Step 80 Global step 80 Train loss 0.25 on epoch=39
04/25/2022 05:29:39 - INFO - __main__ - Step 90 Global step 90 Train loss 0.24 on epoch=44
04/25/2022 05:29:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.17 on epoch=49
04/25/2022 05:29:46 - INFO - __main__ - Global step 100 Train loss 0.24 ACC 0.375 on epoch=49
04/25/2022 05:29:46 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=49, global_step=100
04/25/2022 05:29:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.19 on epoch=54
04/25/2022 05:29:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.17 on epoch=59
04/25/2022 05:30:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.15 on epoch=64
04/25/2022 05:30:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.23 on epoch=69
04/25/2022 05:30:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.16 on epoch=74
04/25/2022 05:30:11 - INFO - __main__ - Global step 150 Train loss 0.18 ACC 0.3125 on epoch=74
04/25/2022 05:30:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.15 on epoch=79
04/25/2022 05:30:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.14 on epoch=84
04/25/2022 05:30:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.13 on epoch=89
04/25/2022 05:30:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.13 on epoch=94
04/25/2022 05:30:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.11 on epoch=99
04/25/2022 05:30:36 - INFO - __main__ - Global step 200 Train loss 0.13 ACC 0.25 on epoch=99
04/25/2022 05:30:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.06 on epoch=104
04/25/2022 05:30:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.12 on epoch=109
04/25/2022 05:30:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.11 on epoch=114
04/25/2022 05:30:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.08 on epoch=119
04/25/2022 05:30:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.11 on epoch=124
04/25/2022 05:31:01 - INFO - __main__ - Global step 250 Train loss 0.09 ACC 0.25 on epoch=124
04/25/2022 05:31:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.11 on epoch=129
04/25/2022 05:31:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.09 on epoch=134
04/25/2022 05:31:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.09 on epoch=139
04/25/2022 05:31:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.07 on epoch=144
04/25/2022 05:31:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.06 on epoch=149
04/25/2022 05:31:26 - INFO - __main__ - Global step 300 Train loss 0.08 ACC 0.21875 on epoch=149
04/25/2022 05:31:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.07 on epoch=154
04/25/2022 05:31:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.09 on epoch=159
04/25/2022 05:31:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.09 on epoch=164
04/25/2022 05:31:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.07 on epoch=169
04/25/2022 05:31:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.07 on epoch=174
04/25/2022 05:31:51 - INFO - __main__ - Global step 350 Train loss 0.08 ACC 0.25 on epoch=174
04/25/2022 05:31:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.05 on epoch=179
04/25/2022 05:32:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.04 on epoch=184
04/25/2022 05:32:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.05 on epoch=189
04/25/2022 05:32:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.05 on epoch=194
04/25/2022 05:32:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.04 on epoch=199
04/25/2022 05:32:15 - INFO - __main__ - Global step 400 Train loss 0.05 ACC 0.3125 on epoch=199
04/25/2022 05:32:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.06 on epoch=204
04/25/2022 05:32:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.04 on epoch=209
04/25/2022 05:32:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.03 on epoch=214
04/25/2022 05:32:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.07 on epoch=219
04/25/2022 05:32:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.05 on epoch=224
04/25/2022 05:32:40 - INFO - __main__ - Global step 450 Train loss 0.05 ACC 0.28125 on epoch=224
04/25/2022 05:32:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.06 on epoch=229
04/25/2022 05:32:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
04/25/2022 05:32:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.04 on epoch=239
04/25/2022 05:32:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
04/25/2022 05:33:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=249
04/25/2022 05:33:05 - INFO - __main__ - Global step 500 Train loss 0.05 ACC 0.25 on epoch=249
04/25/2022 05:33:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.04 on epoch=254
04/25/2022 05:33:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=259
04/25/2022 05:33:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.03 on epoch=264
04/25/2022 05:33:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
04/25/2022 05:33:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
04/25/2022 05:33:30 - INFO - __main__ - Global step 550 Train loss 0.04 ACC 0.25 on epoch=274
04/25/2022 05:33:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
04/25/2022 05:33:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.04 on epoch=284
04/25/2022 05:33:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=289
04/25/2022 05:33:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
04/25/2022 05:33:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
04/25/2022 05:33:55 - INFO - __main__ - Global step 600 Train loss 0.03 ACC 0.3125 on epoch=299
04/25/2022 05:33:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=304
04/25/2022 05:34:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
04/25/2022 05:34:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
04/25/2022 05:34:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
04/25/2022 05:34:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
04/25/2022 05:34:20 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.25 on epoch=324
04/25/2022 05:34:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
04/25/2022 05:34:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
04/25/2022 05:34:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
04/25/2022 05:34:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
04/25/2022 05:34:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
04/25/2022 05:34:44 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.15625 on epoch=349
04/25/2022 05:34:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
04/25/2022 05:34:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
04/25/2022 05:34:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
04/25/2022 05:35:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
04/25/2022 05:35:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
04/25/2022 05:35:09 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.15625 on epoch=374
04/25/2022 05:35:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
04/25/2022 05:35:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
04/25/2022 05:35:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
04/25/2022 05:35:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
04/25/2022 05:35:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
04/25/2022 05:35:35 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.15625 on epoch=399
04/25/2022 05:35:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
04/25/2022 05:35:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
04/25/2022 05:35:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
04/25/2022 05:35:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=419
04/25/2022 05:35:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
04/25/2022 05:36:00 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.1875 on epoch=424
04/25/2022 05:36:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
04/25/2022 05:36:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
04/25/2022 05:36:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
04/25/2022 05:36:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
04/25/2022 05:36:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
04/25/2022 05:36:26 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.25 on epoch=449
04/25/2022 05:36:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
04/25/2022 05:36:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
04/25/2022 05:36:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
04/25/2022 05:36:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
04/25/2022 05:36:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
04/25/2022 05:36:51 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.1875 on epoch=474
04/25/2022 05:36:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
04/25/2022 05:37:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
04/25/2022 05:37:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
04/25/2022 05:37:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
04/25/2022 05:37:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
04/25/2022 05:37:16 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.28125 on epoch=499
04/25/2022 05:37:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
04/25/2022 05:37:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
04/25/2022 05:37:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
04/25/2022 05:37:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
04/25/2022 05:37:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/25/2022 05:37:41 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.1875 on epoch=524
04/25/2022 05:37:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
04/25/2022 05:37:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
04/25/2022 05:37:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/25/2022 05:37:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
04/25/2022 05:38:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
04/25/2022 05:38:06 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.15625 on epoch=549
04/25/2022 05:38:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/25/2022 05:38:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/25/2022 05:38:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/25/2022 05:38:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
04/25/2022 05:38:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
04/25/2022 05:38:31 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.1875 on epoch=574
04/25/2022 05:38:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 05:38:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
04/25/2022 05:38:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
04/25/2022 05:38:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
04/25/2022 05:38:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/25/2022 05:38:56 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.25 on epoch=599
04/25/2022 05:39:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/25/2022 05:39:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 05:39:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 05:39:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
04/25/2022 05:39:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
04/25/2022 05:39:21 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.25 on epoch=624
04/25/2022 05:39:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 05:39:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 05:39:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/25/2022 05:39:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/25/2022 05:39:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/25/2022 05:39:46 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.25 on epoch=649
04/25/2022 05:39:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
04/25/2022 05:39:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 05:40:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/25/2022 05:40:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/25/2022 05:40:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
04/25/2022 05:40:11 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.28125 on epoch=674
04/25/2022 05:40:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 05:40:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 05:40:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/25/2022 05:40:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 05:40:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 05:40:36 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.25 on epoch=699
04/25/2022 05:40:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/25/2022 05:40:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 05:40:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 05:40:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/25/2022 05:40:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
04/25/2022 05:41:01 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.21875 on epoch=724
04/25/2022 05:41:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 05:41:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
04/25/2022 05:41:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 05:41:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
04/25/2022 05:41:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 05:41:25 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.25 on epoch=749
04/25/2022 05:41:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 05:41:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 05:41:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
04/25/2022 05:41:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
04/25/2022 05:41:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/25/2022 05:41:50 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.25 on epoch=774
04/25/2022 05:41:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/25/2022 05:42:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 05:42:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.11 on epoch=789
04/25/2022 05:42:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 05:42:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/25/2022 05:42:15 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.28125 on epoch=799
04/25/2022 05:42:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/25/2022 05:42:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/25/2022 05:42:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
04/25/2022 05:42:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 05:42:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 05:42:40 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.28125 on epoch=824
04/25/2022 05:42:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/25/2022 05:42:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
04/25/2022 05:42:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 05:42:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 05:43:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
04/25/2022 05:43:05 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.34375 on epoch=849
04/25/2022 05:43:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
04/25/2022 05:43:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 05:43:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 05:43:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
04/25/2022 05:43:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
04/25/2022 05:43:30 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.3125 on epoch=874
04/25/2022 05:43:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
04/25/2022 05:43:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
04/25/2022 05:43:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 05:43:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 05:43:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
04/25/2022 05:43:55 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.3125 on epoch=899
04/25/2022 05:43:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 05:44:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
04/25/2022 05:44:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 05:44:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
04/25/2022 05:44:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 05:44:19 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.34375 on epoch=924
04/25/2022 05:44:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 05:44:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 05:44:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 05:44:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
04/25/2022 05:44:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 05:44:45 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.3125 on epoch=949
04/25/2022 05:44:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 05:44:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
04/25/2022 05:44:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 05:45:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
04/25/2022 05:45:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
04/25/2022 05:45:10 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.25 on epoch=974
04/25/2022 05:45:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
04/25/2022 05:45:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 05:45:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/25/2022 05:45:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 05:45:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 05:45:34 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:45:34 - INFO - __main__ - Printing 3 examples
04/25/2022 05:45:34 - INFO - __main__ -  [dream] What are some of the benefits that the company provides? [SEP] Kelly: So, have you found a job yet? Josh: No, but, I have a few leads, so things are looking up. Kelly: But isn't that what you always say? Josh: Well ... uh ... this time is different. Kelly: What are you looking for this time, then? Josh: Actually, I want to work for a Web hosting company. Kelly: What would you do there? Josh: Well, in a nut shell, Web hosting companies provide space for people to store and run their Websites. Does it sound like I know what I'm talking about? Kelly: Oh, yeah, sort of. Josh: Well, And then, sort of? Well, they allow people to run their Web sites without having to buy and maintain their own servers, and I'd like to work in technical support, you know, helping customers resolve computer-related problems with their sites. And you know I'm a good communicator. Kelly: So, how's the pay for that kind of job? Josh: Well, most people I know start out with a very reasonable salary; you can earn pay increases depending on your performance. Kelly: So, what about benefits? Josh: Oh, the benefits are pretty good. They provide health insurance, two weeks (of) paid vacation a year, and opportunities for advancement. And in the end, I'd like to work in a management position. You know, sitting back, enjoying the view out of the twentieth-story window of the office building. Something like that. Kelly: Well, is there any long-term job security in a job like that? Josh: Uhh. That's hard to tell. I mean, the Internet is booming, and these kinds of companies are sprouting up everywhere, which is a good thing, but just like the dot-com era, you never know how long things will last. Kelly: Well, have you ever thought about going back to school to improve your job skills? Josh: Wait, wait. What are you suggesting? Kelly: Well, you know, more training might help you land a better job. Josh: Wh ... wh ... Are you trying to say something about my current job? I mean, is there something going on here? I mean, what are you saying? Kelly: You know, you did drop out of college. Josh: I know, I know, but I don't know. I'm just seeing my current job at McDonalds as a step up. [McDonalds!]. Yeah, but, you know, I don't have the resources to go back to school at the moment; however, the job I am looking at will pay for some classes after I have been with the company for six months. Kelly: Well, it looks like you have things planned out this time. Josh: If I last that long. [SEP]  (A) insurance, paid vacation, and a company vehicle (B) paid vacation, opportunities for promotion, and insurance (C) opportunities for advancement, insurance, and a free bus pass
04/25/2022 05:45:34 - INFO - __main__ - ['paid vacation, opportunities for promotion, and insurance']
04/25/2022 05:45:34 - INFO - __main__ -  [dream] What is the probable result for her exam? [SEP] M: What did you think of the mid-term exam? W: I was expecting it to be easy, but at the end of the first hour, I was still on the first page. I hardly had time to get to the last question. [SEP]  (A) It's OK. (B) Poor. (C) Excellent.
04/25/2022 05:45:34 - INFO - __main__ - ['Poor.']
04/25/2022 05:45:34 - INFO - __main__ -  [dream] What kind of coffee would the woman like? [SEP] M: How would you like your coffee? W: Milky one without sugar, please. [SEP]  (A) Black coffee. (B) With milk and sugar. (C) With milk and no sugar
04/25/2022 05:45:34 - INFO - __main__ - ['With milk and no sugar']
04/25/2022 05:45:34 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:45:34 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:45:34 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 05:45:34 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:45:34 - INFO - __main__ - Printing 3 examples
04/25/2022 05:45:34 - INFO - __main__ -  [dream] Whose wedding dress do the speakers mention? [SEP] M: Catherine, you look great in these photos. W: Oh, thanks. I got into that wedding dress with great difficulty. It was too small. M: Why didn't you get one your size? W: Oh, it was my grandmother's - my mother wore it on her wedding day, and I really wanted to wear it on mine. I suffered for six months so I could wear it. M: How did you do it? W: Do you know this diet that many actors are doing? I don't think it's very healthy if you do it for a long time, but it really works. M: I think meat is not allowed in your diet. W: No, actually, you can eat things like, um, roast chicken, steak, and bacon, but you can't eat any bread and butter. M: Is it the diet that you love but have to avoid? W: That's right. Dieting is so boring. Hopefully, I won't need to wear the wedding dress again anyway. [SEP]  (A) The woman's. (B) The woman's mother's. (C) The woman's grandmother's.
04/25/2022 05:45:34 - INFO - __main__ - ["The woman's grandmother's."]
04/25/2022 05:45:34 - INFO - __main__ -  [dream] What did the woman do in the caf today? [SEP] M: Is that a new computer? W: Yeah. My grandpa gave it to me as a graduation gift. Isn't it nice? M: It is. I can't believe it's so thin! What kind of computer is it? W: It's an Apple MacBook Air. It's very lightweight and has a bunch of built-in apps. I couldn't be happier with it. M: Do they have any other colors besides that one? W: I don't know. I like this color. I've ordered a pink cover for it, but it won't arrive for a couple of weeks. M: Does the computer have a disk drive? W: No, it doesn't, but I don't need one. All of my documents are online. M: Well, you're very lucky. It will be great to have this when you start looking for jobs. W: I already have. Today, I spent the day at the caf working on my rsum. This computer has great battery life. M: Well, that's good. But the caf near here is always so crowded. W: I know. But I like to work in busy cafs. The noise reminds me that everyone else is working hard, so I work hard, too. [SEP]  (A) She talked with a friend. (B) She worked on her rsum. (C) She looked for a job there.
04/25/2022 05:45:34 - INFO - __main__ - ['She worked on her rsum.']
04/25/2022 05:45:34 - INFO - __main__ -  [dream] Why is William Shakespeare mentioned in the conversation? [SEP] W: I enjoy going through a secondhand bookstore, don't you? It's interesting to see what people used to enjoy reading. Did you see this old book of children's stories? M: Some of these books aren't so old, though... See? This mystery was published only six years ago. It cost seventy-five cents. You can't beat that. W: Hey! Look at this! M: What? Are you getting interested in nineteenth century poetry all of a sudden? W: No. Look at the inscription! Someone gave this book as a present, and wrote a note on the inside of the front cover. It's dated 1893. Maybe it's worth something. M: Everything on that shelf is worth fifty cents. W: But if this is a signature of someone who is well known, it might bring a lot more. I hear William Shakespeare's signature is worth about a million dollars. [SEP]  (A) He gave gifts to millions of people. (B) He was a very wealthy man in his times. (C) His signature is worth a lot of money.
04/25/2022 05:45:34 - INFO - __main__ - ['His signature is worth a lot of money.']
04/25/2022 05:45:34 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:45:34 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:45:34 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 05:45:35 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.28125 on epoch=999
04/25/2022 05:45:35 - INFO - __main__ - save last model!
04/25/2022 05:45:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 05:45:35 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 05:45:35 - INFO - __main__ - Printing 3 examples
04/25/2022 05:45:35 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 05:45:35 - INFO - __main__ - ['To change her job.']
04/25/2022 05:45:35 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 05:45:35 - INFO - __main__ - ['Watch TV.']
04/25/2022 05:45:35 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 05:45:35 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 05:45:35 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:45:35 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:45:36 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 05:45:52 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 05:45:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 05:45:53 - INFO - __main__ - Starting training!
04/25/2022 05:46:58 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_21_0.5_8_predictions.txt
04/25/2022 05:46:58 - INFO - __main__ - ACC on test data: 0.3590
04/25/2022 05:46:59 - INFO - __main__ - prefix=dream_32_21, lr=0.5, bsz=8, dev_performance=0.375, test_performance=0.359
04/25/2022 05:46:59 - INFO - __main__ - Running ... prefix=dream_32_21, lr=0.4, bsz=8 ...
04/25/2022 05:47:00 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:47:00 - INFO - __main__ - Printing 3 examples
04/25/2022 05:47:00 - INFO - __main__ -  [dream] What are some of the benefits that the company provides? [SEP] Kelly: So, have you found a job yet? Josh: No, but, I have a few leads, so things are looking up. Kelly: But isn't that what you always say? Josh: Well ... uh ... this time is different. Kelly: What are you looking for this time, then? Josh: Actually, I want to work for a Web hosting company. Kelly: What would you do there? Josh: Well, in a nut shell, Web hosting companies provide space for people to store and run their Websites. Does it sound like I know what I'm talking about? Kelly: Oh, yeah, sort of. Josh: Well, And then, sort of? Well, they allow people to run their Web sites without having to buy and maintain their own servers, and I'd like to work in technical support, you know, helping customers resolve computer-related problems with their sites. And you know I'm a good communicator. Kelly: So, how's the pay for that kind of job? Josh: Well, most people I know start out with a very reasonable salary; you can earn pay increases depending on your performance. Kelly: So, what about benefits? Josh: Oh, the benefits are pretty good. They provide health insurance, two weeks (of) paid vacation a year, and opportunities for advancement. And in the end, I'd like to work in a management position. You know, sitting back, enjoying the view out of the twentieth-story window of the office building. Something like that. Kelly: Well, is there any long-term job security in a job like that? Josh: Uhh. That's hard to tell. I mean, the Internet is booming, and these kinds of companies are sprouting up everywhere, which is a good thing, but just like the dot-com era, you never know how long things will last. Kelly: Well, have you ever thought about going back to school to improve your job skills? Josh: Wait, wait. What are you suggesting? Kelly: Well, you know, more training might help you land a better job. Josh: Wh ... wh ... Are you trying to say something about my current job? I mean, is there something going on here? I mean, what are you saying? Kelly: You know, you did drop out of college. Josh: I know, I know, but I don't know. I'm just seeing my current job at McDonalds as a step up. [McDonalds!]. Yeah, but, you know, I don't have the resources to go back to school at the moment; however, the job I am looking at will pay for some classes after I have been with the company for six months. Kelly: Well, it looks like you have things planned out this time. Josh: If I last that long. [SEP]  (A) insurance, paid vacation, and a company vehicle (B) paid vacation, opportunities for promotion, and insurance (C) opportunities for advancement, insurance, and a free bus pass
04/25/2022 05:47:00 - INFO - __main__ - ['paid vacation, opportunities for promotion, and insurance']
04/25/2022 05:47:00 - INFO - __main__ -  [dream] What is the probable result for her exam? [SEP] M: What did you think of the mid-term exam? W: I was expecting it to be easy, but at the end of the first hour, I was still on the first page. I hardly had time to get to the last question. [SEP]  (A) It's OK. (B) Poor. (C) Excellent.
04/25/2022 05:47:00 - INFO - __main__ - ['Poor.']
04/25/2022 05:47:00 - INFO - __main__ -  [dream] What kind of coffee would the woman like? [SEP] M: How would you like your coffee? W: Milky one without sugar, please. [SEP]  (A) Black coffee. (B) With milk and sugar. (C) With milk and no sugar
04/25/2022 05:47:00 - INFO - __main__ - ['With milk and no sugar']
04/25/2022 05:47:00 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:47:00 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:47:00 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 05:47:00 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 05:47:00 - INFO - __main__ - Printing 3 examples
04/25/2022 05:47:00 - INFO - __main__ -  [dream] Whose wedding dress do the speakers mention? [SEP] M: Catherine, you look great in these photos. W: Oh, thanks. I got into that wedding dress with great difficulty. It was too small. M: Why didn't you get one your size? W: Oh, it was my grandmother's - my mother wore it on her wedding day, and I really wanted to wear it on mine. I suffered for six months so I could wear it. M: How did you do it? W: Do you know this diet that many actors are doing? I don't think it's very healthy if you do it for a long time, but it really works. M: I think meat is not allowed in your diet. W: No, actually, you can eat things like, um, roast chicken, steak, and bacon, but you can't eat any bread and butter. M: Is it the diet that you love but have to avoid? W: That's right. Dieting is so boring. Hopefully, I won't need to wear the wedding dress again anyway. [SEP]  (A) The woman's. (B) The woman's mother's. (C) The woman's grandmother's.
04/25/2022 05:47:00 - INFO - __main__ - ["The woman's grandmother's."]
04/25/2022 05:47:00 - INFO - __main__ -  [dream] What did the woman do in the caf today? [SEP] M: Is that a new computer? W: Yeah. My grandpa gave it to me as a graduation gift. Isn't it nice? M: It is. I can't believe it's so thin! What kind of computer is it? W: It's an Apple MacBook Air. It's very lightweight and has a bunch of built-in apps. I couldn't be happier with it. M: Do they have any other colors besides that one? W: I don't know. I like this color. I've ordered a pink cover for it, but it won't arrive for a couple of weeks. M: Does the computer have a disk drive? W: No, it doesn't, but I don't need one. All of my documents are online. M: Well, you're very lucky. It will be great to have this when you start looking for jobs. W: I already have. Today, I spent the day at the caf working on my rsum. This computer has great battery life. M: Well, that's good. But the caf near here is always so crowded. W: I know. But I like to work in busy cafs. The noise reminds me that everyone else is working hard, so I work hard, too. [SEP]  (A) She talked with a friend. (B) She worked on her rsum. (C) She looked for a job there.
04/25/2022 05:47:00 - INFO - __main__ - ['She worked on her rsum.']
04/25/2022 05:47:00 - INFO - __main__ -  [dream] Why is William Shakespeare mentioned in the conversation? [SEP] W: I enjoy going through a secondhand bookstore, don't you? It's interesting to see what people used to enjoy reading. Did you see this old book of children's stories? M: Some of these books aren't so old, though... See? This mystery was published only six years ago. It cost seventy-five cents. You can't beat that. W: Hey! Look at this! M: What? Are you getting interested in nineteenth century poetry all of a sudden? W: No. Look at the inscription! Someone gave this book as a present, and wrote a note on the inside of the front cover. It's dated 1893. Maybe it's worth something. M: Everything on that shelf is worth fifty cents. W: But if this is a signature of someone who is well known, it might bring a lot more. I hear William Shakespeare's signature is worth about a million dollars. [SEP]  (A) He gave gifts to millions of people. (B) He was a very wealthy man in his times. (C) His signature is worth a lot of money.
04/25/2022 05:47:00 - INFO - __main__ - ['His signature is worth a lot of money.']
04/25/2022 05:47:00 - INFO - __main__ - Tokenizing Input ...
04/25/2022 05:47:00 - INFO - __main__ - Tokenizing Output ...
04/25/2022 05:47:00 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 05:47:18 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 05:47:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 05:47:19 - INFO - __main__ - Starting training!
04/25/2022 05:47:25 - INFO - __main__ - Step 10 Global step 10 Train loss 1.29 on epoch=4
04/25/2022 05:47:29 - INFO - __main__ - Step 20 Global step 20 Train loss 0.93 on epoch=9
04/25/2022 05:47:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.77 on epoch=14
04/25/2022 05:47:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=19
04/25/2022 05:47:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.29 on epoch=24
04/25/2022 05:47:45 - INFO - __main__ - Global step 50 Train loss 0.75 ACC 0.34375 on epoch=24
04/25/2022 05:47:45 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.34375 on epoch=24, global_step=50
04/25/2022 05:47:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.29 on epoch=29
04/25/2022 05:47:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.28 on epoch=34
04/25/2022 05:47:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.23 on epoch=39
04/25/2022 05:48:03 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=44
04/25/2022 05:48:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=49
04/25/2022 05:48:10 - INFO - __main__ - Global step 100 Train loss 0.25 ACC 0.4375 on epoch=49
04/25/2022 05:48:10 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.4375 on epoch=49, global_step=100
04/25/2022 05:48:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.21 on epoch=54
04/25/2022 05:48:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.13 on epoch=59
04/25/2022 05:48:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.18 on epoch=64
04/25/2022 05:48:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.15 on epoch=69
04/25/2022 05:48:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.15 on epoch=74
04/25/2022 05:48:35 - INFO - __main__ - Global step 150 Train loss 0.16 ACC 0.375 on epoch=74
04/25/2022 05:48:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.15 on epoch=79
04/25/2022 05:48:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.16 on epoch=84
04/25/2022 05:48:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.13 on epoch=89
04/25/2022 05:48:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.14 on epoch=94
04/25/2022 05:48:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.13 on epoch=99
04/25/2022 05:48:59 - INFO - __main__ - Global step 200 Train loss 0.14 ACC 0.28125 on epoch=99
04/25/2022 05:49:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.10 on epoch=104
04/25/2022 05:49:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.13 on epoch=109
04/25/2022 05:49:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.12 on epoch=114
04/25/2022 05:49:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.11 on epoch=119
04/25/2022 05:49:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.09 on epoch=124
04/25/2022 05:49:24 - INFO - __main__ - Global step 250 Train loss 0.11 ACC 0.21875 on epoch=124
04/25/2022 05:49:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.09 on epoch=129
04/25/2022 05:49:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.09 on epoch=134
04/25/2022 05:49:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.08 on epoch=139
04/25/2022 05:49:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.08 on epoch=144
04/25/2022 05:49:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.06 on epoch=149
04/25/2022 05:49:49 - INFO - __main__ - Global step 300 Train loss 0.08 ACC 0.34375 on epoch=149
04/25/2022 05:49:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.08 on epoch=154
04/25/2022 05:49:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.10 on epoch=159
04/25/2022 05:50:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.07 on epoch=164
04/25/2022 05:50:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.07 on epoch=169
04/25/2022 05:50:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.05 on epoch=174
04/25/2022 05:50:14 - INFO - __main__ - Global step 350 Train loss 0.08 ACC 0.25 on epoch=174
04/25/2022 05:50:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.04 on epoch=179
04/25/2022 05:50:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.05 on epoch=184
04/25/2022 05:50:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.07 on epoch=189
04/25/2022 05:50:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.08 on epoch=194
04/25/2022 05:50:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.03 on epoch=199
04/25/2022 05:50:39 - INFO - __main__ - Global step 400 Train loss 0.05 ACC 0.28125 on epoch=199
04/25/2022 05:50:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.07 on epoch=204
04/25/2022 05:50:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.05 on epoch=209
04/25/2022 05:50:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.09 on epoch=214
04/25/2022 05:50:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.06 on epoch=219
04/25/2022 05:51:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.05 on epoch=224
04/25/2022 05:51:04 - INFO - __main__ - Global step 450 Train loss 0.06 ACC 0.25 on epoch=224
04/25/2022 05:51:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
04/25/2022 05:51:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.04 on epoch=234
04/25/2022 05:51:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.04 on epoch=239
04/25/2022 05:51:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.03 on epoch=244
04/25/2022 05:51:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=249
04/25/2022 05:51:28 - INFO - __main__ - Global step 500 Train loss 0.05 ACC 0.34375 on epoch=249
04/25/2022 05:51:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.03 on epoch=254
04/25/2022 05:51:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
04/25/2022 05:51:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.03 on epoch=264
04/25/2022 05:51:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
04/25/2022 05:51:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
04/25/2022 05:51:53 - INFO - __main__ - Global step 550 Train loss 0.03 ACC 0.28125 on epoch=274
04/25/2022 05:51:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
04/25/2022 05:52:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.04 on epoch=284
04/25/2022 05:52:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=289
04/25/2022 05:52:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
04/25/2022 05:52:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
04/25/2022 05:52:18 - INFO - __main__ - Global step 600 Train loss 0.03 ACC 0.3125 on epoch=299
04/25/2022 05:52:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
04/25/2022 05:52:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
04/25/2022 05:52:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
04/25/2022 05:52:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
04/25/2022 05:52:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
04/25/2022 05:52:43 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.34375 on epoch=324
04/25/2022 05:52:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
04/25/2022 05:52:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
04/25/2022 05:52:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
04/25/2022 05:53:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
04/25/2022 05:53:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
04/25/2022 05:53:07 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.3125 on epoch=349
04/25/2022 05:53:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
04/25/2022 05:53:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
04/25/2022 05:53:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
04/25/2022 05:53:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
04/25/2022 05:53:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
04/25/2022 05:53:32 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.28125 on epoch=374
04/25/2022 05:53:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
04/25/2022 05:53:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
04/25/2022 05:53:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
04/25/2022 05:53:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
04/25/2022 05:53:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
04/25/2022 05:53:57 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.3125 on epoch=399
04/25/2022 05:54:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
04/25/2022 05:54:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
04/25/2022 05:54:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
04/25/2022 05:54:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
04/25/2022 05:54:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
04/25/2022 05:54:22 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.3125 on epoch=424
04/25/2022 05:54:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
04/25/2022 05:54:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
04/25/2022 05:54:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
04/25/2022 05:54:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
04/25/2022 05:54:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
04/25/2022 05:54:46 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.25 on epoch=449
04/25/2022 05:54:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
04/25/2022 05:54:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
04/25/2022 05:55:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
04/25/2022 05:55:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
04/25/2022 05:55:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
04/25/2022 05:55:12 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.1875 on epoch=474
04/25/2022 05:55:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/25/2022 05:55:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
04/25/2022 05:55:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
04/25/2022 05:55:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
04/25/2022 05:55:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
04/25/2022 05:55:37 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.25 on epoch=499
04/25/2022 05:55:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
04/25/2022 05:55:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
04/25/2022 05:55:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
04/25/2022 05:55:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
04/25/2022 05:55:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
04/25/2022 05:56:02 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.28125 on epoch=524
04/25/2022 05:56:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
04/25/2022 05:56:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
04/25/2022 05:56:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
04/25/2022 05:56:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
04/25/2022 05:56:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/25/2022 05:56:27 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.3125 on epoch=549
04/25/2022 05:56:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
04/25/2022 05:56:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/25/2022 05:56:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
04/25/2022 05:56:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
04/25/2022 05:56:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 05:56:53 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.25 on epoch=574
04/25/2022 05:56:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
04/25/2022 05:57:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
04/25/2022 05:57:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
04/25/2022 05:57:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
04/25/2022 05:57:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
04/25/2022 05:57:18 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.21875 on epoch=599
04/25/2022 05:57:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
04/25/2022 05:57:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 05:57:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
04/25/2022 05:57:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
04/25/2022 05:57:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
04/25/2022 05:57:42 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.21875 on epoch=624
04/25/2022 05:57:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 05:57:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 05:57:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/25/2022 05:58:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/25/2022 05:58:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
04/25/2022 05:58:07 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.25 on epoch=649
04/25/2022 05:58:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
04/25/2022 05:58:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 05:58:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/25/2022 05:58:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
04/25/2022 05:58:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 05:58:32 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.21875 on epoch=674
04/25/2022 05:58:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
04/25/2022 05:58:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
04/25/2022 05:58:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/25/2022 05:58:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 05:58:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 05:58:57 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.25 on epoch=699
04/25/2022 05:59:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 05:59:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 05:59:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/25/2022 05:59:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/25/2022 05:59:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
04/25/2022 05:59:22 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.21875 on epoch=724
04/25/2022 05:59:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 05:59:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/25/2022 05:59:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 05:59:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 05:59:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
04/25/2022 05:59:47 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.25 on epoch=749
04/25/2022 05:59:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 05:59:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 06:00:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 06:00:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
04/25/2022 06:00:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/25/2022 06:00:12 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.21875 on epoch=774
04/25/2022 06:00:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
04/25/2022 06:00:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
04/25/2022 06:00:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
04/25/2022 06:00:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
04/25/2022 06:00:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
04/25/2022 06:00:36 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.25 on epoch=799
04/25/2022 06:00:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 06:00:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 06:00:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 06:00:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
04/25/2022 06:00:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 06:01:01 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.21875 on epoch=824
04/25/2022 06:01:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
04/25/2022 06:01:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
04/25/2022 06:01:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 06:01:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 06:01:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
04/25/2022 06:01:26 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.28125 on epoch=849
04/25/2022 06:01:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
04/25/2022 06:01:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
04/25/2022 06:01:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
04/25/2022 06:01:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
04/25/2022 06:01:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 06:01:51 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.25 on epoch=874
04/25/2022 06:01:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/25/2022 06:02:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 06:02:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
04/25/2022 06:02:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 06:02:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
04/25/2022 06:02:16 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.25 on epoch=899
04/25/2022 06:02:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
04/25/2022 06:02:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/25/2022 06:02:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 06:02:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 06:02:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 06:02:41 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.21875 on epoch=924
04/25/2022 06:02:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
04/25/2022 06:02:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
04/25/2022 06:02:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 06:02:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
04/25/2022 06:03:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
04/25/2022 06:03:06 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.21875 on epoch=949
04/25/2022 06:03:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
04/25/2022 06:03:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
04/25/2022 06:03:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
04/25/2022 06:03:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 06:03:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 06:03:30 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.21875 on epoch=974
04/25/2022 06:03:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 06:03:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 06:03:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
04/25/2022 06:03:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
04/25/2022 06:03:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
04/25/2022 06:03:54 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:03:54 - INFO - __main__ - Printing 3 examples
04/25/2022 06:03:54 - INFO - __main__ -  [dream] What are some of the benefits that the company provides? [SEP] Kelly: So, have you found a job yet? Josh: No, but, I have a few leads, so things are looking up. Kelly: But isn't that what you always say? Josh: Well ... uh ... this time is different. Kelly: What are you looking for this time, then? Josh: Actually, I want to work for a Web hosting company. Kelly: What would you do there? Josh: Well, in a nut shell, Web hosting companies provide space for people to store and run their Websites. Does it sound like I know what I'm talking about? Kelly: Oh, yeah, sort of. Josh: Well, And then, sort of? Well, they allow people to run their Web sites without having to buy and maintain their own servers, and I'd like to work in technical support, you know, helping customers resolve computer-related problems with their sites. And you know I'm a good communicator. Kelly: So, how's the pay for that kind of job? Josh: Well, most people I know start out with a very reasonable salary; you can earn pay increases depending on your performance. Kelly: So, what about benefits? Josh: Oh, the benefits are pretty good. They provide health insurance, two weeks (of) paid vacation a year, and opportunities for advancement. And in the end, I'd like to work in a management position. You know, sitting back, enjoying the view out of the twentieth-story window of the office building. Something like that. Kelly: Well, is there any long-term job security in a job like that? Josh: Uhh. That's hard to tell. I mean, the Internet is booming, and these kinds of companies are sprouting up everywhere, which is a good thing, but just like the dot-com era, you never know how long things will last. Kelly: Well, have you ever thought about going back to school to improve your job skills? Josh: Wait, wait. What are you suggesting? Kelly: Well, you know, more training might help you land a better job. Josh: Wh ... wh ... Are you trying to say something about my current job? I mean, is there something going on here? I mean, what are you saying? Kelly: You know, you did drop out of college. Josh: I know, I know, but I don't know. I'm just seeing my current job at McDonalds as a step up. [McDonalds!]. Yeah, but, you know, I don't have the resources to go back to school at the moment; however, the job I am looking at will pay for some classes after I have been with the company for six months. Kelly: Well, it looks like you have things planned out this time. Josh: If I last that long. [SEP]  (A) insurance, paid vacation, and a company vehicle (B) paid vacation, opportunities for promotion, and insurance (C) opportunities for advancement, insurance, and a free bus pass
04/25/2022 06:03:54 - INFO - __main__ - ['paid vacation, opportunities for promotion, and insurance']
04/25/2022 06:03:54 - INFO - __main__ -  [dream] What is the probable result for her exam? [SEP] M: What did you think of the mid-term exam? W: I was expecting it to be easy, but at the end of the first hour, I was still on the first page. I hardly had time to get to the last question. [SEP]  (A) It's OK. (B) Poor. (C) Excellent.
04/25/2022 06:03:54 - INFO - __main__ - ['Poor.']
04/25/2022 06:03:54 - INFO - __main__ -  [dream] What kind of coffee would the woman like? [SEP] M: How would you like your coffee? W: Milky one without sugar, please. [SEP]  (A) Black coffee. (B) With milk and sugar. (C) With milk and no sugar
04/25/2022 06:03:54 - INFO - __main__ - ['With milk and no sugar']
04/25/2022 06:03:54 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:03:54 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:03:54 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 06:03:54 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:03:54 - INFO - __main__ - Printing 3 examples
04/25/2022 06:03:54 - INFO - __main__ -  [dream] Whose wedding dress do the speakers mention? [SEP] M: Catherine, you look great in these photos. W: Oh, thanks. I got into that wedding dress with great difficulty. It was too small. M: Why didn't you get one your size? W: Oh, it was my grandmother's - my mother wore it on her wedding day, and I really wanted to wear it on mine. I suffered for six months so I could wear it. M: How did you do it? W: Do you know this diet that many actors are doing? I don't think it's very healthy if you do it for a long time, but it really works. M: I think meat is not allowed in your diet. W: No, actually, you can eat things like, um, roast chicken, steak, and bacon, but you can't eat any bread and butter. M: Is it the diet that you love but have to avoid? W: That's right. Dieting is so boring. Hopefully, I won't need to wear the wedding dress again anyway. [SEP]  (A) The woman's. (B) The woman's mother's. (C) The woman's grandmother's.
04/25/2022 06:03:54 - INFO - __main__ - ["The woman's grandmother's."]
04/25/2022 06:03:54 - INFO - __main__ -  [dream] What did the woman do in the caf today? [SEP] M: Is that a new computer? W: Yeah. My grandpa gave it to me as a graduation gift. Isn't it nice? M: It is. I can't believe it's so thin! What kind of computer is it? W: It's an Apple MacBook Air. It's very lightweight and has a bunch of built-in apps. I couldn't be happier with it. M: Do they have any other colors besides that one? W: I don't know. I like this color. I've ordered a pink cover for it, but it won't arrive for a couple of weeks. M: Does the computer have a disk drive? W: No, it doesn't, but I don't need one. All of my documents are online. M: Well, you're very lucky. It will be great to have this when you start looking for jobs. W: I already have. Today, I spent the day at the caf working on my rsum. This computer has great battery life. M: Well, that's good. But the caf near here is always so crowded. W: I know. But I like to work in busy cafs. The noise reminds me that everyone else is working hard, so I work hard, too. [SEP]  (A) She talked with a friend. (B) She worked on her rsum. (C) She looked for a job there.
04/25/2022 06:03:54 - INFO - __main__ - ['She worked on her rsum.']
04/25/2022 06:03:54 - INFO - __main__ -  [dream] Why is William Shakespeare mentioned in the conversation? [SEP] W: I enjoy going through a secondhand bookstore, don't you? It's interesting to see what people used to enjoy reading. Did you see this old book of children's stories? M: Some of these books aren't so old, though... See? This mystery was published only six years ago. It cost seventy-five cents. You can't beat that. W: Hey! Look at this! M: What? Are you getting interested in nineteenth century poetry all of a sudden? W: No. Look at the inscription! Someone gave this book as a present, and wrote a note on the inside of the front cover. It's dated 1893. Maybe it's worth something. M: Everything on that shelf is worth fifty cents. W: But if this is a signature of someone who is well known, it might bring a lot more. I hear William Shakespeare's signature is worth about a million dollars. [SEP]  (A) He gave gifts to millions of people. (B) He was a very wealthy man in his times. (C) His signature is worth a lot of money.
04/25/2022 06:03:54 - INFO - __main__ - ['His signature is worth a lot of money.']
04/25/2022 06:03:54 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:03:54 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:03:54 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 06:03:55 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.21875 on epoch=999
04/25/2022 06:03:55 - INFO - __main__ - save last model!
04/25/2022 06:03:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 06:03:55 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 06:03:55 - INFO - __main__ - Printing 3 examples
04/25/2022 06:03:55 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 06:03:55 - INFO - __main__ - ['To change her job.']
04/25/2022 06:03:55 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 06:03:55 - INFO - __main__ - ['Watch TV.']
04/25/2022 06:03:55 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 06:03:55 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 06:03:55 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:03:56 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:03:57 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 06:04:13 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 06:04:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 06:04:14 - INFO - __main__ - Starting training!
04/25/2022 06:05:17 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_21_0.4_8_predictions.txt
04/25/2022 06:05:17 - INFO - __main__ - ACC on test data: 0.3380
04/25/2022 06:05:18 - INFO - __main__ - prefix=dream_32_21, lr=0.4, bsz=8, dev_performance=0.4375, test_performance=0.338
04/25/2022 06:05:18 - INFO - __main__ - Running ... prefix=dream_32_21, lr=0.3, bsz=8 ...
04/25/2022 06:05:19 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:05:19 - INFO - __main__ - Printing 3 examples
04/25/2022 06:05:19 - INFO - __main__ -  [dream] What are some of the benefits that the company provides? [SEP] Kelly: So, have you found a job yet? Josh: No, but, I have a few leads, so things are looking up. Kelly: But isn't that what you always say? Josh: Well ... uh ... this time is different. Kelly: What are you looking for this time, then? Josh: Actually, I want to work for a Web hosting company. Kelly: What would you do there? Josh: Well, in a nut shell, Web hosting companies provide space for people to store and run their Websites. Does it sound like I know what I'm talking about? Kelly: Oh, yeah, sort of. Josh: Well, And then, sort of? Well, they allow people to run their Web sites without having to buy and maintain their own servers, and I'd like to work in technical support, you know, helping customers resolve computer-related problems with their sites. And you know I'm a good communicator. Kelly: So, how's the pay for that kind of job? Josh: Well, most people I know start out with a very reasonable salary; you can earn pay increases depending on your performance. Kelly: So, what about benefits? Josh: Oh, the benefits are pretty good. They provide health insurance, two weeks (of) paid vacation a year, and opportunities for advancement. And in the end, I'd like to work in a management position. You know, sitting back, enjoying the view out of the twentieth-story window of the office building. Something like that. Kelly: Well, is there any long-term job security in a job like that? Josh: Uhh. That's hard to tell. I mean, the Internet is booming, and these kinds of companies are sprouting up everywhere, which is a good thing, but just like the dot-com era, you never know how long things will last. Kelly: Well, have you ever thought about going back to school to improve your job skills? Josh: Wait, wait. What are you suggesting? Kelly: Well, you know, more training might help you land a better job. Josh: Wh ... wh ... Are you trying to say something about my current job? I mean, is there something going on here? I mean, what are you saying? Kelly: You know, you did drop out of college. Josh: I know, I know, but I don't know. I'm just seeing my current job at McDonalds as a step up. [McDonalds!]. Yeah, but, you know, I don't have the resources to go back to school at the moment; however, the job I am looking at will pay for some classes after I have been with the company for six months. Kelly: Well, it looks like you have things planned out this time. Josh: If I last that long. [SEP]  (A) insurance, paid vacation, and a company vehicle (B) paid vacation, opportunities for promotion, and insurance (C) opportunities for advancement, insurance, and a free bus pass
04/25/2022 06:05:19 - INFO - __main__ - ['paid vacation, opportunities for promotion, and insurance']
04/25/2022 06:05:19 - INFO - __main__ -  [dream] What is the probable result for her exam? [SEP] M: What did you think of the mid-term exam? W: I was expecting it to be easy, but at the end of the first hour, I was still on the first page. I hardly had time to get to the last question. [SEP]  (A) It's OK. (B) Poor. (C) Excellent.
04/25/2022 06:05:19 - INFO - __main__ - ['Poor.']
04/25/2022 06:05:19 - INFO - __main__ -  [dream] What kind of coffee would the woman like? [SEP] M: How would you like your coffee? W: Milky one without sugar, please. [SEP]  (A) Black coffee. (B) With milk and sugar. (C) With milk and no sugar
04/25/2022 06:05:19 - INFO - __main__ - ['With milk and no sugar']
04/25/2022 06:05:19 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:05:19 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:05:19 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 06:05:19 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:05:19 - INFO - __main__ - Printing 3 examples
04/25/2022 06:05:19 - INFO - __main__ -  [dream] Whose wedding dress do the speakers mention? [SEP] M: Catherine, you look great in these photos. W: Oh, thanks. I got into that wedding dress with great difficulty. It was too small. M: Why didn't you get one your size? W: Oh, it was my grandmother's - my mother wore it on her wedding day, and I really wanted to wear it on mine. I suffered for six months so I could wear it. M: How did you do it? W: Do you know this diet that many actors are doing? I don't think it's very healthy if you do it for a long time, but it really works. M: I think meat is not allowed in your diet. W: No, actually, you can eat things like, um, roast chicken, steak, and bacon, but you can't eat any bread and butter. M: Is it the diet that you love but have to avoid? W: That's right. Dieting is so boring. Hopefully, I won't need to wear the wedding dress again anyway. [SEP]  (A) The woman's. (B) The woman's mother's. (C) The woman's grandmother's.
04/25/2022 06:05:19 - INFO - __main__ - ["The woman's grandmother's."]
04/25/2022 06:05:19 - INFO - __main__ -  [dream] What did the woman do in the caf today? [SEP] M: Is that a new computer? W: Yeah. My grandpa gave it to me as a graduation gift. Isn't it nice? M: It is. I can't believe it's so thin! What kind of computer is it? W: It's an Apple MacBook Air. It's very lightweight and has a bunch of built-in apps. I couldn't be happier with it. M: Do they have any other colors besides that one? W: I don't know. I like this color. I've ordered a pink cover for it, but it won't arrive for a couple of weeks. M: Does the computer have a disk drive? W: No, it doesn't, but I don't need one. All of my documents are online. M: Well, you're very lucky. It will be great to have this when you start looking for jobs. W: I already have. Today, I spent the day at the caf working on my rsum. This computer has great battery life. M: Well, that's good. But the caf near here is always so crowded. W: I know. But I like to work in busy cafs. The noise reminds me that everyone else is working hard, so I work hard, too. [SEP]  (A) She talked with a friend. (B) She worked on her rsum. (C) She looked for a job there.
04/25/2022 06:05:19 - INFO - __main__ - ['She worked on her rsum.']
04/25/2022 06:05:19 - INFO - __main__ -  [dream] Why is William Shakespeare mentioned in the conversation? [SEP] W: I enjoy going through a secondhand bookstore, don't you? It's interesting to see what people used to enjoy reading. Did you see this old book of children's stories? M: Some of these books aren't so old, though... See? This mystery was published only six years ago. It cost seventy-five cents. You can't beat that. W: Hey! Look at this! M: What? Are you getting interested in nineteenth century poetry all of a sudden? W: No. Look at the inscription! Someone gave this book as a present, and wrote a note on the inside of the front cover. It's dated 1893. Maybe it's worth something. M: Everything on that shelf is worth fifty cents. W: But if this is a signature of someone who is well known, it might bring a lot more. I hear William Shakespeare's signature is worth about a million dollars. [SEP]  (A) He gave gifts to millions of people. (B) He was a very wealthy man in his times. (C) His signature is worth a lot of money.
04/25/2022 06:05:19 - INFO - __main__ - ['His signature is worth a lot of money.']
04/25/2022 06:05:19 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:05:19 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:05:19 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 06:05:34 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 06:05:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 06:05:35 - INFO - __main__ - Starting training!
04/25/2022 06:05:40 - INFO - __main__ - Step 10 Global step 10 Train loss 1.27 on epoch=4
04/25/2022 06:05:44 - INFO - __main__ - Step 20 Global step 20 Train loss 1.01 on epoch=9
04/25/2022 06:05:49 - INFO - __main__ - Step 30 Global step 30 Train loss 0.91 on epoch=14
04/25/2022 06:05:53 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=19
04/25/2022 06:05:58 - INFO - __main__ - Step 50 Global step 50 Train loss 0.39 on epoch=24
04/25/2022 06:06:00 - INFO - __main__ - Global step 50 Train loss 0.83 ACC 0.3125 on epoch=24
04/25/2022 06:06:00 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.3125 on epoch=24, global_step=50
04/25/2022 06:06:05 - INFO - __main__ - Step 60 Global step 60 Train loss 0.33 on epoch=29
04/25/2022 06:06:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.27 on epoch=34
04/25/2022 06:06:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.23 on epoch=39
04/25/2022 06:06:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.23 on epoch=44
04/25/2022 06:06:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=49
04/25/2022 06:06:25 - INFO - __main__ - Global step 100 Train loss 0.26 ACC 0.28125 on epoch=49
04/25/2022 06:06:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.19 on epoch=54
04/25/2022 06:06:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.22 on epoch=59
04/25/2022 06:06:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.19 on epoch=64
04/25/2022 06:06:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.17 on epoch=69
04/25/2022 06:06:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.20 on epoch=74
04/25/2022 06:06:49 - INFO - __main__ - Global step 150 Train loss 0.19 ACC 0.3125 on epoch=74
04/25/2022 06:06:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.21 on epoch=79
04/25/2022 06:06:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.18 on epoch=84
04/25/2022 06:07:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
04/25/2022 06:07:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.15 on epoch=94
04/25/2022 06:07:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.15 on epoch=99
04/25/2022 06:07:14 - INFO - __main__ - Global step 200 Train loss 0.18 ACC 0.40625 on epoch=99
04/25/2022 06:07:14 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.40625 on epoch=99, global_step=200
04/25/2022 06:07:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.10 on epoch=104
04/25/2022 06:07:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.14 on epoch=109
04/25/2022 06:07:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.10 on epoch=114
04/25/2022 06:07:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.12 on epoch=119
04/25/2022 06:07:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.09 on epoch=124
04/25/2022 06:07:38 - INFO - __main__ - Global step 250 Train loss 0.11 ACC 0.3125 on epoch=124
04/25/2022 06:07:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.11 on epoch=129
04/25/2022 06:07:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.13 on epoch=134
04/25/2022 06:07:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.10 on epoch=139
04/25/2022 06:07:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.09 on epoch=144
04/25/2022 06:08:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.09 on epoch=149
04/25/2022 06:08:03 - INFO - __main__ - Global step 300 Train loss 0.10 ACC 0.3125 on epoch=149
04/25/2022 06:08:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.09 on epoch=154
04/25/2022 06:08:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.11 on epoch=159
04/25/2022 06:08:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.09 on epoch=164
04/25/2022 06:08:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.07 on epoch=169
04/25/2022 06:08:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.09 on epoch=174
04/25/2022 06:08:27 - INFO - __main__ - Global step 350 Train loss 0.09 ACC 0.34375 on epoch=174
04/25/2022 06:08:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.07 on epoch=179
04/25/2022 06:08:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.10 on epoch=184
04/25/2022 06:08:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.08 on epoch=189
04/25/2022 06:08:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.08 on epoch=194
04/25/2022 06:08:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.07 on epoch=199
04/25/2022 06:08:52 - INFO - __main__ - Global step 400 Train loss 0.08 ACC 0.34375 on epoch=199
04/25/2022 06:08:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.07 on epoch=204
04/25/2022 06:09:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
04/25/2022 06:09:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
04/25/2022 06:09:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.07 on epoch=219
04/25/2022 06:09:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.06 on epoch=224
04/25/2022 06:09:17 - INFO - __main__ - Global step 450 Train loss 0.07 ACC 0.375 on epoch=224
04/25/2022 06:09:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
04/25/2022 06:09:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
04/25/2022 06:09:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.06 on epoch=239
04/25/2022 06:09:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
04/25/2022 06:09:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
04/25/2022 06:09:41 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.34375 on epoch=249
04/25/2022 06:09:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
04/25/2022 06:09:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=259
04/25/2022 06:09:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.04 on epoch=264
04/25/2022 06:09:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.04 on epoch=269
04/25/2022 06:10:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
04/25/2022 06:10:05 - INFO - __main__ - Global step 550 Train loss 0.05 ACC 0.34375 on epoch=274
04/25/2022 06:10:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=279
04/25/2022 06:10:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.04 on epoch=284
04/25/2022 06:10:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
04/25/2022 06:10:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=294
04/25/2022 06:10:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
04/25/2022 06:10:30 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.375 on epoch=299
04/25/2022 06:10:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
04/25/2022 06:10:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=309
04/25/2022 06:10:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
04/25/2022 06:10:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
04/25/2022 06:10:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
04/25/2022 06:10:54 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.28125 on epoch=324
04/25/2022 06:10:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
04/25/2022 06:11:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
04/25/2022 06:11:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
04/25/2022 06:11:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=344
04/25/2022 06:11:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
04/25/2022 06:11:19 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.375 on epoch=349
04/25/2022 06:11:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
04/25/2022 06:11:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=359
04/25/2022 06:11:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=364
04/25/2022 06:11:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
04/25/2022 06:11:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
04/25/2022 06:11:43 - INFO - __main__ - Global step 750 Train loss 0.05 ACC 0.40625 on epoch=374
04/25/2022 06:11:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
04/25/2022 06:11:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
04/25/2022 06:11:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
04/25/2022 06:12:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
04/25/2022 06:12:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
04/25/2022 06:12:08 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.34375 on epoch=399
04/25/2022 06:12:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
04/25/2022 06:12:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
04/25/2022 06:12:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
04/25/2022 06:12:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
04/25/2022 06:12:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
04/25/2022 06:12:33 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.4375 on epoch=424
04/25/2022 06:12:33 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.4375 on epoch=424, global_step=850
04/25/2022 06:12:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
04/25/2022 06:12:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
04/25/2022 06:12:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
04/25/2022 06:12:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
04/25/2022 06:12:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
04/25/2022 06:12:57 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.375 on epoch=449
04/25/2022 06:13:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
04/25/2022 06:13:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
04/25/2022 06:13:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
04/25/2022 06:13:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
04/25/2022 06:13:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
04/25/2022 06:13:22 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.375 on epoch=474
04/25/2022 06:13:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/25/2022 06:13:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
04/25/2022 06:13:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
04/25/2022 06:13:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
04/25/2022 06:13:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
04/25/2022 06:13:46 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.34375 on epoch=499
04/25/2022 06:13:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
04/25/2022 06:13:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
04/25/2022 06:13:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
04/25/2022 06:14:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
04/25/2022 06:14:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
04/25/2022 06:14:11 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.375 on epoch=524
04/25/2022 06:14:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
04/25/2022 06:14:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
04/25/2022 06:14:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/25/2022 06:14:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
04/25/2022 06:14:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
04/25/2022 06:14:35 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.3125 on epoch=549
04/25/2022 06:14:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
04/25/2022 06:14:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/25/2022 06:14:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/25/2022 06:14:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
04/25/2022 06:14:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
04/25/2022 06:15:00 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.3125 on epoch=574
04/25/2022 06:15:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/25/2022 06:15:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
04/25/2022 06:15:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/25/2022 06:15:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 06:15:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/25/2022 06:15:24 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.21875 on epoch=599
04/25/2022 06:15:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/25/2022 06:15:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 06:15:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 06:15:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
04/25/2022 06:15:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
04/25/2022 06:15:49 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.40625 on epoch=624
04/25/2022 06:15:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
04/25/2022 06:15:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 06:16:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
04/25/2022 06:16:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/25/2022 06:16:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/25/2022 06:16:13 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.3125 on epoch=649
04/25/2022 06:16:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
04/25/2022 06:16:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 06:16:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/25/2022 06:16:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/25/2022 06:16:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 06:16:38 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.25 on epoch=674
04/25/2022 06:16:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 06:16:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 06:16:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/25/2022 06:16:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
04/25/2022 06:17:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 06:17:02 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.34375 on epoch=699
04/25/2022 06:17:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
04/25/2022 06:17:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 06:17:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 06:17:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 06:17:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/25/2022 06:17:27 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.34375 on epoch=724
04/25/2022 06:17:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/25/2022 06:17:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/25/2022 06:17:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 06:17:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 06:17:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 06:17:51 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.40625 on epoch=749
04/25/2022 06:17:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 06:18:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
04/25/2022 06:18:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 06:18:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 06:18:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/25/2022 06:18:16 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.40625 on epoch=774
04/25/2022 06:18:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 06:18:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 06:18:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 06:18:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 06:18:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/25/2022 06:18:41 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.3125 on epoch=799
04/25/2022 06:18:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
04/25/2022 06:18:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/25/2022 06:18:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/25/2022 06:18:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 06:19:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
04/25/2022 06:19:05 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.3125 on epoch=824
04/25/2022 06:19:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/25/2022 06:19:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 06:19:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 06:19:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 06:19:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
04/25/2022 06:19:29 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.3125 on epoch=849
04/25/2022 06:19:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
04/25/2022 06:19:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
04/25/2022 06:19:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 06:19:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 06:19:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 06:19:54 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.28125 on epoch=874
04/25/2022 06:19:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/25/2022 06:20:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
04/25/2022 06:20:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
04/25/2022 06:20:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 06:20:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 06:20:18 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.3125 on epoch=899
04/25/2022 06:20:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
04/25/2022 06:20:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
04/25/2022 06:20:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
04/25/2022 06:20:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
04/25/2022 06:20:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 06:20:43 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.28125 on epoch=924
04/25/2022 06:20:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 06:20:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 06:20:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 06:21:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 06:21:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 06:21:07 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.3125 on epoch=949
04/25/2022 06:21:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 06:21:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 06:21:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 06:21:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 06:21:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
04/25/2022 06:21:32 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.25 on epoch=974
04/25/2022 06:21:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 06:21:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 06:21:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
04/25/2022 06:21:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 06:21:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
04/25/2022 06:21:56 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:21:56 - INFO - __main__ - Printing 3 examples
04/25/2022 06:21:56 - INFO - __main__ -  [dream] What are some of the benefits that the company provides? [SEP] Kelly: So, have you found a job yet? Josh: No, but, I have a few leads, so things are looking up. Kelly: But isn't that what you always say? Josh: Well ... uh ... this time is different. Kelly: What are you looking for this time, then? Josh: Actually, I want to work for a Web hosting company. Kelly: What would you do there? Josh: Well, in a nut shell, Web hosting companies provide space for people to store and run their Websites. Does it sound like I know what I'm talking about? Kelly: Oh, yeah, sort of. Josh: Well, And then, sort of? Well, they allow people to run their Web sites without having to buy and maintain their own servers, and I'd like to work in technical support, you know, helping customers resolve computer-related problems with their sites. And you know I'm a good communicator. Kelly: So, how's the pay for that kind of job? Josh: Well, most people I know start out with a very reasonable salary; you can earn pay increases depending on your performance. Kelly: So, what about benefits? Josh: Oh, the benefits are pretty good. They provide health insurance, two weeks (of) paid vacation a year, and opportunities for advancement. And in the end, I'd like to work in a management position. You know, sitting back, enjoying the view out of the twentieth-story window of the office building. Something like that. Kelly: Well, is there any long-term job security in a job like that? Josh: Uhh. That's hard to tell. I mean, the Internet is booming, and these kinds of companies are sprouting up everywhere, which is a good thing, but just like the dot-com era, you never know how long things will last. Kelly: Well, have you ever thought about going back to school to improve your job skills? Josh: Wait, wait. What are you suggesting? Kelly: Well, you know, more training might help you land a better job. Josh: Wh ... wh ... Are you trying to say something about my current job? I mean, is there something going on here? I mean, what are you saying? Kelly: You know, you did drop out of college. Josh: I know, I know, but I don't know. I'm just seeing my current job at McDonalds as a step up. [McDonalds!]. Yeah, but, you know, I don't have the resources to go back to school at the moment; however, the job I am looking at will pay for some classes after I have been with the company for six months. Kelly: Well, it looks like you have things planned out this time. Josh: If I last that long. [SEP]  (A) insurance, paid vacation, and a company vehicle (B) paid vacation, opportunities for promotion, and insurance (C) opportunities for advancement, insurance, and a free bus pass
04/25/2022 06:21:56 - INFO - __main__ - ['paid vacation, opportunities for promotion, and insurance']
04/25/2022 06:21:56 - INFO - __main__ -  [dream] What is the probable result for her exam? [SEP] M: What did you think of the mid-term exam? W: I was expecting it to be easy, but at the end of the first hour, I was still on the first page. I hardly had time to get to the last question. [SEP]  (A) It's OK. (B) Poor. (C) Excellent.
04/25/2022 06:21:56 - INFO - __main__ - ['Poor.']
04/25/2022 06:21:56 - INFO - __main__ -  [dream] What kind of coffee would the woman like? [SEP] M: How would you like your coffee? W: Milky one without sugar, please. [SEP]  (A) Black coffee. (B) With milk and sugar. (C) With milk and no sugar
04/25/2022 06:21:56 - INFO - __main__ - ['With milk and no sugar']
04/25/2022 06:21:56 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:21:56 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:21:56 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 06:21:56 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:21:56 - INFO - __main__ - Printing 3 examples
04/25/2022 06:21:56 - INFO - __main__ -  [dream] Whose wedding dress do the speakers mention? [SEP] M: Catherine, you look great in these photos. W: Oh, thanks. I got into that wedding dress with great difficulty. It was too small. M: Why didn't you get one your size? W: Oh, it was my grandmother's - my mother wore it on her wedding day, and I really wanted to wear it on mine. I suffered for six months so I could wear it. M: How did you do it? W: Do you know this diet that many actors are doing? I don't think it's very healthy if you do it for a long time, but it really works. M: I think meat is not allowed in your diet. W: No, actually, you can eat things like, um, roast chicken, steak, and bacon, but you can't eat any bread and butter. M: Is it the diet that you love but have to avoid? W: That's right. Dieting is so boring. Hopefully, I won't need to wear the wedding dress again anyway. [SEP]  (A) The woman's. (B) The woman's mother's. (C) The woman's grandmother's.
04/25/2022 06:21:56 - INFO - __main__ - ["The woman's grandmother's."]
04/25/2022 06:21:56 - INFO - __main__ -  [dream] What did the woman do in the caf today? [SEP] M: Is that a new computer? W: Yeah. My grandpa gave it to me as a graduation gift. Isn't it nice? M: It is. I can't believe it's so thin! What kind of computer is it? W: It's an Apple MacBook Air. It's very lightweight and has a bunch of built-in apps. I couldn't be happier with it. M: Do they have any other colors besides that one? W: I don't know. I like this color. I've ordered a pink cover for it, but it won't arrive for a couple of weeks. M: Does the computer have a disk drive? W: No, it doesn't, but I don't need one. All of my documents are online. M: Well, you're very lucky. It will be great to have this when you start looking for jobs. W: I already have. Today, I spent the day at the caf working on my rsum. This computer has great battery life. M: Well, that's good. But the caf near here is always so crowded. W: I know. But I like to work in busy cafs. The noise reminds me that everyone else is working hard, so I work hard, too. [SEP]  (A) She talked with a friend. (B) She worked on her rsum. (C) She looked for a job there.
04/25/2022 06:21:56 - INFO - __main__ - ['She worked on her rsum.']
04/25/2022 06:21:56 - INFO - __main__ -  [dream] Why is William Shakespeare mentioned in the conversation? [SEP] W: I enjoy going through a secondhand bookstore, don't you? It's interesting to see what people used to enjoy reading. Did you see this old book of children's stories? M: Some of these books aren't so old, though... See? This mystery was published only six years ago. It cost seventy-five cents. You can't beat that. W: Hey! Look at this! M: What? Are you getting interested in nineteenth century poetry all of a sudden? W: No. Look at the inscription! Someone gave this book as a present, and wrote a note on the inside of the front cover. It's dated 1893. Maybe it's worth something. M: Everything on that shelf is worth fifty cents. W: But if this is a signature of someone who is well known, it might bring a lot more. I hear William Shakespeare's signature is worth about a million dollars. [SEP]  (A) He gave gifts to millions of people. (B) He was a very wealthy man in his times. (C) His signature is worth a lot of money.
04/25/2022 06:21:56 - INFO - __main__ - ['His signature is worth a lot of money.']
04/25/2022 06:21:56 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:21:56 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:21:56 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 06:21:56 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.21875 on epoch=999
04/25/2022 06:21:56 - INFO - __main__ - save last model!
04/25/2022 06:21:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 06:21:56 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 06:21:56 - INFO - __main__ - Printing 3 examples
04/25/2022 06:21:56 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 06:21:56 - INFO - __main__ - ['To change her job.']
04/25/2022 06:21:56 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 06:21:56 - INFO - __main__ - ['Watch TV.']
04/25/2022 06:21:56 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 06:21:56 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 06:21:56 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:21:57 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:21:58 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 06:22:14 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 06:22:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 06:22:15 - INFO - __main__ - Starting training!
04/25/2022 06:23:15 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_21_0.3_8_predictions.txt
04/25/2022 06:23:15 - INFO - __main__ - ACC on test data: 0.3230
04/25/2022 06:23:16 - INFO - __main__ - prefix=dream_32_21, lr=0.3, bsz=8, dev_performance=0.4375, test_performance=0.323
04/25/2022 06:23:16 - INFO - __main__ - Running ... prefix=dream_32_21, lr=0.2, bsz=8 ...
04/25/2022 06:23:17 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:23:17 - INFO - __main__ - Printing 3 examples
04/25/2022 06:23:17 - INFO - __main__ -  [dream] What are some of the benefits that the company provides? [SEP] Kelly: So, have you found a job yet? Josh: No, but, I have a few leads, so things are looking up. Kelly: But isn't that what you always say? Josh: Well ... uh ... this time is different. Kelly: What are you looking for this time, then? Josh: Actually, I want to work for a Web hosting company. Kelly: What would you do there? Josh: Well, in a nut shell, Web hosting companies provide space for people to store and run their Websites. Does it sound like I know what I'm talking about? Kelly: Oh, yeah, sort of. Josh: Well, And then, sort of? Well, they allow people to run their Web sites without having to buy and maintain their own servers, and I'd like to work in technical support, you know, helping customers resolve computer-related problems with their sites. And you know I'm a good communicator. Kelly: So, how's the pay for that kind of job? Josh: Well, most people I know start out with a very reasonable salary; you can earn pay increases depending on your performance. Kelly: So, what about benefits? Josh: Oh, the benefits are pretty good. They provide health insurance, two weeks (of) paid vacation a year, and opportunities for advancement. And in the end, I'd like to work in a management position. You know, sitting back, enjoying the view out of the twentieth-story window of the office building. Something like that. Kelly: Well, is there any long-term job security in a job like that? Josh: Uhh. That's hard to tell. I mean, the Internet is booming, and these kinds of companies are sprouting up everywhere, which is a good thing, but just like the dot-com era, you never know how long things will last. Kelly: Well, have you ever thought about going back to school to improve your job skills? Josh: Wait, wait. What are you suggesting? Kelly: Well, you know, more training might help you land a better job. Josh: Wh ... wh ... Are you trying to say something about my current job? I mean, is there something going on here? I mean, what are you saying? Kelly: You know, you did drop out of college. Josh: I know, I know, but I don't know. I'm just seeing my current job at McDonalds as a step up. [McDonalds!]. Yeah, but, you know, I don't have the resources to go back to school at the moment; however, the job I am looking at will pay for some classes after I have been with the company for six months. Kelly: Well, it looks like you have things planned out this time. Josh: If I last that long. [SEP]  (A) insurance, paid vacation, and a company vehicle (B) paid vacation, opportunities for promotion, and insurance (C) opportunities for advancement, insurance, and a free bus pass
04/25/2022 06:23:17 - INFO - __main__ - ['paid vacation, opportunities for promotion, and insurance']
04/25/2022 06:23:17 - INFO - __main__ -  [dream] What is the probable result for her exam? [SEP] M: What did you think of the mid-term exam? W: I was expecting it to be easy, but at the end of the first hour, I was still on the first page. I hardly had time to get to the last question. [SEP]  (A) It's OK. (B) Poor. (C) Excellent.
04/25/2022 06:23:17 - INFO - __main__ - ['Poor.']
04/25/2022 06:23:17 - INFO - __main__ -  [dream] What kind of coffee would the woman like? [SEP] M: How would you like your coffee? W: Milky one without sugar, please. [SEP]  (A) Black coffee. (B) With milk and sugar. (C) With milk and no sugar
04/25/2022 06:23:17 - INFO - __main__ - ['With milk and no sugar']
04/25/2022 06:23:17 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:23:17 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:23:17 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 06:23:17 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:23:17 - INFO - __main__ - Printing 3 examples
04/25/2022 06:23:17 - INFO - __main__ -  [dream] Whose wedding dress do the speakers mention? [SEP] M: Catherine, you look great in these photos. W: Oh, thanks. I got into that wedding dress with great difficulty. It was too small. M: Why didn't you get one your size? W: Oh, it was my grandmother's - my mother wore it on her wedding day, and I really wanted to wear it on mine. I suffered for six months so I could wear it. M: How did you do it? W: Do you know this diet that many actors are doing? I don't think it's very healthy if you do it for a long time, but it really works. M: I think meat is not allowed in your diet. W: No, actually, you can eat things like, um, roast chicken, steak, and bacon, but you can't eat any bread and butter. M: Is it the diet that you love but have to avoid? W: That's right. Dieting is so boring. Hopefully, I won't need to wear the wedding dress again anyway. [SEP]  (A) The woman's. (B) The woman's mother's. (C) The woman's grandmother's.
04/25/2022 06:23:17 - INFO - __main__ - ["The woman's grandmother's."]
04/25/2022 06:23:17 - INFO - __main__ -  [dream] What did the woman do in the caf today? [SEP] M: Is that a new computer? W: Yeah. My grandpa gave it to me as a graduation gift. Isn't it nice? M: It is. I can't believe it's so thin! What kind of computer is it? W: It's an Apple MacBook Air. It's very lightweight and has a bunch of built-in apps. I couldn't be happier with it. M: Do they have any other colors besides that one? W: I don't know. I like this color. I've ordered a pink cover for it, but it won't arrive for a couple of weeks. M: Does the computer have a disk drive? W: No, it doesn't, but I don't need one. All of my documents are online. M: Well, you're very lucky. It will be great to have this when you start looking for jobs. W: I already have. Today, I spent the day at the caf working on my rsum. This computer has great battery life. M: Well, that's good. But the caf near here is always so crowded. W: I know. But I like to work in busy cafs. The noise reminds me that everyone else is working hard, so I work hard, too. [SEP]  (A) She talked with a friend. (B) She worked on her rsum. (C) She looked for a job there.
04/25/2022 06:23:17 - INFO - __main__ - ['She worked on her rsum.']
04/25/2022 06:23:17 - INFO - __main__ -  [dream] Why is William Shakespeare mentioned in the conversation? [SEP] W: I enjoy going through a secondhand bookstore, don't you? It's interesting to see what people used to enjoy reading. Did you see this old book of children's stories? M: Some of these books aren't so old, though... See? This mystery was published only six years ago. It cost seventy-five cents. You can't beat that. W: Hey! Look at this! M: What? Are you getting interested in nineteenth century poetry all of a sudden? W: No. Look at the inscription! Someone gave this book as a present, and wrote a note on the inside of the front cover. It's dated 1893. Maybe it's worth something. M: Everything on that shelf is worth fifty cents. W: But if this is a signature of someone who is well known, it might bring a lot more. I hear William Shakespeare's signature is worth about a million dollars. [SEP]  (A) He gave gifts to millions of people. (B) He was a very wealthy man in his times. (C) His signature is worth a lot of money.
04/25/2022 06:23:17 - INFO - __main__ - ['His signature is worth a lot of money.']
04/25/2022 06:23:17 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:23:17 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:23:17 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 06:23:36 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 06:23:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 06:23:37 - INFO - __main__ - Starting training!
04/25/2022 06:23:42 - INFO - __main__ - Step 10 Global step 10 Train loss 1.38 on epoch=4
04/25/2022 06:23:46 - INFO - __main__ - Step 20 Global step 20 Train loss 1.15 on epoch=9
04/25/2022 06:23:51 - INFO - __main__ - Step 30 Global step 30 Train loss 1.01 on epoch=14
04/25/2022 06:23:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.83 on epoch=19
04/25/2022 06:24:00 - INFO - __main__ - Step 50 Global step 50 Train loss 0.69 on epoch=24
04/25/2022 06:24:03 - INFO - __main__ - Global step 50 Train loss 1.01 ACC 0.1875 on epoch=24
04/25/2022 06:24:03 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
04/25/2022 06:24:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.52 on epoch=29
04/25/2022 06:24:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
04/25/2022 06:24:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.36 on epoch=39
04/25/2022 06:24:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=44
04/25/2022 06:24:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.27 on epoch=49
04/25/2022 06:24:28 - INFO - __main__ - Global step 100 Train loss 0.37 ACC 0.34375 on epoch=49
04/25/2022 06:24:28 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.34375 on epoch=49, global_step=100
04/25/2022 06:24:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.24 on epoch=54
04/25/2022 06:24:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.24 on epoch=59
04/25/2022 06:24:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.22 on epoch=64
04/25/2022 06:24:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.22 on epoch=69
04/25/2022 06:24:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.20 on epoch=74
04/25/2022 06:24:52 - INFO - __main__ - Global step 150 Train loss 0.22 ACC 0.3125 on epoch=74
04/25/2022 06:24:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.19 on epoch=79
04/25/2022 06:25:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.19 on epoch=84
04/25/2022 06:25:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
04/25/2022 06:25:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.17 on epoch=94
04/25/2022 06:25:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.15 on epoch=99
04/25/2022 06:25:17 - INFO - __main__ - Global step 200 Train loss 0.18 ACC 0.46875 on epoch=99
04/25/2022 06:25:17 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.46875 on epoch=99, global_step=200
04/25/2022 06:25:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.13 on epoch=104
04/25/2022 06:25:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.15 on epoch=109
04/25/2022 06:25:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.14 on epoch=114
04/25/2022 06:25:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.16 on epoch=119
04/25/2022 06:25:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.14 on epoch=124
04/25/2022 06:25:42 - INFO - __main__ - Global step 250 Train loss 0.14 ACC 0.375 on epoch=124
04/25/2022 06:25:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.16 on epoch=129
04/25/2022 06:25:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
04/25/2022 06:25:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.11 on epoch=139
04/25/2022 06:26:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.12 on epoch=144
04/25/2022 06:26:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.15 on epoch=149
04/25/2022 06:26:07 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.28125 on epoch=149
04/25/2022 06:26:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.09 on epoch=154
04/25/2022 06:26:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.11 on epoch=159
04/25/2022 06:26:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
04/25/2022 06:26:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.15 on epoch=169
04/25/2022 06:26:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.14 on epoch=174
04/25/2022 06:26:32 - INFO - __main__ - Global step 350 Train loss 0.12 ACC 0.3125 on epoch=174
04/25/2022 06:26:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
04/25/2022 06:26:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.09 on epoch=184
04/25/2022 06:26:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.13 on epoch=189
04/25/2022 06:26:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.10 on epoch=194
04/25/2022 06:26:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.09 on epoch=199
04/25/2022 06:26:56 - INFO - __main__ - Global step 400 Train loss 0.11 ACC 0.25 on epoch=199
04/25/2022 06:27:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
04/25/2022 06:27:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.10 on epoch=209
04/25/2022 06:27:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.09 on epoch=214
04/25/2022 06:27:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.08 on epoch=219
04/25/2022 06:27:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
04/25/2022 06:27:21 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.3125 on epoch=224
04/25/2022 06:27:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
04/25/2022 06:27:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
04/25/2022 06:27:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.06 on epoch=239
04/25/2022 06:27:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
04/25/2022 06:27:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
04/25/2022 06:27:46 - INFO - __main__ - Global step 500 Train loss 0.07 ACC 0.21875 on epoch=249
04/25/2022 06:27:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
04/25/2022 06:27:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
04/25/2022 06:27:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
04/25/2022 06:28:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=269
04/25/2022 06:28:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.05 on epoch=274
04/25/2022 06:28:11 - INFO - __main__ - Global step 550 Train loss 0.07 ACC 0.25 on epoch=274
04/25/2022 06:28:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=279
04/25/2022 06:28:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
04/25/2022 06:28:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
04/25/2022 06:28:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=294
04/25/2022 06:28:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
04/25/2022 06:28:36 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.25 on epoch=299
04/25/2022 06:28:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
04/25/2022 06:28:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=309
04/25/2022 06:28:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
04/25/2022 06:28:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=319
04/25/2022 06:28:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=324
04/25/2022 06:29:00 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.25 on epoch=324
04/25/2022 06:29:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
04/25/2022 06:29:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
04/25/2022 06:29:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
04/25/2022 06:29:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
04/25/2022 06:29:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
04/25/2022 06:29:25 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.28125 on epoch=349
04/25/2022 06:29:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
04/25/2022 06:29:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
04/25/2022 06:29:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
04/25/2022 06:29:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
04/25/2022 06:29:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
04/25/2022 06:29:50 - INFO - __main__ - Global step 750 Train loss 0.05 ACC 0.28125 on epoch=374
04/25/2022 06:29:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=379
04/25/2022 06:29:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
04/25/2022 06:30:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
04/25/2022 06:30:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
04/25/2022 06:30:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
04/25/2022 06:30:14 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.25 on epoch=399
04/25/2022 06:30:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
04/25/2022 06:30:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
04/25/2022 06:30:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
04/25/2022 06:30:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=419
04/25/2022 06:30:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
04/25/2022 06:30:39 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.25 on epoch=424
04/25/2022 06:30:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
04/25/2022 06:30:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
04/25/2022 06:30:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=439
04/25/2022 06:30:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
04/25/2022 06:31:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
04/25/2022 06:31:04 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.25 on epoch=449
04/25/2022 06:31:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
04/25/2022 06:31:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
04/25/2022 06:31:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
04/25/2022 06:31:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
04/25/2022 06:31:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
04/25/2022 06:31:29 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.21875 on epoch=474
04/25/2022 06:31:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
04/25/2022 06:31:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
04/25/2022 06:31:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
04/25/2022 06:31:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
04/25/2022 06:31:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
04/25/2022 06:31:54 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.25 on epoch=499
04/25/2022 06:31:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
04/25/2022 06:32:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
04/25/2022 06:32:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
04/25/2022 06:32:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
04/25/2022 06:32:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
04/25/2022 06:32:18 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.28125 on epoch=524
04/25/2022 06:32:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
04/25/2022 06:32:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
04/25/2022 06:32:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/25/2022 06:32:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
04/25/2022 06:32:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
04/25/2022 06:32:43 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.28125 on epoch=549
04/25/2022 06:32:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
04/25/2022 06:32:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
04/25/2022 06:32:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/25/2022 06:33:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/25/2022 06:33:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 06:33:08 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.28125 on epoch=574
04/25/2022 06:33:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/25/2022 06:33:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
04/25/2022 06:33:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/25/2022 06:33:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 06:33:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
04/25/2022 06:33:33 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.28125 on epoch=599
04/25/2022 06:33:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/25/2022 06:33:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
04/25/2022 06:33:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 06:33:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
04/25/2022 06:33:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=624
04/25/2022 06:33:57 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.28125 on epoch=624
04/25/2022 06:34:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
04/25/2022 06:34:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 06:34:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=639
04/25/2022 06:34:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/25/2022 06:34:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/25/2022 06:34:22 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.28125 on epoch=649
04/25/2022 06:34:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 06:34:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 06:34:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
04/25/2022 06:34:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/25/2022 06:34:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 06:34:47 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.3125 on epoch=674
04/25/2022 06:34:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/25/2022 06:34:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/25/2022 06:35:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/25/2022 06:35:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 06:35:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
04/25/2022 06:35:12 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.3125 on epoch=699
04/25/2022 06:35:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/25/2022 06:35:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
04/25/2022 06:35:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 06:35:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 06:35:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/25/2022 06:35:37 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.34375 on epoch=724
04/25/2022 06:35:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/25/2022 06:35:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/25/2022 06:35:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 06:35:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
04/25/2022 06:35:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 06:36:01 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.34375 on epoch=749
04/25/2022 06:36:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
04/25/2022 06:36:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 06:36:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 06:36:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
04/25/2022 06:36:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/25/2022 06:36:26 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.25 on epoch=774
04/25/2022 06:36:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
04/25/2022 06:36:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 06:36:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 06:36:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 06:36:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
04/25/2022 06:36:51 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.25 on epoch=799
04/25/2022 06:36:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 06:37:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/25/2022 06:37:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 06:37:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 06:37:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 06:37:15 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.25 on epoch=824
04/25/2022 06:37:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/25/2022 06:37:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 06:37:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 06:37:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/25/2022 06:37:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 06:37:40 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.3125 on epoch=849
04/25/2022 06:37:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 06:37:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 06:37:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 06:37:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 06:38:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 06:38:04 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.34375 on epoch=874
04/25/2022 06:38:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/25/2022 06:38:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
04/25/2022 06:38:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
04/25/2022 06:38:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 06:38:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 06:38:29 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.3125 on epoch=899
04/25/2022 06:38:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 06:38:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/25/2022 06:38:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
04/25/2022 06:38:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 06:38:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 06:38:53 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.3125 on epoch=924
04/25/2022 06:38:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
04/25/2022 06:39:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 06:39:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 06:39:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 06:39:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
04/25/2022 06:39:18 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.28125 on epoch=949
04/25/2022 06:39:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 06:39:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
04/25/2022 06:39:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 06:39:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 06:39:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 06:39:43 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.28125 on epoch=974
04/25/2022 06:39:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 06:39:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 06:39:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/25/2022 06:40:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
04/25/2022 06:40:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 06:40:06 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:40:06 - INFO - __main__ - Printing 3 examples
04/25/2022 06:40:06 - INFO - __main__ -  [dream] What does Terry say about the garbage in New York? [SEP] Frank: Hi, Terry, you're just back from New York. Terry: Yes. Frank: It must be an exciting trip. Terry: Well, I've really seen lots of things. I saw the most spectacular view when I was crossing a bridge to Manhattan at dusk, and the skyscrapers were lit up producing a classic nighttime view of Manhattan. Frank: That's really beautiful. Terry: But that's not the whole picture. Some of the streets in New York are very rough. I saw large piles of garbage cans at the roadside, and graffiti all over garage doors and store shutters. Frank: I can't believe it. Terry: The garbage are tidily bagged and boxed, though. Frank: Did you stay in a hotel? Terry: Yes. The hotel we stayed at turned out to be fairly decent, though the room was small, with a tiny bathroom that was only about 3 feet larger than the bed. As I was inexperienced with tourist-area hotels, I was startled, I mean, the room was $129 a night. But at least the room was clean and the bed reasonably comfortable. Frank: What's your general impression of New York? Terry: Well, restaurants pack their tiny tables very tightly; grocery stores and bookstores have aisles that are narrow; the sidewalks are cluttered with newsstands, vendors and their carts, and places that aren't restrictively small, such as the lawns around the Natural History Museum, are full of people, so they're no escape. [SEP]  (A) Smelly. (B) Scattered. (C) Put in bags and boxes.
04/25/2022 06:40:06 - INFO - __main__ - ['Put in bags and boxes.']
04/25/2022 06:40:06 - INFO - __main__ -  [dream] Which would the woman like to apply for next year? [SEP] M: Good morning, Lucy. Can I help you? W: Good morning, sir. I'd like to talk with you about my studies for a minute, if I may. M: Certainly, come in and have a seat. W: Thank you. I have a record of my studies for last year. Would you like to see it? M: Yes, let me see now. You are studying mathematics, aren't you?. W: Yes, I am. But I'd like to apply for admission to the engineering college next year. M: I see. Have you asked your parents for their advice about this? W: Yes, I have. They think it is a good idea. M: Well, your record here has been very good. I don't think you will have much trouble. W: I hope not. Anyway, I am going to apply. And I'd like to ask you to write a recommendation for me, if it is not too much trouble. M: No trouble at all. I'd be glad to do it. Is there anything else? W: No, sir. I think that is all. Thank you very much. M: All right, Lucy. Good luck to you. W: Thank you, Good-bye. [SEP]  (A) Economics. (B) Engineering. (C) Science.
04/25/2022 06:40:06 - INFO - __main__ - ['Engineering.']
04/25/2022 06:40:06 - INFO - __main__ -  [dream] At what time must the passenger arrive at the airport for Flight NH906? [SEP] M: Miss, what time is Flight NH906 for New York due to depart? W: It leaves at 3:45. but you must check in one hour ahead of departure. [SEP]  (A) 2:45. (B) 1:15. (C) 3:45.
04/25/2022 06:40:06 - INFO - __main__ - ['2:45.']
04/25/2022 06:40:06 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:40:06 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:40:07 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 06:40:07 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:40:07 - INFO - __main__ - Printing 3 examples
04/25/2022 06:40:07 - INFO - __main__ -  [dream] How did Jim get hue? [SEP] W: Did you get the injury playing football, Jim? M: Yes. One of the boys kicked my foot instead of the ball. W: The best thing for you to do is to rest your leg for 48 hours. M: When can I go back to school? W: You can go back in 4 days, but you'll need to avoid sports for a month. [SEP]  (A) He was kicked by a boy. (B) He was hit by a football. (C) He fell down on the ground.
04/25/2022 06:40:07 - INFO - __main__ - ['He was kicked by a boy.']
04/25/2022 06:40:07 - INFO - __main__ -  [dream] What is the monthly salary of the woman? [SEP] M: Do you mind telling me about your work in the factory? W: Certainly not. I'm a machine operator working in the assembly workshop. M: Oh, are you? Then you have a six-day week, don't you? W: Yes. And we work eight hours a day, but this includes a one -hour break. M: What about the huge machines used in your factory? I know you make heavy machines yourselves. But I wonder if you also use machines imported from abroad. W: Most of them are made in China, some even by our factory. Only a few were bought from other countries. M: But I see the machines turned out here are quite modem and efficient. W: Following the national policy of self - reliance and hard - working struggle, we've tried our best in the past few years to improve our equipment. However, there is still a lot more to do. M: Now the question about yourself. I hope you don't mind it. How much do you earn each month? W: My monthly wage is 3,500 RMB, not including the extra pay of about 500 for extra work I put in. M: That sounds reasonable indeed. I'm very interested in worker's welfare here. What other benefits do you workers get, I wonder? W: Like other factories in China, our factory offers free medical service for workers. [SEP]  (A) $3,500. (B) $500. (C) 3,500RMB.
04/25/2022 06:40:07 - INFO - __main__ - ['3,500RMB.']
04/25/2022 06:40:07 - INFO - __main__ -  [dream] What are Paula and her roommate planning to do? [SEP] M: Hello? W: Hello, Sam. This is Paula Hanson, Sorry to bother you, but I'm having a small problem I thought you might be able to help me with. M: Sure, Paula. What's up? W: Well, you know Sarah and I moved into an off-campus apartment in the fall,on the west side of town. Any way, we've been happy with it until the past couple of months. M: Yeah. What happened? W: Well,the dishwasher broken down,so we reported it to Ms. Corners,the owner. She said she'd take care of it, but a month went by and nothing happened. M: Did you get back in touch with her? W: I got a repairperson to give me an estimate,and then I sent it to her. When I didn't hear from her,I had the repair done. And I deducted the cost from the rent check. M: So what's the problem? W: She called here madly. She said she could have gotten the repair done for less money. Now, she's threatening to expel us for not paying the full rent. M: Hold on, Paula. It does sound pretty serious. But I'm sure you can all sit down and work this out. W: Well,you're over at the law school. So, I wondered if you would mind coming with Sarah and me when we go to talk to Ms. Corners. We're supposed to meet with her tomorrow night at eight. M: Sure. I haven't studied a lot about contracts yet,but I'll be glad to help you straighten things out. Why don't I stop by about 7:30? W: Thanks Sam. You are our lifesaver. [SEP]  (A) To talk to their landlord. (B) To buy a new dishwasher. (C) To break their contract.
04/25/2022 06:40:07 - INFO - __main__ - ['To talk to their landlord.']
04/25/2022 06:40:07 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:40:07 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:40:07 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 06:40:07 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.34375 on epoch=999
04/25/2022 06:40:07 - INFO - __main__ - save last model!
04/25/2022 06:40:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 06:40:07 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 06:40:07 - INFO - __main__ - Printing 3 examples
04/25/2022 06:40:07 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 06:40:07 - INFO - __main__ - ['To change her job.']
04/25/2022 06:40:07 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 06:40:07 - INFO - __main__ - ['Watch TV.']
04/25/2022 06:40:07 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 06:40:07 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 06:40:07 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:40:08 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:40:09 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 06:40:25 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 06:40:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 06:40:26 - INFO - __main__ - Starting training!
04/25/2022 06:41:29 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_21_0.2_8_predictions.txt
04/25/2022 06:41:29 - INFO - __main__ - ACC on test data: 0.3260
04/25/2022 06:41:29 - INFO - __main__ - prefix=dream_32_21, lr=0.2, bsz=8, dev_performance=0.46875, test_performance=0.326
04/25/2022 06:41:29 - INFO - __main__ - Running ... prefix=dream_32_42, lr=0.5, bsz=8 ...
04/25/2022 06:41:30 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:41:30 - INFO - __main__ - Printing 3 examples
04/25/2022 06:41:30 - INFO - __main__ -  [dream] What does Terry say about the garbage in New York? [SEP] Frank: Hi, Terry, you're just back from New York. Terry: Yes. Frank: It must be an exciting trip. Terry: Well, I've really seen lots of things. I saw the most spectacular view when I was crossing a bridge to Manhattan at dusk, and the skyscrapers were lit up producing a classic nighttime view of Manhattan. Frank: That's really beautiful. Terry: But that's not the whole picture. Some of the streets in New York are very rough. I saw large piles of garbage cans at the roadside, and graffiti all over garage doors and store shutters. Frank: I can't believe it. Terry: The garbage are tidily bagged and boxed, though. Frank: Did you stay in a hotel? Terry: Yes. The hotel we stayed at turned out to be fairly decent, though the room was small, with a tiny bathroom that was only about 3 feet larger than the bed. As I was inexperienced with tourist-area hotels, I was startled, I mean, the room was $129 a night. But at least the room was clean and the bed reasonably comfortable. Frank: What's your general impression of New York? Terry: Well, restaurants pack their tiny tables very tightly; grocery stores and bookstores have aisles that are narrow; the sidewalks are cluttered with newsstands, vendors and their carts, and places that aren't restrictively small, such as the lawns around the Natural History Museum, are full of people, so they're no escape. [SEP]  (A) Smelly. (B) Scattered. (C) Put in bags and boxes.
04/25/2022 06:41:30 - INFO - __main__ - ['Put in bags and boxes.']
04/25/2022 06:41:30 - INFO - __main__ -  [dream] Which would the woman like to apply for next year? [SEP] M: Good morning, Lucy. Can I help you? W: Good morning, sir. I'd like to talk with you about my studies for a minute, if I may. M: Certainly, come in and have a seat. W: Thank you. I have a record of my studies for last year. Would you like to see it? M: Yes, let me see now. You are studying mathematics, aren't you?. W: Yes, I am. But I'd like to apply for admission to the engineering college next year. M: I see. Have you asked your parents for their advice about this? W: Yes, I have. They think it is a good idea. M: Well, your record here has been very good. I don't think you will have much trouble. W: I hope not. Anyway, I am going to apply. And I'd like to ask you to write a recommendation for me, if it is not too much trouble. M: No trouble at all. I'd be glad to do it. Is there anything else? W: No, sir. I think that is all. Thank you very much. M: All right, Lucy. Good luck to you. W: Thank you, Good-bye. [SEP]  (A) Economics. (B) Engineering. (C) Science.
04/25/2022 06:41:30 - INFO - __main__ - ['Engineering.']
04/25/2022 06:41:30 - INFO - __main__ -  [dream] At what time must the passenger arrive at the airport for Flight NH906? [SEP] M: Miss, what time is Flight NH906 for New York due to depart? W: It leaves at 3:45. but you must check in one hour ahead of departure. [SEP]  (A) 2:45. (B) 1:15. (C) 3:45.
04/25/2022 06:41:30 - INFO - __main__ - ['2:45.']
04/25/2022 06:41:30 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:41:30 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:41:30 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 06:41:30 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:41:30 - INFO - __main__ - Printing 3 examples
04/25/2022 06:41:30 - INFO - __main__ -  [dream] How did Jim get hue? [SEP] W: Did you get the injury playing football, Jim? M: Yes. One of the boys kicked my foot instead of the ball. W: The best thing for you to do is to rest your leg for 48 hours. M: When can I go back to school? W: You can go back in 4 days, but you'll need to avoid sports for a month. [SEP]  (A) He was kicked by a boy. (B) He was hit by a football. (C) He fell down on the ground.
04/25/2022 06:41:30 - INFO - __main__ - ['He was kicked by a boy.']
04/25/2022 06:41:30 - INFO - __main__ -  [dream] What is the monthly salary of the woman? [SEP] M: Do you mind telling me about your work in the factory? W: Certainly not. I'm a machine operator working in the assembly workshop. M: Oh, are you? Then you have a six-day week, don't you? W: Yes. And we work eight hours a day, but this includes a one -hour break. M: What about the huge machines used in your factory? I know you make heavy machines yourselves. But I wonder if you also use machines imported from abroad. W: Most of them are made in China, some even by our factory. Only a few were bought from other countries. M: But I see the machines turned out here are quite modem and efficient. W: Following the national policy of self - reliance and hard - working struggle, we've tried our best in the past few years to improve our equipment. However, there is still a lot more to do. M: Now the question about yourself. I hope you don't mind it. How much do you earn each month? W: My monthly wage is 3,500 RMB, not including the extra pay of about 500 for extra work I put in. M: That sounds reasonable indeed. I'm very interested in worker's welfare here. What other benefits do you workers get, I wonder? W: Like other factories in China, our factory offers free medical service for workers. [SEP]  (A) $3,500. (B) $500. (C) 3,500RMB.
04/25/2022 06:41:30 - INFO - __main__ - ['3,500RMB.']
04/25/2022 06:41:30 - INFO - __main__ -  [dream] What are Paula and her roommate planning to do? [SEP] M: Hello? W: Hello, Sam. This is Paula Hanson, Sorry to bother you, but I'm having a small problem I thought you might be able to help me with. M: Sure, Paula. What's up? W: Well, you know Sarah and I moved into an off-campus apartment in the fall,on the west side of town. Any way, we've been happy with it until the past couple of months. M: Yeah. What happened? W: Well,the dishwasher broken down,so we reported it to Ms. Corners,the owner. She said she'd take care of it, but a month went by and nothing happened. M: Did you get back in touch with her? W: I got a repairperson to give me an estimate,and then I sent it to her. When I didn't hear from her,I had the repair done. And I deducted the cost from the rent check. M: So what's the problem? W: She called here madly. She said she could have gotten the repair done for less money. Now, she's threatening to expel us for not paying the full rent. M: Hold on, Paula. It does sound pretty serious. But I'm sure you can all sit down and work this out. W: Well,you're over at the law school. So, I wondered if you would mind coming with Sarah and me when we go to talk to Ms. Corners. We're supposed to meet with her tomorrow night at eight. M: Sure. I haven't studied a lot about contracts yet,but I'll be glad to help you straighten things out. Why don't I stop by about 7:30? W: Thanks Sam. You are our lifesaver. [SEP]  (A) To talk to their landlord. (B) To buy a new dishwasher. (C) To break their contract.
04/25/2022 06:41:30 - INFO - __main__ - ['To talk to their landlord.']
04/25/2022 06:41:30 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:41:30 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:41:31 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 06:41:49 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 06:41:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 06:41:50 - INFO - __main__ - Starting training!
04/25/2022 06:41:55 - INFO - __main__ - Step 10 Global step 10 Train loss 1.50 on epoch=4
04/25/2022 06:41:59 - INFO - __main__ - Step 20 Global step 20 Train loss 1.24 on epoch=9
04/25/2022 06:42:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.81 on epoch=14
04/25/2022 06:42:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.62 on epoch=19
04/25/2022 06:42:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=24
04/25/2022 06:42:16 - INFO - __main__ - Global step 50 Train loss 0.93 ACC 0.15625 on epoch=24
04/25/2022 06:42:16 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
04/25/2022 06:42:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=29
04/25/2022 06:42:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
04/25/2022 06:42:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.35 on epoch=39
04/25/2022 06:42:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=44
04/25/2022 06:42:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.35 on epoch=49
04/25/2022 06:42:41 - INFO - __main__ - Global step 100 Train loss 0.40 ACC 0.21875 on epoch=49
04/25/2022 06:42:41 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.21875 on epoch=49, global_step=100
04/25/2022 06:42:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=54
04/25/2022 06:42:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
04/25/2022 06:42:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=64
04/25/2022 06:42:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=69
04/25/2022 06:43:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.25 on epoch=74
04/25/2022 06:43:06 - INFO - __main__ - Global step 150 Train loss 0.28 ACC 0.15625 on epoch=74
04/25/2022 06:43:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=79
04/25/2022 06:43:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=84
04/25/2022 06:43:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.20 on epoch=89
04/25/2022 06:43:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=94
04/25/2022 06:43:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.19 on epoch=99
04/25/2022 06:43:31 - INFO - __main__ - Global step 200 Train loss 0.21 ACC 0.21875 on epoch=99
04/25/2022 06:43:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.15 on epoch=104
04/25/2022 06:43:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
04/25/2022 06:43:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.18 on epoch=114
04/25/2022 06:43:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.16 on epoch=119
04/25/2022 06:43:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.16 on epoch=124
04/25/2022 06:43:55 - INFO - __main__ - Global step 250 Train loss 0.17 ACC 0.21875 on epoch=124
04/25/2022 06:44:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.13 on epoch=129
04/25/2022 06:44:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.12 on epoch=134
04/25/2022 06:44:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.13 on epoch=139
04/25/2022 06:44:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.12 on epoch=144
04/25/2022 06:44:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.18 on epoch=149
04/25/2022 06:44:20 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.15625 on epoch=149
04/25/2022 06:44:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.19 on epoch=154
04/25/2022 06:44:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.10 on epoch=159
04/25/2022 06:44:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.09 on epoch=164
04/25/2022 06:44:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.11 on epoch=169
04/25/2022 06:44:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
04/25/2022 06:44:45 - INFO - __main__ - Global step 350 Train loss 0.12 ACC 0.21875 on epoch=174
04/25/2022 06:44:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.11 on epoch=179
04/25/2022 06:44:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
04/25/2022 06:44:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.10 on epoch=189
04/25/2022 06:45:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.09 on epoch=194
04/25/2022 06:45:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.16 on epoch=199
04/25/2022 06:45:09 - INFO - __main__ - Global step 400 Train loss 0.12 ACC 0.34375 on epoch=199
04/25/2022 06:45:09 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.34375 on epoch=199, global_step=400
04/25/2022 06:45:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
04/25/2022 06:45:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.10 on epoch=209
04/25/2022 06:45:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
04/25/2022 06:45:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=219
04/25/2022 06:45:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.06 on epoch=224
04/25/2022 06:45:34 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.28125 on epoch=224
04/25/2022 06:45:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
04/25/2022 06:45:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
04/25/2022 06:45:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
04/25/2022 06:45:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.06 on epoch=244
04/25/2022 06:45:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
04/25/2022 06:45:58 - INFO - __main__ - Global step 500 Train loss 0.07 ACC 0.34375 on epoch=249
04/25/2022 06:46:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
04/25/2022 06:46:07 - INFO - __main__ - Step 520 Global step 520 Train loss 1.17 on epoch=259
04/25/2022 06:46:11 - INFO - __main__ - Step 530 Global step 530 Train loss 1.70 on epoch=264
04/25/2022 06:46:15 - INFO - __main__ - Step 540 Global step 540 Train loss 2.09 on epoch=269
04/25/2022 06:46:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.90 on epoch=274
04/25/2022 06:46:22 - INFO - __main__ - Global step 550 Train loss 1.18 ACC 0.25 on epoch=274
04/25/2022 06:46:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=279
04/25/2022 06:46:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=284
04/25/2022 06:46:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=289
04/25/2022 06:46:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=294
04/25/2022 06:46:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
04/25/2022 06:46:47 - INFO - __main__ - Global step 600 Train loss 0.22 ACC 0.3125 on epoch=299
04/25/2022 06:46:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=304
04/25/2022 06:46:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=309
04/25/2022 06:47:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
04/25/2022 06:47:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
04/25/2022 06:47:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=324
04/25/2022 06:47:12 - INFO - __main__ - Global step 650 Train loss 0.15 ACC 0.25 on epoch=324
04/25/2022 06:47:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.10 on epoch=329
04/25/2022 06:47:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.17 on epoch=334
04/25/2022 06:47:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=339
04/25/2022 06:47:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=344
04/25/2022 06:47:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=349
04/25/2022 06:47:36 - INFO - __main__ - Global step 700 Train loss 0.13 ACC 0.25 on epoch=349
04/25/2022 06:47:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=354
04/25/2022 06:47:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=359
04/25/2022 06:47:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
04/25/2022 06:47:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=369
04/25/2022 06:47:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
04/25/2022 06:48:00 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.25 on epoch=374
04/25/2022 06:48:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=379
04/25/2022 06:48:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=384
04/25/2022 06:48:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=389
04/25/2022 06:48:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=394
04/25/2022 06:48:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=399
04/25/2022 06:48:25 - INFO - __main__ - Global step 800 Train loss 0.10 ACC 0.28125 on epoch=399
04/25/2022 06:48:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=404
04/25/2022 06:48:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=409
04/25/2022 06:48:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=414
04/25/2022 06:48:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=419
04/25/2022 06:48:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=424
04/25/2022 06:48:49 - INFO - __main__ - Global step 850 Train loss 0.14 ACC 0.3125 on epoch=424
04/25/2022 06:48:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=429
04/25/2022 06:48:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.13 on epoch=434
04/25/2022 06:49:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=439
04/25/2022 06:49:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=444
04/25/2022 06:49:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=449
04/25/2022 06:49:14 - INFO - __main__ - Global step 900 Train loss 0.12 ACC 0.28125 on epoch=449
04/25/2022 06:49:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=454
04/25/2022 06:49:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
04/25/2022 06:49:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=464
04/25/2022 06:49:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=469
04/25/2022 06:49:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=474
04/25/2022 06:49:38 - INFO - __main__ - Global step 950 Train loss 0.08 ACC 0.28125 on epoch=474
04/25/2022 06:49:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=479
04/25/2022 06:49:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
04/25/2022 06:49:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=489
04/25/2022 06:49:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=494
04/25/2022 06:50:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=499
04/25/2022 06:50:03 - INFO - __main__ - Global step 1000 Train loss 0.09 ACC 0.28125 on epoch=499
04/25/2022 06:50:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=504
04/25/2022 06:50:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
04/25/2022 06:50:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=514
04/25/2022 06:50:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
04/25/2022 06:50:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=524
04/25/2022 06:50:27 - INFO - __main__ - Global step 1050 Train loss 0.08 ACC 0.28125 on epoch=524
04/25/2022 06:50:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=529
04/25/2022 06:50:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=534
04/25/2022 06:50:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=539
04/25/2022 06:50:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
04/25/2022 06:50:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
04/25/2022 06:50:52 - INFO - __main__ - Global step 1100 Train loss 0.07 ACC 0.28125 on epoch=549
04/25/2022 06:50:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=554
04/25/2022 06:51:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=559
04/25/2022 06:51:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=564
04/25/2022 06:51:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=569
04/25/2022 06:51:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=574
04/25/2022 06:51:16 - INFO - __main__ - Global step 1150 Train loss 0.07 ACC 0.25 on epoch=574
04/25/2022 06:51:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
04/25/2022 06:51:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=584
04/25/2022 06:51:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=589
04/25/2022 06:51:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
04/25/2022 06:51:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
04/25/2022 06:51:41 - INFO - __main__ - Global step 1200 Train loss 0.06 ACC 0.25 on epoch=599
04/25/2022 06:51:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
04/25/2022 06:51:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
04/25/2022 06:51:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=614
04/25/2022 06:51:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=619
04/25/2022 06:52:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=624
04/25/2022 06:52:05 - INFO - __main__ - Global step 1250 Train loss 0.07 ACC 0.25 on epoch=624
04/25/2022 06:52:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
04/25/2022 06:52:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=634
04/25/2022 06:52:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=639
04/25/2022 06:52:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
04/25/2022 06:52:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=649
04/25/2022 06:52:29 - INFO - __main__ - Global step 1300 Train loss 0.06 ACC 0.25 on epoch=649
04/25/2022 06:52:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=654
04/25/2022 06:52:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=659
04/25/2022 06:52:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=664
04/25/2022 06:52:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=669
04/25/2022 06:52:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=674
04/25/2022 06:52:54 - INFO - __main__ - Global step 1350 Train loss 0.08 ACC 0.25 on epoch=674
04/25/2022 06:52:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=679
04/25/2022 06:53:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=684
04/25/2022 06:53:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=689
04/25/2022 06:53:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=694
04/25/2022 06:53:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=699
04/25/2022 06:53:18 - INFO - __main__ - Global step 1400 Train loss 0.07 ACC 0.25 on epoch=699
04/25/2022 06:53:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=704
04/25/2022 06:53:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=709
04/25/2022 06:53:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=714
04/25/2022 06:53:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=719
04/25/2022 06:53:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=724
04/25/2022 06:53:42 - INFO - __main__ - Global step 1450 Train loss 0.06 ACC 0.25 on epoch=724
04/25/2022 06:53:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=729
04/25/2022 06:53:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=734
04/25/2022 06:53:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=739
04/25/2022 06:54:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
04/25/2022 06:54:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
04/25/2022 06:54:07 - INFO - __main__ - Global step 1500 Train loss 0.06 ACC 0.25 on epoch=749
04/25/2022 06:54:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=754
04/25/2022 06:54:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=759
04/25/2022 06:54:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=764
04/25/2022 06:54:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=769
04/25/2022 06:54:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=774
04/25/2022 06:54:31 - INFO - __main__ - Global step 1550 Train loss 0.06 ACC 0.28125 on epoch=774
04/25/2022 06:54:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=779
04/25/2022 06:54:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
04/25/2022 06:54:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=789
04/25/2022 06:54:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=794
04/25/2022 06:54:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=799
04/25/2022 06:54:56 - INFO - __main__ - Global step 1600 Train loss 0.07 ACC 0.25 on epoch=799
04/25/2022 06:55:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=804
04/25/2022 06:55:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=809
04/25/2022 06:55:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=814
04/25/2022 06:55:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=819
04/25/2022 06:55:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=824
04/25/2022 06:55:20 - INFO - __main__ - Global step 1650 Train loss 0.06 ACC 0.25 on epoch=824
04/25/2022 06:55:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=829
04/25/2022 06:55:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=834
04/25/2022 06:55:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=839
04/25/2022 06:55:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=844
04/25/2022 06:55:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=849
04/25/2022 06:55:44 - INFO - __main__ - Global step 1700 Train loss 0.06 ACC 0.21875 on epoch=849
04/25/2022 06:55:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=854
04/25/2022 06:55:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=859
04/25/2022 06:55:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=864
04/25/2022 06:56:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=869
04/25/2022 06:56:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=874
04/25/2022 06:56:09 - INFO - __main__ - Global step 1750 Train loss 0.05 ACC 0.28125 on epoch=874
04/25/2022 06:56:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=879
04/25/2022 06:56:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
04/25/2022 06:56:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=889
04/25/2022 06:56:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=894
04/25/2022 06:56:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=899
04/25/2022 06:56:33 - INFO - __main__ - Global step 1800 Train loss 0.05 ACC 0.25 on epoch=899
04/25/2022 06:56:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=904
04/25/2022 06:56:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=909
04/25/2022 06:56:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=914
04/25/2022 06:56:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
04/25/2022 06:56:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=924
04/25/2022 06:56:57 - INFO - __main__ - Global step 1850 Train loss 0.05 ACC 0.25 on epoch=924
04/25/2022 06:57:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=929
04/25/2022 06:57:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=934
04/25/2022 06:57:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=939
04/25/2022 06:57:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
04/25/2022 06:57:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=949
04/25/2022 06:57:22 - INFO - __main__ - Global step 1900 Train loss 0.05 ACC 0.28125 on epoch=949
04/25/2022 06:57:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
04/25/2022 06:57:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
04/25/2022 06:57:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=964
04/25/2022 06:57:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=969
04/25/2022 06:57:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=974
04/25/2022 06:57:46 - INFO - __main__ - Global step 1950 Train loss 0.04 ACC 0.28125 on epoch=974
04/25/2022 06:57:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=979
04/25/2022 06:57:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=984
04/25/2022 06:57:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=989
04/25/2022 06:58:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=994
04/25/2022 06:58:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=999
04/25/2022 06:58:10 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:58:10 - INFO - __main__ - Printing 3 examples
04/25/2022 06:58:10 - INFO - __main__ -  [dream] What does Terry say about the garbage in New York? [SEP] Frank: Hi, Terry, you're just back from New York. Terry: Yes. Frank: It must be an exciting trip. Terry: Well, I've really seen lots of things. I saw the most spectacular view when I was crossing a bridge to Manhattan at dusk, and the skyscrapers were lit up producing a classic nighttime view of Manhattan. Frank: That's really beautiful. Terry: But that's not the whole picture. Some of the streets in New York are very rough. I saw large piles of garbage cans at the roadside, and graffiti all over garage doors and store shutters. Frank: I can't believe it. Terry: The garbage are tidily bagged and boxed, though. Frank: Did you stay in a hotel? Terry: Yes. The hotel we stayed at turned out to be fairly decent, though the room was small, with a tiny bathroom that was only about 3 feet larger than the bed. As I was inexperienced with tourist-area hotels, I was startled, I mean, the room was $129 a night. But at least the room was clean and the bed reasonably comfortable. Frank: What's your general impression of New York? Terry: Well, restaurants pack their tiny tables very tightly; grocery stores and bookstores have aisles that are narrow; the sidewalks are cluttered with newsstands, vendors and their carts, and places that aren't restrictively small, such as the lawns around the Natural History Museum, are full of people, so they're no escape. [SEP]  (A) Smelly. (B) Scattered. (C) Put in bags and boxes.
04/25/2022 06:58:10 - INFO - __main__ - ['Put in bags and boxes.']
04/25/2022 06:58:10 - INFO - __main__ -  [dream] Which would the woman like to apply for next year? [SEP] M: Good morning, Lucy. Can I help you? W: Good morning, sir. I'd like to talk with you about my studies for a minute, if I may. M: Certainly, come in and have a seat. W: Thank you. I have a record of my studies for last year. Would you like to see it? M: Yes, let me see now. You are studying mathematics, aren't you?. W: Yes, I am. But I'd like to apply for admission to the engineering college next year. M: I see. Have you asked your parents for their advice about this? W: Yes, I have. They think it is a good idea. M: Well, your record here has been very good. I don't think you will have much trouble. W: I hope not. Anyway, I am going to apply. And I'd like to ask you to write a recommendation for me, if it is not too much trouble. M: No trouble at all. I'd be glad to do it. Is there anything else? W: No, sir. I think that is all. Thank you very much. M: All right, Lucy. Good luck to you. W: Thank you, Good-bye. [SEP]  (A) Economics. (B) Engineering. (C) Science.
04/25/2022 06:58:10 - INFO - __main__ - ['Engineering.']
04/25/2022 06:58:10 - INFO - __main__ -  [dream] At what time must the passenger arrive at the airport for Flight NH906? [SEP] M: Miss, what time is Flight NH906 for New York due to depart? W: It leaves at 3:45. but you must check in one hour ahead of departure. [SEP]  (A) 2:45. (B) 1:15. (C) 3:45.
04/25/2022 06:58:10 - INFO - __main__ - ['2:45.']
04/25/2022 06:58:10 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:58:10 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:58:10 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 06:58:10 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:58:10 - INFO - __main__ - Printing 3 examples
04/25/2022 06:58:10 - INFO - __main__ -  [dream] How did Jim get hue? [SEP] W: Did you get the injury playing football, Jim? M: Yes. One of the boys kicked my foot instead of the ball. W: The best thing for you to do is to rest your leg for 48 hours. M: When can I go back to school? W: You can go back in 4 days, but you'll need to avoid sports for a month. [SEP]  (A) He was kicked by a boy. (B) He was hit by a football. (C) He fell down on the ground.
04/25/2022 06:58:10 - INFO - __main__ - ['He was kicked by a boy.']
04/25/2022 06:58:10 - INFO - __main__ -  [dream] What is the monthly salary of the woman? [SEP] M: Do you mind telling me about your work in the factory? W: Certainly not. I'm a machine operator working in the assembly workshop. M: Oh, are you? Then you have a six-day week, don't you? W: Yes. And we work eight hours a day, but this includes a one -hour break. M: What about the huge machines used in your factory? I know you make heavy machines yourselves. But I wonder if you also use machines imported from abroad. W: Most of them are made in China, some even by our factory. Only a few were bought from other countries. M: But I see the machines turned out here are quite modem and efficient. W: Following the national policy of self - reliance and hard - working struggle, we've tried our best in the past few years to improve our equipment. However, there is still a lot more to do. M: Now the question about yourself. I hope you don't mind it. How much do you earn each month? W: My monthly wage is 3,500 RMB, not including the extra pay of about 500 for extra work I put in. M: That sounds reasonable indeed. I'm very interested in worker's welfare here. What other benefits do you workers get, I wonder? W: Like other factories in China, our factory offers free medical service for workers. [SEP]  (A) $3,500. (B) $500. (C) 3,500RMB.
04/25/2022 06:58:10 - INFO - __main__ - ['3,500RMB.']
04/25/2022 06:58:10 - INFO - __main__ -  [dream] What are Paula and her roommate planning to do? [SEP] M: Hello? W: Hello, Sam. This is Paula Hanson, Sorry to bother you, but I'm having a small problem I thought you might be able to help me with. M: Sure, Paula. What's up? W: Well, you know Sarah and I moved into an off-campus apartment in the fall,on the west side of town. Any way, we've been happy with it until the past couple of months. M: Yeah. What happened? W: Well,the dishwasher broken down,so we reported it to Ms. Corners,the owner. She said she'd take care of it, but a month went by and nothing happened. M: Did you get back in touch with her? W: I got a repairperson to give me an estimate,and then I sent it to her. When I didn't hear from her,I had the repair done. And I deducted the cost from the rent check. M: So what's the problem? W: She called here madly. She said she could have gotten the repair done for less money. Now, she's threatening to expel us for not paying the full rent. M: Hold on, Paula. It does sound pretty serious. But I'm sure you can all sit down and work this out. W: Well,you're over at the law school. So, I wondered if you would mind coming with Sarah and me when we go to talk to Ms. Corners. We're supposed to meet with her tomorrow night at eight. M: Sure. I haven't studied a lot about contracts yet,but I'll be glad to help you straighten things out. Why don't I stop by about 7:30? W: Thanks Sam. You are our lifesaver. [SEP]  (A) To talk to their landlord. (B) To buy a new dishwasher. (C) To break their contract.
04/25/2022 06:58:10 - INFO - __main__ - ['To talk to their landlord.']
04/25/2022 06:58:10 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:58:10 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:58:10 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 06:58:11 - INFO - __main__ - Global step 2000 Train loss 0.05 ACC 0.3125 on epoch=999
04/25/2022 06:58:11 - INFO - __main__ - save last model!
04/25/2022 06:58:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 06:58:11 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 06:58:11 - INFO - __main__ - Printing 3 examples
04/25/2022 06:58:11 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 06:58:11 - INFO - __main__ - ['To change her job.']
04/25/2022 06:58:11 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 06:58:11 - INFO - __main__ - ['Watch TV.']
04/25/2022 06:58:11 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 06:58:11 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 06:58:11 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:58:12 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:58:13 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 06:58:28 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 06:58:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 06:58:29 - INFO - __main__ - Starting training!
04/25/2022 06:59:32 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_42_0.5_8_predictions.txt
04/25/2022 06:59:32 - INFO - __main__ - ACC on test data: 0.3110
04/25/2022 06:59:33 - INFO - __main__ - prefix=dream_32_42, lr=0.5, bsz=8, dev_performance=0.34375, test_performance=0.311
04/25/2022 06:59:33 - INFO - __main__ - Running ... prefix=dream_32_42, lr=0.4, bsz=8 ...
04/25/2022 06:59:34 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:59:34 - INFO - __main__ - Printing 3 examples
04/25/2022 06:59:34 - INFO - __main__ -  [dream] What does Terry say about the garbage in New York? [SEP] Frank: Hi, Terry, you're just back from New York. Terry: Yes. Frank: It must be an exciting trip. Terry: Well, I've really seen lots of things. I saw the most spectacular view when I was crossing a bridge to Manhattan at dusk, and the skyscrapers were lit up producing a classic nighttime view of Manhattan. Frank: That's really beautiful. Terry: But that's not the whole picture. Some of the streets in New York are very rough. I saw large piles of garbage cans at the roadside, and graffiti all over garage doors and store shutters. Frank: I can't believe it. Terry: The garbage are tidily bagged and boxed, though. Frank: Did you stay in a hotel? Terry: Yes. The hotel we stayed at turned out to be fairly decent, though the room was small, with a tiny bathroom that was only about 3 feet larger than the bed. As I was inexperienced with tourist-area hotels, I was startled, I mean, the room was $129 a night. But at least the room was clean and the bed reasonably comfortable. Frank: What's your general impression of New York? Terry: Well, restaurants pack their tiny tables very tightly; grocery stores and bookstores have aisles that are narrow; the sidewalks are cluttered with newsstands, vendors and their carts, and places that aren't restrictively small, such as the lawns around the Natural History Museum, are full of people, so they're no escape. [SEP]  (A) Smelly. (B) Scattered. (C) Put in bags and boxes.
04/25/2022 06:59:34 - INFO - __main__ - ['Put in bags and boxes.']
04/25/2022 06:59:34 - INFO - __main__ -  [dream] Which would the woman like to apply for next year? [SEP] M: Good morning, Lucy. Can I help you? W: Good morning, sir. I'd like to talk with you about my studies for a minute, if I may. M: Certainly, come in and have a seat. W: Thank you. I have a record of my studies for last year. Would you like to see it? M: Yes, let me see now. You are studying mathematics, aren't you?. W: Yes, I am. But I'd like to apply for admission to the engineering college next year. M: I see. Have you asked your parents for their advice about this? W: Yes, I have. They think it is a good idea. M: Well, your record here has been very good. I don't think you will have much trouble. W: I hope not. Anyway, I am going to apply. And I'd like to ask you to write a recommendation for me, if it is not too much trouble. M: No trouble at all. I'd be glad to do it. Is there anything else? W: No, sir. I think that is all. Thank you very much. M: All right, Lucy. Good luck to you. W: Thank you, Good-bye. [SEP]  (A) Economics. (B) Engineering. (C) Science.
04/25/2022 06:59:34 - INFO - __main__ - ['Engineering.']
04/25/2022 06:59:34 - INFO - __main__ -  [dream] At what time must the passenger arrive at the airport for Flight NH906? [SEP] M: Miss, what time is Flight NH906 for New York due to depart? W: It leaves at 3:45. but you must check in one hour ahead of departure. [SEP]  (A) 2:45. (B) 1:15. (C) 3:45.
04/25/2022 06:59:34 - INFO - __main__ - ['2:45.']
04/25/2022 06:59:34 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:59:34 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:59:34 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 06:59:34 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 06:59:34 - INFO - __main__ - Printing 3 examples
04/25/2022 06:59:34 - INFO - __main__ -  [dream] How did Jim get hue? [SEP] W: Did you get the injury playing football, Jim? M: Yes. One of the boys kicked my foot instead of the ball. W: The best thing for you to do is to rest your leg for 48 hours. M: When can I go back to school? W: You can go back in 4 days, but you'll need to avoid sports for a month. [SEP]  (A) He was kicked by a boy. (B) He was hit by a football. (C) He fell down on the ground.
04/25/2022 06:59:34 - INFO - __main__ - ['He was kicked by a boy.']
04/25/2022 06:59:34 - INFO - __main__ -  [dream] What is the monthly salary of the woman? [SEP] M: Do you mind telling me about your work in the factory? W: Certainly not. I'm a machine operator working in the assembly workshop. M: Oh, are you? Then you have a six-day week, don't you? W: Yes. And we work eight hours a day, but this includes a one -hour break. M: What about the huge machines used in your factory? I know you make heavy machines yourselves. But I wonder if you also use machines imported from abroad. W: Most of them are made in China, some even by our factory. Only a few were bought from other countries. M: But I see the machines turned out here are quite modem and efficient. W: Following the national policy of self - reliance and hard - working struggle, we've tried our best in the past few years to improve our equipment. However, there is still a lot more to do. M: Now the question about yourself. I hope you don't mind it. How much do you earn each month? W: My monthly wage is 3,500 RMB, not including the extra pay of about 500 for extra work I put in. M: That sounds reasonable indeed. I'm very interested in worker's welfare here. What other benefits do you workers get, I wonder? W: Like other factories in China, our factory offers free medical service for workers. [SEP]  (A) $3,500. (B) $500. (C) 3,500RMB.
04/25/2022 06:59:34 - INFO - __main__ - ['3,500RMB.']
04/25/2022 06:59:34 - INFO - __main__ -  [dream] What are Paula and her roommate planning to do? [SEP] M: Hello? W: Hello, Sam. This is Paula Hanson, Sorry to bother you, but I'm having a small problem I thought you might be able to help me with. M: Sure, Paula. What's up? W: Well, you know Sarah and I moved into an off-campus apartment in the fall,on the west side of town. Any way, we've been happy with it until the past couple of months. M: Yeah. What happened? W: Well,the dishwasher broken down,so we reported it to Ms. Corners,the owner. She said she'd take care of it, but a month went by and nothing happened. M: Did you get back in touch with her? W: I got a repairperson to give me an estimate,and then I sent it to her. When I didn't hear from her,I had the repair done. And I deducted the cost from the rent check. M: So what's the problem? W: She called here madly. She said she could have gotten the repair done for less money. Now, she's threatening to expel us for not paying the full rent. M: Hold on, Paula. It does sound pretty serious. But I'm sure you can all sit down and work this out. W: Well,you're over at the law school. So, I wondered if you would mind coming with Sarah and me when we go to talk to Ms. Corners. We're supposed to meet with her tomorrow night at eight. M: Sure. I haven't studied a lot about contracts yet,but I'll be glad to help you straighten things out. Why don't I stop by about 7:30? W: Thanks Sam. You are our lifesaver. [SEP]  (A) To talk to their landlord. (B) To buy a new dishwasher. (C) To break their contract.
04/25/2022 06:59:34 - INFO - __main__ - ['To talk to their landlord.']
04/25/2022 06:59:34 - INFO - __main__ - Tokenizing Input ...
04/25/2022 06:59:34 - INFO - __main__ - Tokenizing Output ...
04/25/2022 06:59:34 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 06:59:49 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 06:59:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 06:59:50 - INFO - __main__ - Starting training!
04/25/2022 06:59:55 - INFO - __main__ - Step 10 Global step 10 Train loss 1.51 on epoch=4
04/25/2022 07:00:00 - INFO - __main__ - Step 20 Global step 20 Train loss 1.14 on epoch=9
04/25/2022 07:00:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.87 on epoch=14
04/25/2022 07:00:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.62 on epoch=19
04/25/2022 07:00:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=24
04/25/2022 07:00:16 - INFO - __main__ - Global step 50 Train loss 0.93 ACC 0.28125 on epoch=24
04/25/2022 07:00:16 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.28125 on epoch=24, global_step=50
04/25/2022 07:00:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=29
04/25/2022 07:00:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=34
04/25/2022 07:00:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=39
04/25/2022 07:00:33 - INFO - __main__ - Step 90 Global step 90 Train loss 0.35 on epoch=44
04/25/2022 07:00:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=49
04/25/2022 07:00:40 - INFO - __main__ - Global step 100 Train loss 0.39 ACC 0.375 on epoch=49
04/25/2022 07:00:40 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.375 on epoch=49, global_step=100
04/25/2022 07:00:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=54
04/25/2022 07:00:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
04/25/2022 07:00:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
04/25/2022 07:00:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=69
04/25/2022 07:01:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=74
04/25/2022 07:01:06 - INFO - __main__ - Global step 150 Train loss 0.29 ACC 0.28125 on epoch=74
04/25/2022 07:01:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
04/25/2022 07:01:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.24 on epoch=84
04/25/2022 07:01:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=89
04/25/2022 07:01:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.21 on epoch=94
04/25/2022 07:01:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.18 on epoch=99
04/25/2022 07:01:30 - INFO - __main__ - Global step 200 Train loss 0.23 ACC 0.3125 on epoch=99
04/25/2022 07:01:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.18 on epoch=104
04/25/2022 07:01:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.20 on epoch=109
04/25/2022 07:01:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.20 on epoch=114
04/25/2022 07:01:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=119
04/25/2022 07:01:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.17 on epoch=124
04/25/2022 07:01:55 - INFO - __main__ - Global step 250 Train loss 0.19 ACC 0.28125 on epoch=124
04/25/2022 07:01:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.16 on epoch=129
04/25/2022 07:02:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
04/25/2022 07:02:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.13 on epoch=139
04/25/2022 07:02:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
04/25/2022 07:02:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.16 on epoch=149
04/25/2022 07:02:20 - INFO - __main__ - Global step 300 Train loss 0.15 ACC 0.28125 on epoch=149
04/25/2022 07:02:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.18 on epoch=154
04/25/2022 07:02:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.11 on epoch=159
04/25/2022 07:02:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.15 on epoch=164
04/25/2022 07:02:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
04/25/2022 07:02:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=174
04/25/2022 07:02:45 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.25 on epoch=174
04/25/2022 07:02:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
04/25/2022 07:02:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
04/25/2022 07:02:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.08 on epoch=189
04/25/2022 07:03:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.15 on epoch=194
04/25/2022 07:03:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.08 on epoch=199
04/25/2022 07:03:10 - INFO - __main__ - Global step 400 Train loss 0.11 ACC 0.25 on epoch=199
04/25/2022 07:03:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
04/25/2022 07:03:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=209
04/25/2022 07:03:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.11 on epoch=214
04/25/2022 07:03:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.15 on epoch=219
04/25/2022 07:03:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=224
04/25/2022 07:03:34 - INFO - __main__ - Global step 450 Train loss 0.18 ACC 0.21875 on epoch=224
04/25/2022 07:03:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=229
04/25/2022 07:03:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.14 on epoch=234
04/25/2022 07:03:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.09 on epoch=239
04/25/2022 07:03:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
04/25/2022 07:03:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.08 on epoch=249
04/25/2022 07:03:59 - INFO - __main__ - Global step 500 Train loss 0.11 ACC 0.28125 on epoch=249
04/25/2022 07:04:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
04/25/2022 07:04:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=259
04/25/2022 07:04:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=264
04/25/2022 07:04:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=269
04/25/2022 07:04:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.10 on epoch=274
04/25/2022 07:04:24 - INFO - __main__ - Global step 550 Train loss 0.09 ACC 0.28125 on epoch=274
04/25/2022 07:04:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=279
04/25/2022 07:04:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=284
04/25/2022 07:04:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.08 on epoch=289
04/25/2022 07:04:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=294
04/25/2022 07:04:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
04/25/2022 07:04:48 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.1875 on epoch=299
04/25/2022 07:04:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=304
04/25/2022 07:04:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=309
04/25/2022 07:05:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
04/25/2022 07:05:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
04/25/2022 07:05:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
04/25/2022 07:05:13 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.25 on epoch=324
04/25/2022 07:05:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
04/25/2022 07:05:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
04/25/2022 07:05:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
04/25/2022 07:05:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
04/25/2022 07:05:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=349
04/25/2022 07:05:38 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.3125 on epoch=349
04/25/2022 07:05:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
04/25/2022 07:05:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
04/25/2022 07:05:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
04/25/2022 07:05:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
04/25/2022 07:06:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
04/25/2022 07:06:03 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.21875 on epoch=374
04/25/2022 07:06:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=379
04/25/2022 07:06:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=384
04/25/2022 07:06:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
04/25/2022 07:06:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
04/25/2022 07:06:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
04/25/2022 07:06:27 - INFO - __main__ - Global step 800 Train loss 0.04 ACC 0.21875 on epoch=399
04/25/2022 07:06:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
04/25/2022 07:06:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
04/25/2022 07:06:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=414
04/25/2022 07:06:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
04/25/2022 07:06:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
04/25/2022 07:06:52 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.28125 on epoch=424
04/25/2022 07:06:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
04/25/2022 07:07:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
04/25/2022 07:07:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
04/25/2022 07:07:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=444
04/25/2022 07:07:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
04/25/2022 07:07:17 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.25 on epoch=449
04/25/2022 07:07:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
04/25/2022 07:07:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
04/25/2022 07:07:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
04/25/2022 07:07:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
04/25/2022 07:07:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
04/25/2022 07:07:42 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.28125 on epoch=474
04/25/2022 07:07:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
04/25/2022 07:07:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
04/25/2022 07:07:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
04/25/2022 07:07:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
04/25/2022 07:08:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
04/25/2022 07:08:06 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.25 on epoch=499
04/25/2022 07:08:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
04/25/2022 07:08:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
04/25/2022 07:08:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
04/25/2022 07:08:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
04/25/2022 07:08:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
04/25/2022 07:08:31 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.25 on epoch=524
04/25/2022 07:08:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
04/25/2022 07:08:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
04/25/2022 07:08:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
04/25/2022 07:08:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
04/25/2022 07:08:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
04/25/2022 07:08:56 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.15625 on epoch=549
04/25/2022 07:09:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
04/25/2022 07:09:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
04/25/2022 07:09:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/25/2022 07:09:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/25/2022 07:09:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
04/25/2022 07:09:21 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.25 on epoch=574
04/25/2022 07:09:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 07:09:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
04/25/2022 07:09:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/25/2022 07:09:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
04/25/2022 07:09:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/25/2022 07:09:45 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.21875 on epoch=599
04/25/2022 07:09:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
04/25/2022 07:09:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
04/25/2022 07:09:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 07:10:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
04/25/2022 07:10:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/25/2022 07:10:10 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.21875 on epoch=624
04/25/2022 07:10:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
04/25/2022 07:10:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
04/25/2022 07:10:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
04/25/2022 07:10:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/25/2022 07:10:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/25/2022 07:10:35 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.1875 on epoch=649
04/25/2022 07:10:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 07:10:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 07:10:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/25/2022 07:10:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/25/2022 07:10:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 07:11:00 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.21875 on epoch=674
04/25/2022 07:11:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
04/25/2022 07:11:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/25/2022 07:11:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/25/2022 07:11:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
04/25/2022 07:11:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 07:11:25 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.28125 on epoch=699
04/25/2022 07:11:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 07:11:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 07:11:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/25/2022 07:11:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/25/2022 07:11:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
04/25/2022 07:11:49 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.21875 on epoch=724
04/25/2022 07:11:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 07:11:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/25/2022 07:12:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
04/25/2022 07:12:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 07:12:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 07:12:14 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.125 on epoch=749
04/25/2022 07:12:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
04/25/2022 07:12:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 07:12:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
04/25/2022 07:12:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 07:12:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/25/2022 07:12:39 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.28125 on epoch=774
04/25/2022 07:12:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/25/2022 07:12:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/25/2022 07:12:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 07:12:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/25/2022 07:13:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/25/2022 07:13:04 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.21875 on epoch=799
04/25/2022 07:13:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/25/2022 07:13:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 07:13:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/25/2022 07:13:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 07:13:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 07:13:29 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.21875 on epoch=824
04/25/2022 07:13:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/25/2022 07:13:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/25/2022 07:13:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
04/25/2022 07:13:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 07:13:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 07:13:54 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.21875 on epoch=849
04/25/2022 07:13:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 07:14:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 07:14:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
04/25/2022 07:14:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
04/25/2022 07:14:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 07:14:19 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.21875 on epoch=874
04/25/2022 07:14:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/25/2022 07:14:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
04/25/2022 07:14:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 07:14:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
04/25/2022 07:14:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
04/25/2022 07:14:44 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.125 on epoch=899
04/25/2022 07:14:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 07:14:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
04/25/2022 07:14:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 07:15:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 07:15:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
04/25/2022 07:15:09 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.25 on epoch=924
04/25/2022 07:15:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 07:15:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 07:15:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
04/25/2022 07:15:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 07:15:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 07:15:34 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.25 on epoch=949
04/25/2022 07:15:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 07:15:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 07:15:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 07:15:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 07:15:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
04/25/2022 07:15:59 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.21875 on epoch=974
04/25/2022 07:16:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
04/25/2022 07:16:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 07:16:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/25/2022 07:16:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 07:16:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
04/25/2022 07:16:23 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:16:23 - INFO - __main__ - Printing 3 examples
04/25/2022 07:16:23 - INFO - __main__ -  [dream] What does Terry say about the garbage in New York? [SEP] Frank: Hi, Terry, you're just back from New York. Terry: Yes. Frank: It must be an exciting trip. Terry: Well, I've really seen lots of things. I saw the most spectacular view when I was crossing a bridge to Manhattan at dusk, and the skyscrapers were lit up producing a classic nighttime view of Manhattan. Frank: That's really beautiful. Terry: But that's not the whole picture. Some of the streets in New York are very rough. I saw large piles of garbage cans at the roadside, and graffiti all over garage doors and store shutters. Frank: I can't believe it. Terry: The garbage are tidily bagged and boxed, though. Frank: Did you stay in a hotel? Terry: Yes. The hotel we stayed at turned out to be fairly decent, though the room was small, with a tiny bathroom that was only about 3 feet larger than the bed. As I was inexperienced with tourist-area hotels, I was startled, I mean, the room was $129 a night. But at least the room was clean and the bed reasonably comfortable. Frank: What's your general impression of New York? Terry: Well, restaurants pack their tiny tables very tightly; grocery stores and bookstores have aisles that are narrow; the sidewalks are cluttered with newsstands, vendors and their carts, and places that aren't restrictively small, such as the lawns around the Natural History Museum, are full of people, so they're no escape. [SEP]  (A) Smelly. (B) Scattered. (C) Put in bags and boxes.
04/25/2022 07:16:23 - INFO - __main__ - ['Put in bags and boxes.']
04/25/2022 07:16:23 - INFO - __main__ -  [dream] Which would the woman like to apply for next year? [SEP] M: Good morning, Lucy. Can I help you? W: Good morning, sir. I'd like to talk with you about my studies for a minute, if I may. M: Certainly, come in and have a seat. W: Thank you. I have a record of my studies for last year. Would you like to see it? M: Yes, let me see now. You are studying mathematics, aren't you?. W: Yes, I am. But I'd like to apply for admission to the engineering college next year. M: I see. Have you asked your parents for their advice about this? W: Yes, I have. They think it is a good idea. M: Well, your record here has been very good. I don't think you will have much trouble. W: I hope not. Anyway, I am going to apply. And I'd like to ask you to write a recommendation for me, if it is not too much trouble. M: No trouble at all. I'd be glad to do it. Is there anything else? W: No, sir. I think that is all. Thank you very much. M: All right, Lucy. Good luck to you. W: Thank you, Good-bye. [SEP]  (A) Economics. (B) Engineering. (C) Science.
04/25/2022 07:16:23 - INFO - __main__ - ['Engineering.']
04/25/2022 07:16:23 - INFO - __main__ -  [dream] At what time must the passenger arrive at the airport for Flight NH906? [SEP] M: Miss, what time is Flight NH906 for New York due to depart? W: It leaves at 3:45. but you must check in one hour ahead of departure. [SEP]  (A) 2:45. (B) 1:15. (C) 3:45.
04/25/2022 07:16:23 - INFO - __main__ - ['2:45.']
04/25/2022 07:16:23 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:16:23 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:16:23 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 07:16:23 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:16:23 - INFO - __main__ - Printing 3 examples
04/25/2022 07:16:23 - INFO - __main__ -  [dream] How did Jim get hue? [SEP] W: Did you get the injury playing football, Jim? M: Yes. One of the boys kicked my foot instead of the ball. W: The best thing for you to do is to rest your leg for 48 hours. M: When can I go back to school? W: You can go back in 4 days, but you'll need to avoid sports for a month. [SEP]  (A) He was kicked by a boy. (B) He was hit by a football. (C) He fell down on the ground.
04/25/2022 07:16:23 - INFO - __main__ - ['He was kicked by a boy.']
04/25/2022 07:16:23 - INFO - __main__ -  [dream] What is the monthly salary of the woman? [SEP] M: Do you mind telling me about your work in the factory? W: Certainly not. I'm a machine operator working in the assembly workshop. M: Oh, are you? Then you have a six-day week, don't you? W: Yes. And we work eight hours a day, but this includes a one -hour break. M: What about the huge machines used in your factory? I know you make heavy machines yourselves. But I wonder if you also use machines imported from abroad. W: Most of them are made in China, some even by our factory. Only a few were bought from other countries. M: But I see the machines turned out here are quite modem and efficient. W: Following the national policy of self - reliance and hard - working struggle, we've tried our best in the past few years to improve our equipment. However, there is still a lot more to do. M: Now the question about yourself. I hope you don't mind it. How much do you earn each month? W: My monthly wage is 3,500 RMB, not including the extra pay of about 500 for extra work I put in. M: That sounds reasonable indeed. I'm very interested in worker's welfare here. What other benefits do you workers get, I wonder? W: Like other factories in China, our factory offers free medical service for workers. [SEP]  (A) $3,500. (B) $500. (C) 3,500RMB.
04/25/2022 07:16:23 - INFO - __main__ - ['3,500RMB.']
04/25/2022 07:16:23 - INFO - __main__ -  [dream] What are Paula and her roommate planning to do? [SEP] M: Hello? W: Hello, Sam. This is Paula Hanson, Sorry to bother you, but I'm having a small problem I thought you might be able to help me with. M: Sure, Paula. What's up? W: Well, you know Sarah and I moved into an off-campus apartment in the fall,on the west side of town. Any way, we've been happy with it until the past couple of months. M: Yeah. What happened? W: Well,the dishwasher broken down,so we reported it to Ms. Corners,the owner. She said she'd take care of it, but a month went by and nothing happened. M: Did you get back in touch with her? W: I got a repairperson to give me an estimate,and then I sent it to her. When I didn't hear from her,I had the repair done. And I deducted the cost from the rent check. M: So what's the problem? W: She called here madly. She said she could have gotten the repair done for less money. Now, she's threatening to expel us for not paying the full rent. M: Hold on, Paula. It does sound pretty serious. But I'm sure you can all sit down and work this out. W: Well,you're over at the law school. So, I wondered if you would mind coming with Sarah and me when we go to talk to Ms. Corners. We're supposed to meet with her tomorrow night at eight. M: Sure. I haven't studied a lot about contracts yet,but I'll be glad to help you straighten things out. Why don't I stop by about 7:30? W: Thanks Sam. You are our lifesaver. [SEP]  (A) To talk to their landlord. (B) To buy a new dishwasher. (C) To break their contract.
04/25/2022 07:16:23 - INFO - __main__ - ['To talk to their landlord.']
04/25/2022 07:16:23 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:16:23 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:16:23 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 07:16:25 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.1875 on epoch=999
04/25/2022 07:16:25 - INFO - __main__ - save last model!
04/25/2022 07:16:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 07:16:25 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 07:16:25 - INFO - __main__ - Printing 3 examples
04/25/2022 07:16:25 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 07:16:25 - INFO - __main__ - ['To change her job.']
04/25/2022 07:16:25 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 07:16:25 - INFO - __main__ - ['Watch TV.']
04/25/2022 07:16:25 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 07:16:25 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 07:16:25 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:16:26 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:16:27 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 07:16:42 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 07:16:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 07:16:43 - INFO - __main__ - Starting training!
04/25/2022 07:17:50 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_42_0.4_8_predictions.txt
04/25/2022 07:17:50 - INFO - __main__ - ACC on test data: 0.2990
04/25/2022 07:17:50 - INFO - __main__ - prefix=dream_32_42, lr=0.4, bsz=8, dev_performance=0.375, test_performance=0.299
04/25/2022 07:17:50 - INFO - __main__ - Running ... prefix=dream_32_42, lr=0.3, bsz=8 ...
04/25/2022 07:17:51 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:17:51 - INFO - __main__ - Printing 3 examples
04/25/2022 07:17:51 - INFO - __main__ -  [dream] What does Terry say about the garbage in New York? [SEP] Frank: Hi, Terry, you're just back from New York. Terry: Yes. Frank: It must be an exciting trip. Terry: Well, I've really seen lots of things. I saw the most spectacular view when I was crossing a bridge to Manhattan at dusk, and the skyscrapers were lit up producing a classic nighttime view of Manhattan. Frank: That's really beautiful. Terry: But that's not the whole picture. Some of the streets in New York are very rough. I saw large piles of garbage cans at the roadside, and graffiti all over garage doors and store shutters. Frank: I can't believe it. Terry: The garbage are tidily bagged and boxed, though. Frank: Did you stay in a hotel? Terry: Yes. The hotel we stayed at turned out to be fairly decent, though the room was small, with a tiny bathroom that was only about 3 feet larger than the bed. As I was inexperienced with tourist-area hotels, I was startled, I mean, the room was $129 a night. But at least the room was clean and the bed reasonably comfortable. Frank: What's your general impression of New York? Terry: Well, restaurants pack their tiny tables very tightly; grocery stores and bookstores have aisles that are narrow; the sidewalks are cluttered with newsstands, vendors and their carts, and places that aren't restrictively small, such as the lawns around the Natural History Museum, are full of people, so they're no escape. [SEP]  (A) Smelly. (B) Scattered. (C) Put in bags and boxes.
04/25/2022 07:17:51 - INFO - __main__ - ['Put in bags and boxes.']
04/25/2022 07:17:51 - INFO - __main__ -  [dream] Which would the woman like to apply for next year? [SEP] M: Good morning, Lucy. Can I help you? W: Good morning, sir. I'd like to talk with you about my studies for a minute, if I may. M: Certainly, come in and have a seat. W: Thank you. I have a record of my studies for last year. Would you like to see it? M: Yes, let me see now. You are studying mathematics, aren't you?. W: Yes, I am. But I'd like to apply for admission to the engineering college next year. M: I see. Have you asked your parents for their advice about this? W: Yes, I have. They think it is a good idea. M: Well, your record here has been very good. I don't think you will have much trouble. W: I hope not. Anyway, I am going to apply. And I'd like to ask you to write a recommendation for me, if it is not too much trouble. M: No trouble at all. I'd be glad to do it. Is there anything else? W: No, sir. I think that is all. Thank you very much. M: All right, Lucy. Good luck to you. W: Thank you, Good-bye. [SEP]  (A) Economics. (B) Engineering. (C) Science.
04/25/2022 07:17:51 - INFO - __main__ - ['Engineering.']
04/25/2022 07:17:51 - INFO - __main__ -  [dream] At what time must the passenger arrive at the airport for Flight NH906? [SEP] M: Miss, what time is Flight NH906 for New York due to depart? W: It leaves at 3:45. but you must check in one hour ahead of departure. [SEP]  (A) 2:45. (B) 1:15. (C) 3:45.
04/25/2022 07:17:51 - INFO - __main__ - ['2:45.']
04/25/2022 07:17:51 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:17:51 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:17:51 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 07:17:51 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:17:51 - INFO - __main__ - Printing 3 examples
04/25/2022 07:17:51 - INFO - __main__ -  [dream] How did Jim get hue? [SEP] W: Did you get the injury playing football, Jim? M: Yes. One of the boys kicked my foot instead of the ball. W: The best thing for you to do is to rest your leg for 48 hours. M: When can I go back to school? W: You can go back in 4 days, but you'll need to avoid sports for a month. [SEP]  (A) He was kicked by a boy. (B) He was hit by a football. (C) He fell down on the ground.
04/25/2022 07:17:51 - INFO - __main__ - ['He was kicked by a boy.']
04/25/2022 07:17:51 - INFO - __main__ -  [dream] What is the monthly salary of the woman? [SEP] M: Do you mind telling me about your work in the factory? W: Certainly not. I'm a machine operator working in the assembly workshop. M: Oh, are you? Then you have a six-day week, don't you? W: Yes. And we work eight hours a day, but this includes a one -hour break. M: What about the huge machines used in your factory? I know you make heavy machines yourselves. But I wonder if you also use machines imported from abroad. W: Most of them are made in China, some even by our factory. Only a few were bought from other countries. M: But I see the machines turned out here are quite modem and efficient. W: Following the national policy of self - reliance and hard - working struggle, we've tried our best in the past few years to improve our equipment. However, there is still a lot more to do. M: Now the question about yourself. I hope you don't mind it. How much do you earn each month? W: My monthly wage is 3,500 RMB, not including the extra pay of about 500 for extra work I put in. M: That sounds reasonable indeed. I'm very interested in worker's welfare here. What other benefits do you workers get, I wonder? W: Like other factories in China, our factory offers free medical service for workers. [SEP]  (A) $3,500. (B) $500. (C) 3,500RMB.
04/25/2022 07:17:51 - INFO - __main__ - ['3,500RMB.']
04/25/2022 07:17:51 - INFO - __main__ -  [dream] What are Paula and her roommate planning to do? [SEP] M: Hello? W: Hello, Sam. This is Paula Hanson, Sorry to bother you, but I'm having a small problem I thought you might be able to help me with. M: Sure, Paula. What's up? W: Well, you know Sarah and I moved into an off-campus apartment in the fall,on the west side of town. Any way, we've been happy with it until the past couple of months. M: Yeah. What happened? W: Well,the dishwasher broken down,so we reported it to Ms. Corners,the owner. She said she'd take care of it, but a month went by and nothing happened. M: Did you get back in touch with her? W: I got a repairperson to give me an estimate,and then I sent it to her. When I didn't hear from her,I had the repair done. And I deducted the cost from the rent check. M: So what's the problem? W: She called here madly. She said she could have gotten the repair done for less money. Now, she's threatening to expel us for not paying the full rent. M: Hold on, Paula. It does sound pretty serious. But I'm sure you can all sit down and work this out. W: Well,you're over at the law school. So, I wondered if you would mind coming with Sarah and me when we go to talk to Ms. Corners. We're supposed to meet with her tomorrow night at eight. M: Sure. I haven't studied a lot about contracts yet,but I'll be glad to help you straighten things out. Why don't I stop by about 7:30? W: Thanks Sam. You are our lifesaver. [SEP]  (A) To talk to their landlord. (B) To buy a new dishwasher. (C) To break their contract.
04/25/2022 07:17:51 - INFO - __main__ - ['To talk to their landlord.']
04/25/2022 07:17:51 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:17:51 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:17:51 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 07:18:10 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 07:18:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 07:18:10 - INFO - __main__ - Starting training!
04/25/2022 07:18:16 - INFO - __main__ - Step 10 Global step 10 Train loss 1.51 on epoch=4
04/25/2022 07:18:20 - INFO - __main__ - Step 20 Global step 20 Train loss 1.29 on epoch=9
04/25/2022 07:18:25 - INFO - __main__ - Step 30 Global step 30 Train loss 1.32 on epoch=14
04/25/2022 07:18:29 - INFO - __main__ - Step 40 Global step 40 Train loss 1.34 on epoch=19
04/25/2022 07:18:34 - INFO - __main__ - Step 50 Global step 50 Train loss 1.20 on epoch=24
04/25/2022 07:18:37 - INFO - __main__ - Global step 50 Train loss 1.33 ACC 0.0625 on epoch=24
04/25/2022 07:18:37 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0625 on epoch=24, global_step=50
04/25/2022 07:18:42 - INFO - __main__ - Step 60 Global step 60 Train loss 1.10 on epoch=29
04/25/2022 07:18:46 - INFO - __main__ - Step 70 Global step 70 Train loss 1.08 on epoch=34
04/25/2022 07:18:51 - INFO - __main__ - Step 80 Global step 80 Train loss 1.06 on epoch=39
04/25/2022 07:18:55 - INFO - __main__ - Step 90 Global step 90 Train loss 1.00 on epoch=44
04/25/2022 07:19:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.98 on epoch=49
04/25/2022 07:19:03 - INFO - __main__ - Global step 100 Train loss 1.04 ACC 0.1875 on epoch=49
04/25/2022 07:19:03 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.1875 on epoch=49, global_step=100
04/25/2022 07:19:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.89 on epoch=54
04/25/2022 07:19:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.77 on epoch=59
04/25/2022 07:19:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.61 on epoch=64
04/25/2022 07:19:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.62 on epoch=69
04/25/2022 07:19:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=74
04/25/2022 07:19:28 - INFO - __main__ - Global step 150 Train loss 0.69 ACC 0.28125 on epoch=74
04/25/2022 07:19:28 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.28125 on epoch=74, global_step=150
04/25/2022 07:19:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=79
04/25/2022 07:19:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=84
04/25/2022 07:19:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.55 on epoch=89
04/25/2022 07:19:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=94
04/25/2022 07:19:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=99
04/25/2022 07:19:54 - INFO - __main__ - Global step 200 Train loss 0.49 ACC 0.25 on epoch=99
04/25/2022 07:19:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=104
04/25/2022 07:20:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=109
04/25/2022 07:20:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=114
04/25/2022 07:20:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=119
04/25/2022 07:20:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=124
04/25/2022 07:20:19 - INFO - __main__ - Global step 250 Train loss 0.42 ACC 0.25 on epoch=124
04/25/2022 07:20:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.34 on epoch=129
04/25/2022 07:20:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
04/25/2022 07:20:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
04/25/2022 07:20:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.33 on epoch=144
04/25/2022 07:20:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.35 on epoch=149
04/25/2022 07:20:44 - INFO - __main__ - Global step 300 Train loss 0.34 ACC 0.375 on epoch=149
04/25/2022 07:20:44 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.375 on epoch=149, global_step=300
04/25/2022 07:20:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=154
04/25/2022 07:20:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=159
04/25/2022 07:20:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.31 on epoch=164
04/25/2022 07:21:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=169
04/25/2022 07:21:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
04/25/2022 07:21:09 - INFO - __main__ - Global step 350 Train loss 0.31 ACC 0.34375 on epoch=174
04/25/2022 07:21:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
04/25/2022 07:21:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=184
04/25/2022 07:21:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
04/25/2022 07:21:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=194
04/25/2022 07:21:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
04/25/2022 07:21:34 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.25 on epoch=199
04/25/2022 07:21:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=204
04/25/2022 07:21:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
04/25/2022 07:21:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=214
04/25/2022 07:21:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=219
04/25/2022 07:21:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=224
04/25/2022 07:21:59 - INFO - __main__ - Global step 450 Train loss 0.23 ACC 0.25 on epoch=224
04/25/2022 07:22:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
04/25/2022 07:22:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.16 on epoch=234
04/25/2022 07:22:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=239
04/25/2022 07:22:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.17 on epoch=244
04/25/2022 07:22:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=249
04/25/2022 07:22:24 - INFO - __main__ - Global step 500 Train loss 0.19 ACC 0.25 on epoch=249
04/25/2022 07:22:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.17 on epoch=254
04/25/2022 07:22:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.17 on epoch=259
04/25/2022 07:22:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=264
04/25/2022 07:22:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=269
04/25/2022 07:22:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=274
04/25/2022 07:22:49 - INFO - __main__ - Global step 550 Train loss 0.16 ACC 0.28125 on epoch=274
04/25/2022 07:22:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=279
04/25/2022 07:22:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=284
04/25/2022 07:23:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=289
04/25/2022 07:23:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=294
04/25/2022 07:23:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=299
04/25/2022 07:23:14 - INFO - __main__ - Global step 600 Train loss 0.14 ACC 0.3125 on epoch=299
04/25/2022 07:23:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=304
04/25/2022 07:23:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=309
04/25/2022 07:23:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=314
04/25/2022 07:23:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=319
04/25/2022 07:23:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
04/25/2022 07:23:39 - INFO - __main__ - Global step 650 Train loss 0.14 ACC 0.21875 on epoch=324
04/25/2022 07:23:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=329
04/25/2022 07:23:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=334
04/25/2022 07:23:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=339
04/25/2022 07:23:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=344
04/25/2022 07:24:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=349
04/25/2022 07:24:04 - INFO - __main__ - Global step 700 Train loss 0.12 ACC 0.15625 on epoch=349
04/25/2022 07:24:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=354
04/25/2022 07:24:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.13 on epoch=359
04/25/2022 07:24:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=364
04/25/2022 07:24:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=369
04/25/2022 07:24:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=374
04/25/2022 07:24:29 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.21875 on epoch=374
04/25/2022 07:24:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=379
04/25/2022 07:24:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=384
04/25/2022 07:24:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=389
04/25/2022 07:24:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=394
04/25/2022 07:24:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=399
04/25/2022 07:24:54 - INFO - __main__ - Global step 800 Train loss 0.09 ACC 0.21875 on epoch=399
04/25/2022 07:24:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=404
04/25/2022 07:25:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=409
04/25/2022 07:25:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=414
04/25/2022 07:25:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=419
04/25/2022 07:25:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=424
04/25/2022 07:25:19 - INFO - __main__ - Global step 850 Train loss 0.09 ACC 0.21875 on epoch=424
04/25/2022 07:25:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
04/25/2022 07:25:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=434
04/25/2022 07:25:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=439
04/25/2022 07:25:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=444
04/25/2022 07:25:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=449
04/25/2022 07:25:45 - INFO - __main__ - Global step 900 Train loss 0.07 ACC 0.1875 on epoch=449
04/25/2022 07:25:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=454
04/25/2022 07:25:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
04/25/2022 07:25:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
04/25/2022 07:26:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=469
04/25/2022 07:26:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
04/25/2022 07:26:10 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.21875 on epoch=474
04/25/2022 07:26:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
04/25/2022 07:26:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
04/25/2022 07:26:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=489
04/25/2022 07:26:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
04/25/2022 07:26:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=499
04/25/2022 07:26:35 - INFO - __main__ - Global step 1000 Train loss 0.06 ACC 0.21875 on epoch=499
04/25/2022 07:26:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=504
04/25/2022 07:26:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=509
04/25/2022 07:26:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
04/25/2022 07:26:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=519
04/25/2022 07:26:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
04/25/2022 07:27:00 - INFO - __main__ - Global step 1050 Train loss 0.06 ACC 0.125 on epoch=524
04/25/2022 07:27:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
04/25/2022 07:27:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
04/25/2022 07:27:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
04/25/2022 07:27:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
04/25/2022 07:27:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
04/25/2022 07:27:25 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.15625 on epoch=549
04/25/2022 07:27:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
04/25/2022 07:27:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
04/25/2022 07:27:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
04/25/2022 07:27:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
04/25/2022 07:27:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
04/25/2022 07:27:50 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.21875 on epoch=574
04/25/2022 07:27:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/25/2022 07:27:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/25/2022 07:28:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
04/25/2022 07:28:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
04/25/2022 07:28:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
04/25/2022 07:28:16 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.21875 on epoch=599
04/25/2022 07:28:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
04/25/2022 07:28:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
04/25/2022 07:28:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
04/25/2022 07:28:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
04/25/2022 07:28:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
04/25/2022 07:28:41 - INFO - __main__ - Global step 1250 Train loss 0.04 ACC 0.125 on epoch=624
04/25/2022 07:28:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
04/25/2022 07:28:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
04/25/2022 07:28:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
04/25/2022 07:28:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
04/25/2022 07:29:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
04/25/2022 07:29:06 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.21875 on epoch=649
04/25/2022 07:29:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=654
04/25/2022 07:29:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 07:29:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
04/25/2022 07:29:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
04/25/2022 07:29:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=674
04/25/2022 07:29:32 - INFO - __main__ - Global step 1350 Train loss 0.04 ACC 0.1875 on epoch=674
04/25/2022 07:29:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
04/25/2022 07:29:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/25/2022 07:29:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=689
04/25/2022 07:29:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
04/25/2022 07:29:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=699
04/25/2022 07:29:57 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.15625 on epoch=699
04/25/2022 07:30:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
04/25/2022 07:30:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
04/25/2022 07:30:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 07:30:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
04/25/2022 07:30:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
04/25/2022 07:30:22 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.25 on epoch=724
04/25/2022 07:30:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
04/25/2022 07:30:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/25/2022 07:30:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
04/25/2022 07:30:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
04/25/2022 07:30:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 07:30:47 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.21875 on epoch=749
04/25/2022 07:30:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
04/25/2022 07:30:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
04/25/2022 07:31:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
04/25/2022 07:31:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 07:31:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/25/2022 07:31:13 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.1875 on epoch=774
04/25/2022 07:31:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
04/25/2022 07:31:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/25/2022 07:31:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
04/25/2022 07:31:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/25/2022 07:31:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
04/25/2022 07:31:38 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.125 on epoch=799
04/25/2022 07:31:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/25/2022 07:31:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 07:31:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
04/25/2022 07:31:56 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
04/25/2022 07:32:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
04/25/2022 07:32:03 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.15625 on epoch=824
04/25/2022 07:32:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/25/2022 07:32:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
04/25/2022 07:32:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
04/25/2022 07:32:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/25/2022 07:32:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
04/25/2022 07:32:28 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.1875 on epoch=849
04/25/2022 07:32:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 07:32:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
04/25/2022 07:32:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
04/25/2022 07:32:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 07:32:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/25/2022 07:32:53 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.15625 on epoch=874
04/25/2022 07:32:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
04/25/2022 07:33:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
04/25/2022 07:33:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
04/25/2022 07:33:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
04/25/2022 07:33:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=899
04/25/2022 07:33:18 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.25 on epoch=899
04/25/2022 07:33:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
04/25/2022 07:33:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/25/2022 07:33:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
04/25/2022 07:33:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
04/25/2022 07:33:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 07:33:44 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.1875 on epoch=924
04/25/2022 07:33:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
04/25/2022 07:33:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
04/25/2022 07:33:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
04/25/2022 07:34:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
04/25/2022 07:34:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 07:34:09 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.15625 on epoch=949
04/25/2022 07:34:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 07:34:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 07:34:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 07:34:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 07:34:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 07:34:34 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.28125 on epoch=974
04/25/2022 07:34:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 07:34:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 07:34:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
04/25/2022 07:34:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 07:34:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
04/25/2022 07:34:58 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:34:58 - INFO - __main__ - Printing 3 examples
04/25/2022 07:34:58 - INFO - __main__ -  [dream] What does Terry say about the garbage in New York? [SEP] Frank: Hi, Terry, you're just back from New York. Terry: Yes. Frank: It must be an exciting trip. Terry: Well, I've really seen lots of things. I saw the most spectacular view when I was crossing a bridge to Manhattan at dusk, and the skyscrapers were lit up producing a classic nighttime view of Manhattan. Frank: That's really beautiful. Terry: But that's not the whole picture. Some of the streets in New York are very rough. I saw large piles of garbage cans at the roadside, and graffiti all over garage doors and store shutters. Frank: I can't believe it. Terry: The garbage are tidily bagged and boxed, though. Frank: Did you stay in a hotel? Terry: Yes. The hotel we stayed at turned out to be fairly decent, though the room was small, with a tiny bathroom that was only about 3 feet larger than the bed. As I was inexperienced with tourist-area hotels, I was startled, I mean, the room was $129 a night. But at least the room was clean and the bed reasonably comfortable. Frank: What's your general impression of New York? Terry: Well, restaurants pack their tiny tables very tightly; grocery stores and bookstores have aisles that are narrow; the sidewalks are cluttered with newsstands, vendors and their carts, and places that aren't restrictively small, such as the lawns around the Natural History Museum, are full of people, so they're no escape. [SEP]  (A) Smelly. (B) Scattered. (C) Put in bags and boxes.
04/25/2022 07:34:58 - INFO - __main__ - ['Put in bags and boxes.']
04/25/2022 07:34:58 - INFO - __main__ -  [dream] Which would the woman like to apply for next year? [SEP] M: Good morning, Lucy. Can I help you? W: Good morning, sir. I'd like to talk with you about my studies for a minute, if I may. M: Certainly, come in and have a seat. W: Thank you. I have a record of my studies for last year. Would you like to see it? M: Yes, let me see now. You are studying mathematics, aren't you?. W: Yes, I am. But I'd like to apply for admission to the engineering college next year. M: I see. Have you asked your parents for their advice about this? W: Yes, I have. They think it is a good idea. M: Well, your record here has been very good. I don't think you will have much trouble. W: I hope not. Anyway, I am going to apply. And I'd like to ask you to write a recommendation for me, if it is not too much trouble. M: No trouble at all. I'd be glad to do it. Is there anything else? W: No, sir. I think that is all. Thank you very much. M: All right, Lucy. Good luck to you. W: Thank you, Good-bye. [SEP]  (A) Economics. (B) Engineering. (C) Science.
04/25/2022 07:34:58 - INFO - __main__ - ['Engineering.']
04/25/2022 07:34:58 - INFO - __main__ -  [dream] At what time must the passenger arrive at the airport for Flight NH906? [SEP] M: Miss, what time is Flight NH906 for New York due to depart? W: It leaves at 3:45. but you must check in one hour ahead of departure. [SEP]  (A) 2:45. (B) 1:15. (C) 3:45.
04/25/2022 07:34:58 - INFO - __main__ - ['2:45.']
04/25/2022 07:34:58 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:34:58 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:34:58 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 07:34:58 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:34:58 - INFO - __main__ - Printing 3 examples
04/25/2022 07:34:58 - INFO - __main__ -  [dream] How did Jim get hue? [SEP] W: Did you get the injury playing football, Jim? M: Yes. One of the boys kicked my foot instead of the ball. W: The best thing for you to do is to rest your leg for 48 hours. M: When can I go back to school? W: You can go back in 4 days, but you'll need to avoid sports for a month. [SEP]  (A) He was kicked by a boy. (B) He was hit by a football. (C) He fell down on the ground.
04/25/2022 07:34:58 - INFO - __main__ - ['He was kicked by a boy.']
04/25/2022 07:34:58 - INFO - __main__ -  [dream] What is the monthly salary of the woman? [SEP] M: Do you mind telling me about your work in the factory? W: Certainly not. I'm a machine operator working in the assembly workshop. M: Oh, are you? Then you have a six-day week, don't you? W: Yes. And we work eight hours a day, but this includes a one -hour break. M: What about the huge machines used in your factory? I know you make heavy machines yourselves. But I wonder if you also use machines imported from abroad. W: Most of them are made in China, some even by our factory. Only a few were bought from other countries. M: But I see the machines turned out here are quite modem and efficient. W: Following the national policy of self - reliance and hard - working struggle, we've tried our best in the past few years to improve our equipment. However, there is still a lot more to do. M: Now the question about yourself. I hope you don't mind it. How much do you earn each month? W: My monthly wage is 3,500 RMB, not including the extra pay of about 500 for extra work I put in. M: That sounds reasonable indeed. I'm very interested in worker's welfare here. What other benefits do you workers get, I wonder? W: Like other factories in China, our factory offers free medical service for workers. [SEP]  (A) $3,500. (B) $500. (C) 3,500RMB.
04/25/2022 07:34:58 - INFO - __main__ - ['3,500RMB.']
04/25/2022 07:34:58 - INFO - __main__ -  [dream] What are Paula and her roommate planning to do? [SEP] M: Hello? W: Hello, Sam. This is Paula Hanson, Sorry to bother you, but I'm having a small problem I thought you might be able to help me with. M: Sure, Paula. What's up? W: Well, you know Sarah and I moved into an off-campus apartment in the fall,on the west side of town. Any way, we've been happy with it until the past couple of months. M: Yeah. What happened? W: Well,the dishwasher broken down,so we reported it to Ms. Corners,the owner. She said she'd take care of it, but a month went by and nothing happened. M: Did you get back in touch with her? W: I got a repairperson to give me an estimate,and then I sent it to her. When I didn't hear from her,I had the repair done. And I deducted the cost from the rent check. M: So what's the problem? W: She called here madly. She said she could have gotten the repair done for less money. Now, she's threatening to expel us for not paying the full rent. M: Hold on, Paula. It does sound pretty serious. But I'm sure you can all sit down and work this out. W: Well,you're over at the law school. So, I wondered if you would mind coming with Sarah and me when we go to talk to Ms. Corners. We're supposed to meet with her tomorrow night at eight. M: Sure. I haven't studied a lot about contracts yet,but I'll be glad to help you straighten things out. Why don't I stop by about 7:30? W: Thanks Sam. You are our lifesaver. [SEP]  (A) To talk to their landlord. (B) To buy a new dishwasher. (C) To break their contract.
04/25/2022 07:34:58 - INFO - __main__ - ['To talk to their landlord.']
04/25/2022 07:34:58 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:34:58 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:34:58 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 07:34:59 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.21875 on epoch=999
04/25/2022 07:34:59 - INFO - __main__ - save last model!
04/25/2022 07:34:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 07:34:59 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 07:34:59 - INFO - __main__ - Printing 3 examples
04/25/2022 07:34:59 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 07:34:59 - INFO - __main__ - ['To change her job.']
04/25/2022 07:34:59 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 07:34:59 - INFO - __main__ - ['Watch TV.']
04/25/2022 07:34:59 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 07:34:59 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 07:34:59 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:35:00 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:35:01 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 07:35:17 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 07:35:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 07:35:18 - INFO - __main__ - Starting training!
04/25/2022 07:36:20 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_42_0.3_8_predictions.txt
04/25/2022 07:36:20 - INFO - __main__ - ACC on test data: 0.3130
04/25/2022 07:36:21 - INFO - __main__ - prefix=dream_32_42, lr=0.3, bsz=8, dev_performance=0.375, test_performance=0.313
04/25/2022 07:36:21 - INFO - __main__ - Running ... prefix=dream_32_42, lr=0.2, bsz=8 ...
04/25/2022 07:36:21 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:36:21 - INFO - __main__ - Printing 3 examples
04/25/2022 07:36:21 - INFO - __main__ -  [dream] What does Terry say about the garbage in New York? [SEP] Frank: Hi, Terry, you're just back from New York. Terry: Yes. Frank: It must be an exciting trip. Terry: Well, I've really seen lots of things. I saw the most spectacular view when I was crossing a bridge to Manhattan at dusk, and the skyscrapers were lit up producing a classic nighttime view of Manhattan. Frank: That's really beautiful. Terry: But that's not the whole picture. Some of the streets in New York are very rough. I saw large piles of garbage cans at the roadside, and graffiti all over garage doors and store shutters. Frank: I can't believe it. Terry: The garbage are tidily bagged and boxed, though. Frank: Did you stay in a hotel? Terry: Yes. The hotel we stayed at turned out to be fairly decent, though the room was small, with a tiny bathroom that was only about 3 feet larger than the bed. As I was inexperienced with tourist-area hotels, I was startled, I mean, the room was $129 a night. But at least the room was clean and the bed reasonably comfortable. Frank: What's your general impression of New York? Terry: Well, restaurants pack their tiny tables very tightly; grocery stores and bookstores have aisles that are narrow; the sidewalks are cluttered with newsstands, vendors and their carts, and places that aren't restrictively small, such as the lawns around the Natural History Museum, are full of people, so they're no escape. [SEP]  (A) Smelly. (B) Scattered. (C) Put in bags and boxes.
04/25/2022 07:36:21 - INFO - __main__ - ['Put in bags and boxes.']
04/25/2022 07:36:21 - INFO - __main__ -  [dream] Which would the woman like to apply for next year? [SEP] M: Good morning, Lucy. Can I help you? W: Good morning, sir. I'd like to talk with you about my studies for a minute, if I may. M: Certainly, come in and have a seat. W: Thank you. I have a record of my studies for last year. Would you like to see it? M: Yes, let me see now. You are studying mathematics, aren't you?. W: Yes, I am. But I'd like to apply for admission to the engineering college next year. M: I see. Have you asked your parents for their advice about this? W: Yes, I have. They think it is a good idea. M: Well, your record here has been very good. I don't think you will have much trouble. W: I hope not. Anyway, I am going to apply. And I'd like to ask you to write a recommendation for me, if it is not too much trouble. M: No trouble at all. I'd be glad to do it. Is there anything else? W: No, sir. I think that is all. Thank you very much. M: All right, Lucy. Good luck to you. W: Thank you, Good-bye. [SEP]  (A) Economics. (B) Engineering. (C) Science.
04/25/2022 07:36:21 - INFO - __main__ - ['Engineering.']
04/25/2022 07:36:21 - INFO - __main__ -  [dream] At what time must the passenger arrive at the airport for Flight NH906? [SEP] M: Miss, what time is Flight NH906 for New York due to depart? W: It leaves at 3:45. but you must check in one hour ahead of departure. [SEP]  (A) 2:45. (B) 1:15. (C) 3:45.
04/25/2022 07:36:21 - INFO - __main__ - ['2:45.']
04/25/2022 07:36:21 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:36:22 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:36:22 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 07:36:22 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:36:22 - INFO - __main__ - Printing 3 examples
04/25/2022 07:36:22 - INFO - __main__ -  [dream] How did Jim get hue? [SEP] W: Did you get the injury playing football, Jim? M: Yes. One of the boys kicked my foot instead of the ball. W: The best thing for you to do is to rest your leg for 48 hours. M: When can I go back to school? W: You can go back in 4 days, but you'll need to avoid sports for a month. [SEP]  (A) He was kicked by a boy. (B) He was hit by a football. (C) He fell down on the ground.
04/25/2022 07:36:22 - INFO - __main__ - ['He was kicked by a boy.']
04/25/2022 07:36:22 - INFO - __main__ -  [dream] What is the monthly salary of the woman? [SEP] M: Do you mind telling me about your work in the factory? W: Certainly not. I'm a machine operator working in the assembly workshop. M: Oh, are you? Then you have a six-day week, don't you? W: Yes. And we work eight hours a day, but this includes a one -hour break. M: What about the huge machines used in your factory? I know you make heavy machines yourselves. But I wonder if you also use machines imported from abroad. W: Most of them are made in China, some even by our factory. Only a few were bought from other countries. M: But I see the machines turned out here are quite modem and efficient. W: Following the national policy of self - reliance and hard - working struggle, we've tried our best in the past few years to improve our equipment. However, there is still a lot more to do. M: Now the question about yourself. I hope you don't mind it. How much do you earn each month? W: My monthly wage is 3,500 RMB, not including the extra pay of about 500 for extra work I put in. M: That sounds reasonable indeed. I'm very interested in worker's welfare here. What other benefits do you workers get, I wonder? W: Like other factories in China, our factory offers free medical service for workers. [SEP]  (A) $3,500. (B) $500. (C) 3,500RMB.
04/25/2022 07:36:22 - INFO - __main__ - ['3,500RMB.']
04/25/2022 07:36:22 - INFO - __main__ -  [dream] What are Paula and her roommate planning to do? [SEP] M: Hello? W: Hello, Sam. This is Paula Hanson, Sorry to bother you, but I'm having a small problem I thought you might be able to help me with. M: Sure, Paula. What's up? W: Well, you know Sarah and I moved into an off-campus apartment in the fall,on the west side of town. Any way, we've been happy with it until the past couple of months. M: Yeah. What happened? W: Well,the dishwasher broken down,so we reported it to Ms. Corners,the owner. She said she'd take care of it, but a month went by and nothing happened. M: Did you get back in touch with her? W: I got a repairperson to give me an estimate,and then I sent it to her. When I didn't hear from her,I had the repair done. And I deducted the cost from the rent check. M: So what's the problem? W: She called here madly. She said she could have gotten the repair done for less money. Now, she's threatening to expel us for not paying the full rent. M: Hold on, Paula. It does sound pretty serious. But I'm sure you can all sit down and work this out. W: Well,you're over at the law school. So, I wondered if you would mind coming with Sarah and me when we go to talk to Ms. Corners. We're supposed to meet with her tomorrow night at eight. M: Sure. I haven't studied a lot about contracts yet,but I'll be glad to help you straighten things out. Why don't I stop by about 7:30? W: Thanks Sam. You are our lifesaver. [SEP]  (A) To talk to their landlord. (B) To buy a new dishwasher. (C) To break their contract.
04/25/2022 07:36:22 - INFO - __main__ - ['To talk to their landlord.']
04/25/2022 07:36:22 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:36:22 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:36:22 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 07:36:40 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 07:36:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 07:36:41 - INFO - __main__ - Starting training!
04/25/2022 07:36:46 - INFO - __main__ - Step 10 Global step 10 Train loss 1.62 on epoch=4
04/25/2022 07:36:51 - INFO - __main__ - Step 20 Global step 20 Train loss 1.34 on epoch=9
04/25/2022 07:36:55 - INFO - __main__ - Step 30 Global step 30 Train loss 1.22 on epoch=14
04/25/2022 07:37:00 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=19
04/25/2022 07:37:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.94 on epoch=24
04/25/2022 07:37:10 - INFO - __main__ - Global step 50 Train loss 1.23 ACC 0.21875 on epoch=24
04/25/2022 07:37:10 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.21875 on epoch=24, global_step=50
04/25/2022 07:37:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.79 on epoch=29
04/25/2022 07:37:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.71 on epoch=34
04/25/2022 07:37:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.58 on epoch=39
04/25/2022 07:37:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=44
04/25/2022 07:37:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=49
04/25/2022 07:37:36 - INFO - __main__ - Global step 100 Train loss 0.63 ACC 0.1875 on epoch=49
04/25/2022 07:37:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=54
04/25/2022 07:37:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=59
04/25/2022 07:37:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.36 on epoch=64
04/25/2022 07:37:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
04/25/2022 07:37:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.35 on epoch=74
04/25/2022 07:38:01 - INFO - __main__ - Global step 150 Train loss 0.41 ACC 0.34375 on epoch=74
04/25/2022 07:38:01 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.34375 on epoch=74, global_step=150
04/25/2022 07:38:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=79
04/25/2022 07:38:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
04/25/2022 07:38:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.32 on epoch=89
04/25/2022 07:38:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
04/25/2022 07:38:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=99
04/25/2022 07:38:27 - INFO - __main__ - Global step 200 Train loss 0.35 ACC 0.25 on epoch=99
04/25/2022 07:38:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=104
04/25/2022 07:38:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=109
04/25/2022 07:38:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=114
04/25/2022 07:38:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
04/25/2022 07:38:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
04/25/2022 07:38:53 - INFO - __main__ - Global step 250 Train loss 0.29 ACC 0.25 on epoch=124
04/25/2022 07:38:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
04/25/2022 07:39:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
04/25/2022 07:39:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=139
04/25/2022 07:39:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
04/25/2022 07:39:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=149
04/25/2022 07:39:18 - INFO - __main__ - Global step 300 Train loss 0.26 ACC 0.3125 on epoch=149
04/25/2022 07:39:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=154
04/25/2022 07:39:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.22 on epoch=159
04/25/2022 07:39:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=164
04/25/2022 07:39:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.17 on epoch=169
04/25/2022 07:39:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
04/25/2022 07:39:43 - INFO - __main__ - Global step 350 Train loss 0.21 ACC 0.3125 on epoch=174
04/25/2022 07:39:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
04/25/2022 07:39:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=184
04/25/2022 07:39:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.16 on epoch=189
04/25/2022 07:40:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=194
04/25/2022 07:40:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=199
04/25/2022 07:40:07 - INFO - __main__ - Global step 400 Train loss 0.20 ACC 0.3125 on epoch=199
04/25/2022 07:40:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=204
04/25/2022 07:40:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.17 on epoch=209
04/25/2022 07:40:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.18 on epoch=214
04/25/2022 07:40:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.16 on epoch=219
04/25/2022 07:40:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=224
04/25/2022 07:40:32 - INFO - __main__ - Global step 450 Train loss 0.18 ACC 0.3125 on epoch=224
04/25/2022 07:40:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=229
04/25/2022 07:40:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=234
04/25/2022 07:40:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.12 on epoch=239
04/25/2022 07:40:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.15 on epoch=244
04/25/2022 07:40:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.16 on epoch=249
04/25/2022 07:40:56 - INFO - __main__ - Global step 500 Train loss 0.15 ACC 0.21875 on epoch=249
04/25/2022 07:41:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=254
04/25/2022 07:41:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=259
04/25/2022 07:41:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=264
04/25/2022 07:41:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
04/25/2022 07:41:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=274
04/25/2022 07:41:21 - INFO - __main__ - Global step 550 Train loss 0.13 ACC 0.21875 on epoch=274
04/25/2022 07:41:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=279
04/25/2022 07:41:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=284
04/25/2022 07:41:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.09 on epoch=289
04/25/2022 07:41:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
04/25/2022 07:41:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=299
04/25/2022 07:41:45 - INFO - __main__ - Global step 600 Train loss 0.11 ACC 0.25 on epoch=299
04/25/2022 07:41:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
04/25/2022 07:41:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=309
04/25/2022 07:41:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.14 on epoch=314
04/25/2022 07:42:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=319
04/25/2022 07:42:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=324
04/25/2022 07:42:09 - INFO - __main__ - Global step 650 Train loss 0.12 ACC 0.21875 on epoch=324
04/25/2022 07:42:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.07 on epoch=329
04/25/2022 07:42:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=334
04/25/2022 07:42:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=339
04/25/2022 07:42:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=344
04/25/2022 07:42:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
04/25/2022 07:42:34 - INFO - __main__ - Global step 700 Train loss 0.08 ACC 0.25 on epoch=349
04/25/2022 07:42:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
04/25/2022 07:42:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=359
04/25/2022 07:42:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=364
04/25/2022 07:42:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=369
04/25/2022 07:42:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=374
04/25/2022 07:42:58 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.28125 on epoch=374
04/25/2022 07:43:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=379
04/25/2022 07:43:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
04/25/2022 07:43:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
04/25/2022 07:43:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
04/25/2022 07:43:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
04/25/2022 07:43:23 - INFO - __main__ - Global step 800 Train loss 0.05 ACC 0.15625 on epoch=399
04/25/2022 07:43:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=404
04/25/2022 07:43:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
04/25/2022 07:43:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=414
04/25/2022 07:43:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
04/25/2022 07:43:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=424
04/25/2022 07:43:47 - INFO - __main__ - Global step 850 Train loss 0.07 ACC 0.28125 on epoch=424
04/25/2022 07:43:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
04/25/2022 07:43:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
04/25/2022 07:44:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=439
04/25/2022 07:44:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
04/25/2022 07:44:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=449
04/25/2022 07:44:11 - INFO - __main__ - Global step 900 Train loss 0.05 ACC 0.28125 on epoch=449
04/25/2022 07:44:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
04/25/2022 07:44:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=459
04/25/2022 07:44:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
04/25/2022 07:44:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=469
04/25/2022 07:44:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
04/25/2022 07:44:36 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.34375 on epoch=474
04/25/2022 07:44:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
04/25/2022 07:44:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
04/25/2022 07:44:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
04/25/2022 07:44:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
04/25/2022 07:44:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/25/2022 07:45:00 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.34375 on epoch=499
04/25/2022 07:45:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
04/25/2022 07:45:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
04/25/2022 07:45:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
04/25/2022 07:45:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
04/25/2022 07:45:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
04/25/2022 07:45:25 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.3125 on epoch=524
04/25/2022 07:45:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
04/25/2022 07:45:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
04/25/2022 07:45:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
04/25/2022 07:45:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
04/25/2022 07:45:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
04/25/2022 07:45:49 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.40625 on epoch=549
04/25/2022 07:45:49 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.40625 on epoch=549, global_step=1100
04/25/2022 07:45:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
04/25/2022 07:45:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=559
04/25/2022 07:46:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
04/25/2022 07:46:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/25/2022 07:46:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
04/25/2022 07:46:13 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.34375 on epoch=574
04/25/2022 07:46:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
04/25/2022 07:46:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/25/2022 07:46:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
04/25/2022 07:46:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
04/25/2022 07:46:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
04/25/2022 07:46:38 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.375 on epoch=599
04/25/2022 07:46:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
04/25/2022 07:46:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
04/25/2022 07:46:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
04/25/2022 07:46:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
04/25/2022 07:47:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
04/25/2022 07:47:02 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.34375 on epoch=624
04/25/2022 07:47:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
04/25/2022 07:47:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
04/25/2022 07:47:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
04/25/2022 07:47:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/25/2022 07:47:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
04/25/2022 07:47:27 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.34375 on epoch=649
04/25/2022 07:47:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
04/25/2022 07:47:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 07:47:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/25/2022 07:47:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
04/25/2022 07:47:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
04/25/2022 07:47:51 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.375 on epoch=674
04/25/2022 07:47:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 07:48:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/25/2022 07:48:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=689
04/25/2022 07:48:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 07:48:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
04/25/2022 07:48:16 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.34375 on epoch=699
04/25/2022 07:48:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
04/25/2022 07:48:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
04/25/2022 07:48:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 07:48:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/25/2022 07:48:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
04/25/2022 07:48:41 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.375 on epoch=724
04/25/2022 07:48:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/25/2022 07:48:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
04/25/2022 07:48:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
04/25/2022 07:48:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
04/25/2022 07:49:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
04/25/2022 07:49:05 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.375 on epoch=749
04/25/2022 07:49:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
04/25/2022 07:49:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
04/25/2022 07:49:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
04/25/2022 07:49:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
04/25/2022 07:49:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/25/2022 07:49:30 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.34375 on epoch=774
04/25/2022 07:49:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 07:49:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/25/2022 07:49:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
04/25/2022 07:49:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 07:49:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
04/25/2022 07:49:55 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.34375 on epoch=799
04/25/2022 07:49:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 07:50:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 07:50:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/25/2022 07:50:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 07:50:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 07:50:20 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.40625 on epoch=824
04/25/2022 07:50:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/25/2022 07:50:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 07:50:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
04/25/2022 07:50:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/25/2022 07:50:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
04/25/2022 07:50:44 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.40625 on epoch=849
04/25/2022 07:50:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
04/25/2022 07:50:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
04/25/2022 07:50:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 07:51:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
04/25/2022 07:51:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 07:51:09 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.28125 on epoch=874
04/25/2022 07:51:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/25/2022 07:51:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 07:51:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 07:51:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 07:51:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
04/25/2022 07:51:34 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.34375 on epoch=899
04/25/2022 07:51:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 07:51:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/25/2022 07:51:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
04/25/2022 07:51:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 07:51:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
04/25/2022 07:51:59 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.3125 on epoch=924
04/25/2022 07:52:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
04/25/2022 07:52:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 07:52:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
04/25/2022 07:52:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 07:52:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 07:52:24 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.40625 on epoch=949
04/25/2022 07:52:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 07:52:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
04/25/2022 07:52:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 07:52:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=969
04/25/2022 07:52:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 07:52:49 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.375 on epoch=974
04/25/2022 07:52:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 07:52:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
04/25/2022 07:53:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
04/25/2022 07:53:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
04/25/2022 07:53:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 07:53:13 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:53:13 - INFO - __main__ - Printing 3 examples
04/25/2022 07:53:13 - INFO - __main__ -  [dream] Why do the woman's parents want her to do Business Studies? [SEP] M: Hi. I think I'm sitting next to you. Seat 35B. W: Oh,sorry. I'll just move my things. Hold on a minute. M: Thanks. Phwoo...I've been waiting in departure for ages. W: Mmm...The flight was delayed leaving Beijing. Security checks, you know. M: Yeah. Same here. W: Are you going to London, then? M: Yes. I'm going to the LSE to do a master's degree in International Relations. W: Really? That sounds interesting. You must be really clever. M: So are you going to London too? W: Well actually, I'm going to the University of Middlesex to do Business Studies. M: That sounds interesting. W: Well actually, it's my parents who want me to do Business Studies. I'd rather study Philosophy, but my dad wants me to take over the family business. He thinks Business Studies will make me rich. M: Well, nobody wants to be poor...ah, here's the drinks trolley. What would you like? W: A Coke, please,I'm sorry...I don't know your name... M: Oh, sorry. I forgot...Ali. [SEP]  (A) Because they want her to run a company. (B) Because they want her to be a career woman. (C) Because they want her to take over the family business.
04/25/2022 07:53:13 - INFO - __main__ - ['Because they want her to take over the family business.']
04/25/2022 07:53:13 - INFO - __main__ -  [dream] What is the woman? [SEP] M: Well, Jenny, the school year is almost over. We just have two more weeks before exams. What do you plan to do this summer? W: I'm going to teach English to some immigrants in the university's community service program. M: That sounds interesting. Don't you need to speak a foreign language for a job like that? W: No, you don't. You just have to present the language simply and give the students a chance to practice speaking. M: Come to think of it, that's the way I was taught to speak Chinese. But speaking didn't help me learn to read and write Chinese. W: My students won't want to read and write English, at lease not now. They are more interested in speaking. M: You sound very knowledgeable about all this. How do you know so much? W: I took a Teaching English as a Second Language course last year when you were in China. I've also talked with the experienced teachers quite a lot. I think I would like to be an ESL teacher when I graduate. [SEP]  (A) A student. (B) A teacher. (C) A tourist.
04/25/2022 07:53:13 - INFO - __main__ - ['A student.']
04/25/2022 07:53:13 - INFO - __main__ -  [dream] What does the man think ridiculous? [SEP] M: I don't enjoy dating anymore. I can't seem to find anyone I have anything in common with. W: Don't feel discouraged. Be patient. As you are so distinguished, you will definitely find the person who is right for you. M: To tell you the truth, I am tired of being alone. I hope to find my Mrs. Right. What should I do? W: Do you believe in Internet matchmaking service? M: That's really a new walk of life. What is it exactly? W: It helps match up singles the world over, and helps find the man and woman of their dreams. M: Oh, it must be to the taste of a certain group of people. W: The advertisement said Dream Dates has matched up thousands of singles the world over! M: Unbelievable! They must be exaggerating the figure! W: Look at the way they manage their business: they collect applicants' photos, and give the applicants questionnaires to fill out as to what type of character they are. M: I don't believe several questions can decide the type of person you're. People's characters are complicated and keep changing all the time. W: Anyway, it seems that things work well this way. The information and specifications will be entered in a large computer database. M: A computer to decide your best date? That's really ridiculous! W: Look, it promises: Dream Dates provides expert dating service and a place for singles to meet. We'll introduce you to the person uniquely qualified to be your partner. M: Sheer slogans! Not reliable! W: It says you can enroll in a free trial membership! M: I won't do it even they pay me for that! W: Well, we don't have to believe this. Maybe I can tell John, and see whether he'd like to try it. [SEP]  (A) Entering a large company without application. (B) Programming human feelings into machines. (C) Deciding one's best partner through a computer.
04/25/2022 07:53:13 - INFO - __main__ - ["Deciding one's best partner through a computer."]
04/25/2022 07:53:13 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:53:13 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:53:13 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 07:53:13 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:53:13 - INFO - __main__ - Printing 3 examples
04/25/2022 07:53:13 - INFO - __main__ -  [dream] Why can't the man concentrate on his study? [SEP] W: Come to the movies with us. Everybody needs to take a break every once in a while. M: I guess I might as well. I've been studying so long I just can't concentrate. [SEP]  (A) He keeps thinking of going to the movies. (B) His classmates are taking a break. (C) He's been studying for too long.
04/25/2022 07:53:13 - INFO - __main__ - ["He's been studying for too long."]
04/25/2022 07:53:13 - INFO - __main__ -  [dream] What has she found? [SEP] M: Have you found your key? F: Yes, I've found it. [SEP]  (A) Her bag. (B) Her key. (C) Her kite.
04/25/2022 07:53:13 - INFO - __main__ - ['Her key.']
04/25/2022 07:53:13 - INFO - __main__ -  [dream] At what temperature should you bake the cookies? [SEP] Chef Randall: Well, hello everyone, and welcome to today's show. And joining me today is my daughter, Ashley, who has had to endure my cooking experiments over the years. Are we ready, Ashley? [Ready to eat.] No, let's wait for a few minutes. We'll get to that. But as you know, my faithful listeners, I starting cooking and baking almost 30 years ago when my grandmother taught me in her humble kitchen. In fact, she taught almost me everything I know, and I've never attended cooking classes [You should have ...] Wait, wait, wait ... I know my daughter's going to mention to you faithful listeners that recently as I was helping the kids prepare for our kitchen for chicken meal, I forgot to take the chicken out of the oven, burned the bird to a crisp, and we ended up ordering pizza for dinner. Kids: We had to use the fire extinguisher. Chef Randall: But that's another story. So, anyway, today I'd like to share with you our favorite ... at least my favorite ... chocolate chip cookie recipe. Now, before you switch the TV channel, I know what you are thinking. "Another fattening cookie recipe." But wait. What makes this recipe great is that it offers a wonderful low-fat, low-calorie, low-cholesterol dessert for the entire family. Kids: We still like the fat though. Chef Randall: Well, I know we do. But let's see. We have all the ingredients, so we can start by mixing all of the ingredients, the sugars, the flour, the egg whites, the low-fat butter, vanilla, baking soda, and a pinch of salt in a large mixing bowl. Then, we add the mini chocolate chips. Now, my kids would like me to add the big ones but we start with the mini-chocolate chips. And don't forget to preheat the oven to 350 degrees (Fahrenheit). And finally, when the cookies are done, take them out of the oven, remove them from the cookie sheet, and let them cool before their fingers get into them. Did I forget anything? Kids: Yeah, if you have college-age kids, be sure to make a few extra batches they can take back to school for their roommates. And don't forget the kids still at home. Chef Randall: Oh, well yeah. We can't do that. We can't forget them. And unfortunately, by the time your kids get the cookies, you, the cook, will be left with a single cookie - your instant diet plan for you - and a dirty kitchen. So, that's all for today. On next week's show, we will be showing you how to feed hungry teenagers on a budget without having to sell the family car. Until then. [SEP]  (A) at 305 degrees (B) at 315 degrees (C) at 350 degrees
04/25/2022 07:53:13 - INFO - __main__ - ['at 350 degrees']
04/25/2022 07:53:13 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:53:13 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:53:13 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 07:53:14 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.375 on epoch=999
04/25/2022 07:53:14 - INFO - __main__ - save last model!
04/25/2022 07:53:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 07:53:14 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 07:53:14 - INFO - __main__ - Printing 3 examples
04/25/2022 07:53:14 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 07:53:14 - INFO - __main__ - ['To change her job.']
04/25/2022 07:53:14 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 07:53:14 - INFO - __main__ - ['Watch TV.']
04/25/2022 07:53:14 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 07:53:14 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 07:53:14 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:53:15 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:53:16 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 07:53:28 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 07:53:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 07:53:29 - INFO - __main__ - Starting training!
04/25/2022 07:54:31 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_42_0.2_8_predictions.txt
04/25/2022 07:54:31 - INFO - __main__ - ACC on test data: 0.3090
04/25/2022 07:54:31 - INFO - __main__ - prefix=dream_32_42, lr=0.2, bsz=8, dev_performance=0.40625, test_performance=0.309
04/25/2022 07:54:31 - INFO - __main__ - Running ... prefix=dream_32_87, lr=0.5, bsz=8 ...
04/25/2022 07:54:32 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:54:32 - INFO - __main__ - Printing 3 examples
04/25/2022 07:54:32 - INFO - __main__ -  [dream] Why do the woman's parents want her to do Business Studies? [SEP] M: Hi. I think I'm sitting next to you. Seat 35B. W: Oh,sorry. I'll just move my things. Hold on a minute. M: Thanks. Phwoo...I've been waiting in departure for ages. W: Mmm...The flight was delayed leaving Beijing. Security checks, you know. M: Yeah. Same here. W: Are you going to London, then? M: Yes. I'm going to the LSE to do a master's degree in International Relations. W: Really? That sounds interesting. You must be really clever. M: So are you going to London too? W: Well actually, I'm going to the University of Middlesex to do Business Studies. M: That sounds interesting. W: Well actually, it's my parents who want me to do Business Studies. I'd rather study Philosophy, but my dad wants me to take over the family business. He thinks Business Studies will make me rich. M: Well, nobody wants to be poor...ah, here's the drinks trolley. What would you like? W: A Coke, please,I'm sorry...I don't know your name... M: Oh, sorry. I forgot...Ali. [SEP]  (A) Because they want her to run a company. (B) Because they want her to be a career woman. (C) Because they want her to take over the family business.
04/25/2022 07:54:32 - INFO - __main__ - ['Because they want her to take over the family business.']
04/25/2022 07:54:32 - INFO - __main__ -  [dream] What is the woman? [SEP] M: Well, Jenny, the school year is almost over. We just have two more weeks before exams. What do you plan to do this summer? W: I'm going to teach English to some immigrants in the university's community service program. M: That sounds interesting. Don't you need to speak a foreign language for a job like that? W: No, you don't. You just have to present the language simply and give the students a chance to practice speaking. M: Come to think of it, that's the way I was taught to speak Chinese. But speaking didn't help me learn to read and write Chinese. W: My students won't want to read and write English, at lease not now. They are more interested in speaking. M: You sound very knowledgeable about all this. How do you know so much? W: I took a Teaching English as a Second Language course last year when you were in China. I've also talked with the experienced teachers quite a lot. I think I would like to be an ESL teacher when I graduate. [SEP]  (A) A student. (B) A teacher. (C) A tourist.
04/25/2022 07:54:32 - INFO - __main__ - ['A student.']
04/25/2022 07:54:32 - INFO - __main__ -  [dream] What does the man think ridiculous? [SEP] M: I don't enjoy dating anymore. I can't seem to find anyone I have anything in common with. W: Don't feel discouraged. Be patient. As you are so distinguished, you will definitely find the person who is right for you. M: To tell you the truth, I am tired of being alone. I hope to find my Mrs. Right. What should I do? W: Do you believe in Internet matchmaking service? M: That's really a new walk of life. What is it exactly? W: It helps match up singles the world over, and helps find the man and woman of their dreams. M: Oh, it must be to the taste of a certain group of people. W: The advertisement said Dream Dates has matched up thousands of singles the world over! M: Unbelievable! They must be exaggerating the figure! W: Look at the way they manage their business: they collect applicants' photos, and give the applicants questionnaires to fill out as to what type of character they are. M: I don't believe several questions can decide the type of person you're. People's characters are complicated and keep changing all the time. W: Anyway, it seems that things work well this way. The information and specifications will be entered in a large computer database. M: A computer to decide your best date? That's really ridiculous! W: Look, it promises: Dream Dates provides expert dating service and a place for singles to meet. We'll introduce you to the person uniquely qualified to be your partner. M: Sheer slogans! Not reliable! W: It says you can enroll in a free trial membership! M: I won't do it even they pay me for that! W: Well, we don't have to believe this. Maybe I can tell John, and see whether he'd like to try it. [SEP]  (A) Entering a large company without application. (B) Programming human feelings into machines. (C) Deciding one's best partner through a computer.
04/25/2022 07:54:32 - INFO - __main__ - ["Deciding one's best partner through a computer."]
04/25/2022 07:54:32 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:54:32 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:54:32 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 07:54:32 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 07:54:32 - INFO - __main__ - Printing 3 examples
04/25/2022 07:54:32 - INFO - __main__ -  [dream] Why can't the man concentrate on his study? [SEP] W: Come to the movies with us. Everybody needs to take a break every once in a while. M: I guess I might as well. I've been studying so long I just can't concentrate. [SEP]  (A) He keeps thinking of going to the movies. (B) His classmates are taking a break. (C) He's been studying for too long.
04/25/2022 07:54:32 - INFO - __main__ - ["He's been studying for too long."]
04/25/2022 07:54:32 - INFO - __main__ -  [dream] What has she found? [SEP] M: Have you found your key? F: Yes, I've found it. [SEP]  (A) Her bag. (B) Her key. (C) Her kite.
04/25/2022 07:54:32 - INFO - __main__ - ['Her key.']
04/25/2022 07:54:32 - INFO - __main__ -  [dream] At what temperature should you bake the cookies? [SEP] Chef Randall: Well, hello everyone, and welcome to today's show. And joining me today is my daughter, Ashley, who has had to endure my cooking experiments over the years. Are we ready, Ashley? [Ready to eat.] No, let's wait for a few minutes. We'll get to that. But as you know, my faithful listeners, I starting cooking and baking almost 30 years ago when my grandmother taught me in her humble kitchen. In fact, she taught almost me everything I know, and I've never attended cooking classes [You should have ...] Wait, wait, wait ... I know my daughter's going to mention to you faithful listeners that recently as I was helping the kids prepare for our kitchen for chicken meal, I forgot to take the chicken out of the oven, burned the bird to a crisp, and we ended up ordering pizza for dinner. Kids: We had to use the fire extinguisher. Chef Randall: But that's another story. So, anyway, today I'd like to share with you our favorite ... at least my favorite ... chocolate chip cookie recipe. Now, before you switch the TV channel, I know what you are thinking. "Another fattening cookie recipe." But wait. What makes this recipe great is that it offers a wonderful low-fat, low-calorie, low-cholesterol dessert for the entire family. Kids: We still like the fat though. Chef Randall: Well, I know we do. But let's see. We have all the ingredients, so we can start by mixing all of the ingredients, the sugars, the flour, the egg whites, the low-fat butter, vanilla, baking soda, and a pinch of salt in a large mixing bowl. Then, we add the mini chocolate chips. Now, my kids would like me to add the big ones but we start with the mini-chocolate chips. And don't forget to preheat the oven to 350 degrees (Fahrenheit). And finally, when the cookies are done, take them out of the oven, remove them from the cookie sheet, and let them cool before their fingers get into them. Did I forget anything? Kids: Yeah, if you have college-age kids, be sure to make a few extra batches they can take back to school for their roommates. And don't forget the kids still at home. Chef Randall: Oh, well yeah. We can't do that. We can't forget them. And unfortunately, by the time your kids get the cookies, you, the cook, will be left with a single cookie - your instant diet plan for you - and a dirty kitchen. So, that's all for today. On next week's show, we will be showing you how to feed hungry teenagers on a budget without having to sell the family car. Until then. [SEP]  (A) at 305 degrees (B) at 315 degrees (C) at 350 degrees
04/25/2022 07:54:32 - INFO - __main__ - ['at 350 degrees']
04/25/2022 07:54:32 - INFO - __main__ - Tokenizing Input ...
04/25/2022 07:54:32 - INFO - __main__ - Tokenizing Output ...
04/25/2022 07:54:32 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 07:54:47 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 07:54:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 07:54:48 - INFO - __main__ - Starting training!
04/25/2022 07:54:54 - INFO - __main__ - Step 10 Global step 10 Train loss 1.31 on epoch=4
04/25/2022 07:54:58 - INFO - __main__ - Step 20 Global step 20 Train loss 0.95 on epoch=9
04/25/2022 07:55:02 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=14
04/25/2022 07:55:06 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=19
04/25/2022 07:55:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.38 on epoch=24
04/25/2022 07:55:13 - INFO - __main__ - Global step 50 Train loss 0.75 ACC 0.125 on epoch=24
04/25/2022 07:55:13 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.125 on epoch=24, global_step=50
04/25/2022 07:55:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.33 on epoch=29
04/25/2022 07:55:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.39 on epoch=34
04/25/2022 07:55:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=39
04/25/2022 07:55:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.23 on epoch=44
04/25/2022 07:55:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.21 on epoch=49
04/25/2022 07:55:37 - INFO - __main__ - Global step 100 Train loss 0.28 ACC 0.25 on epoch=49
04/25/2022 07:55:37 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.25 on epoch=49, global_step=100
04/25/2022 07:55:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=54
04/25/2022 07:55:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.16 on epoch=59
04/25/2022 07:55:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.19 on epoch=64
04/25/2022 07:55:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.73 on epoch=69
04/25/2022 07:55:58 - INFO - __main__ - Step 150 Global step 150 Train loss 1.08 on epoch=74
04/25/2022 07:56:01 - INFO - __main__ - Global step 150 Train loss 0.51 ACC 0.1875 on epoch=74
04/25/2022 07:56:05 - INFO - __main__ - Step 160 Global step 160 Train loss 1.75 on epoch=79
04/25/2022 07:56:09 - INFO - __main__ - Step 170 Global step 170 Train loss 2.33 on epoch=84
04/25/2022 07:56:13 - INFO - __main__ - Step 180 Global step 180 Train loss 2.48 on epoch=89
04/25/2022 07:56:17 - INFO - __main__ - Step 190 Global step 190 Train loss 1.94 on epoch=94
04/25/2022 07:56:21 - INFO - __main__ - Step 200 Global step 200 Train loss 1.32 on epoch=99
04/25/2022 07:56:25 - INFO - __main__ - Global step 200 Train loss 1.97 ACC 0.3125 on epoch=99
04/25/2022 07:56:25 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.3125 on epoch=99, global_step=200
04/25/2022 07:56:29 - INFO - __main__ - Step 210 Global step 210 Train loss 1.06 on epoch=104
04/25/2022 07:56:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.98 on epoch=109
04/25/2022 07:56:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.81 on epoch=114
04/25/2022 07:56:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.68 on epoch=119
04/25/2022 07:56:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.68 on epoch=124
04/25/2022 07:56:48 - INFO - __main__ - Global step 250 Train loss 0.84 ACC 0.21875 on epoch=124
04/25/2022 07:56:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=129
04/25/2022 07:56:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=134
04/25/2022 07:57:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=139
04/25/2022 07:57:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=144
04/25/2022 07:57:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=149
04/25/2022 07:57:12 - INFO - __main__ - Global step 300 Train loss 0.52 ACC 0.1875 on epoch=149
04/25/2022 07:57:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=154
04/25/2022 07:57:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=159
04/25/2022 07:57:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=164
04/25/2022 07:57:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=169
04/25/2022 07:57:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
04/25/2022 07:57:36 - INFO - __main__ - Global step 350 Train loss 0.38 ACC 0.21875 on epoch=174
04/25/2022 07:57:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=179
04/25/2022 07:57:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=184
04/25/2022 07:57:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=189
04/25/2022 07:57:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=194
04/25/2022 07:57:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=199
04/25/2022 07:58:00 - INFO - __main__ - Global step 400 Train loss 0.39 ACC 0.25 on epoch=199
04/25/2022 07:58:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=204
04/25/2022 07:58:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
04/25/2022 07:58:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
04/25/2022 07:58:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=219
04/25/2022 07:58:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
04/25/2022 07:58:23 - INFO - __main__ - Global step 450 Train loss 0.28 ACC 0.28125 on epoch=224
04/25/2022 07:58:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=229
04/25/2022 07:58:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
04/25/2022 07:58:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=239
04/25/2022 07:58:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=244
04/25/2022 07:58:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
04/25/2022 07:58:47 - INFO - __main__ - Global step 500 Train loss 0.28 ACC 0.25 on epoch=249
04/25/2022 07:58:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.33 on epoch=254
04/25/2022 07:58:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
04/25/2022 07:59:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
04/25/2022 07:59:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
04/25/2022 07:59:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=274
04/25/2022 07:59:11 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.3125 on epoch=274
04/25/2022 07:59:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
04/25/2022 07:59:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=284
04/25/2022 07:59:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
04/25/2022 07:59:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
04/25/2022 07:59:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
04/25/2022 07:59:35 - INFO - __main__ - Global step 600 Train loss 0.24 ACC 0.3125 on epoch=299
04/25/2022 07:59:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=304
04/25/2022 07:59:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
04/25/2022 07:59:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=314
04/25/2022 07:59:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=319
04/25/2022 07:59:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=324
04/25/2022 07:59:58 - INFO - __main__ - Global step 650 Train loss 0.23 ACC 0.3125 on epoch=324
04/25/2022 08:00:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=329
04/25/2022 08:00:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=334
04/25/2022 08:00:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=339
04/25/2022 08:00:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=344
04/25/2022 08:00:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.17 on epoch=349
04/25/2022 08:00:22 - INFO - __main__ - Global step 700 Train loss 0.21 ACC 0.34375 on epoch=349
04/25/2022 08:00:22 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.34375 on epoch=349, global_step=700
04/25/2022 08:00:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=354
04/25/2022 08:00:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=359
04/25/2022 08:00:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=364
04/25/2022 08:00:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=369
04/25/2022 08:00:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=374
04/25/2022 08:00:46 - INFO - __main__ - Global step 750 Train loss 0.18 ACC 0.34375 on epoch=374
04/25/2022 08:00:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=379
04/25/2022 08:00:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=384
04/25/2022 08:00:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=389
04/25/2022 08:01:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=394
04/25/2022 08:01:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=399
04/25/2022 08:01:10 - INFO - __main__ - Global step 800 Train loss 0.18 ACC 0.34375 on epoch=399
04/25/2022 08:01:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=404
04/25/2022 08:01:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=409
04/25/2022 08:01:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=414
04/25/2022 08:01:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.17 on epoch=419
04/25/2022 08:01:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=424
04/25/2022 08:01:33 - INFO - __main__ - Global step 850 Train loss 0.16 ACC 0.375 on epoch=424
04/25/2022 08:01:33 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=424, global_step=850
04/25/2022 08:01:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=429
04/25/2022 08:01:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=434
04/25/2022 08:01:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=439
04/25/2022 08:01:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=444
04/25/2022 08:01:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=449
04/25/2022 08:01:57 - INFO - __main__ - Global step 900 Train loss 0.17 ACC 0.28125 on epoch=449
04/25/2022 08:02:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=454
04/25/2022 08:02:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=459
04/25/2022 08:02:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=464
04/25/2022 08:02:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=469
04/25/2022 08:02:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=474
04/25/2022 08:02:21 - INFO - __main__ - Global step 950 Train loss 0.16 ACC 0.375 on epoch=474
04/25/2022 08:02:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=479
04/25/2022 08:02:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=484
04/25/2022 08:02:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=489
04/25/2022 08:02:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=494
04/25/2022 08:02:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.13 on epoch=499
04/25/2022 08:02:44 - INFO - __main__ - Global step 1000 Train loss 0.15 ACC 0.28125 on epoch=499
04/25/2022 08:02:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=504
04/25/2022 08:02:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=509
04/25/2022 08:02:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=514
04/25/2022 08:03:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=519
04/25/2022 08:03:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=524
04/25/2022 08:03:08 - INFO - __main__ - Global step 1050 Train loss 0.14 ACC 0.3125 on epoch=524
04/25/2022 08:03:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=529
04/25/2022 08:03:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=534
04/25/2022 08:03:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=539
04/25/2022 08:03:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=544
04/25/2022 08:03:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=549
04/25/2022 08:03:32 - INFO - __main__ - Global step 1100 Train loss 0.15 ACC 0.3125 on epoch=549
04/25/2022 08:03:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=554
04/25/2022 08:03:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=559
04/25/2022 08:03:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=564
04/25/2022 08:03:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=569
04/25/2022 08:03:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=574
04/25/2022 08:03:55 - INFO - __main__ - Global step 1150 Train loss 0.13 ACC 0.34375 on epoch=574
04/25/2022 08:03:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=579
04/25/2022 08:04:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=584
04/25/2022 08:04:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=589
04/25/2022 08:04:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=594
04/25/2022 08:04:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=599
04/25/2022 08:04:19 - INFO - __main__ - Global step 1200 Train loss 0.09 ACC 0.34375 on epoch=599
04/25/2022 08:04:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=604
04/25/2022 08:04:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=609
04/25/2022 08:04:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=614
04/25/2022 08:04:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=619
04/25/2022 08:04:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=624
04/25/2022 08:04:43 - INFO - __main__ - Global step 1250 Train loss 0.11 ACC 0.375 on epoch=624
04/25/2022 08:04:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=629
04/25/2022 08:04:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=634
04/25/2022 08:04:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=639
04/25/2022 08:04:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=644
04/25/2022 08:05:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=649
04/25/2022 08:05:06 - INFO - __main__ - Global step 1300 Train loss 0.11 ACC 0.34375 on epoch=649
04/25/2022 08:05:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=654
04/25/2022 08:05:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.11 on epoch=659
04/25/2022 08:05:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=664
04/25/2022 08:05:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=669
04/25/2022 08:05:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=674
04/25/2022 08:05:30 - INFO - __main__ - Global step 1350 Train loss 0.10 ACC 0.34375 on epoch=674
04/25/2022 08:05:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=679
04/25/2022 08:05:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=684
04/25/2022 08:05:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=689
04/25/2022 08:05:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=694
04/25/2022 08:05:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=699
04/25/2022 08:05:54 - INFO - __main__ - Global step 1400 Train loss 0.08 ACC 0.34375 on epoch=699
04/25/2022 08:05:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=704
04/25/2022 08:06:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=709
04/25/2022 08:06:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=714
04/25/2022 08:06:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=719
04/25/2022 08:06:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=724
04/25/2022 08:06:18 - INFO - __main__ - Global step 1450 Train loss 0.08 ACC 0.34375 on epoch=724
04/25/2022 08:06:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=729
04/25/2022 08:06:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=734
04/25/2022 08:06:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=739
04/25/2022 08:06:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=744
04/25/2022 08:06:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=749
04/25/2022 08:06:41 - INFO - __main__ - Global step 1500 Train loss 0.07 ACC 0.375 on epoch=749
04/25/2022 08:06:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=754
04/25/2022 08:06:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=759
04/25/2022 08:06:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=764
04/25/2022 08:06:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=769
04/25/2022 08:07:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=774
04/25/2022 08:07:05 - INFO - __main__ - Global step 1550 Train loss 0.08 ACC 0.375 on epoch=774
04/25/2022 08:07:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=779
04/25/2022 08:07:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=784
04/25/2022 08:07:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=789
04/25/2022 08:07:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=794
04/25/2022 08:07:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=799
04/25/2022 08:07:29 - INFO - __main__ - Global step 1600 Train loss 0.08 ACC 0.34375 on epoch=799
04/25/2022 08:07:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=804
04/25/2022 08:07:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=809
04/25/2022 08:07:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=814
04/25/2022 08:07:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=819
04/25/2022 08:07:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=824
04/25/2022 08:07:52 - INFO - __main__ - Global step 1650 Train loss 0.06 ACC 0.375 on epoch=824
04/25/2022 08:07:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=829
04/25/2022 08:08:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=834
04/25/2022 08:08:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=839
04/25/2022 08:08:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=844
04/25/2022 08:08:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=849
04/25/2022 08:08:16 - INFO - __main__ - Global step 1700 Train loss 0.06 ACC 0.375 on epoch=849
04/25/2022 08:08:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=854
04/25/2022 08:08:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=859
04/25/2022 08:08:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=864
04/25/2022 08:08:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=869
04/25/2022 08:08:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=874
04/25/2022 08:08:40 - INFO - __main__ - Global step 1750 Train loss 0.06 ACC 0.34375 on epoch=874
04/25/2022 08:08:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=879
04/25/2022 08:08:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=884
04/25/2022 08:08:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=889
04/25/2022 08:08:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
04/25/2022 08:09:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=899
04/25/2022 08:09:04 - INFO - __main__ - Global step 1800 Train loss 0.05 ACC 0.3125 on epoch=899
04/25/2022 08:09:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=904
04/25/2022 08:09:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=909
04/25/2022 08:09:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=914
04/25/2022 08:09:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
04/25/2022 08:09:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=924
04/25/2022 08:09:28 - INFO - __main__ - Global step 1850 Train loss 0.04 ACC 0.3125 on epoch=924
04/25/2022 08:09:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=929
04/25/2022 08:09:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=934
04/25/2022 08:09:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=939
04/25/2022 08:09:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=944
04/25/2022 08:09:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=949
04/25/2022 08:09:52 - INFO - __main__ - Global step 1900 Train loss 0.05 ACC 0.375 on epoch=949
04/25/2022 08:09:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=954
04/25/2022 08:10:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=959
04/25/2022 08:10:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=964
04/25/2022 08:10:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=969
04/25/2022 08:10:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
04/25/2022 08:10:15 - INFO - __main__ - Global step 1950 Train loss 0.05 ACC 0.28125 on epoch=974
04/25/2022 08:10:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=979
04/25/2022 08:10:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=984
04/25/2022 08:10:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
04/25/2022 08:10:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=994
04/25/2022 08:10:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=999
04/25/2022 08:10:37 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:10:37 - INFO - __main__ - Printing 3 examples
04/25/2022 08:10:37 - INFO - __main__ -  [dream] Why do the woman's parents want her to do Business Studies? [SEP] M: Hi. I think I'm sitting next to you. Seat 35B. W: Oh,sorry. I'll just move my things. Hold on a minute. M: Thanks. Phwoo...I've been waiting in departure for ages. W: Mmm...The flight was delayed leaving Beijing. Security checks, you know. M: Yeah. Same here. W: Are you going to London, then? M: Yes. I'm going to the LSE to do a master's degree in International Relations. W: Really? That sounds interesting. You must be really clever. M: So are you going to London too? W: Well actually, I'm going to the University of Middlesex to do Business Studies. M: That sounds interesting. W: Well actually, it's my parents who want me to do Business Studies. I'd rather study Philosophy, but my dad wants me to take over the family business. He thinks Business Studies will make me rich. M: Well, nobody wants to be poor...ah, here's the drinks trolley. What would you like? W: A Coke, please,I'm sorry...I don't know your name... M: Oh, sorry. I forgot...Ali. [SEP]  (A) Because they want her to run a company. (B) Because they want her to be a career woman. (C) Because they want her to take over the family business.
04/25/2022 08:10:37 - INFO - __main__ - ['Because they want her to take over the family business.']
04/25/2022 08:10:37 - INFO - __main__ -  [dream] What is the woman? [SEP] M: Well, Jenny, the school year is almost over. We just have two more weeks before exams. What do you plan to do this summer? W: I'm going to teach English to some immigrants in the university's community service program. M: That sounds interesting. Don't you need to speak a foreign language for a job like that? W: No, you don't. You just have to present the language simply and give the students a chance to practice speaking. M: Come to think of it, that's the way I was taught to speak Chinese. But speaking didn't help me learn to read and write Chinese. W: My students won't want to read and write English, at lease not now. They are more interested in speaking. M: You sound very knowledgeable about all this. How do you know so much? W: I took a Teaching English as a Second Language course last year when you were in China. I've also talked with the experienced teachers quite a lot. I think I would like to be an ESL teacher when I graduate. [SEP]  (A) A student. (B) A teacher. (C) A tourist.
04/25/2022 08:10:37 - INFO - __main__ - ['A student.']
04/25/2022 08:10:37 - INFO - __main__ -  [dream] What does the man think ridiculous? [SEP] M: I don't enjoy dating anymore. I can't seem to find anyone I have anything in common with. W: Don't feel discouraged. Be patient. As you are so distinguished, you will definitely find the person who is right for you. M: To tell you the truth, I am tired of being alone. I hope to find my Mrs. Right. What should I do? W: Do you believe in Internet matchmaking service? M: That's really a new walk of life. What is it exactly? W: It helps match up singles the world over, and helps find the man and woman of their dreams. M: Oh, it must be to the taste of a certain group of people. W: The advertisement said Dream Dates has matched up thousands of singles the world over! M: Unbelievable! They must be exaggerating the figure! W: Look at the way they manage their business: they collect applicants' photos, and give the applicants questionnaires to fill out as to what type of character they are. M: I don't believe several questions can decide the type of person you're. People's characters are complicated and keep changing all the time. W: Anyway, it seems that things work well this way. The information and specifications will be entered in a large computer database. M: A computer to decide your best date? That's really ridiculous! W: Look, it promises: Dream Dates provides expert dating service and a place for singles to meet. We'll introduce you to the person uniquely qualified to be your partner. M: Sheer slogans! Not reliable! W: It says you can enroll in a free trial membership! M: I won't do it even they pay me for that! W: Well, we don't have to believe this. Maybe I can tell John, and see whether he'd like to try it. [SEP]  (A) Entering a large company without application. (B) Programming human feelings into machines. (C) Deciding one's best partner through a computer.
04/25/2022 08:10:37 - INFO - __main__ - ["Deciding one's best partner through a computer."]
04/25/2022 08:10:37 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:10:37 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:10:37 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 08:10:37 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:10:37 - INFO - __main__ - Printing 3 examples
04/25/2022 08:10:37 - INFO - __main__ -  [dream] Why can't the man concentrate on his study? [SEP] W: Come to the movies with us. Everybody needs to take a break every once in a while. M: I guess I might as well. I've been studying so long I just can't concentrate. [SEP]  (A) He keeps thinking of going to the movies. (B) His classmates are taking a break. (C) He's been studying for too long.
04/25/2022 08:10:37 - INFO - __main__ - ["He's been studying for too long."]
04/25/2022 08:10:37 - INFO - __main__ -  [dream] What has she found? [SEP] M: Have you found your key? F: Yes, I've found it. [SEP]  (A) Her bag. (B) Her key. (C) Her kite.
04/25/2022 08:10:37 - INFO - __main__ - ['Her key.']
04/25/2022 08:10:37 - INFO - __main__ -  [dream] At what temperature should you bake the cookies? [SEP] Chef Randall: Well, hello everyone, and welcome to today's show. And joining me today is my daughter, Ashley, who has had to endure my cooking experiments over the years. Are we ready, Ashley? [Ready to eat.] No, let's wait for a few minutes. We'll get to that. But as you know, my faithful listeners, I starting cooking and baking almost 30 years ago when my grandmother taught me in her humble kitchen. In fact, she taught almost me everything I know, and I've never attended cooking classes [You should have ...] Wait, wait, wait ... I know my daughter's going to mention to you faithful listeners that recently as I was helping the kids prepare for our kitchen for chicken meal, I forgot to take the chicken out of the oven, burned the bird to a crisp, and we ended up ordering pizza for dinner. Kids: We had to use the fire extinguisher. Chef Randall: But that's another story. So, anyway, today I'd like to share with you our favorite ... at least my favorite ... chocolate chip cookie recipe. Now, before you switch the TV channel, I know what you are thinking. "Another fattening cookie recipe." But wait. What makes this recipe great is that it offers a wonderful low-fat, low-calorie, low-cholesterol dessert for the entire family. Kids: We still like the fat though. Chef Randall: Well, I know we do. But let's see. We have all the ingredients, so we can start by mixing all of the ingredients, the sugars, the flour, the egg whites, the low-fat butter, vanilla, baking soda, and a pinch of salt in a large mixing bowl. Then, we add the mini chocolate chips. Now, my kids would like me to add the big ones but we start with the mini-chocolate chips. And don't forget to preheat the oven to 350 degrees (Fahrenheit). And finally, when the cookies are done, take them out of the oven, remove them from the cookie sheet, and let them cool before their fingers get into them. Did I forget anything? Kids: Yeah, if you have college-age kids, be sure to make a few extra batches they can take back to school for their roommates. And don't forget the kids still at home. Chef Randall: Oh, well yeah. We can't do that. We can't forget them. And unfortunately, by the time your kids get the cookies, you, the cook, will be left with a single cookie - your instant diet plan for you - and a dirty kitchen. So, that's all for today. On next week's show, we will be showing you how to feed hungry teenagers on a budget without having to sell the family car. Until then. [SEP]  (A) at 305 degrees (B) at 315 degrees (C) at 350 degrees
04/25/2022 08:10:37 - INFO - __main__ - ['at 350 degrees']
04/25/2022 08:10:37 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:10:37 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:10:37 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 08:10:39 - INFO - __main__ - Global step 2000 Train loss 0.04 ACC 0.34375 on epoch=999
04/25/2022 08:10:39 - INFO - __main__ - save last model!
04/25/2022 08:10:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 08:10:39 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 08:10:39 - INFO - __main__ - Printing 3 examples
04/25/2022 08:10:39 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 08:10:39 - INFO - __main__ - ['To change her job.']
04/25/2022 08:10:39 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 08:10:39 - INFO - __main__ - ['Watch TV.']
04/25/2022 08:10:39 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 08:10:39 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 08:10:39 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:10:40 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:10:41 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 08:10:52 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 08:10:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 08:10:53 - INFO - __main__ - Starting training!
04/25/2022 08:11:56 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_87_0.5_8_predictions.txt
04/25/2022 08:11:56 - INFO - __main__ - ACC on test data: 0.3380
04/25/2022 08:11:56 - INFO - __main__ - prefix=dream_32_87, lr=0.5, bsz=8, dev_performance=0.375, test_performance=0.338
04/25/2022 08:11:56 - INFO - __main__ - Running ... prefix=dream_32_87, lr=0.4, bsz=8 ...
04/25/2022 08:11:57 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:11:57 - INFO - __main__ - Printing 3 examples
04/25/2022 08:11:57 - INFO - __main__ -  [dream] Why do the woman's parents want her to do Business Studies? [SEP] M: Hi. I think I'm sitting next to you. Seat 35B. W: Oh,sorry. I'll just move my things. Hold on a minute. M: Thanks. Phwoo...I've been waiting in departure for ages. W: Mmm...The flight was delayed leaving Beijing. Security checks, you know. M: Yeah. Same here. W: Are you going to London, then? M: Yes. I'm going to the LSE to do a master's degree in International Relations. W: Really? That sounds interesting. You must be really clever. M: So are you going to London too? W: Well actually, I'm going to the University of Middlesex to do Business Studies. M: That sounds interesting. W: Well actually, it's my parents who want me to do Business Studies. I'd rather study Philosophy, but my dad wants me to take over the family business. He thinks Business Studies will make me rich. M: Well, nobody wants to be poor...ah, here's the drinks trolley. What would you like? W: A Coke, please,I'm sorry...I don't know your name... M: Oh, sorry. I forgot...Ali. [SEP]  (A) Because they want her to run a company. (B) Because they want her to be a career woman. (C) Because they want her to take over the family business.
04/25/2022 08:11:57 - INFO - __main__ - ['Because they want her to take over the family business.']
04/25/2022 08:11:57 - INFO - __main__ -  [dream] What is the woman? [SEP] M: Well, Jenny, the school year is almost over. We just have two more weeks before exams. What do you plan to do this summer? W: I'm going to teach English to some immigrants in the university's community service program. M: That sounds interesting. Don't you need to speak a foreign language for a job like that? W: No, you don't. You just have to present the language simply and give the students a chance to practice speaking. M: Come to think of it, that's the way I was taught to speak Chinese. But speaking didn't help me learn to read and write Chinese. W: My students won't want to read and write English, at lease not now. They are more interested in speaking. M: You sound very knowledgeable about all this. How do you know so much? W: I took a Teaching English as a Second Language course last year when you were in China. I've also talked with the experienced teachers quite a lot. I think I would like to be an ESL teacher when I graduate. [SEP]  (A) A student. (B) A teacher. (C) A tourist.
04/25/2022 08:11:57 - INFO - __main__ - ['A student.']
04/25/2022 08:11:57 - INFO - __main__ -  [dream] What does the man think ridiculous? [SEP] M: I don't enjoy dating anymore. I can't seem to find anyone I have anything in common with. W: Don't feel discouraged. Be patient. As you are so distinguished, you will definitely find the person who is right for you. M: To tell you the truth, I am tired of being alone. I hope to find my Mrs. Right. What should I do? W: Do you believe in Internet matchmaking service? M: That's really a new walk of life. What is it exactly? W: It helps match up singles the world over, and helps find the man and woman of their dreams. M: Oh, it must be to the taste of a certain group of people. W: The advertisement said Dream Dates has matched up thousands of singles the world over! M: Unbelievable! They must be exaggerating the figure! W: Look at the way they manage their business: they collect applicants' photos, and give the applicants questionnaires to fill out as to what type of character they are. M: I don't believe several questions can decide the type of person you're. People's characters are complicated and keep changing all the time. W: Anyway, it seems that things work well this way. The information and specifications will be entered in a large computer database. M: A computer to decide your best date? That's really ridiculous! W: Look, it promises: Dream Dates provides expert dating service and a place for singles to meet. We'll introduce you to the person uniquely qualified to be your partner. M: Sheer slogans! Not reliable! W: It says you can enroll in a free trial membership! M: I won't do it even they pay me for that! W: Well, we don't have to believe this. Maybe I can tell John, and see whether he'd like to try it. [SEP]  (A) Entering a large company without application. (B) Programming human feelings into machines. (C) Deciding one's best partner through a computer.
04/25/2022 08:11:57 - INFO - __main__ - ["Deciding one's best partner through a computer."]
04/25/2022 08:11:57 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:11:57 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:11:57 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 08:11:57 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:11:57 - INFO - __main__ - Printing 3 examples
04/25/2022 08:11:57 - INFO - __main__ -  [dream] Why can't the man concentrate on his study? [SEP] W: Come to the movies with us. Everybody needs to take a break every once in a while. M: I guess I might as well. I've been studying so long I just can't concentrate. [SEP]  (A) He keeps thinking of going to the movies. (B) His classmates are taking a break. (C) He's been studying for too long.
04/25/2022 08:11:57 - INFO - __main__ - ["He's been studying for too long."]
04/25/2022 08:11:57 - INFO - __main__ -  [dream] What has she found? [SEP] M: Have you found your key? F: Yes, I've found it. [SEP]  (A) Her bag. (B) Her key. (C) Her kite.
04/25/2022 08:11:57 - INFO - __main__ - ['Her key.']
04/25/2022 08:11:57 - INFO - __main__ -  [dream] At what temperature should you bake the cookies? [SEP] Chef Randall: Well, hello everyone, and welcome to today's show. And joining me today is my daughter, Ashley, who has had to endure my cooking experiments over the years. Are we ready, Ashley? [Ready to eat.] No, let's wait for a few minutes. We'll get to that. But as you know, my faithful listeners, I starting cooking and baking almost 30 years ago when my grandmother taught me in her humble kitchen. In fact, she taught almost me everything I know, and I've never attended cooking classes [You should have ...] Wait, wait, wait ... I know my daughter's going to mention to you faithful listeners that recently as I was helping the kids prepare for our kitchen for chicken meal, I forgot to take the chicken out of the oven, burned the bird to a crisp, and we ended up ordering pizza for dinner. Kids: We had to use the fire extinguisher. Chef Randall: But that's another story. So, anyway, today I'd like to share with you our favorite ... at least my favorite ... chocolate chip cookie recipe. Now, before you switch the TV channel, I know what you are thinking. "Another fattening cookie recipe." But wait. What makes this recipe great is that it offers a wonderful low-fat, low-calorie, low-cholesterol dessert for the entire family. Kids: We still like the fat though. Chef Randall: Well, I know we do. But let's see. We have all the ingredients, so we can start by mixing all of the ingredients, the sugars, the flour, the egg whites, the low-fat butter, vanilla, baking soda, and a pinch of salt in a large mixing bowl. Then, we add the mini chocolate chips. Now, my kids would like me to add the big ones but we start with the mini-chocolate chips. And don't forget to preheat the oven to 350 degrees (Fahrenheit). And finally, when the cookies are done, take them out of the oven, remove them from the cookie sheet, and let them cool before their fingers get into them. Did I forget anything? Kids: Yeah, if you have college-age kids, be sure to make a few extra batches they can take back to school for their roommates. And don't forget the kids still at home. Chef Randall: Oh, well yeah. We can't do that. We can't forget them. And unfortunately, by the time your kids get the cookies, you, the cook, will be left with a single cookie - your instant diet plan for you - and a dirty kitchen. So, that's all for today. On next week's show, we will be showing you how to feed hungry teenagers on a budget without having to sell the family car. Until then. [SEP]  (A) at 305 degrees (B) at 315 degrees (C) at 350 degrees
04/25/2022 08:11:57 - INFO - __main__ - ['at 350 degrees']
04/25/2022 08:11:57 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:11:57 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:11:57 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 08:12:12 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 08:12:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 08:12:13 - INFO - __main__ - Starting training!
04/25/2022 08:12:19 - INFO - __main__ - Step 10 Global step 10 Train loss 1.37 on epoch=4
04/25/2022 08:12:23 - INFO - __main__ - Step 20 Global step 20 Train loss 0.94 on epoch=9
04/25/2022 08:12:27 - INFO - __main__ - Step 30 Global step 30 Train loss 0.66 on epoch=14
04/25/2022 08:12:32 - INFO - __main__ - Step 40 Global step 40 Train loss 0.60 on epoch=19
04/25/2022 08:12:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=24
04/25/2022 08:12:39 - INFO - __main__ - Global step 50 Train loss 0.80 ACC 0.1875 on epoch=24
04/25/2022 08:12:39 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
04/25/2022 08:12:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.38 on epoch=29
04/25/2022 08:12:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.34 on epoch=34
04/25/2022 08:12:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=39
04/25/2022 08:12:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=44
04/25/2022 08:13:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.28 on epoch=49
04/25/2022 08:13:03 - INFO - __main__ - Global step 100 Train loss 0.33 ACC 0.3125 on epoch=49
04/25/2022 08:13:03 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.3125 on epoch=49, global_step=100
04/25/2022 08:13:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=54
04/25/2022 08:13:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.23 on epoch=59
04/25/2022 08:13:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.22 on epoch=64
04/25/2022 08:13:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.19 on epoch=69
04/25/2022 08:13:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.19 on epoch=74
04/25/2022 08:13:26 - INFO - __main__ - Global step 150 Train loss 0.22 ACC 0.25 on epoch=74
04/25/2022 08:13:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.15 on epoch=79
04/25/2022 08:13:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.17 on epoch=84
04/25/2022 08:13:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.15 on epoch=89
04/25/2022 08:13:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.17 on epoch=94
04/25/2022 08:13:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.14 on epoch=99
04/25/2022 08:13:49 - INFO - __main__ - Global step 200 Train loss 0.16 ACC 0.21875 on epoch=99
04/25/2022 08:13:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.15 on epoch=104
04/25/2022 08:13:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.12 on epoch=109
04/25/2022 08:14:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.12 on epoch=114
04/25/2022 08:14:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=119
04/25/2022 08:14:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=124
04/25/2022 08:14:13 - INFO - __main__ - Global step 250 Train loss 0.18 ACC 0.21875 on epoch=124
04/25/2022 08:14:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=129
04/25/2022 08:14:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=134
04/25/2022 08:14:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.09 on epoch=139
04/25/2022 08:14:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.11 on epoch=144
04/25/2022 08:14:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.10 on epoch=149
04/25/2022 08:14:37 - INFO - __main__ - Global step 300 Train loss 0.18 ACC 0.21875 on epoch=149
04/25/2022 08:14:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.09 on epoch=154
04/25/2022 08:14:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.05 on epoch=159
04/25/2022 08:14:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
04/25/2022 08:14:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.06 on epoch=169
04/25/2022 08:14:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.07 on epoch=174
04/25/2022 08:15:00 - INFO - __main__ - Global step 350 Train loss 0.08 ACC 0.25 on epoch=174
04/25/2022 08:15:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.06 on epoch=179
04/25/2022 08:15:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.05 on epoch=184
04/25/2022 08:15:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.06 on epoch=189
04/25/2022 08:15:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.06 on epoch=194
04/25/2022 08:15:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.05 on epoch=199
04/25/2022 08:15:24 - INFO - __main__ - Global step 400 Train loss 0.06 ACC 0.25 on epoch=199
04/25/2022 08:15:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.04 on epoch=204
04/25/2022 08:15:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.03 on epoch=209
04/25/2022 08:15:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.05 on epoch=214
04/25/2022 08:15:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.07 on epoch=219
04/25/2022 08:15:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.05 on epoch=224
04/25/2022 08:15:48 - INFO - __main__ - Global step 450 Train loss 0.05 ACC 0.3125 on epoch=224
04/25/2022 08:15:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.02 on epoch=229
04/25/2022 08:15:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.03 on epoch=234
04/25/2022 08:16:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.04 on epoch=239
04/25/2022 08:16:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.02 on epoch=244
04/25/2022 08:16:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.02 on epoch=249
04/25/2022 08:16:11 - INFO - __main__ - Global step 500 Train loss 0.03 ACC 0.21875 on epoch=249
04/25/2022 08:16:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.03 on epoch=254
04/25/2022 08:16:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
04/25/2022 08:16:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
04/25/2022 08:16:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
04/25/2022 08:16:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
04/25/2022 08:16:35 - INFO - __main__ - Global step 550 Train loss 0.05 ACC 0.25 on epoch=274
04/25/2022 08:16:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
04/25/2022 08:16:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
04/25/2022 08:16:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=289
04/25/2022 08:16:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
04/25/2022 08:16:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.06 on epoch=299
04/25/2022 08:16:59 - INFO - __main__ - Global step 600 Train loss 0.04 ACC 0.1875 on epoch=299
04/25/2022 08:17:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
04/25/2022 08:17:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
04/25/2022 08:17:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
04/25/2022 08:17:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
04/25/2022 08:17:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
04/25/2022 08:17:22 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.15625 on epoch=324
04/25/2022 08:17:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
04/25/2022 08:17:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
04/25/2022 08:17:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
04/25/2022 08:17:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
04/25/2022 08:17:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
04/25/2022 08:17:46 - INFO - __main__ - Global step 700 Train loss 0.02 ACC 0.21875 on epoch=349
04/25/2022 08:17:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
04/25/2022 08:17:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
04/25/2022 08:17:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
04/25/2022 08:18:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
04/25/2022 08:18:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
04/25/2022 08:18:10 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.21875 on epoch=374
04/25/2022 08:18:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
04/25/2022 08:18:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
04/25/2022 08:18:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
04/25/2022 08:18:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
04/25/2022 08:18:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
04/25/2022 08:18:33 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.1875 on epoch=399
04/25/2022 08:18:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
04/25/2022 08:18:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
04/25/2022 08:18:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
04/25/2022 08:18:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
04/25/2022 08:18:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
04/25/2022 08:18:57 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.09375 on epoch=424
04/25/2022 08:19:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
04/25/2022 08:19:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
04/25/2022 08:19:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
04/25/2022 08:19:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
04/25/2022 08:19:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
04/25/2022 08:19:21 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.15625 on epoch=449
04/25/2022 08:19:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
04/25/2022 08:19:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
04/25/2022 08:19:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
04/25/2022 08:19:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
04/25/2022 08:19:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
04/25/2022 08:19:45 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.21875 on epoch=474
04/25/2022 08:19:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/25/2022 08:19:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
04/25/2022 08:19:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
04/25/2022 08:20:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
04/25/2022 08:20:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
04/25/2022 08:20:08 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.125 on epoch=499
04/25/2022 08:20:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
04/25/2022 08:20:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
04/25/2022 08:20:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
04/25/2022 08:20:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
04/25/2022 08:20:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/25/2022 08:20:32 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.21875 on epoch=524
04/25/2022 08:20:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
04/25/2022 08:20:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
04/25/2022 08:20:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/25/2022 08:20:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
04/25/2022 08:20:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
04/25/2022 08:20:55 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.15625 on epoch=549
04/25/2022 08:20:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/25/2022 08:21:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
04/25/2022 08:21:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
04/25/2022 08:21:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
04/25/2022 08:21:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 08:21:19 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.25 on epoch=574
04/25/2022 08:21:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 08:21:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
04/25/2022 08:21:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
04/25/2022 08:21:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 08:21:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/25/2022 08:21:42 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.21875 on epoch=599
04/25/2022 08:21:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
04/25/2022 08:21:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
04/25/2022 08:21:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
04/25/2022 08:21:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
04/25/2022 08:22:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
04/25/2022 08:22:06 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.21875 on epoch=624
04/25/2022 08:22:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
04/25/2022 08:22:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 08:22:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/25/2022 08:22:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/25/2022 08:22:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
04/25/2022 08:22:30 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.21875 on epoch=649
04/25/2022 08:22:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 08:22:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 08:22:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
04/25/2022 08:22:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
04/25/2022 08:22:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 08:22:54 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.28125 on epoch=674
04/25/2022 08:22:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
04/25/2022 08:23:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 08:23:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/25/2022 08:23:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
04/25/2022 08:23:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 08:23:17 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.21875 on epoch=699
04/25/2022 08:23:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 08:23:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 08:23:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/25/2022 08:23:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/25/2022 08:23:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/25/2022 08:23:41 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.1875 on epoch=724
04/25/2022 08:23:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 08:23:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
04/25/2022 08:23:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 08:23:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 08:24:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
04/25/2022 08:24:05 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.1875 on epoch=749
04/25/2022 08:24:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
04/25/2022 08:24:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 08:24:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 08:24:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
04/25/2022 08:24:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
04/25/2022 08:24:28 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.125 on epoch=774
04/25/2022 08:24:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
04/25/2022 08:24:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
04/25/2022 08:24:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 08:24:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
04/25/2022 08:24:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
04/25/2022 08:24:52 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.125 on epoch=799
04/25/2022 08:24:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
04/25/2022 08:25:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
04/25/2022 08:25:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
04/25/2022 08:25:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
04/25/2022 08:25:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
04/25/2022 08:25:15 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.1875 on epoch=824
04/25/2022 08:25:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/25/2022 08:25:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
04/25/2022 08:25:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 08:25:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 08:25:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
04/25/2022 08:25:39 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.125 on epoch=849
04/25/2022 08:25:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
04/25/2022 08:25:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
04/25/2022 08:25:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
04/25/2022 08:25:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
04/25/2022 08:26:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 08:26:03 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.09375 on epoch=874
04/25/2022 08:26:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/25/2022 08:26:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
04/25/2022 08:26:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
04/25/2022 08:26:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
04/25/2022 08:26:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
04/25/2022 08:26:27 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.15625 on epoch=899
04/25/2022 08:26:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
04/25/2022 08:26:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
04/25/2022 08:26:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 08:26:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
04/25/2022 08:26:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
04/25/2022 08:26:50 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.25 on epoch=924
04/25/2022 08:26:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
04/25/2022 08:26:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 08:27:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 08:27:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 08:27:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 08:27:14 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.28125 on epoch=949
04/25/2022 08:27:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
04/25/2022 08:27:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 08:27:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 08:27:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
04/25/2022 08:27:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
04/25/2022 08:27:38 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.3125 on epoch=974
04/25/2022 08:27:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 08:27:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 08:27:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
04/25/2022 08:27:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
04/25/2022 08:27:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
04/25/2022 08:28:00 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:28:00 - INFO - __main__ - Printing 3 examples
04/25/2022 08:28:00 - INFO - __main__ -  [dream] Why do the woman's parents want her to do Business Studies? [SEP] M: Hi. I think I'm sitting next to you. Seat 35B. W: Oh,sorry. I'll just move my things. Hold on a minute. M: Thanks. Phwoo...I've been waiting in departure for ages. W: Mmm...The flight was delayed leaving Beijing. Security checks, you know. M: Yeah. Same here. W: Are you going to London, then? M: Yes. I'm going to the LSE to do a master's degree in International Relations. W: Really? That sounds interesting. You must be really clever. M: So are you going to London too? W: Well actually, I'm going to the University of Middlesex to do Business Studies. M: That sounds interesting. W: Well actually, it's my parents who want me to do Business Studies. I'd rather study Philosophy, but my dad wants me to take over the family business. He thinks Business Studies will make me rich. M: Well, nobody wants to be poor...ah, here's the drinks trolley. What would you like? W: A Coke, please,I'm sorry...I don't know your name... M: Oh, sorry. I forgot...Ali. [SEP]  (A) Because they want her to run a company. (B) Because they want her to be a career woman. (C) Because they want her to take over the family business.
04/25/2022 08:28:00 - INFO - __main__ - ['Because they want her to take over the family business.']
04/25/2022 08:28:00 - INFO - __main__ -  [dream] What is the woman? [SEP] M: Well, Jenny, the school year is almost over. We just have two more weeks before exams. What do you plan to do this summer? W: I'm going to teach English to some immigrants in the university's community service program. M: That sounds interesting. Don't you need to speak a foreign language for a job like that? W: No, you don't. You just have to present the language simply and give the students a chance to practice speaking. M: Come to think of it, that's the way I was taught to speak Chinese. But speaking didn't help me learn to read and write Chinese. W: My students won't want to read and write English, at lease not now. They are more interested in speaking. M: You sound very knowledgeable about all this. How do you know so much? W: I took a Teaching English as a Second Language course last year when you were in China. I've also talked with the experienced teachers quite a lot. I think I would like to be an ESL teacher when I graduate. [SEP]  (A) A student. (B) A teacher. (C) A tourist.
04/25/2022 08:28:00 - INFO - __main__ - ['A student.']
04/25/2022 08:28:00 - INFO - __main__ -  [dream] What does the man think ridiculous? [SEP] M: I don't enjoy dating anymore. I can't seem to find anyone I have anything in common with. W: Don't feel discouraged. Be patient. As you are so distinguished, you will definitely find the person who is right for you. M: To tell you the truth, I am tired of being alone. I hope to find my Mrs. Right. What should I do? W: Do you believe in Internet matchmaking service? M: That's really a new walk of life. What is it exactly? W: It helps match up singles the world over, and helps find the man and woman of their dreams. M: Oh, it must be to the taste of a certain group of people. W: The advertisement said Dream Dates has matched up thousands of singles the world over! M: Unbelievable! They must be exaggerating the figure! W: Look at the way they manage their business: they collect applicants' photos, and give the applicants questionnaires to fill out as to what type of character they are. M: I don't believe several questions can decide the type of person you're. People's characters are complicated and keep changing all the time. W: Anyway, it seems that things work well this way. The information and specifications will be entered in a large computer database. M: A computer to decide your best date? That's really ridiculous! W: Look, it promises: Dream Dates provides expert dating service and a place for singles to meet. We'll introduce you to the person uniquely qualified to be your partner. M: Sheer slogans! Not reliable! W: It says you can enroll in a free trial membership! M: I won't do it even they pay me for that! W: Well, we don't have to believe this. Maybe I can tell John, and see whether he'd like to try it. [SEP]  (A) Entering a large company without application. (B) Programming human feelings into machines. (C) Deciding one's best partner through a computer.
04/25/2022 08:28:00 - INFO - __main__ - ["Deciding one's best partner through a computer."]
04/25/2022 08:28:00 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:28:00 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:28:00 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 08:28:00 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:28:00 - INFO - __main__ - Printing 3 examples
04/25/2022 08:28:00 - INFO - __main__ -  [dream] Why can't the man concentrate on his study? [SEP] W: Come to the movies with us. Everybody needs to take a break every once in a while. M: I guess I might as well. I've been studying so long I just can't concentrate. [SEP]  (A) He keeps thinking of going to the movies. (B) His classmates are taking a break. (C) He's been studying for too long.
04/25/2022 08:28:00 - INFO - __main__ - ["He's been studying for too long."]
04/25/2022 08:28:00 - INFO - __main__ -  [dream] What has she found? [SEP] M: Have you found your key? F: Yes, I've found it. [SEP]  (A) Her bag. (B) Her key. (C) Her kite.
04/25/2022 08:28:00 - INFO - __main__ - ['Her key.']
04/25/2022 08:28:00 - INFO - __main__ -  [dream] At what temperature should you bake the cookies? [SEP] Chef Randall: Well, hello everyone, and welcome to today's show. And joining me today is my daughter, Ashley, who has had to endure my cooking experiments over the years. Are we ready, Ashley? [Ready to eat.] No, let's wait for a few minutes. We'll get to that. But as you know, my faithful listeners, I starting cooking and baking almost 30 years ago when my grandmother taught me in her humble kitchen. In fact, she taught almost me everything I know, and I've never attended cooking classes [You should have ...] Wait, wait, wait ... I know my daughter's going to mention to you faithful listeners that recently as I was helping the kids prepare for our kitchen for chicken meal, I forgot to take the chicken out of the oven, burned the bird to a crisp, and we ended up ordering pizza for dinner. Kids: We had to use the fire extinguisher. Chef Randall: But that's another story. So, anyway, today I'd like to share with you our favorite ... at least my favorite ... chocolate chip cookie recipe. Now, before you switch the TV channel, I know what you are thinking. "Another fattening cookie recipe." But wait. What makes this recipe great is that it offers a wonderful low-fat, low-calorie, low-cholesterol dessert for the entire family. Kids: We still like the fat though. Chef Randall: Well, I know we do. But let's see. We have all the ingredients, so we can start by mixing all of the ingredients, the sugars, the flour, the egg whites, the low-fat butter, vanilla, baking soda, and a pinch of salt in a large mixing bowl. Then, we add the mini chocolate chips. Now, my kids would like me to add the big ones but we start with the mini-chocolate chips. And don't forget to preheat the oven to 350 degrees (Fahrenheit). And finally, when the cookies are done, take them out of the oven, remove them from the cookie sheet, and let them cool before their fingers get into them. Did I forget anything? Kids: Yeah, if you have college-age kids, be sure to make a few extra batches they can take back to school for their roommates. And don't forget the kids still at home. Chef Randall: Oh, well yeah. We can't do that. We can't forget them. And unfortunately, by the time your kids get the cookies, you, the cook, will be left with a single cookie - your instant diet plan for you - and a dirty kitchen. So, that's all for today. On next week's show, we will be showing you how to feed hungry teenagers on a budget without having to sell the family car. Until then. [SEP]  (A) at 305 degrees (B) at 315 degrees (C) at 350 degrees
04/25/2022 08:28:00 - INFO - __main__ - ['at 350 degrees']
04/25/2022 08:28:00 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:28:00 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:28:00 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 08:28:02 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.28125 on epoch=999
04/25/2022 08:28:02 - INFO - __main__ - save last model!
04/25/2022 08:28:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 08:28:02 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 08:28:02 - INFO - __main__ - Printing 3 examples
04/25/2022 08:28:02 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 08:28:02 - INFO - __main__ - ['To change her job.']
04/25/2022 08:28:02 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 08:28:02 - INFO - __main__ - ['Watch TV.']
04/25/2022 08:28:02 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 08:28:02 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 08:28:02 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:28:03 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:28:04 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 08:28:15 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 08:28:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 08:28:16 - INFO - __main__ - Starting training!
04/25/2022 08:29:21 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_87_0.4_8_predictions.txt
04/25/2022 08:29:21 - INFO - __main__ - ACC on test data: 0.3050
04/25/2022 08:29:22 - INFO - __main__ - prefix=dream_32_87, lr=0.4, bsz=8, dev_performance=0.3125, test_performance=0.305
04/25/2022 08:29:22 - INFO - __main__ - Running ... prefix=dream_32_87, lr=0.3, bsz=8 ...
04/25/2022 08:29:23 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:29:23 - INFO - __main__ - Printing 3 examples
04/25/2022 08:29:23 - INFO - __main__ -  [dream] Why do the woman's parents want her to do Business Studies? [SEP] M: Hi. I think I'm sitting next to you. Seat 35B. W: Oh,sorry. I'll just move my things. Hold on a minute. M: Thanks. Phwoo...I've been waiting in departure for ages. W: Mmm...The flight was delayed leaving Beijing. Security checks, you know. M: Yeah. Same here. W: Are you going to London, then? M: Yes. I'm going to the LSE to do a master's degree in International Relations. W: Really? That sounds interesting. You must be really clever. M: So are you going to London too? W: Well actually, I'm going to the University of Middlesex to do Business Studies. M: That sounds interesting. W: Well actually, it's my parents who want me to do Business Studies. I'd rather study Philosophy, but my dad wants me to take over the family business. He thinks Business Studies will make me rich. M: Well, nobody wants to be poor...ah, here's the drinks trolley. What would you like? W: A Coke, please,I'm sorry...I don't know your name... M: Oh, sorry. I forgot...Ali. [SEP]  (A) Because they want her to run a company. (B) Because they want her to be a career woman. (C) Because they want her to take over the family business.
04/25/2022 08:29:23 - INFO - __main__ - ['Because they want her to take over the family business.']
04/25/2022 08:29:23 - INFO - __main__ -  [dream] What is the woman? [SEP] M: Well, Jenny, the school year is almost over. We just have two more weeks before exams. What do you plan to do this summer? W: I'm going to teach English to some immigrants in the university's community service program. M: That sounds interesting. Don't you need to speak a foreign language for a job like that? W: No, you don't. You just have to present the language simply and give the students a chance to practice speaking. M: Come to think of it, that's the way I was taught to speak Chinese. But speaking didn't help me learn to read and write Chinese. W: My students won't want to read and write English, at lease not now. They are more interested in speaking. M: You sound very knowledgeable about all this. How do you know so much? W: I took a Teaching English as a Second Language course last year when you were in China. I've also talked with the experienced teachers quite a lot. I think I would like to be an ESL teacher when I graduate. [SEP]  (A) A student. (B) A teacher. (C) A tourist.
04/25/2022 08:29:23 - INFO - __main__ - ['A student.']
04/25/2022 08:29:23 - INFO - __main__ -  [dream] What does the man think ridiculous? [SEP] M: I don't enjoy dating anymore. I can't seem to find anyone I have anything in common with. W: Don't feel discouraged. Be patient. As you are so distinguished, you will definitely find the person who is right for you. M: To tell you the truth, I am tired of being alone. I hope to find my Mrs. Right. What should I do? W: Do you believe in Internet matchmaking service? M: That's really a new walk of life. What is it exactly? W: It helps match up singles the world over, and helps find the man and woman of their dreams. M: Oh, it must be to the taste of a certain group of people. W: The advertisement said Dream Dates has matched up thousands of singles the world over! M: Unbelievable! They must be exaggerating the figure! W: Look at the way they manage their business: they collect applicants' photos, and give the applicants questionnaires to fill out as to what type of character they are. M: I don't believe several questions can decide the type of person you're. People's characters are complicated and keep changing all the time. W: Anyway, it seems that things work well this way. The information and specifications will be entered in a large computer database. M: A computer to decide your best date? That's really ridiculous! W: Look, it promises: Dream Dates provides expert dating service and a place for singles to meet. We'll introduce you to the person uniquely qualified to be your partner. M: Sheer slogans! Not reliable! W: It says you can enroll in a free trial membership! M: I won't do it even they pay me for that! W: Well, we don't have to believe this. Maybe I can tell John, and see whether he'd like to try it. [SEP]  (A) Entering a large company without application. (B) Programming human feelings into machines. (C) Deciding one's best partner through a computer.
04/25/2022 08:29:23 - INFO - __main__ - ["Deciding one's best partner through a computer."]
04/25/2022 08:29:23 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:29:23 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:29:23 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 08:29:23 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:29:23 - INFO - __main__ - Printing 3 examples
04/25/2022 08:29:23 - INFO - __main__ -  [dream] Why can't the man concentrate on his study? [SEP] W: Come to the movies with us. Everybody needs to take a break every once in a while. M: I guess I might as well. I've been studying so long I just can't concentrate. [SEP]  (A) He keeps thinking of going to the movies. (B) His classmates are taking a break. (C) He's been studying for too long.
04/25/2022 08:29:23 - INFO - __main__ - ["He's been studying for too long."]
04/25/2022 08:29:23 - INFO - __main__ -  [dream] What has she found? [SEP] M: Have you found your key? F: Yes, I've found it. [SEP]  (A) Her bag. (B) Her key. (C) Her kite.
04/25/2022 08:29:23 - INFO - __main__ - ['Her key.']
04/25/2022 08:29:23 - INFO - __main__ -  [dream] At what temperature should you bake the cookies? [SEP] Chef Randall: Well, hello everyone, and welcome to today's show. And joining me today is my daughter, Ashley, who has had to endure my cooking experiments over the years. Are we ready, Ashley? [Ready to eat.] No, let's wait for a few minutes. We'll get to that. But as you know, my faithful listeners, I starting cooking and baking almost 30 years ago when my grandmother taught me in her humble kitchen. In fact, she taught almost me everything I know, and I've never attended cooking classes [You should have ...] Wait, wait, wait ... I know my daughter's going to mention to you faithful listeners that recently as I was helping the kids prepare for our kitchen for chicken meal, I forgot to take the chicken out of the oven, burned the bird to a crisp, and we ended up ordering pizza for dinner. Kids: We had to use the fire extinguisher. Chef Randall: But that's another story. So, anyway, today I'd like to share with you our favorite ... at least my favorite ... chocolate chip cookie recipe. Now, before you switch the TV channel, I know what you are thinking. "Another fattening cookie recipe." But wait. What makes this recipe great is that it offers a wonderful low-fat, low-calorie, low-cholesterol dessert for the entire family. Kids: We still like the fat though. Chef Randall: Well, I know we do. But let's see. We have all the ingredients, so we can start by mixing all of the ingredients, the sugars, the flour, the egg whites, the low-fat butter, vanilla, baking soda, and a pinch of salt in a large mixing bowl. Then, we add the mini chocolate chips. Now, my kids would like me to add the big ones but we start with the mini-chocolate chips. And don't forget to preheat the oven to 350 degrees (Fahrenheit). And finally, when the cookies are done, take them out of the oven, remove them from the cookie sheet, and let them cool before their fingers get into them. Did I forget anything? Kids: Yeah, if you have college-age kids, be sure to make a few extra batches they can take back to school for their roommates. And don't forget the kids still at home. Chef Randall: Oh, well yeah. We can't do that. We can't forget them. And unfortunately, by the time your kids get the cookies, you, the cook, will be left with a single cookie - your instant diet plan for you - and a dirty kitchen. So, that's all for today. On next week's show, we will be showing you how to feed hungry teenagers on a budget without having to sell the family car. Until then. [SEP]  (A) at 305 degrees (B) at 315 degrees (C) at 350 degrees
04/25/2022 08:29:23 - INFO - __main__ - ['at 350 degrees']
04/25/2022 08:29:23 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:29:23 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:29:23 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 08:29:38 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 08:29:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 08:29:39 - INFO - __main__ - Starting training!
04/25/2022 08:29:43 - INFO - __main__ - Step 10 Global step 10 Train loss 1.40 on epoch=4
04/25/2022 08:29:47 - INFO - __main__ - Step 20 Global step 20 Train loss 1.05 on epoch=9
04/25/2022 08:29:51 - INFO - __main__ - Step 30 Global step 30 Train loss 0.82 on epoch=14
04/25/2022 08:29:56 - INFO - __main__ - Step 40 Global step 40 Train loss 0.70 on epoch=19
04/25/2022 08:30:00 - INFO - __main__ - Step 50 Global step 50 Train loss 0.60 on epoch=24
04/25/2022 08:30:03 - INFO - __main__ - Global step 50 Train loss 0.92 ACC 0.25 on epoch=24
04/25/2022 08:30:03 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.25 on epoch=24, global_step=50
04/25/2022 08:30:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=29
04/25/2022 08:30:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.39 on epoch=34
04/25/2022 08:30:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
04/25/2022 08:30:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.33 on epoch=44
04/25/2022 08:30:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=49
04/25/2022 08:30:27 - INFO - __main__ - Global step 100 Train loss 0.37 ACC 0.09375 on epoch=49
04/25/2022 08:30:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=54
04/25/2022 08:30:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.23 on epoch=59
04/25/2022 08:30:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.24 on epoch=64
04/25/2022 08:30:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.20 on epoch=69
04/25/2022 08:30:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.23 on epoch=74
04/25/2022 08:30:50 - INFO - __main__ - Global step 150 Train loss 0.23 ACC 0.125 on epoch=74
04/25/2022 08:30:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.15 on epoch=79
04/25/2022 08:30:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=84
04/25/2022 08:31:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
04/25/2022 08:31:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=94
04/25/2022 08:31:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.31 on epoch=99
04/25/2022 08:31:14 - INFO - __main__ - Global step 200 Train loss 0.29 ACC 0.1875 on epoch=99
04/25/2022 08:31:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=104
04/25/2022 08:31:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
04/25/2022 08:31:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.14 on epoch=114
04/25/2022 08:31:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.18 on epoch=119
04/25/2022 08:31:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.18 on epoch=124
04/25/2022 08:31:37 - INFO - __main__ - Global step 250 Train loss 0.18 ACC 0.125 on epoch=124
04/25/2022 08:31:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.15 on epoch=129
04/25/2022 08:31:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.17 on epoch=134
04/25/2022 08:31:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.16 on epoch=139
04/25/2022 08:31:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.15 on epoch=144
04/25/2022 08:31:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.16 on epoch=149
04/25/2022 08:32:01 - INFO - __main__ - Global step 300 Train loss 0.16 ACC 0.15625 on epoch=149
04/25/2022 08:32:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.16 on epoch=154
04/25/2022 08:32:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=159
04/25/2022 08:32:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=164
04/25/2022 08:32:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.17 on epoch=169
04/25/2022 08:32:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
04/25/2022 08:32:24 - INFO - __main__ - Global step 350 Train loss 0.15 ACC 0.1875 on epoch=174
04/25/2022 08:32:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
04/25/2022 08:32:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.13 on epoch=184
04/25/2022 08:32:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.17 on epoch=189
04/25/2022 08:32:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.14 on epoch=194
04/25/2022 08:32:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=199
04/25/2022 08:32:48 - INFO - __main__ - Global step 400 Train loss 0.14 ACC 0.28125 on epoch=199
04/25/2022 08:32:48 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=199, global_step=400
04/25/2022 08:32:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.12 on epoch=204
04/25/2022 08:32:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.12 on epoch=209
04/25/2022 08:33:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.14 on epoch=214
04/25/2022 08:33:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.10 on epoch=219
04/25/2022 08:33:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=224
04/25/2022 08:33:11 - INFO - __main__ - Global step 450 Train loss 0.13 ACC 0.28125 on epoch=224
04/25/2022 08:33:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=229
04/25/2022 08:33:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=234
04/25/2022 08:33:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
04/25/2022 08:33:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.14 on epoch=244
04/25/2022 08:33:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=249
04/25/2022 08:33:35 - INFO - __main__ - Global step 500 Train loss 0.15 ACC 0.25 on epoch=249
04/25/2022 08:33:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=254
04/25/2022 08:33:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.13 on epoch=259
04/25/2022 08:33:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
04/25/2022 08:33:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=269
04/25/2022 08:33:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=274
04/25/2022 08:33:58 - INFO - __main__ - Global step 550 Train loss 0.11 ACC 0.1875 on epoch=274
04/25/2022 08:34:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=279
04/25/2022 08:34:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=284
04/25/2022 08:34:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=289
04/25/2022 08:34:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
04/25/2022 08:34:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=299
04/25/2022 08:34:22 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.1875 on epoch=299
04/25/2022 08:34:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.10 on epoch=304
04/25/2022 08:34:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=309
04/25/2022 08:34:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=314
04/25/2022 08:34:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.09 on epoch=319
04/25/2022 08:34:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=324
04/25/2022 08:34:45 - INFO - __main__ - Global step 650 Train loss 0.09 ACC 0.28125 on epoch=324
04/25/2022 08:34:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=329
04/25/2022 08:34:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=334
04/25/2022 08:34:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=339
04/25/2022 08:35:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=344
04/25/2022 08:35:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=349
04/25/2022 08:35:09 - INFO - __main__ - Global step 700 Train loss 0.10 ACC 0.25 on epoch=349
04/25/2022 08:35:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.09 on epoch=354
04/25/2022 08:35:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=359
04/25/2022 08:35:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=364
04/25/2022 08:35:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=369
04/25/2022 08:35:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=374
04/25/2022 08:35:33 - INFO - __main__ - Global step 750 Train loss 0.09 ACC 0.28125 on epoch=374
04/25/2022 08:35:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=379
04/25/2022 08:35:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
04/25/2022 08:35:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=389
04/25/2022 08:35:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
04/25/2022 08:35:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
04/25/2022 08:35:56 - INFO - __main__ - Global step 800 Train loss 0.08 ACC 0.28125 on epoch=399
04/25/2022 08:36:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
04/25/2022 08:36:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=409
04/25/2022 08:36:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=414
04/25/2022 08:36:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=419
04/25/2022 08:36:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=424
04/25/2022 08:36:20 - INFO - __main__ - Global step 850 Train loss 0.08 ACC 0.21875 on epoch=424
04/25/2022 08:36:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
04/25/2022 08:36:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=434
04/25/2022 08:36:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=439
04/25/2022 08:36:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
04/25/2022 08:36:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=449
04/25/2022 08:36:43 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.21875 on epoch=449
04/25/2022 08:36:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=454
04/25/2022 08:36:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=459
04/25/2022 08:36:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=464
04/25/2022 08:37:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=469
04/25/2022 08:37:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
04/25/2022 08:37:07 - INFO - __main__ - Global step 950 Train loss 0.07 ACC 0.21875 on epoch=474
04/25/2022 08:37:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=479
04/25/2022 08:37:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
04/25/2022 08:37:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
04/25/2022 08:37:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
04/25/2022 08:37:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/25/2022 08:37:31 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.21875 on epoch=499
04/25/2022 08:37:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
04/25/2022 08:37:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=509
04/25/2022 08:37:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
04/25/2022 08:37:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
04/25/2022 08:37:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
04/25/2022 08:37:54 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.28125 on epoch=524
04/25/2022 08:37:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
04/25/2022 08:38:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
04/25/2022 08:38:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=539
04/25/2022 08:38:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
04/25/2022 08:38:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
04/25/2022 08:38:18 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.1875 on epoch=549
04/25/2022 08:38:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
04/25/2022 08:38:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
04/25/2022 08:38:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
04/25/2022 08:38:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
04/25/2022 08:38:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
04/25/2022 08:38:42 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.25 on epoch=574
04/25/2022 08:38:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
04/25/2022 08:38:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/25/2022 08:38:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/25/2022 08:38:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
04/25/2022 08:39:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
04/25/2022 08:39:06 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.21875 on epoch=599
04/25/2022 08:39:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/25/2022 08:39:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=609
04/25/2022 08:39:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
04/25/2022 08:39:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
04/25/2022 08:39:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
04/25/2022 08:39:30 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.28125 on epoch=624
04/25/2022 08:39:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
04/25/2022 08:39:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
04/25/2022 08:39:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
04/25/2022 08:39:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
04/25/2022 08:39:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/25/2022 08:39:53 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.28125 on epoch=649
04/25/2022 08:39:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
04/25/2022 08:40:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
04/25/2022 08:40:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
04/25/2022 08:40:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/25/2022 08:40:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
04/25/2022 08:40:17 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.25 on epoch=674
04/25/2022 08:40:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/25/2022 08:40:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/25/2022 08:40:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
04/25/2022 08:40:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
04/25/2022 08:40:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
04/25/2022 08:40:41 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.375 on epoch=699
04/25/2022 08:40:41 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.375 on epoch=699, global_step=1400
04/25/2022 08:40:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
04/25/2022 08:40:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
04/25/2022 08:40:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
04/25/2022 08:40:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 08:41:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
04/25/2022 08:41:05 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.28125 on epoch=724
04/25/2022 08:41:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/25/2022 08:41:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/25/2022 08:41:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
04/25/2022 08:41:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
04/25/2022 08:41:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 08:41:28 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.25 on epoch=749
04/25/2022 08:41:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
04/25/2022 08:41:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=759
04/25/2022 08:41:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 08:41:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 08:41:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/25/2022 08:41:52 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.34375 on epoch=774
04/25/2022 08:41:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/25/2022 08:42:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/25/2022 08:42:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
04/25/2022 08:42:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 08:42:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
04/25/2022 08:42:16 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.3125 on epoch=799
04/25/2022 08:42:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 08:42:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
04/25/2022 08:42:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 08:42:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 08:42:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 08:42:40 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.21875 on epoch=824
04/25/2022 08:42:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/25/2022 08:42:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 08:42:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 08:42:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 08:43:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 08:43:04 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.3125 on epoch=849
04/25/2022 08:43:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 08:43:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 08:43:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 08:43:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 08:43:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 08:43:27 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.3125 on epoch=874
04/25/2022 08:43:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/25/2022 08:43:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
04/25/2022 08:43:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 08:43:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
04/25/2022 08:43:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
04/25/2022 08:43:51 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.3125 on epoch=899
04/25/2022 08:43:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 08:43:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
04/25/2022 08:44:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 08:44:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 08:44:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
04/25/2022 08:44:15 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.25 on epoch=924
04/25/2022 08:44:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
04/25/2022 08:44:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
04/25/2022 08:44:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 08:44:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 08:44:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
04/25/2022 08:44:38 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.375 on epoch=949
04/25/2022 08:44:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
04/25/2022 08:44:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
04/25/2022 08:44:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 08:44:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 08:44:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 08:45:02 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.34375 on epoch=974
04/25/2022 08:45:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 08:45:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 08:45:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/25/2022 08:45:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 08:45:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 08:45:24 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:45:24 - INFO - __main__ - Printing 3 examples
04/25/2022 08:45:24 - INFO - __main__ -  [dream] Why do the woman's parents want her to do Business Studies? [SEP] M: Hi. I think I'm sitting next to you. Seat 35B. W: Oh,sorry. I'll just move my things. Hold on a minute. M: Thanks. Phwoo...I've been waiting in departure for ages. W: Mmm...The flight was delayed leaving Beijing. Security checks, you know. M: Yeah. Same here. W: Are you going to London, then? M: Yes. I'm going to the LSE to do a master's degree in International Relations. W: Really? That sounds interesting. You must be really clever. M: So are you going to London too? W: Well actually, I'm going to the University of Middlesex to do Business Studies. M: That sounds interesting. W: Well actually, it's my parents who want me to do Business Studies. I'd rather study Philosophy, but my dad wants me to take over the family business. He thinks Business Studies will make me rich. M: Well, nobody wants to be poor...ah, here's the drinks trolley. What would you like? W: A Coke, please,I'm sorry...I don't know your name... M: Oh, sorry. I forgot...Ali. [SEP]  (A) Because they want her to run a company. (B) Because they want her to be a career woman. (C) Because they want her to take over the family business.
04/25/2022 08:45:24 - INFO - __main__ - ['Because they want her to take over the family business.']
04/25/2022 08:45:24 - INFO - __main__ -  [dream] What is the woman? [SEP] M: Well, Jenny, the school year is almost over. We just have two more weeks before exams. What do you plan to do this summer? W: I'm going to teach English to some immigrants in the university's community service program. M: That sounds interesting. Don't you need to speak a foreign language for a job like that? W: No, you don't. You just have to present the language simply and give the students a chance to practice speaking. M: Come to think of it, that's the way I was taught to speak Chinese. But speaking didn't help me learn to read and write Chinese. W: My students won't want to read and write English, at lease not now. They are more interested in speaking. M: You sound very knowledgeable about all this. How do you know so much? W: I took a Teaching English as a Second Language course last year when you were in China. I've also talked with the experienced teachers quite a lot. I think I would like to be an ESL teacher when I graduate. [SEP]  (A) A student. (B) A teacher. (C) A tourist.
04/25/2022 08:45:24 - INFO - __main__ - ['A student.']
04/25/2022 08:45:24 - INFO - __main__ -  [dream] What does the man think ridiculous? [SEP] M: I don't enjoy dating anymore. I can't seem to find anyone I have anything in common with. W: Don't feel discouraged. Be patient. As you are so distinguished, you will definitely find the person who is right for you. M: To tell you the truth, I am tired of being alone. I hope to find my Mrs. Right. What should I do? W: Do you believe in Internet matchmaking service? M: That's really a new walk of life. What is it exactly? W: It helps match up singles the world over, and helps find the man and woman of their dreams. M: Oh, it must be to the taste of a certain group of people. W: The advertisement said Dream Dates has matched up thousands of singles the world over! M: Unbelievable! They must be exaggerating the figure! W: Look at the way they manage their business: they collect applicants' photos, and give the applicants questionnaires to fill out as to what type of character they are. M: I don't believe several questions can decide the type of person you're. People's characters are complicated and keep changing all the time. W: Anyway, it seems that things work well this way. The information and specifications will be entered in a large computer database. M: A computer to decide your best date? That's really ridiculous! W: Look, it promises: Dream Dates provides expert dating service and a place for singles to meet. We'll introduce you to the person uniquely qualified to be your partner. M: Sheer slogans! Not reliable! W: It says you can enroll in a free trial membership! M: I won't do it even they pay me for that! W: Well, we don't have to believe this. Maybe I can tell John, and see whether he'd like to try it. [SEP]  (A) Entering a large company without application. (B) Programming human feelings into machines. (C) Deciding one's best partner through a computer.
04/25/2022 08:45:24 - INFO - __main__ - ["Deciding one's best partner through a computer."]
04/25/2022 08:45:24 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:45:24 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:45:24 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 08:45:24 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:45:24 - INFO - __main__ - Printing 3 examples
04/25/2022 08:45:24 - INFO - __main__ -  [dream] Why can't the man concentrate on his study? [SEP] W: Come to the movies with us. Everybody needs to take a break every once in a while. M: I guess I might as well. I've been studying so long I just can't concentrate. [SEP]  (A) He keeps thinking of going to the movies. (B) His classmates are taking a break. (C) He's been studying for too long.
04/25/2022 08:45:24 - INFO - __main__ - ["He's been studying for too long."]
04/25/2022 08:45:24 - INFO - __main__ -  [dream] What has she found? [SEP] M: Have you found your key? F: Yes, I've found it. [SEP]  (A) Her bag. (B) Her key. (C) Her kite.
04/25/2022 08:45:24 - INFO - __main__ - ['Her key.']
04/25/2022 08:45:24 - INFO - __main__ -  [dream] At what temperature should you bake the cookies? [SEP] Chef Randall: Well, hello everyone, and welcome to today's show. And joining me today is my daughter, Ashley, who has had to endure my cooking experiments over the years. Are we ready, Ashley? [Ready to eat.] No, let's wait for a few minutes. We'll get to that. But as you know, my faithful listeners, I starting cooking and baking almost 30 years ago when my grandmother taught me in her humble kitchen. In fact, she taught almost me everything I know, and I've never attended cooking classes [You should have ...] Wait, wait, wait ... I know my daughter's going to mention to you faithful listeners that recently as I was helping the kids prepare for our kitchen for chicken meal, I forgot to take the chicken out of the oven, burned the bird to a crisp, and we ended up ordering pizza for dinner. Kids: We had to use the fire extinguisher. Chef Randall: But that's another story. So, anyway, today I'd like to share with you our favorite ... at least my favorite ... chocolate chip cookie recipe. Now, before you switch the TV channel, I know what you are thinking. "Another fattening cookie recipe." But wait. What makes this recipe great is that it offers a wonderful low-fat, low-calorie, low-cholesterol dessert for the entire family. Kids: We still like the fat though. Chef Randall: Well, I know we do. But let's see. We have all the ingredients, so we can start by mixing all of the ingredients, the sugars, the flour, the egg whites, the low-fat butter, vanilla, baking soda, and a pinch of salt in a large mixing bowl. Then, we add the mini chocolate chips. Now, my kids would like me to add the big ones but we start with the mini-chocolate chips. And don't forget to preheat the oven to 350 degrees (Fahrenheit). And finally, when the cookies are done, take them out of the oven, remove them from the cookie sheet, and let them cool before their fingers get into them. Did I forget anything? Kids: Yeah, if you have college-age kids, be sure to make a few extra batches they can take back to school for their roommates. And don't forget the kids still at home. Chef Randall: Oh, well yeah. We can't do that. We can't forget them. And unfortunately, by the time your kids get the cookies, you, the cook, will be left with a single cookie - your instant diet plan for you - and a dirty kitchen. So, that's all for today. On next week's show, we will be showing you how to feed hungry teenagers on a budget without having to sell the family car. Until then. [SEP]  (A) at 305 degrees (B) at 315 degrees (C) at 350 degrees
04/25/2022 08:45:24 - INFO - __main__ - ['at 350 degrees']
04/25/2022 08:45:24 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:45:24 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:45:24 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 08:45:26 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.34375 on epoch=999
04/25/2022 08:45:26 - INFO - __main__ - save last model!
04/25/2022 08:45:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 08:45:26 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 08:45:26 - INFO - __main__ - Printing 3 examples
04/25/2022 08:45:26 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 08:45:26 - INFO - __main__ - ['To change her job.']
04/25/2022 08:45:26 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 08:45:26 - INFO - __main__ - ['Watch TV.']
04/25/2022 08:45:26 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 08:45:26 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 08:45:26 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:45:27 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:45:28 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 08:45:43 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 08:45:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 08:45:44 - INFO - __main__ - Starting training!
04/25/2022 08:46:47 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_87_0.3_8_predictions.txt
04/25/2022 08:46:47 - INFO - __main__ - ACC on test data: 0.3170
04/25/2022 08:46:48 - INFO - __main__ - prefix=dream_32_87, lr=0.3, bsz=8, dev_performance=0.375, test_performance=0.317
04/25/2022 08:46:48 - INFO - __main__ - Running ... prefix=dream_32_87, lr=0.2, bsz=8 ...
04/25/2022 08:46:49 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:46:49 - INFO - __main__ - Printing 3 examples
04/25/2022 08:46:49 - INFO - __main__ -  [dream] Why do the woman's parents want her to do Business Studies? [SEP] M: Hi. I think I'm sitting next to you. Seat 35B. W: Oh,sorry. I'll just move my things. Hold on a minute. M: Thanks. Phwoo...I've been waiting in departure for ages. W: Mmm...The flight was delayed leaving Beijing. Security checks, you know. M: Yeah. Same here. W: Are you going to London, then? M: Yes. I'm going to the LSE to do a master's degree in International Relations. W: Really? That sounds interesting. You must be really clever. M: So are you going to London too? W: Well actually, I'm going to the University of Middlesex to do Business Studies. M: That sounds interesting. W: Well actually, it's my parents who want me to do Business Studies. I'd rather study Philosophy, but my dad wants me to take over the family business. He thinks Business Studies will make me rich. M: Well, nobody wants to be poor...ah, here's the drinks trolley. What would you like? W: A Coke, please,I'm sorry...I don't know your name... M: Oh, sorry. I forgot...Ali. [SEP]  (A) Because they want her to run a company. (B) Because they want her to be a career woman. (C) Because they want her to take over the family business.
04/25/2022 08:46:49 - INFO - __main__ - ['Because they want her to take over the family business.']
04/25/2022 08:46:49 - INFO - __main__ -  [dream] What is the woman? [SEP] M: Well, Jenny, the school year is almost over. We just have two more weeks before exams. What do you plan to do this summer? W: I'm going to teach English to some immigrants in the university's community service program. M: That sounds interesting. Don't you need to speak a foreign language for a job like that? W: No, you don't. You just have to present the language simply and give the students a chance to practice speaking. M: Come to think of it, that's the way I was taught to speak Chinese. But speaking didn't help me learn to read and write Chinese. W: My students won't want to read and write English, at lease not now. They are more interested in speaking. M: You sound very knowledgeable about all this. How do you know so much? W: I took a Teaching English as a Second Language course last year when you were in China. I've also talked with the experienced teachers quite a lot. I think I would like to be an ESL teacher when I graduate. [SEP]  (A) A student. (B) A teacher. (C) A tourist.
04/25/2022 08:46:49 - INFO - __main__ - ['A student.']
04/25/2022 08:46:49 - INFO - __main__ -  [dream] What does the man think ridiculous? [SEP] M: I don't enjoy dating anymore. I can't seem to find anyone I have anything in common with. W: Don't feel discouraged. Be patient. As you are so distinguished, you will definitely find the person who is right for you. M: To tell you the truth, I am tired of being alone. I hope to find my Mrs. Right. What should I do? W: Do you believe in Internet matchmaking service? M: That's really a new walk of life. What is it exactly? W: It helps match up singles the world over, and helps find the man and woman of their dreams. M: Oh, it must be to the taste of a certain group of people. W: The advertisement said Dream Dates has matched up thousands of singles the world over! M: Unbelievable! They must be exaggerating the figure! W: Look at the way they manage their business: they collect applicants' photos, and give the applicants questionnaires to fill out as to what type of character they are. M: I don't believe several questions can decide the type of person you're. People's characters are complicated and keep changing all the time. W: Anyway, it seems that things work well this way. The information and specifications will be entered in a large computer database. M: A computer to decide your best date? That's really ridiculous! W: Look, it promises: Dream Dates provides expert dating service and a place for singles to meet. We'll introduce you to the person uniquely qualified to be your partner. M: Sheer slogans! Not reliable! W: It says you can enroll in a free trial membership! M: I won't do it even they pay me for that! W: Well, we don't have to believe this. Maybe I can tell John, and see whether he'd like to try it. [SEP]  (A) Entering a large company without application. (B) Programming human feelings into machines. (C) Deciding one's best partner through a computer.
04/25/2022 08:46:49 - INFO - __main__ - ["Deciding one's best partner through a computer."]
04/25/2022 08:46:49 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:46:49 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:46:49 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 08:46:49 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 08:46:49 - INFO - __main__ - Printing 3 examples
04/25/2022 08:46:49 - INFO - __main__ -  [dream] Why can't the man concentrate on his study? [SEP] W: Come to the movies with us. Everybody needs to take a break every once in a while. M: I guess I might as well. I've been studying so long I just can't concentrate. [SEP]  (A) He keeps thinking of going to the movies. (B) His classmates are taking a break. (C) He's been studying for too long.
04/25/2022 08:46:49 - INFO - __main__ - ["He's been studying for too long."]
04/25/2022 08:46:49 - INFO - __main__ -  [dream] What has she found? [SEP] M: Have you found your key? F: Yes, I've found it. [SEP]  (A) Her bag. (B) Her key. (C) Her kite.
04/25/2022 08:46:49 - INFO - __main__ - ['Her key.']
04/25/2022 08:46:49 - INFO - __main__ -  [dream] At what temperature should you bake the cookies? [SEP] Chef Randall: Well, hello everyone, and welcome to today's show. And joining me today is my daughter, Ashley, who has had to endure my cooking experiments over the years. Are we ready, Ashley? [Ready to eat.] No, let's wait for a few minutes. We'll get to that. But as you know, my faithful listeners, I starting cooking and baking almost 30 years ago when my grandmother taught me in her humble kitchen. In fact, she taught almost me everything I know, and I've never attended cooking classes [You should have ...] Wait, wait, wait ... I know my daughter's going to mention to you faithful listeners that recently as I was helping the kids prepare for our kitchen for chicken meal, I forgot to take the chicken out of the oven, burned the bird to a crisp, and we ended up ordering pizza for dinner. Kids: We had to use the fire extinguisher. Chef Randall: But that's another story. So, anyway, today I'd like to share with you our favorite ... at least my favorite ... chocolate chip cookie recipe. Now, before you switch the TV channel, I know what you are thinking. "Another fattening cookie recipe." But wait. What makes this recipe great is that it offers a wonderful low-fat, low-calorie, low-cholesterol dessert for the entire family. Kids: We still like the fat though. Chef Randall: Well, I know we do. But let's see. We have all the ingredients, so we can start by mixing all of the ingredients, the sugars, the flour, the egg whites, the low-fat butter, vanilla, baking soda, and a pinch of salt in a large mixing bowl. Then, we add the mini chocolate chips. Now, my kids would like me to add the big ones but we start with the mini-chocolate chips. And don't forget to preheat the oven to 350 degrees (Fahrenheit). And finally, when the cookies are done, take them out of the oven, remove them from the cookie sheet, and let them cool before their fingers get into them. Did I forget anything? Kids: Yeah, if you have college-age kids, be sure to make a few extra batches they can take back to school for their roommates. And don't forget the kids still at home. Chef Randall: Oh, well yeah. We can't do that. We can't forget them. And unfortunately, by the time your kids get the cookies, you, the cook, will be left with a single cookie - your instant diet plan for you - and a dirty kitchen. So, that's all for today. On next week's show, we will be showing you how to feed hungry teenagers on a budget without having to sell the family car. Until then. [SEP]  (A) at 305 degrees (B) at 315 degrees (C) at 350 degrees
04/25/2022 08:46:49 - INFO - __main__ - ['at 350 degrees']
04/25/2022 08:46:49 - INFO - __main__ - Tokenizing Input ...
04/25/2022 08:46:49 - INFO - __main__ - Tokenizing Output ...
04/25/2022 08:46:49 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 08:47:07 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 08:47:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 08:47:08 - INFO - __main__ - Starting training!
04/25/2022 08:47:13 - INFO - __main__ - Step 10 Global step 10 Train loss 1.46 on epoch=4
04/25/2022 08:47:17 - INFO - __main__ - Step 20 Global step 20 Train loss 1.20 on epoch=9
04/25/2022 08:47:21 - INFO - __main__ - Step 30 Global step 30 Train loss 1.02 on epoch=14
04/25/2022 08:47:25 - INFO - __main__ - Step 40 Global step 40 Train loss 0.90 on epoch=19
04/25/2022 08:47:29 - INFO - __main__ - Step 50 Global step 50 Train loss 0.76 on epoch=24
04/25/2022 08:47:34 - INFO - __main__ - Global step 50 Train loss 1.07 ACC 0.125 on epoch=24
04/25/2022 08:47:34 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.125 on epoch=24, global_step=50
04/25/2022 08:47:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.65 on epoch=29
04/25/2022 08:47:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=34
04/25/2022 08:47:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=39
04/25/2022 08:47:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=44
04/25/2022 08:47:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=49
04/25/2022 08:47:58 - INFO - __main__ - Global step 100 Train loss 0.50 ACC 0.21875 on epoch=49
04/25/2022 08:47:58 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.21875 on epoch=49, global_step=100
04/25/2022 08:48:02 - INFO - __main__ - Step 110 Global step 110 Train loss 0.34 on epoch=54
04/25/2022 08:48:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.34 on epoch=59
04/25/2022 08:48:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
04/25/2022 08:48:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=69
04/25/2022 08:48:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.31 on epoch=74
04/25/2022 08:48:21 - INFO - __main__ - Global step 150 Train loss 0.31 ACC 0.28125 on epoch=74
04/25/2022 08:48:21 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.28125 on epoch=74, global_step=150
04/25/2022 08:48:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
04/25/2022 08:48:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.21 on epoch=84
04/25/2022 08:48:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
04/25/2022 08:48:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
04/25/2022 08:48:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
04/25/2022 08:48:45 - INFO - __main__ - Global step 200 Train loss 0.23 ACC 0.125 on epoch=99
04/25/2022 08:48:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.21 on epoch=104
04/25/2022 08:48:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
04/25/2022 08:48:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=114
04/25/2022 08:49:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.20 on epoch=119
04/25/2022 08:49:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.17 on epoch=124
04/25/2022 08:49:09 - INFO - __main__ - Global step 250 Train loss 0.21 ACC 0.21875 on epoch=124
04/25/2022 08:49:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.17 on epoch=129
04/25/2022 08:49:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
04/25/2022 08:49:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.15 on epoch=139
04/25/2022 08:49:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.18 on epoch=144
04/25/2022 08:49:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.11 on epoch=149
04/25/2022 08:49:32 - INFO - __main__ - Global step 300 Train loss 0.15 ACC 0.28125 on epoch=149
04/25/2022 08:49:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=154
04/25/2022 08:49:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.12 on epoch=159
04/25/2022 08:49:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.10 on epoch=164
04/25/2022 08:49:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
04/25/2022 08:49:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.14 on epoch=174
04/25/2022 08:49:56 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.34375 on epoch=174
04/25/2022 08:49:56 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.34375 on epoch=174, global_step=350
04/25/2022 08:50:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.11 on epoch=179
04/25/2022 08:50:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
04/25/2022 08:50:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.15 on epoch=189
04/25/2022 08:50:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.10 on epoch=194
04/25/2022 08:50:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.12 on epoch=199
04/25/2022 08:50:19 - INFO - __main__ - Global step 400 Train loss 0.13 ACC 0.34375 on epoch=199
04/25/2022 08:50:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.09 on epoch=204
04/25/2022 08:50:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
04/25/2022 08:50:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
04/25/2022 08:50:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
04/25/2022 08:50:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
04/25/2022 08:50:43 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.3125 on epoch=224
04/25/2022 08:50:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=229
04/25/2022 08:50:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=234
04/25/2022 08:50:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.09 on epoch=239
04/25/2022 08:50:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.06 on epoch=244
04/25/2022 08:51:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=249
04/25/2022 08:51:06 - INFO - __main__ - Global step 500 Train loss 0.08 ACC 0.28125 on epoch=249
04/25/2022 08:51:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
04/25/2022 08:51:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=259
04/25/2022 08:51:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=264
04/25/2022 08:51:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
04/25/2022 08:51:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
04/25/2022 08:51:30 - INFO - __main__ - Global step 550 Train loss 0.08 ACC 0.28125 on epoch=274
04/25/2022 08:51:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
04/25/2022 08:51:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
04/25/2022 08:51:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
04/25/2022 08:51:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
04/25/2022 08:51:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
04/25/2022 08:51:54 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.25 on epoch=299
04/25/2022 08:51:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
04/25/2022 08:52:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
04/25/2022 08:52:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
04/25/2022 08:52:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
04/25/2022 08:52:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
04/25/2022 08:52:17 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.375 on epoch=324
04/25/2022 08:52:17 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=324, global_step=650
04/25/2022 08:52:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
04/25/2022 08:52:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=334
04/25/2022 08:52:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
04/25/2022 08:52:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
04/25/2022 08:52:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
04/25/2022 08:52:41 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.21875 on epoch=349
04/25/2022 08:52:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
04/25/2022 08:52:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
04/25/2022 08:52:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
04/25/2022 08:52:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
04/25/2022 08:53:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
04/25/2022 08:53:04 - INFO - __main__ - Global step 750 Train loss 0.05 ACC 0.1875 on epoch=374
04/25/2022 08:53:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=379
04/25/2022 08:53:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
04/25/2022 08:53:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
04/25/2022 08:53:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
04/25/2022 08:53:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
04/25/2022 08:53:28 - INFO - __main__ - Global step 800 Train loss 0.04 ACC 0.25 on epoch=399
04/25/2022 08:53:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
04/25/2022 08:53:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
04/25/2022 08:53:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
04/25/2022 08:53:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=419
04/25/2022 08:53:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
04/25/2022 08:53:52 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.25 on epoch=424
04/25/2022 08:53:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
04/25/2022 08:54:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
04/25/2022 08:54:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
04/25/2022 08:54:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
04/25/2022 08:54:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
04/25/2022 08:54:16 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.1875 on epoch=449
04/25/2022 08:54:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
04/25/2022 08:54:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
04/25/2022 08:54:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
04/25/2022 08:54:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
04/25/2022 08:54:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
04/25/2022 08:54:39 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.1875 on epoch=474
04/25/2022 08:54:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
04/25/2022 08:54:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
04/25/2022 08:54:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
04/25/2022 08:54:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
04/25/2022 08:55:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/25/2022 08:55:03 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.25 on epoch=499
04/25/2022 08:55:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
04/25/2022 08:55:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
04/25/2022 08:55:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
04/25/2022 08:55:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
04/25/2022 08:55:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
04/25/2022 08:55:26 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.25 on epoch=524
04/25/2022 08:55:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
04/25/2022 08:55:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
04/25/2022 08:55:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
04/25/2022 08:55:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
04/25/2022 08:55:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/25/2022 08:55:50 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.25 on epoch=549
04/25/2022 08:55:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/25/2022 08:55:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
04/25/2022 08:56:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/25/2022 08:56:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/25/2022 08:56:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
04/25/2022 08:56:14 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.21875 on epoch=574
04/25/2022 08:56:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/25/2022 08:56:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
04/25/2022 08:56:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
04/25/2022 08:56:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 08:56:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/25/2022 08:56:38 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.34375 on epoch=599
04/25/2022 08:56:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
04/25/2022 08:56:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 08:56:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 08:56:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
04/25/2022 08:56:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/25/2022 08:57:01 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.3125 on epoch=624
04/25/2022 08:57:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 08:57:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
04/25/2022 08:57:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/25/2022 08:57:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/25/2022 08:57:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/25/2022 08:57:24 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.28125 on epoch=649
04/25/2022 08:57:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 08:57:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 08:57:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/25/2022 08:57:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
04/25/2022 08:57:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 08:57:48 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.25 on epoch=674
04/25/2022 08:57:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 08:57:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 08:58:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/25/2022 08:58:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
04/25/2022 08:58:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 08:58:11 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.1875 on epoch=699
04/25/2022 08:58:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 08:58:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 08:58:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 08:58:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 08:58:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/25/2022 08:58:35 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.28125 on epoch=724
04/25/2022 08:58:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 08:58:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/25/2022 08:58:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
04/25/2022 08:58:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 08:58:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 08:58:59 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.3125 on epoch=749
04/25/2022 08:59:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 08:59:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
04/25/2022 08:59:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 08:59:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 08:59:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
04/25/2022 08:59:22 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.3125 on epoch=774
04/25/2022 08:59:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
04/25/2022 08:59:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 08:59:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 08:59:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 08:59:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
04/25/2022 08:59:46 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.3125 on epoch=799
04/25/2022 08:59:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/25/2022 08:59:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/25/2022 08:59:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/25/2022 09:00:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
04/25/2022 09:00:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
04/25/2022 09:00:10 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.28125 on epoch=824
04/25/2022 09:00:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
04/25/2022 09:00:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 09:00:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 09:00:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/25/2022 09:00:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 09:00:34 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.375 on epoch=849
04/25/2022 09:00:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 09:00:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
04/25/2022 09:00:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 09:00:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 09:00:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 09:00:58 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.125 on epoch=874
04/25/2022 09:01:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/25/2022 09:01:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 09:01:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 09:01:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 09:01:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 09:01:21 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.25 on epoch=899
04/25/2022 09:01:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
04/25/2022 09:01:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/25/2022 09:01:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
04/25/2022 09:01:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 09:01:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
04/25/2022 09:01:45 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.28125 on epoch=924
04/25/2022 09:01:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 09:01:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 09:01:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
04/25/2022 09:02:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
04/25/2022 09:02:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
04/25/2022 09:02:09 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.21875 on epoch=949
04/25/2022 09:02:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 09:02:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 09:02:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 09:02:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 09:02:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
04/25/2022 09:02:33 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.34375 on epoch=974
04/25/2022 09:02:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 09:02:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 09:02:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/25/2022 09:02:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 09:02:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 09:02:56 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.28125 on epoch=999
04/25/2022 09:02:56 - INFO - __main__ - save last model!
04/25/2022 09:02:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 09:02:56 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 09:02:56 - INFO - __main__ - Printing 3 examples
04/25/2022 09:02:56 - INFO - __main__ -  [dream] What's the woman probably going to do? [SEP] M: How long have you been teaching in this middle school? W: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new. [SEP]  (A) To teach a different textbook. (B) To change her job. (C) To learn a different textbook.
04/25/2022 09:02:56 - INFO - __main__ - ['To change her job.']
04/25/2022 09:02:56 - INFO - __main__ -  [dream] What did the man probably do yesterday evening? [SEP] M: Did you watch TV yesterday evening? F: No, I saw a film instead. [SEP]  (A) Saw a film. (B) Read a book. (C) Watch TV.
04/25/2022 09:02:56 - INFO - __main__ - ['Watch TV.']
04/25/2022 09:02:56 - INFO - __main__ -  [dream] What can we learn about the ten-day tour? [SEP] M: Could you give me some information on your European tours? W: Our pleasure. We have several package tours you may choose, from ten days to three weeks in Europe. M: I would be interested in a ten-day trip around Christmas time. W: I have one ten-day tour that is still available. It will depart from New York on December 24. M: What is the cost? W: The price for one person for a ten-day tour is only $1,088, which includes round-trip airfare. M: That sounds reasonable. By the way, do you have a discount for two? W: Yes, you can have a 10% discount. [SEP]  (A) It has all been booked out. (B) The price sounds reasonable. (C) The price includes one-way airfare.
04/25/2022 09:02:56 - INFO - __main__ - ['The price sounds reasonable.']
04/25/2022 09:02:56 - INFO - __main__ - Tokenizing Input ...
04/25/2022 09:02:57 - INFO - __main__ - Tokenizing Output ...
04/25/2022 09:02:58 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 09:04:18 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-dream/dream_32_87_0.2_8_predictions.txt
04/25/2022 09:04:18 - INFO - __main__ - ACC on test data: 0.3050
04/25/2022 09:04:19 - INFO - __main__ - prefix=dream_32_87, lr=0.2, bsz=8, dev_performance=0.375, test_performance=0.305
