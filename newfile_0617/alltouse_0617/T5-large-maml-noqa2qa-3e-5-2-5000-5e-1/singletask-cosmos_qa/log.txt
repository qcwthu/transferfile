04/06/2022 14:10:22 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=False, bsz_list=[4], cache_dir='/data/qin/cache/', checkpoint='None', cuda='4', dataset='nlp_forest_single', debug=False, dev_file='data', do_lowercase=False, do_predict=True, do_train=True, eval_period=50, freeze_embeds=False, gradient_accumulation_steps=2, identifier='T5-large-maml-noqa2qa-3e-5-2-5000-5e-1', learning_rate=0.5, learning_rate_list=[0.5], lm_adapted_path='/data/qin/lm_adapted_t5model/torch_ckpt/large/pytorch_model.bin', local_rank=0, log_step=10, max_grad_norm=1.0, max_input_length=512, max_output_length=128, model='google/t5-v1_1-large', num_beams=4, num_train_epochs=1000.0, output_dir='models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa', predict_batch_size=16, predict_checkpoint='best-model.pt', prefix='', prompt_number=100, quiet=False, seed=42, task_dir='data/cosmos_qa/', task_name='cosmos_qa', test_file='data', total_steps=3000, train_batch_size=4, train_file='data', wait_step=10000000000, warmup_steps=50, weight_decay=1e-05)
04/06/2022 14:10:22 - INFO - __main__ - models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa
04/24/2022 22:09:55 - INFO - __main__ - Namespace(task_dir='data/cosmos_qa/', task_name='cosmos_qa', identifier='T5-large-maml-noqa2qa-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-noqa2qa-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
04/24/2022 22:09:55 - INFO - __main__ - models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa
04/24/2022 22:09:55 - INFO - __main__ - Namespace(task_dir='data/cosmos_qa/', task_name='cosmos_qa', identifier='T5-large-maml-noqa2qa-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-noqa2qa-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
04/24/2022 22:09:55 - INFO - __main__ - models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa
04/24/2022 22:09:56 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
04/24/2022 22:09:56 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
04/24/2022 22:09:56 - INFO - __main__ - args.device: cuda:0
04/24/2022 22:09:56 - INFO - __main__ - Using 2 gpus
04/24/2022 22:09:56 - INFO - __main__ - args.device: cuda:1
04/24/2022 22:09:56 - INFO - __main__ - Using 2 gpus
04/24/2022 22:09:56 - INFO - __main__ - Fine-tuning the following samples: ['cosmos_qa_32_100', 'cosmos_qa_32_13', 'cosmos_qa_32_21', 'cosmos_qa_32_42', 'cosmos_qa_32_87']
04/24/2022 22:09:56 - INFO - __main__ - Fine-tuning the following samples: ['cosmos_qa_32_100', 'cosmos_qa_32_13', 'cosmos_qa_32_21', 'cosmos_qa_32_42', 'cosmos_qa_32_87']
04/24/2022 22:10:01 - INFO - __main__ - Running ... prefix=cosmos_qa_32_100, lr=0.5, bsz=8 ...
04/24/2022 22:10:02 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:10:02 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:10:02 - INFO - __main__ - Printing 3 examples
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] What is the narrator desiring next once her prep is done ? [SEP] So this morning I thought I was being proactive and getting dinner in the crock pot to cook all day . The idea of smelling a brisket with potatoes and carrots and onions cooking all say sounded heavenly . I got everything all chopped up and into the crock pot . I threw in some chopped up garlic and put the lid on . [SEP]  (A) The narrator will want to go on a fast . (B) The narrator will want to make food . (C) The narrator will want to eat . (D) None of the above choices .
04/24/2022 22:10:02 - INFO - __main__ - Printing 3 examples
04/24/2022 22:10:02 - INFO - __main__ - ['The narrator will want to eat .']
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] What is the narrator desiring next once her prep is done ? [SEP] So this morning I thought I was being proactive and getting dinner in the crock pot to cook all day . The idea of smelling a brisket with potatoes and carrots and onions cooking all say sounded heavenly . I got everything all chopped up and into the crock pot . I threw in some chopped up garlic and put the lid on . [SEP]  (A) The narrator will want to go on a fast . (B) The narrator will want to make food . (C) The narrator will want to eat . (D) None of the above choices .
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] What would happen if the cake was put in the oven for 50 minutes ? [SEP] Alternately add flour mixture and milk ( starting and ending with the flour mixture ) , while mixing on low speed . Continue to mix until smooth . Pour into prepared pans . Bake for 30 to 35 minutes or until a cake tester inserted in the center of cake comes out clean . [SEP]  (A) Cake would taste better . (B) Cake would be moist . (C) Cake would be burnt . (D) None of the above choices .
04/24/2022 22:10:02 - INFO - __main__ - ['Cake would be burnt .']
04/24/2022 22:10:02 - INFO - __main__ - ['The narrator will want to eat .']
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] What type of game might be being discussed ? [SEP] Well well , if you look at the bonus I will get back later on I ' m up =) . I played about 1200 hands which was ok considering that I still have n't really got of my could . Tomorrow is gon na be a tough day thou . [SEP]  (A) None of the above choices . (B) A game of Scrabble (C) A poker game (D) A game of draughts
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] What would happen if the cake was put in the oven for 50 minutes ? [SEP] Alternately add flour mixture and milk ( starting and ending with the flour mixture ) , while mixing on low speed . Continue to mix until smooth . Pour into prepared pans . Bake for 30 to 35 minutes or until a cake tester inserted in the center of cake comes out clean . [SEP]  (A) Cake would taste better . (B) Cake would be moist . (C) Cake would be burnt . (D) None of the above choices .
04/24/2022 22:10:02 - INFO - __main__ - ['A poker game']
04/24/2022 22:10:02 - INFO - __main__ - ['Cake would be burnt .']
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] What type of game might be being discussed ? [SEP] Well well , if you look at the bonus I will get back later on I ' m up =) . I played about 1200 hands which was ok considering that I still have n't really got of my could . Tomorrow is gon na be a tough day thou . [SEP]  (A) None of the above choices . (B) A game of Scrabble (C) A poker game (D) A game of draughts
04/24/2022 22:10:02 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:10:02 - INFO - __main__ - ['A poker game']
04/24/2022 22:10:02 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:10:02 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:10:02 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:10:02 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 22:10:02 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:10:02 - INFO - __main__ - Printing 3 examples
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] How does this person feel about his service to the community ? [SEP] I am always one to share , have been like that for a long time . And that includes the estate . I am behind the scenes in day to day operations but want to emphasize that there are several meeting places that are open to anyone for free use . Second Life is all about collaboration and providing space is a little part I can add back to the community . [SEP]  (A) None of the above choices . (B) He is saddened about his service to the community (C) He is proud about his service to the community (D) He is boastful about his service tothe community
04/24/2022 22:10:02 - INFO - __main__ - ['He is proud about his service to the community']
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] What may be the reason I took a bus tour ? [SEP] Maybe even go to bed early ! Shocking I know!The bus tour was okay , but ya , I ' m definitely not a fan of bus tours . The stops were interesting but rushed . [SEP]  (A) I am taking the bus to work . (B) I work in another city . (C) I am a bus driver . (D) None of the above choices .
04/24/2022 22:10:02 - INFO - __main__ - ['None of the above choices .']
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] Why is the narrator going on a trip to Antigua with friends ? [SEP] Next weekend we had planned to go to la Antigua from Friday to Satudar , because Sunday was the school race of 7 kilometers , which I hope I would finish without problems , but I really doubt about it . Anyway , Friday my friends and I , after school , will go in a bus to La Antigua . It is our last year in school , so we are trying to plan everything together , because it could be our last time having fun in that way . [SEP]  (A) None of the above choices . (B) The narrator and friends are having a scuba excursion . (C) The narrator and friends are having a farewell bash . (D) The narrator and friends are having a light jaunt .
04/24/2022 22:10:02 - INFO - __main__ - ['The narrator and friends are having a farewell bash .']
04/24/2022 22:10:02 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:10:02 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 22:10:02 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:10:02 - INFO - __main__ - Printing 3 examples
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] How does this person feel about his service to the community ? [SEP] I am always one to share , have been like that for a long time . And that includes the estate . I am behind the scenes in day to day operations but want to emphasize that there are several meeting places that are open to anyone for free use . Second Life is all about collaboration and providing space is a little part I can add back to the community . [SEP]  (A) None of the above choices . (B) He is saddened about his service to the community (C) He is proud about his service to the community (D) He is boastful about his service tothe community
04/24/2022 22:10:02 - INFO - __main__ - ['He is proud about his service to the community']
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] What may be the reason I took a bus tour ? [SEP] Maybe even go to bed early ! Shocking I know!The bus tour was okay , but ya , I ' m definitely not a fan of bus tours . The stops were interesting but rushed . [SEP]  (A) I am taking the bus to work . (B) I work in another city . (C) I am a bus driver . (D) None of the above choices .
04/24/2022 22:10:02 - INFO - __main__ - ['None of the above choices .']
04/24/2022 22:10:02 - INFO - __main__ -  [cosmos_qa] Why is the narrator going on a trip to Antigua with friends ? [SEP] Next weekend we had planned to go to la Antigua from Friday to Satudar , because Sunday was the school race of 7 kilometers , which I hope I would finish without problems , but I really doubt about it . Anyway , Friday my friends and I , after school , will go in a bus to La Antigua . It is our last year in school , so we are trying to plan everything together , because it could be our last time having fun in that way . [SEP]  (A) None of the above choices . (B) The narrator and friends are having a scuba excursion . (C) The narrator and friends are having a farewell bash . (D) The narrator and friends are having a light jaunt .
04/24/2022 22:10:02 - INFO - __main__ - ['The narrator and friends are having a farewell bash .']
04/24/2022 22:10:02 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:10:02 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:10:02 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:10:02 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 22:10:02 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 22:10:20 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 22:10:20 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 22:10:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 22:10:21 - INFO - __main__ - Starting training!
04/24/2022 22:10:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 22:10:25 - INFO - __main__ - Starting training!
04/24/2022 22:10:29 - INFO - __main__ - Step 10 Global step 10 Train loss 1.30 on epoch=4
04/24/2022 22:10:32 - INFO - __main__ - Step 20 Global step 20 Train loss 1.15 on epoch=9
04/24/2022 22:10:35 - INFO - __main__ - Step 30 Global step 30 Train loss 1.06 on epoch=14
04/24/2022 22:10:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.91 on epoch=19
04/24/2022 22:10:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.84 on epoch=24
04/24/2022 22:10:46 - INFO - __main__ - Global step 50 Train loss 1.05 ACC 0.1875 on epoch=24
04/24/2022 22:10:46 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
04/24/2022 22:10:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.78 on epoch=29
04/24/2022 22:10:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.72 on epoch=34
04/24/2022 22:10:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.62 on epoch=39
04/24/2022 22:10:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=44
04/24/2022 22:11:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=49
04/24/2022 22:11:05 - INFO - __main__ - Global step 100 Train loss 0.65 ACC 0.25 on epoch=49
04/24/2022 22:11:05 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.25 on epoch=49, global_step=100
04/24/2022 22:11:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=54
04/24/2022 22:11:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=59
04/24/2022 22:11:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=64
04/24/2022 22:11:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=69
04/24/2022 22:11:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=74
04/24/2022 22:11:22 - INFO - __main__ - Global step 150 Train loss 0.41 ACC 0.21875 on epoch=74
04/24/2022 22:11:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
04/24/2022 22:11:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=84
04/24/2022 22:11:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.30 on epoch=89
04/24/2022 22:11:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
04/24/2022 22:11:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=99
04/24/2022 22:11:40 - INFO - __main__ - Global step 200 Train loss 0.28 ACC 0.21875 on epoch=99
04/24/2022 22:11:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=104
04/24/2022 22:11:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=109
04/24/2022 22:11:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.21 on epoch=114
04/24/2022 22:11:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=119
04/24/2022 22:11:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
04/24/2022 22:11:58 - INFO - __main__ - Global step 250 Train loss 0.23 ACC 0.25 on epoch=124
04/24/2022 22:12:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.19 on epoch=129
04/24/2022 22:12:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.20 on epoch=134
04/24/2022 22:12:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.18 on epoch=139
04/24/2022 22:12:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
04/24/2022 22:12:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.19 on epoch=149
04/24/2022 22:12:16 - INFO - __main__ - Global step 300 Train loss 0.18 ACC 0.25 on epoch=149
04/24/2022 22:12:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.15 on epoch=154
04/24/2022 22:12:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.16 on epoch=159
04/24/2022 22:12:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.12 on epoch=164
04/24/2022 22:12:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.15 on epoch=169
04/24/2022 22:12:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.16 on epoch=174
04/24/2022 22:12:33 - INFO - __main__ - Global step 350 Train loss 0.15 ACC 0.28125 on epoch=174
04/24/2022 22:12:34 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=174, global_step=350
04/24/2022 22:12:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.12 on epoch=179
04/24/2022 22:12:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
04/24/2022 22:12:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
04/24/2022 22:12:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.09 on epoch=194
04/24/2022 22:12:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=199
04/24/2022 22:12:51 - INFO - __main__ - Global step 400 Train loss 0.12 ACC 0.25 on epoch=199
04/24/2022 22:12:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.12 on epoch=204
04/24/2022 22:12:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.13 on epoch=209
04/24/2022 22:13:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=214
04/24/2022 22:13:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
04/24/2022 22:13:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.08 on epoch=224
04/24/2022 22:13:09 - INFO - __main__ - Global step 450 Train loss 0.15 ACC 0.25 on epoch=224
04/24/2022 22:13:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.12 on epoch=229
04/24/2022 22:13:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
04/24/2022 22:13:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.10 on epoch=239
04/24/2022 22:13:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
04/24/2022 22:13:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=249
04/24/2022 22:13:27 - INFO - __main__ - Global step 500 Train loss 0.10 ACC 0.34375 on epoch=249
04/24/2022 22:13:27 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.34375 on epoch=249, global_step=500
04/24/2022 22:13:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
04/24/2022 22:13:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=259
04/24/2022 22:13:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
04/24/2022 22:13:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
04/24/2022 22:13:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=274
04/24/2022 22:13:45 - INFO - __main__ - Global step 550 Train loss 0.09 ACC 0.28125 on epoch=274
04/24/2022 22:13:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=279
04/24/2022 22:13:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=284
04/24/2022 22:13:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.09 on epoch=289
04/24/2022 22:13:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=294
04/24/2022 22:14:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=299
04/24/2022 22:14:03 - INFO - __main__ - Global step 600 Train loss 0.08 ACC 0.28125 on epoch=299
04/24/2022 22:14:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
04/24/2022 22:14:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
04/24/2022 22:14:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=314
04/24/2022 22:14:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=319
04/24/2022 22:14:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=324
04/24/2022 22:14:20 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.34375 on epoch=324
04/24/2022 22:14:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
04/24/2022 22:14:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=334
04/24/2022 22:14:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
04/24/2022 22:14:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=344
04/24/2022 22:14:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=349
04/24/2022 22:14:38 - INFO - __main__ - Global step 700 Train loss 0.07 ACC 0.21875 on epoch=349
04/24/2022 22:14:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
04/24/2022 22:14:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
04/24/2022 22:14:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
04/24/2022 22:14:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=369
04/24/2022 22:14:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
04/24/2022 22:14:56 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.21875 on epoch=374
04/24/2022 22:14:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=379
04/24/2022 22:15:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
04/24/2022 22:15:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
04/24/2022 22:15:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
04/24/2022 22:15:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
04/24/2022 22:15:14 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.21875 on epoch=399
04/24/2022 22:15:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
04/24/2022 22:15:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
04/24/2022 22:15:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
04/24/2022 22:15:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
04/24/2022 22:15:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
04/24/2022 22:15:32 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.25 on epoch=424
04/24/2022 22:15:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
04/24/2022 22:15:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
04/24/2022 22:15:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
04/24/2022 22:15:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
04/24/2022 22:15:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
04/24/2022 22:15:51 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.3125 on epoch=449
04/24/2022 22:15:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
04/24/2022 22:15:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
04/24/2022 22:16:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
04/24/2022 22:16:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
04/24/2022 22:16:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
04/24/2022 22:16:09 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.25 on epoch=474
04/24/2022 22:16:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
04/24/2022 22:16:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
04/24/2022 22:16:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
04/24/2022 22:16:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
04/24/2022 22:16:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
04/24/2022 22:16:27 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.3125 on epoch=499
04/24/2022 22:16:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
04/24/2022 22:16:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
04/24/2022 22:16:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
04/24/2022 22:16:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
04/24/2022 22:16:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/24/2022 22:16:45 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.28125 on epoch=524
04/24/2022 22:16:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
04/24/2022 22:16:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
04/24/2022 22:16:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
04/24/2022 22:16:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
04/24/2022 22:17:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/24/2022 22:17:03 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.21875 on epoch=549
04/24/2022 22:17:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
04/24/2022 22:17:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
04/24/2022 22:17:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
04/24/2022 22:17:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
04/24/2022 22:17:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/24/2022 22:17:21 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.21875 on epoch=574
04/24/2022 22:17:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/24/2022 22:17:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
04/24/2022 22:17:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/24/2022 22:17:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
04/24/2022 22:17:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=599
04/24/2022 22:17:39 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.21875 on epoch=599
04/24/2022 22:17:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/24/2022 22:17:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/24/2022 22:17:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/24/2022 22:17:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
04/24/2022 22:17:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/24/2022 22:17:57 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.3125 on epoch=624
04/24/2022 22:18:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/24/2022 22:18:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/24/2022 22:18:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/24/2022 22:18:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/24/2022 22:18:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/24/2022 22:18:15 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.21875 on epoch=649
04/24/2022 22:18:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
04/24/2022 22:18:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/24/2022 22:18:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/24/2022 22:18:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/24/2022 22:18:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/24/2022 22:18:32 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.25 on epoch=674
04/24/2022 22:18:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/24/2022 22:18:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/24/2022 22:18:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/24/2022 22:18:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
04/24/2022 22:18:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/24/2022 22:18:50 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.28125 on epoch=699
04/24/2022 22:18:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/24/2022 22:18:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/24/2022 22:18:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/24/2022 22:19:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/24/2022 22:19:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
04/24/2022 22:19:08 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.25 on epoch=724
04/24/2022 22:19:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/24/2022 22:19:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/24/2022 22:19:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/24/2022 22:19:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/24/2022 22:19:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/24/2022 22:19:26 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.25 on epoch=749
04/24/2022 22:19:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
04/24/2022 22:19:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/24/2022 22:19:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/24/2022 22:19:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
04/24/2022 22:19:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/24/2022 22:19:45 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.21875 on epoch=774
04/24/2022 22:19:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/24/2022 22:19:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/24/2022 22:19:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=789
04/24/2022 22:19:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/24/2022 22:20:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/24/2022 22:20:02 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.21875 on epoch=799
04/24/2022 22:20:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/24/2022 22:20:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/24/2022 22:20:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
04/24/2022 22:20:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/24/2022 22:20:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
04/24/2022 22:20:21 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.28125 on epoch=824
04/24/2022 22:20:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
04/24/2022 22:20:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/24/2022 22:20:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/24/2022 22:20:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
04/24/2022 22:20:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
04/24/2022 22:20:39 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.25 on epoch=849
04/24/2022 22:20:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/24/2022 22:20:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
04/24/2022 22:20:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/24/2022 22:20:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
04/24/2022 22:20:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/24/2022 22:20:57 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.1875 on epoch=874
04/24/2022 22:21:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
04/24/2022 22:21:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/24/2022 22:21:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
04/24/2022 22:21:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/24/2022 22:21:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
04/24/2022 22:21:15 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.15625 on epoch=899
04/24/2022 22:21:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/24/2022 22:21:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/24/2022 22:21:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/24/2022 22:21:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
04/24/2022 22:21:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/24/2022 22:21:33 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.1875 on epoch=924
04/24/2022 22:21:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/24/2022 22:21:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
04/24/2022 22:21:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
04/24/2022 22:21:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/24/2022 22:21:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/24/2022 22:21:51 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.1875 on epoch=949
04/24/2022 22:21:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/24/2022 22:21:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
04/24/2022 22:22:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
04/24/2022 22:22:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/24/2022 22:22:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
04/24/2022 22:22:09 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.15625 on epoch=974
04/24/2022 22:22:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
04/24/2022 22:22:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
04/24/2022 22:22:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
04/24/2022 22:22:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
04/24/2022 22:22:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/24/2022 22:22:26 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:22:26 - INFO - __main__ - Printing 3 examples
04/24/2022 22:22:26 - INFO - __main__ -  [cosmos_qa] What is the narrator desiring next once her prep is done ? [SEP] So this morning I thought I was being proactive and getting dinner in the crock pot to cook all day . The idea of smelling a brisket with potatoes and carrots and onions cooking all say sounded heavenly . I got everything all chopped up and into the crock pot . I threw in some chopped up garlic and put the lid on . [SEP]  (A) The narrator will want to go on a fast . (B) The narrator will want to make food . (C) The narrator will want to eat . (D) None of the above choices .
04/24/2022 22:22:26 - INFO - __main__ - ['The narrator will want to eat .']
04/24/2022 22:22:26 - INFO - __main__ -  [cosmos_qa] What would happen if the cake was put in the oven for 50 minutes ? [SEP] Alternately add flour mixture and milk ( starting and ending with the flour mixture ) , while mixing on low speed . Continue to mix until smooth . Pour into prepared pans . Bake for 30 to 35 minutes or until a cake tester inserted in the center of cake comes out clean . [SEP]  (A) Cake would taste better . (B) Cake would be moist . (C) Cake would be burnt . (D) None of the above choices .
04/24/2022 22:22:26 - INFO - __main__ - ['Cake would be burnt .']
04/24/2022 22:22:26 - INFO - __main__ -  [cosmos_qa] What type of game might be being discussed ? [SEP] Well well , if you look at the bonus I will get back later on I ' m up =) . I played about 1200 hands which was ok considering that I still have n't really got of my could . Tomorrow is gon na be a tough day thou . [SEP]  (A) None of the above choices . (B) A game of Scrabble (C) A poker game (D) A game of draughts
04/24/2022 22:22:26 - INFO - __main__ - ['A poker game']
04/24/2022 22:22:26 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:22:26 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:22:26 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 22:22:26 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:22:26 - INFO - __main__ - Printing 3 examples
04/24/2022 22:22:26 - INFO - __main__ -  [cosmos_qa] How does this person feel about his service to the community ? [SEP] I am always one to share , have been like that for a long time . And that includes the estate . I am behind the scenes in day to day operations but want to emphasize that there are several meeting places that are open to anyone for free use . Second Life is all about collaboration and providing space is a little part I can add back to the community . [SEP]  (A) None of the above choices . (B) He is saddened about his service to the community (C) He is proud about his service to the community (D) He is boastful about his service tothe community
04/24/2022 22:22:26 - INFO - __main__ - ['He is proud about his service to the community']
04/24/2022 22:22:26 - INFO - __main__ -  [cosmos_qa] What may be the reason I took a bus tour ? [SEP] Maybe even go to bed early ! Shocking I know!The bus tour was okay , but ya , I ' m definitely not a fan of bus tours . The stops were interesting but rushed . [SEP]  (A) I am taking the bus to work . (B) I work in another city . (C) I am a bus driver . (D) None of the above choices .
04/24/2022 22:22:26 - INFO - __main__ - ['None of the above choices .']
04/24/2022 22:22:26 - INFO - __main__ -  [cosmos_qa] Why is the narrator going on a trip to Antigua with friends ? [SEP] Next weekend we had planned to go to la Antigua from Friday to Satudar , because Sunday was the school race of 7 kilometers , which I hope I would finish without problems , but I really doubt about it . Anyway , Friday my friends and I , after school , will go in a bus to La Antigua . It is our last year in school , so we are trying to plan everything together , because it could be our last time having fun in that way . [SEP]  (A) None of the above choices . (B) The narrator and friends are having a scuba excursion . (C) The narrator and friends are having a farewell bash . (D) The narrator and friends are having a light jaunt .
04/24/2022 22:22:26 - INFO - __main__ - ['The narrator and friends are having a farewell bash .']
04/24/2022 22:22:26 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:22:26 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:22:26 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 22:22:27 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.15625 on epoch=999
04/24/2022 22:22:27 - INFO - __main__ - save last model!
04/24/2022 22:22:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/24/2022 22:22:28 - INFO - __main__ - Start tokenizing ... 1000 instances
04/24/2022 22:22:28 - INFO - __main__ - Printing 3 examples
04/24/2022 22:22:28 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/24/2022 22:22:28 - INFO - __main__ - ['He wants to get married to a different person .']
04/24/2022 22:22:28 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/24/2022 22:22:28 - INFO - __main__ - ['He was married before and she might come back one day .']
04/24/2022 22:22:28 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/24/2022 22:22:28 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/24/2022 22:22:28 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:22:28 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:22:29 - INFO - __main__ - Loaded 1000 examples from test data
04/24/2022 22:22:42 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 22:22:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 22:22:43 - INFO - __main__ - Starting training!
04/24/2022 22:24:16 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_100_0.5_8_predictions.txt
04/24/2022 22:24:16 - INFO - __main__ - ACC on test data: 0.2490
04/24/2022 22:24:17 - INFO - __main__ - prefix=cosmos_qa_32_100, lr=0.5, bsz=8, dev_performance=0.34375, test_performance=0.249
04/24/2022 22:24:17 - INFO - __main__ - Running ... prefix=cosmos_qa_32_100, lr=0.4, bsz=8 ...
04/24/2022 22:24:18 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:24:18 - INFO - __main__ - Printing 3 examples
04/24/2022 22:24:18 - INFO - __main__ -  [cosmos_qa] What is the narrator desiring next once her prep is done ? [SEP] So this morning I thought I was being proactive and getting dinner in the crock pot to cook all day . The idea of smelling a brisket with potatoes and carrots and onions cooking all say sounded heavenly . I got everything all chopped up and into the crock pot . I threw in some chopped up garlic and put the lid on . [SEP]  (A) The narrator will want to go on a fast . (B) The narrator will want to make food . (C) The narrator will want to eat . (D) None of the above choices .
04/24/2022 22:24:18 - INFO - __main__ - ['The narrator will want to eat .']
04/24/2022 22:24:18 - INFO - __main__ -  [cosmos_qa] What would happen if the cake was put in the oven for 50 minutes ? [SEP] Alternately add flour mixture and milk ( starting and ending with the flour mixture ) , while mixing on low speed . Continue to mix until smooth . Pour into prepared pans . Bake for 30 to 35 minutes or until a cake tester inserted in the center of cake comes out clean . [SEP]  (A) Cake would taste better . (B) Cake would be moist . (C) Cake would be burnt . (D) None of the above choices .
04/24/2022 22:24:18 - INFO - __main__ - ['Cake would be burnt .']
04/24/2022 22:24:18 - INFO - __main__ -  [cosmos_qa] What type of game might be being discussed ? [SEP] Well well , if you look at the bonus I will get back later on I ' m up =) . I played about 1200 hands which was ok considering that I still have n't really got of my could . Tomorrow is gon na be a tough day thou . [SEP]  (A) None of the above choices . (B) A game of Scrabble (C) A poker game (D) A game of draughts
04/24/2022 22:24:18 - INFO - __main__ - ['A poker game']
04/24/2022 22:24:18 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:24:18 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:24:18 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 22:24:18 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:24:18 - INFO - __main__ - Printing 3 examples
04/24/2022 22:24:18 - INFO - __main__ -  [cosmos_qa] How does this person feel about his service to the community ? [SEP] I am always one to share , have been like that for a long time . And that includes the estate . I am behind the scenes in day to day operations but want to emphasize that there are several meeting places that are open to anyone for free use . Second Life is all about collaboration and providing space is a little part I can add back to the community . [SEP]  (A) None of the above choices . (B) He is saddened about his service to the community (C) He is proud about his service to the community (D) He is boastful about his service tothe community
04/24/2022 22:24:18 - INFO - __main__ - ['He is proud about his service to the community']
04/24/2022 22:24:18 - INFO - __main__ -  [cosmos_qa] What may be the reason I took a bus tour ? [SEP] Maybe even go to bed early ! Shocking I know!The bus tour was okay , but ya , I ' m definitely not a fan of bus tours . The stops were interesting but rushed . [SEP]  (A) I am taking the bus to work . (B) I work in another city . (C) I am a bus driver . (D) None of the above choices .
04/24/2022 22:24:18 - INFO - __main__ - ['None of the above choices .']
04/24/2022 22:24:18 - INFO - __main__ -  [cosmos_qa] Why is the narrator going on a trip to Antigua with friends ? [SEP] Next weekend we had planned to go to la Antigua from Friday to Satudar , because Sunday was the school race of 7 kilometers , which I hope I would finish without problems , but I really doubt about it . Anyway , Friday my friends and I , after school , will go in a bus to La Antigua . It is our last year in school , so we are trying to plan everything together , because it could be our last time having fun in that way . [SEP]  (A) None of the above choices . (B) The narrator and friends are having a scuba excursion . (C) The narrator and friends are having a farewell bash . (D) The narrator and friends are having a light jaunt .
04/24/2022 22:24:18 - INFO - __main__ - ['The narrator and friends are having a farewell bash .']
04/24/2022 22:24:18 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:24:18 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:24:18 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 22:24:37 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 22:24:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 22:24:38 - INFO - __main__ - Starting training!
04/24/2022 22:24:42 - INFO - __main__ - Step 10 Global step 10 Train loss 1.26 on epoch=4
04/24/2022 22:24:45 - INFO - __main__ - Step 20 Global step 20 Train loss 0.79 on epoch=9
04/24/2022 22:24:48 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=14
04/24/2022 22:24:51 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=19
04/24/2022 22:24:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.40 on epoch=24
04/24/2022 22:24:57 - INFO - __main__ - Global step 50 Train loss 0.71 ACC 0.28125 on epoch=24
04/24/2022 22:24:57 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.28125 on epoch=24, global_step=50
04/24/2022 22:25:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.39 on epoch=29
04/24/2022 22:25:03 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=34
04/24/2022 22:25:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.28 on epoch=39
04/24/2022 22:25:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.29 on epoch=44
04/24/2022 22:25:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.26 on epoch=49
04/24/2022 22:25:16 - INFO - __main__ - Global step 100 Train loss 0.30 ACC 0.3125 on epoch=49
04/24/2022 22:25:16 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=49, global_step=100
04/24/2022 22:25:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.25 on epoch=54
04/24/2022 22:25:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.21 on epoch=59
04/24/2022 22:25:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.20 on epoch=64
04/24/2022 22:25:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.20 on epoch=69
04/24/2022 22:25:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.18 on epoch=74
04/24/2022 22:25:34 - INFO - __main__ - Global step 150 Train loss 0.21 ACC 0.25 on epoch=74
04/24/2022 22:25:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.20 on epoch=79
04/24/2022 22:25:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.16 on epoch=84
04/24/2022 22:25:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.16 on epoch=89
04/24/2022 22:25:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.15 on epoch=94
04/24/2022 22:25:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.19 on epoch=99
04/24/2022 22:25:52 - INFO - __main__ - Global step 200 Train loss 0.17 ACC 0.1875 on epoch=99
04/24/2022 22:25:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.15 on epoch=104
04/24/2022 22:25:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.09 on epoch=109
04/24/2022 22:26:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.13 on epoch=114
04/24/2022 22:26:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.13 on epoch=119
04/24/2022 22:26:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.11 on epoch=124
04/24/2022 22:26:10 - INFO - __main__ - Global step 250 Train loss 0.12 ACC 0.28125 on epoch=124
04/24/2022 22:26:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.16 on epoch=129
04/24/2022 22:26:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.18 on epoch=134
04/24/2022 22:26:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.15 on epoch=139
04/24/2022 22:26:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.11 on epoch=144
04/24/2022 22:26:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.13 on epoch=149
04/24/2022 22:26:28 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.25 on epoch=149
04/24/2022 22:26:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
04/24/2022 22:26:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.08 on epoch=159
04/24/2022 22:26:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.10 on epoch=164
04/24/2022 22:26:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.12 on epoch=169
04/24/2022 22:26:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.08 on epoch=174
04/24/2022 22:26:46 - INFO - __main__ - Global step 350 Train loss 0.10 ACC 0.1875 on epoch=174
04/24/2022 22:26:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.08 on epoch=179
04/24/2022 22:26:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.07 on epoch=184
04/24/2022 22:26:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.09 on epoch=189
04/24/2022 22:26:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.09 on epoch=194
04/24/2022 22:27:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.08 on epoch=199
04/24/2022 22:27:04 - INFO - __main__ - Global step 400 Train loss 0.08 ACC 0.1875 on epoch=199
04/24/2022 22:27:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.07 on epoch=204
04/24/2022 22:27:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=209
04/24/2022 22:27:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.07 on epoch=214
04/24/2022 22:27:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=219
04/24/2022 22:27:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
04/24/2022 22:27:22 - INFO - __main__ - Global step 450 Train loss 0.08 ACC 0.25 on epoch=224
04/24/2022 22:27:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
04/24/2022 22:27:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
04/24/2022 22:27:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
04/24/2022 22:27:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.04 on epoch=244
04/24/2022 22:27:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
04/24/2022 22:27:40 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.21875 on epoch=249
04/24/2022 22:27:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.04 on epoch=254
04/24/2022 22:27:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
04/24/2022 22:27:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.03 on epoch=264
04/24/2022 22:27:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
04/24/2022 22:27:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
04/24/2022 22:27:59 - INFO - __main__ - Global step 550 Train loss 0.05 ACC 0.3125 on epoch=274
04/24/2022 22:28:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
04/24/2022 22:28:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.04 on epoch=284
04/24/2022 22:28:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
04/24/2022 22:28:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=294
04/24/2022 22:28:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
04/24/2022 22:28:17 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.25 on epoch=299
04/24/2022 22:28:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
04/24/2022 22:28:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=309
04/24/2022 22:28:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
04/24/2022 22:28:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.05 on epoch=319
04/24/2022 22:28:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=324
04/24/2022 22:28:35 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.3125 on epoch=324
04/24/2022 22:28:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
04/24/2022 22:28:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
04/24/2022 22:28:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
04/24/2022 22:28:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
04/24/2022 22:28:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=349
04/24/2022 22:28:53 - INFO - __main__ - Global step 700 Train loss 0.06 ACC 0.3125 on epoch=349
04/24/2022 22:28:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
04/24/2022 22:28:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
04/24/2022 22:29:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
04/24/2022 22:29:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
04/24/2022 22:29:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
04/24/2022 22:29:11 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.25 on epoch=374
04/24/2022 22:29:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=379
04/24/2022 22:29:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
04/24/2022 22:29:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
04/24/2022 22:29:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
04/24/2022 22:29:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
04/24/2022 22:29:30 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.25 on epoch=399
04/24/2022 22:29:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
04/24/2022 22:29:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
04/24/2022 22:29:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
04/24/2022 22:29:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
04/24/2022 22:29:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
04/24/2022 22:29:48 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.3125 on epoch=424
04/24/2022 22:29:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
04/24/2022 22:29:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
04/24/2022 22:29:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
04/24/2022 22:30:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
04/24/2022 22:30:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
04/24/2022 22:30:06 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.3125 on epoch=449
04/24/2022 22:30:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
04/24/2022 22:30:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
04/24/2022 22:30:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
04/24/2022 22:30:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
04/24/2022 22:30:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
04/24/2022 22:30:25 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.25 on epoch=474
04/24/2022 22:30:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/24/2022 22:30:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
04/24/2022 22:30:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
04/24/2022 22:30:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
04/24/2022 22:30:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/24/2022 22:30:43 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.28125 on epoch=499
04/24/2022 22:30:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
04/24/2022 22:30:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
04/24/2022 22:30:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
04/24/2022 22:30:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
04/24/2022 22:30:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
04/24/2022 22:31:02 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.21875 on epoch=524
04/24/2022 22:31:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
04/24/2022 22:31:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
04/24/2022 22:31:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
04/24/2022 22:31:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
04/24/2022 22:31:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/24/2022 22:31:20 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.1875 on epoch=549
04/24/2022 22:31:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/24/2022 22:31:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
04/24/2022 22:31:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/24/2022 22:31:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/24/2022 22:31:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
04/24/2022 22:31:38 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.21875 on epoch=574
04/24/2022 22:31:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/24/2022 22:31:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
04/24/2022 22:31:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
04/24/2022 22:31:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
04/24/2022 22:31:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/24/2022 22:31:57 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.28125 on epoch=599
04/24/2022 22:32:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/24/2022 22:32:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/24/2022 22:32:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
04/24/2022 22:32:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
04/24/2022 22:32:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/24/2022 22:32:15 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.25 on epoch=624
04/24/2022 22:32:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/24/2022 22:32:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
04/24/2022 22:32:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
04/24/2022 22:32:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
04/24/2022 22:32:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/24/2022 22:32:34 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.28125 on epoch=649
04/24/2022 22:32:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/24/2022 22:32:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
04/24/2022 22:32:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/24/2022 22:32:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/24/2022 22:32:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/24/2022 22:32:53 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.21875 on epoch=674
04/24/2022 22:32:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/24/2022 22:32:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/24/2022 22:33:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/24/2022 22:33:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
04/24/2022 22:33:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
04/24/2022 22:33:11 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.3125 on epoch=699
04/24/2022 22:33:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/24/2022 22:33:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
04/24/2022 22:33:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/24/2022 22:33:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/24/2022 22:33:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/24/2022 22:33:30 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.21875 on epoch=724
04/24/2022 22:33:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/24/2022 22:33:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/24/2022 22:33:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
04/24/2022 22:33:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/24/2022 22:33:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/24/2022 22:33:48 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.15625 on epoch=749
04/24/2022 22:33:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
04/24/2022 22:33:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/24/2022 22:33:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/24/2022 22:34:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/24/2022 22:34:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/24/2022 22:34:07 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.25 on epoch=774
04/24/2022 22:34:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/24/2022 22:34:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/24/2022 22:34:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
04/24/2022 22:34:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/24/2022 22:34:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/24/2022 22:34:25 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.25 on epoch=799
04/24/2022 22:34:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/24/2022 22:34:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/24/2022 22:34:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/24/2022 22:34:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
04/24/2022 22:34:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/24/2022 22:34:44 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.1875 on epoch=824
04/24/2022 22:34:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/24/2022 22:34:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
04/24/2022 22:34:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/24/2022 22:34:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/24/2022 22:34:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/24/2022 22:35:02 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.1875 on epoch=849
04/24/2022 22:35:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/24/2022 22:35:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/24/2022 22:35:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/24/2022 22:35:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
04/24/2022 22:35:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/24/2022 22:35:21 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.15625 on epoch=874
04/24/2022 22:35:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/24/2022 22:35:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/24/2022 22:35:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
04/24/2022 22:35:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/24/2022 22:35:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/24/2022 22:35:39 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.15625 on epoch=899
04/24/2022 22:35:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/24/2022 22:35:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/24/2022 22:35:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/24/2022 22:35:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/24/2022 22:35:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
04/24/2022 22:35:58 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.15625 on epoch=924
04/24/2022 22:36:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
04/24/2022 22:36:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/24/2022 22:36:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=939
04/24/2022 22:36:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/24/2022 22:36:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
04/24/2022 22:36:16 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.125 on epoch=949
04/24/2022 22:36:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
04/24/2022 22:36:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/24/2022 22:36:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/24/2022 22:36:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/24/2022 22:36:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/24/2022 22:36:35 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.125 on epoch=974
04/24/2022 22:36:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=979
04/24/2022 22:36:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
04/24/2022 22:36:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
04/24/2022 22:36:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/24/2022 22:36:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
04/24/2022 22:36:52 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:36:52 - INFO - __main__ - Printing 3 examples
04/24/2022 22:36:52 - INFO - __main__ -  [cosmos_qa] What is the narrator desiring next once her prep is done ? [SEP] So this morning I thought I was being proactive and getting dinner in the crock pot to cook all day . The idea of smelling a brisket with potatoes and carrots and onions cooking all say sounded heavenly . I got everything all chopped up and into the crock pot . I threw in some chopped up garlic and put the lid on . [SEP]  (A) The narrator will want to go on a fast . (B) The narrator will want to make food . (C) The narrator will want to eat . (D) None of the above choices .
04/24/2022 22:36:52 - INFO - __main__ - ['The narrator will want to eat .']
04/24/2022 22:36:52 - INFO - __main__ -  [cosmos_qa] What would happen if the cake was put in the oven for 50 minutes ? [SEP] Alternately add flour mixture and milk ( starting and ending with the flour mixture ) , while mixing on low speed . Continue to mix until smooth . Pour into prepared pans . Bake for 30 to 35 minutes or until a cake tester inserted in the center of cake comes out clean . [SEP]  (A) Cake would taste better . (B) Cake would be moist . (C) Cake would be burnt . (D) None of the above choices .
04/24/2022 22:36:52 - INFO - __main__ - ['Cake would be burnt .']
04/24/2022 22:36:52 - INFO - __main__ -  [cosmos_qa] What type of game might be being discussed ? [SEP] Well well , if you look at the bonus I will get back later on I ' m up =) . I played about 1200 hands which was ok considering that I still have n't really got of my could . Tomorrow is gon na be a tough day thou . [SEP]  (A) None of the above choices . (B) A game of Scrabble (C) A poker game (D) A game of draughts
04/24/2022 22:36:52 - INFO - __main__ - ['A poker game']
04/24/2022 22:36:52 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:36:52 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:36:52 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 22:36:52 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:36:52 - INFO - __main__ - Printing 3 examples
04/24/2022 22:36:52 - INFO - __main__ -  [cosmos_qa] How does this person feel about his service to the community ? [SEP] I am always one to share , have been like that for a long time . And that includes the estate . I am behind the scenes in day to day operations but want to emphasize that there are several meeting places that are open to anyone for free use . Second Life is all about collaboration and providing space is a little part I can add back to the community . [SEP]  (A) None of the above choices . (B) He is saddened about his service to the community (C) He is proud about his service to the community (D) He is boastful about his service tothe community
04/24/2022 22:36:52 - INFO - __main__ - ['He is proud about his service to the community']
04/24/2022 22:36:52 - INFO - __main__ -  [cosmos_qa] What may be the reason I took a bus tour ? [SEP] Maybe even go to bed early ! Shocking I know!The bus tour was okay , but ya , I ' m definitely not a fan of bus tours . The stops were interesting but rushed . [SEP]  (A) I am taking the bus to work . (B) I work in another city . (C) I am a bus driver . (D) None of the above choices .
04/24/2022 22:36:52 - INFO - __main__ - ['None of the above choices .']
04/24/2022 22:36:52 - INFO - __main__ -  [cosmos_qa] Why is the narrator going on a trip to Antigua with friends ? [SEP] Next weekend we had planned to go to la Antigua from Friday to Satudar , because Sunday was the school race of 7 kilometers , which I hope I would finish without problems , but I really doubt about it . Anyway , Friday my friends and I , after school , will go in a bus to La Antigua . It is our last year in school , so we are trying to plan everything together , because it could be our last time having fun in that way . [SEP]  (A) None of the above choices . (B) The narrator and friends are having a scuba excursion . (C) The narrator and friends are having a farewell bash . (D) The narrator and friends are having a light jaunt .
04/24/2022 22:36:52 - INFO - __main__ - ['The narrator and friends are having a farewell bash .']
04/24/2022 22:36:52 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:36:52 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:36:52 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 22:36:54 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.25 on epoch=999
04/24/2022 22:36:54 - INFO - __main__ - save last model!
04/24/2022 22:36:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/24/2022 22:36:54 - INFO - __main__ - Start tokenizing ... 1000 instances
04/24/2022 22:36:54 - INFO - __main__ - Printing 3 examples
04/24/2022 22:36:54 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/24/2022 22:36:54 - INFO - __main__ - ['He wants to get married to a different person .']
04/24/2022 22:36:54 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/24/2022 22:36:54 - INFO - __main__ - ['He was married before and she might come back one day .']
04/24/2022 22:36:54 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/24/2022 22:36:54 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/24/2022 22:36:54 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:36:54 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:36:55 - INFO - __main__ - Loaded 1000 examples from test data
04/24/2022 22:37:08 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 22:37:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 22:37:08 - INFO - __main__ - Starting training!
04/24/2022 22:38:51 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_100_0.4_8_predictions.txt
04/24/2022 22:38:51 - INFO - __main__ - ACC on test data: 0.2590
04/24/2022 22:38:51 - INFO - __main__ - prefix=cosmos_qa_32_100, lr=0.4, bsz=8, dev_performance=0.3125, test_performance=0.259
04/24/2022 22:38:52 - INFO - __main__ - Running ... prefix=cosmos_qa_32_100, lr=0.3, bsz=8 ...
04/24/2022 22:38:53 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:38:53 - INFO - __main__ - Printing 3 examples
04/24/2022 22:38:53 - INFO - __main__ -  [cosmos_qa] What is the narrator desiring next once her prep is done ? [SEP] So this morning I thought I was being proactive and getting dinner in the crock pot to cook all day . The idea of smelling a brisket with potatoes and carrots and onions cooking all say sounded heavenly . I got everything all chopped up and into the crock pot . I threw in some chopped up garlic and put the lid on . [SEP]  (A) The narrator will want to go on a fast . (B) The narrator will want to make food . (C) The narrator will want to eat . (D) None of the above choices .
04/24/2022 22:38:53 - INFO - __main__ - ['The narrator will want to eat .']
04/24/2022 22:38:53 - INFO - __main__ -  [cosmos_qa] What would happen if the cake was put in the oven for 50 minutes ? [SEP] Alternately add flour mixture and milk ( starting and ending with the flour mixture ) , while mixing on low speed . Continue to mix until smooth . Pour into prepared pans . Bake for 30 to 35 minutes or until a cake tester inserted in the center of cake comes out clean . [SEP]  (A) Cake would taste better . (B) Cake would be moist . (C) Cake would be burnt . (D) None of the above choices .
04/24/2022 22:38:53 - INFO - __main__ - ['Cake would be burnt .']
04/24/2022 22:38:53 - INFO - __main__ -  [cosmos_qa] What type of game might be being discussed ? [SEP] Well well , if you look at the bonus I will get back later on I ' m up =) . I played about 1200 hands which was ok considering that I still have n't really got of my could . Tomorrow is gon na be a tough day thou . [SEP]  (A) None of the above choices . (B) A game of Scrabble (C) A poker game (D) A game of draughts
04/24/2022 22:38:53 - INFO - __main__ - ['A poker game']
04/24/2022 22:38:53 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:38:53 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:38:53 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 22:38:53 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:38:53 - INFO - __main__ - Printing 3 examples
04/24/2022 22:38:53 - INFO - __main__ -  [cosmos_qa] How does this person feel about his service to the community ? [SEP] I am always one to share , have been like that for a long time . And that includes the estate . I am behind the scenes in day to day operations but want to emphasize that there are several meeting places that are open to anyone for free use . Second Life is all about collaboration and providing space is a little part I can add back to the community . [SEP]  (A) None of the above choices . (B) He is saddened about his service to the community (C) He is proud about his service to the community (D) He is boastful about his service tothe community
04/24/2022 22:38:53 - INFO - __main__ - ['He is proud about his service to the community']
04/24/2022 22:38:53 - INFO - __main__ -  [cosmos_qa] What may be the reason I took a bus tour ? [SEP] Maybe even go to bed early ! Shocking I know!The bus tour was okay , but ya , I ' m definitely not a fan of bus tours . The stops were interesting but rushed . [SEP]  (A) I am taking the bus to work . (B) I work in another city . (C) I am a bus driver . (D) None of the above choices .
04/24/2022 22:38:53 - INFO - __main__ - ['None of the above choices .']
04/24/2022 22:38:53 - INFO - __main__ -  [cosmos_qa] Why is the narrator going on a trip to Antigua with friends ? [SEP] Next weekend we had planned to go to la Antigua from Friday to Satudar , because Sunday was the school race of 7 kilometers , which I hope I would finish without problems , but I really doubt about it . Anyway , Friday my friends and I , after school , will go in a bus to La Antigua . It is our last year in school , so we are trying to plan everything together , because it could be our last time having fun in that way . [SEP]  (A) None of the above choices . (B) The narrator and friends are having a scuba excursion . (C) The narrator and friends are having a farewell bash . (D) The narrator and friends are having a light jaunt .
04/24/2022 22:38:53 - INFO - __main__ - ['The narrator and friends are having a farewell bash .']
04/24/2022 22:38:53 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:38:53 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:38:53 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 22:39:11 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 22:39:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 22:39:12 - INFO - __main__ - Starting training!
04/24/2022 22:39:16 - INFO - __main__ - Step 10 Global step 10 Train loss 1.28 on epoch=4
04/24/2022 22:39:19 - INFO - __main__ - Step 20 Global step 20 Train loss 0.93 on epoch=9
04/24/2022 22:39:22 - INFO - __main__ - Step 30 Global step 30 Train loss 0.72 on epoch=14
04/24/2022 22:39:25 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=19
04/24/2022 22:39:29 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=24
04/24/2022 22:39:31 - INFO - __main__ - Global step 50 Train loss 0.79 ACC 0.1875 on epoch=24
04/24/2022 22:39:31 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
04/24/2022 22:39:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=29
04/24/2022 22:39:37 - INFO - __main__ - Step 70 Global step 70 Train loss 0.36 on epoch=34
04/24/2022 22:39:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=39
04/24/2022 22:39:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.34 on epoch=44
04/24/2022 22:39:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
04/24/2022 22:39:49 - INFO - __main__ - Global step 100 Train loss 0.34 ACC 0.25 on epoch=49
04/24/2022 22:39:49 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.25 on epoch=49, global_step=100
04/24/2022 22:39:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.31 on epoch=54
04/24/2022 22:39:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.26 on epoch=59
04/24/2022 22:39:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.19 on epoch=64
04/24/2022 22:40:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
04/24/2022 22:40:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.23 on epoch=74
04/24/2022 22:40:07 - INFO - __main__ - Global step 150 Train loss 0.25 ACC 0.21875 on epoch=74
04/24/2022 22:40:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.21 on epoch=79
04/24/2022 22:40:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.15 on epoch=84
04/24/2022 22:40:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.18 on epoch=89
04/24/2022 22:40:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.15 on epoch=94
04/24/2022 22:40:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.18 on epoch=99
04/24/2022 22:40:25 - INFO - __main__ - Global step 200 Train loss 0.17 ACC 0.3125 on epoch=99
04/24/2022 22:40:25 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.3125 on epoch=99, global_step=200
04/24/2022 22:40:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=104
04/24/2022 22:40:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.15 on epoch=109
04/24/2022 22:40:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.13 on epoch=114
04/24/2022 22:40:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.12 on epoch=119
04/24/2022 22:40:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.16 on epoch=124
04/24/2022 22:40:43 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.25 on epoch=124
04/24/2022 22:40:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.11 on epoch=129
04/24/2022 22:40:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.10 on epoch=134
04/24/2022 22:40:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.16 on epoch=139
04/24/2022 22:40:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.11 on epoch=144
04/24/2022 22:40:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.11 on epoch=149
04/24/2022 22:41:01 - INFO - __main__ - Global step 300 Train loss 0.12 ACC 0.21875 on epoch=149
04/24/2022 22:41:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.11 on epoch=154
04/24/2022 22:41:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.11 on epoch=159
04/24/2022 22:41:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.10 on epoch=164
04/24/2022 22:41:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.07 on epoch=169
04/24/2022 22:41:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.15 on epoch=174
04/24/2022 22:41:19 - INFO - __main__ - Global step 350 Train loss 0.11 ACC 0.28125 on epoch=174
04/24/2022 22:41:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.12 on epoch=179
04/24/2022 22:41:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.11 on epoch=184
04/24/2022 22:41:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.08 on epoch=189
04/24/2022 22:41:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.09 on epoch=194
04/24/2022 22:41:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.06 on epoch=199
04/24/2022 22:41:38 - INFO - __main__ - Global step 400 Train loss 0.09 ACC 0.15625 on epoch=199
04/24/2022 22:41:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.08 on epoch=204
04/24/2022 22:41:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=209
04/24/2022 22:41:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
04/24/2022 22:41:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
04/24/2022 22:41:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
04/24/2022 22:41:56 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.21875 on epoch=224
04/24/2022 22:41:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
04/24/2022 22:42:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=234
04/24/2022 22:42:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.06 on epoch=239
04/24/2022 22:42:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.06 on epoch=244
04/24/2022 22:42:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=249
04/24/2022 22:42:14 - INFO - __main__ - Global step 500 Train loss 0.07 ACC 0.25 on epoch=249
04/24/2022 22:42:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
04/24/2022 22:42:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.07 on epoch=259
04/24/2022 22:42:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
04/24/2022 22:42:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
04/24/2022 22:42:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.05 on epoch=274
04/24/2022 22:42:32 - INFO - __main__ - Global step 550 Train loss 0.07 ACC 0.21875 on epoch=274
04/24/2022 22:42:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=279
04/24/2022 22:42:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
04/24/2022 22:42:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
04/24/2022 22:42:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=294
04/24/2022 22:42:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
04/24/2022 22:42:51 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.28125 on epoch=299
04/24/2022 22:42:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
04/24/2022 22:42:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=309
04/24/2022 22:43:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
04/24/2022 22:43:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
04/24/2022 22:43:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=324
04/24/2022 22:43:09 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.21875 on epoch=324
04/24/2022 22:43:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
04/24/2022 22:43:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
04/24/2022 22:43:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
04/24/2022 22:43:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
04/24/2022 22:43:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=349
04/24/2022 22:43:28 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.3125 on epoch=349
04/24/2022 22:43:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
04/24/2022 22:43:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
04/24/2022 22:43:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
04/24/2022 22:43:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
04/24/2022 22:43:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
04/24/2022 22:43:46 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.28125 on epoch=374
04/24/2022 22:43:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=379
04/24/2022 22:43:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
04/24/2022 22:43:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
04/24/2022 22:43:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
04/24/2022 22:44:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
04/24/2022 22:44:04 - INFO - __main__ - Global step 800 Train loss 0.04 ACC 0.25 on epoch=399
04/24/2022 22:44:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
04/24/2022 22:44:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
04/24/2022 22:44:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
04/24/2022 22:44:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
04/24/2022 22:44:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
04/24/2022 22:44:22 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.21875 on epoch=424
04/24/2022 22:44:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
04/24/2022 22:44:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
04/24/2022 22:44:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
04/24/2022 22:44:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=444
04/24/2022 22:44:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
04/24/2022 22:44:41 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.28125 on epoch=449
04/24/2022 22:44:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
04/24/2022 22:44:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
04/24/2022 22:44:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
04/24/2022 22:44:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
04/24/2022 22:44:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
04/24/2022 22:44:59 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.25 on epoch=474
04/24/2022 22:45:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=479
04/24/2022 22:45:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
04/24/2022 22:45:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
04/24/2022 22:45:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
04/24/2022 22:45:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/24/2022 22:45:18 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.21875 on epoch=499
04/24/2022 22:45:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
04/24/2022 22:45:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
04/24/2022 22:45:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
04/24/2022 22:45:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
04/24/2022 22:45:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
04/24/2022 22:45:36 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.25 on epoch=524
04/24/2022 22:45:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
04/24/2022 22:45:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
04/24/2022 22:45:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/24/2022 22:45:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
04/24/2022 22:45:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
04/24/2022 22:45:54 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.28125 on epoch=549
04/24/2022 22:45:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
04/24/2022 22:46:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/24/2022 22:46:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/24/2022 22:46:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/24/2022 22:46:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
04/24/2022 22:46:13 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.28125 on epoch=574
04/24/2022 22:46:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/24/2022 22:46:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
04/24/2022 22:46:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/24/2022 22:46:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/24/2022 22:46:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
04/24/2022 22:46:31 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.28125 on epoch=599
04/24/2022 22:46:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
04/24/2022 22:46:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
04/24/2022 22:46:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
04/24/2022 22:46:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
04/24/2022 22:46:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/24/2022 22:46:50 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.28125 on epoch=624
04/24/2022 22:46:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
04/24/2022 22:46:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/24/2022 22:46:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
04/24/2022 22:47:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/24/2022 22:47:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/24/2022 22:47:08 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.25 on epoch=649
04/24/2022 22:47:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
04/24/2022 22:47:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/24/2022 22:47:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/24/2022 22:47:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/24/2022 22:47:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
04/24/2022 22:47:27 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.28125 on epoch=674
04/24/2022 22:47:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/24/2022 22:47:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/24/2022 22:47:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/24/2022 22:47:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
04/24/2022 22:47:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
04/24/2022 22:47:45 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.25 on epoch=699
04/24/2022 22:47:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/24/2022 22:47:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
04/24/2022 22:47:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/24/2022 22:47:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/24/2022 22:48:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
04/24/2022 22:48:03 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.21875 on epoch=724
04/24/2022 22:48:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/24/2022 22:48:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/24/2022 22:48:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/24/2022 22:48:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
04/24/2022 22:48:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/24/2022 22:48:22 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.3125 on epoch=749
04/24/2022 22:48:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/24/2022 22:48:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/24/2022 22:48:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
04/24/2022 22:48:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/24/2022 22:48:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/24/2022 22:48:40 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.25 on epoch=774
04/24/2022 22:48:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/24/2022 22:48:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
04/24/2022 22:48:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/24/2022 22:48:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/24/2022 22:48:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/24/2022 22:48:59 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.3125 on epoch=799
04/24/2022 22:49:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/24/2022 22:49:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/24/2022 22:49:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/24/2022 22:49:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
04/24/2022 22:49:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/24/2022 22:49:17 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.25 on epoch=824
04/24/2022 22:49:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/24/2022 22:49:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/24/2022 22:49:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/24/2022 22:49:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/24/2022 22:49:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/24/2022 22:49:36 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.21875 on epoch=849
04/24/2022 22:49:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/24/2022 22:49:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
04/24/2022 22:49:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/24/2022 22:49:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/24/2022 22:49:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/24/2022 22:49:54 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.21875 on epoch=874
04/24/2022 22:49:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
04/24/2022 22:50:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/24/2022 22:50:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
04/24/2022 22:50:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
04/24/2022 22:50:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/24/2022 22:50:12 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.25 on epoch=899
04/24/2022 22:50:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/24/2022 22:50:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/24/2022 22:50:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/24/2022 22:50:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/24/2022 22:50:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
04/24/2022 22:50:31 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.28125 on epoch=924
04/24/2022 22:50:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
04/24/2022 22:50:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/24/2022 22:50:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
04/24/2022 22:50:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/24/2022 22:50:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
04/24/2022 22:50:49 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.34375 on epoch=949
04/24/2022 22:50:49 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.34375 on epoch=949, global_step=1900
04/24/2022 22:50:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/24/2022 22:50:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
04/24/2022 22:50:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/24/2022 22:51:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/24/2022 22:51:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/24/2022 22:51:08 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.28125 on epoch=974
04/24/2022 22:51:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/24/2022 22:51:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/24/2022 22:51:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/24/2022 22:51:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
04/24/2022 22:51:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/24/2022 22:51:25 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:51:25 - INFO - __main__ - Printing 3 examples
04/24/2022 22:51:25 - INFO - __main__ -  [cosmos_qa] What is the narrator desiring next once her prep is done ? [SEP] So this morning I thought I was being proactive and getting dinner in the crock pot to cook all day . The idea of smelling a brisket with potatoes and carrots and onions cooking all say sounded heavenly . I got everything all chopped up and into the crock pot . I threw in some chopped up garlic and put the lid on . [SEP]  (A) The narrator will want to go on a fast . (B) The narrator will want to make food . (C) The narrator will want to eat . (D) None of the above choices .
04/24/2022 22:51:25 - INFO - __main__ - ['The narrator will want to eat .']
04/24/2022 22:51:25 - INFO - __main__ -  [cosmos_qa] What would happen if the cake was put in the oven for 50 minutes ? [SEP] Alternately add flour mixture and milk ( starting and ending with the flour mixture ) , while mixing on low speed . Continue to mix until smooth . Pour into prepared pans . Bake for 30 to 35 minutes or until a cake tester inserted in the center of cake comes out clean . [SEP]  (A) Cake would taste better . (B) Cake would be moist . (C) Cake would be burnt . (D) None of the above choices .
04/24/2022 22:51:25 - INFO - __main__ - ['Cake would be burnt .']
04/24/2022 22:51:25 - INFO - __main__ -  [cosmos_qa] What type of game might be being discussed ? [SEP] Well well , if you look at the bonus I will get back later on I ' m up =) . I played about 1200 hands which was ok considering that I still have n't really got of my could . Tomorrow is gon na be a tough day thou . [SEP]  (A) None of the above choices . (B) A game of Scrabble (C) A poker game (D) A game of draughts
04/24/2022 22:51:25 - INFO - __main__ - ['A poker game']
04/24/2022 22:51:25 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:51:25 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:51:25 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 22:51:25 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:51:25 - INFO - __main__ - Printing 3 examples
04/24/2022 22:51:25 - INFO - __main__ -  [cosmos_qa] How does this person feel about his service to the community ? [SEP] I am always one to share , have been like that for a long time . And that includes the estate . I am behind the scenes in day to day operations but want to emphasize that there are several meeting places that are open to anyone for free use . Second Life is all about collaboration and providing space is a little part I can add back to the community . [SEP]  (A) None of the above choices . (B) He is saddened about his service to the community (C) He is proud about his service to the community (D) He is boastful about his service tothe community
04/24/2022 22:51:25 - INFO - __main__ - ['He is proud about his service to the community']
04/24/2022 22:51:25 - INFO - __main__ -  [cosmos_qa] What may be the reason I took a bus tour ? [SEP] Maybe even go to bed early ! Shocking I know!The bus tour was okay , but ya , I ' m definitely not a fan of bus tours . The stops were interesting but rushed . [SEP]  (A) I am taking the bus to work . (B) I work in another city . (C) I am a bus driver . (D) None of the above choices .
04/24/2022 22:51:25 - INFO - __main__ - ['None of the above choices .']
04/24/2022 22:51:25 - INFO - __main__ -  [cosmos_qa] Why is the narrator going on a trip to Antigua with friends ? [SEP] Next weekend we had planned to go to la Antigua from Friday to Satudar , because Sunday was the school race of 7 kilometers , which I hope I would finish without problems , but I really doubt about it . Anyway , Friday my friends and I , after school , will go in a bus to La Antigua . It is our last year in school , so we are trying to plan everything together , because it could be our last time having fun in that way . [SEP]  (A) None of the above choices . (B) The narrator and friends are having a scuba excursion . (C) The narrator and friends are having a farewell bash . (D) The narrator and friends are having a light jaunt .
04/24/2022 22:51:25 - INFO - __main__ - ['The narrator and friends are having a farewell bash .']
04/24/2022 22:51:25 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:51:25 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:51:25 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 22:51:26 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.3125 on epoch=999
04/24/2022 22:51:26 - INFO - __main__ - save last model!
04/24/2022 22:51:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/24/2022 22:51:26 - INFO - __main__ - Start tokenizing ... 1000 instances
04/24/2022 22:51:26 - INFO - __main__ - Printing 3 examples
04/24/2022 22:51:26 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/24/2022 22:51:26 - INFO - __main__ - ['He wants to get married to a different person .']
04/24/2022 22:51:26 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/24/2022 22:51:26 - INFO - __main__ - ['He was married before and she might come back one day .']
04/24/2022 22:51:26 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/24/2022 22:51:26 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/24/2022 22:51:26 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:51:27 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:51:28 - INFO - __main__ - Loaded 1000 examples from test data
04/24/2022 22:51:40 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 22:51:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 22:51:40 - INFO - __main__ - Starting training!
04/24/2022 22:53:19 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_100_0.3_8_predictions.txt
04/24/2022 22:53:19 - INFO - __main__ - ACC on test data: 0.2660
04/24/2022 22:53:19 - INFO - __main__ - prefix=cosmos_qa_32_100, lr=0.3, bsz=8, dev_performance=0.34375, test_performance=0.266
04/24/2022 22:53:19 - INFO - __main__ - Running ... prefix=cosmos_qa_32_100, lr=0.2, bsz=8 ...
04/24/2022 22:53:20 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:53:20 - INFO - __main__ - Printing 3 examples
04/24/2022 22:53:20 - INFO - __main__ -  [cosmos_qa] What is the narrator desiring next once her prep is done ? [SEP] So this morning I thought I was being proactive and getting dinner in the crock pot to cook all day . The idea of smelling a brisket with potatoes and carrots and onions cooking all say sounded heavenly . I got everything all chopped up and into the crock pot . I threw in some chopped up garlic and put the lid on . [SEP]  (A) The narrator will want to go on a fast . (B) The narrator will want to make food . (C) The narrator will want to eat . (D) None of the above choices .
04/24/2022 22:53:20 - INFO - __main__ - ['The narrator will want to eat .']
04/24/2022 22:53:20 - INFO - __main__ -  [cosmos_qa] What would happen if the cake was put in the oven for 50 minutes ? [SEP] Alternately add flour mixture and milk ( starting and ending with the flour mixture ) , while mixing on low speed . Continue to mix until smooth . Pour into prepared pans . Bake for 30 to 35 minutes or until a cake tester inserted in the center of cake comes out clean . [SEP]  (A) Cake would taste better . (B) Cake would be moist . (C) Cake would be burnt . (D) None of the above choices .
04/24/2022 22:53:20 - INFO - __main__ - ['Cake would be burnt .']
04/24/2022 22:53:20 - INFO - __main__ -  [cosmos_qa] What type of game might be being discussed ? [SEP] Well well , if you look at the bonus I will get back later on I ' m up =) . I played about 1200 hands which was ok considering that I still have n't really got of my could . Tomorrow is gon na be a tough day thou . [SEP]  (A) None of the above choices . (B) A game of Scrabble (C) A poker game (D) A game of draughts
04/24/2022 22:53:20 - INFO - __main__ - ['A poker game']
04/24/2022 22:53:20 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:53:20 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:53:20 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 22:53:20 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 22:53:20 - INFO - __main__ - Printing 3 examples
04/24/2022 22:53:20 - INFO - __main__ -  [cosmos_qa] How does this person feel about his service to the community ? [SEP] I am always one to share , have been like that for a long time . And that includes the estate . I am behind the scenes in day to day operations but want to emphasize that there are several meeting places that are open to anyone for free use . Second Life is all about collaboration and providing space is a little part I can add back to the community . [SEP]  (A) None of the above choices . (B) He is saddened about his service to the community (C) He is proud about his service to the community (D) He is boastful about his service tothe community
04/24/2022 22:53:20 - INFO - __main__ - ['He is proud about his service to the community']
04/24/2022 22:53:20 - INFO - __main__ -  [cosmos_qa] What may be the reason I took a bus tour ? [SEP] Maybe even go to bed early ! Shocking I know!The bus tour was okay , but ya , I ' m definitely not a fan of bus tours . The stops were interesting but rushed . [SEP]  (A) I am taking the bus to work . (B) I work in another city . (C) I am a bus driver . (D) None of the above choices .
04/24/2022 22:53:20 - INFO - __main__ - ['None of the above choices .']
04/24/2022 22:53:20 - INFO - __main__ -  [cosmos_qa] Why is the narrator going on a trip to Antigua with friends ? [SEP] Next weekend we had planned to go to la Antigua from Friday to Satudar , because Sunday was the school race of 7 kilometers , which I hope I would finish without problems , but I really doubt about it . Anyway , Friday my friends and I , after school , will go in a bus to La Antigua . It is our last year in school , so we are trying to plan everything together , because it could be our last time having fun in that way . [SEP]  (A) None of the above choices . (B) The narrator and friends are having a scuba excursion . (C) The narrator and friends are having a farewell bash . (D) The narrator and friends are having a light jaunt .
04/24/2022 22:53:20 - INFO - __main__ - ['The narrator and friends are having a farewell bash .']
04/24/2022 22:53:20 - INFO - __main__ - Tokenizing Input ...
04/24/2022 22:53:20 - INFO - __main__ - Tokenizing Output ...
04/24/2022 22:53:20 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 22:53:36 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 22:53:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 22:53:37 - INFO - __main__ - Starting training!
04/24/2022 22:53:41 - INFO - __main__ - Step 10 Global step 10 Train loss 1.27 on epoch=4
04/24/2022 22:53:44 - INFO - __main__ - Step 20 Global step 20 Train loss 1.05 on epoch=9
04/24/2022 22:53:47 - INFO - __main__ - Step 30 Global step 30 Train loss 0.81 on epoch=14
04/24/2022 22:53:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.63 on epoch=19
04/24/2022 22:53:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.60 on epoch=24
04/24/2022 22:53:56 - INFO - __main__ - Global step 50 Train loss 0.87 ACC 0.1875 on epoch=24
04/24/2022 22:53:56 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
04/24/2022 22:53:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=29
04/24/2022 22:54:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=34
04/24/2022 22:54:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=39
04/24/2022 22:54:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.37 on epoch=44
04/24/2022 22:54:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=49
04/24/2022 22:54:14 - INFO - __main__ - Global step 100 Train loss 0.43 ACC 0.25 on epoch=49
04/24/2022 22:54:14 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.25 on epoch=49, global_step=100
04/24/2022 22:54:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.32 on epoch=54
04/24/2022 22:54:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=59
04/24/2022 22:54:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.31 on epoch=64
04/24/2022 22:54:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=69
04/24/2022 22:54:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.25 on epoch=74
04/24/2022 22:54:32 - INFO - __main__ - Global step 150 Train loss 0.31 ACC 0.21875 on epoch=74
04/24/2022 22:54:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=79
04/24/2022 22:54:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=84
04/24/2022 22:54:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
04/24/2022 22:54:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=94
04/24/2022 22:54:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.19 on epoch=99
04/24/2022 22:54:50 - INFO - __main__ - Global step 200 Train loss 0.23 ACC 0.34375 on epoch=99
04/24/2022 22:54:50 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.34375 on epoch=99, global_step=200
04/24/2022 22:54:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.23 on epoch=104
04/24/2022 22:54:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.20 on epoch=109
04/24/2022 22:54:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.15 on epoch=114
04/24/2022 22:55:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=119
04/24/2022 22:55:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
04/24/2022 22:55:08 - INFO - __main__ - Global step 250 Train loss 0.20 ACC 0.25 on epoch=124
04/24/2022 22:55:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.14 on epoch=129
04/24/2022 22:55:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.19 on epoch=134
04/24/2022 22:55:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.19 on epoch=139
04/24/2022 22:55:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=144
04/24/2022 22:55:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.17 on epoch=149
04/24/2022 22:55:26 - INFO - __main__ - Global step 300 Train loss 0.18 ACC 0.21875 on epoch=149
04/24/2022 22:55:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
04/24/2022 22:55:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.15 on epoch=159
04/24/2022 22:55:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
04/24/2022 22:55:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.12 on epoch=169
04/24/2022 22:55:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=174
04/24/2022 22:55:44 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.28125 on epoch=174
04/24/2022 22:55:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
04/24/2022 22:55:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
04/24/2022 22:55:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.11 on epoch=189
04/24/2022 22:55:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.11 on epoch=194
04/24/2022 22:56:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=199
04/24/2022 22:56:02 - INFO - __main__ - Global step 400 Train loss 0.12 ACC 0.3125 on epoch=199
04/24/2022 22:56:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.14 on epoch=204
04/24/2022 22:56:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=209
04/24/2022 22:56:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.11 on epoch=214
04/24/2022 22:56:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
04/24/2022 22:56:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
04/24/2022 22:56:20 - INFO - __main__ - Global step 450 Train loss 0.11 ACC 0.1875 on epoch=224
04/24/2022 22:56:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
04/24/2022 22:56:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.11 on epoch=234
04/24/2022 22:56:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.09 on epoch=239
04/24/2022 22:56:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.10 on epoch=244
04/24/2022 22:56:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
04/24/2022 22:56:39 - INFO - __main__ - Global step 500 Train loss 0.10 ACC 0.25 on epoch=249
04/24/2022 22:56:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
04/24/2022 22:56:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=259
04/24/2022 22:56:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
04/24/2022 22:56:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
04/24/2022 22:56:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.10 on epoch=274
04/24/2022 22:56:57 - INFO - __main__ - Global step 550 Train loss 0.08 ACC 0.3125 on epoch=274
04/24/2022 22:57:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=279
04/24/2022 22:57:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
04/24/2022 22:57:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
04/24/2022 22:57:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=294
04/24/2022 22:57:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
04/24/2022 22:57:15 - INFO - __main__ - Global step 600 Train loss 0.08 ACC 0.25 on epoch=299
04/24/2022 22:57:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
04/24/2022 22:57:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=309
04/24/2022 22:57:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=314
04/24/2022 22:57:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
04/24/2022 22:57:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=324
04/24/2022 22:57:33 - INFO - __main__ - Global step 650 Train loss 0.08 ACC 0.3125 on epoch=324
04/24/2022 22:57:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.07 on epoch=329
04/24/2022 22:57:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=334
04/24/2022 22:57:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
04/24/2022 22:57:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=344
04/24/2022 22:57:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=349
04/24/2022 22:57:52 - INFO - __main__ - Global step 700 Train loss 0.07 ACC 0.25 on epoch=349
04/24/2022 22:57:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
04/24/2022 22:57:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=359
04/24/2022 22:58:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=364
04/24/2022 22:58:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=369
04/24/2022 22:58:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=374
04/24/2022 22:58:10 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.21875 on epoch=374
04/24/2022 22:58:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=379
04/24/2022 22:58:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
04/24/2022 22:58:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
04/24/2022 22:58:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
04/24/2022 22:58:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
04/24/2022 22:58:29 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.25 on epoch=399
04/24/2022 22:58:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
04/24/2022 22:58:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
04/24/2022 22:58:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=414
04/24/2022 22:58:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
04/24/2022 22:58:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
04/24/2022 22:58:47 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.3125 on epoch=424
04/24/2022 22:58:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
04/24/2022 22:58:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
04/24/2022 22:58:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=439
04/24/2022 22:59:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
04/24/2022 22:59:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
04/24/2022 22:59:05 - INFO - __main__ - Global step 900 Train loss 0.05 ACC 0.28125 on epoch=449
04/24/2022 22:59:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
04/24/2022 22:59:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
04/24/2022 22:59:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
04/24/2022 22:59:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
04/24/2022 22:59:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
04/24/2022 22:59:24 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.21875 on epoch=474
04/24/2022 22:59:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
04/24/2022 22:59:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
04/24/2022 22:59:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
04/24/2022 22:59:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=494
04/24/2022 22:59:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
04/24/2022 22:59:42 - INFO - __main__ - Global step 1000 Train loss 0.05 ACC 0.21875 on epoch=499
04/24/2022 22:59:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
04/24/2022 22:59:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
04/24/2022 22:59:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
04/24/2022 22:59:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
04/24/2022 22:59:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
04/24/2022 23:00:00 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.28125 on epoch=524
04/24/2022 23:00:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
04/24/2022 23:00:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
04/24/2022 23:00:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
04/24/2022 23:00:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=544
04/24/2022 23:00:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/24/2022 23:00:18 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.25 on epoch=549
04/24/2022 23:00:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
04/24/2022 23:00:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
04/24/2022 23:00:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
04/24/2022 23:00:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/24/2022 23:00:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
04/24/2022 23:00:37 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.34375 on epoch=574
04/24/2022 23:00:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
04/24/2022 23:00:43 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/24/2022 23:00:46 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
04/24/2022 23:00:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
04/24/2022 23:00:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
04/24/2022 23:00:55 - INFO - __main__ - Global step 1200 Train loss 0.04 ACC 0.28125 on epoch=599
04/24/2022 23:00:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
04/24/2022 23:01:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
04/24/2022 23:01:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
04/24/2022 23:01:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
04/24/2022 23:01:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/24/2022 23:01:13 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.28125 on epoch=624
04/24/2022 23:01:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/24/2022 23:01:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
04/24/2022 23:01:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
04/24/2022 23:01:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
04/24/2022 23:01:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
04/24/2022 23:01:32 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.28125 on epoch=649
04/24/2022 23:01:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/24/2022 23:01:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
04/24/2022 23:01:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
04/24/2022 23:01:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/24/2022 23:01:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
04/24/2022 23:01:50 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.1875 on epoch=674
04/24/2022 23:01:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/24/2022 23:01:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/24/2022 23:02:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/24/2022 23:02:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
04/24/2022 23:02:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/24/2022 23:02:09 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.21875 on epoch=699
04/24/2022 23:02:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/24/2022 23:02:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
04/24/2022 23:02:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
04/24/2022 23:02:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=719
04/24/2022 23:02:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
04/24/2022 23:02:27 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.3125 on epoch=724
04/24/2022 23:02:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
04/24/2022 23:02:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/24/2022 23:02:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
04/24/2022 23:02:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/24/2022 23:02:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/24/2022 23:02:46 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.25 on epoch=749
04/24/2022 23:02:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
04/24/2022 23:02:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
04/24/2022 23:02:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
04/24/2022 23:02:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
04/24/2022 23:03:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/24/2022 23:03:04 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.28125 on epoch=774
04/24/2022 23:03:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/24/2022 23:03:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
04/24/2022 23:03:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
04/24/2022 23:03:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/24/2022 23:03:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
04/24/2022 23:03:23 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.28125 on epoch=799
04/24/2022 23:03:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/24/2022 23:03:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/24/2022 23:03:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/24/2022 23:03:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
04/24/2022 23:03:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/24/2022 23:03:42 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.28125 on epoch=824
04/24/2022 23:03:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/24/2022 23:03:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/24/2022 23:03:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/24/2022 23:03:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/24/2022 23:03:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
04/24/2022 23:04:00 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.25 on epoch=849
04/24/2022 23:04:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
04/24/2022 23:04:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
04/24/2022 23:04:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
04/24/2022 23:04:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=869
04/24/2022 23:04:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/24/2022 23:04:19 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.25 on epoch=874
04/24/2022 23:04:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/24/2022 23:04:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/24/2022 23:04:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/24/2022 23:04:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
04/24/2022 23:04:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
04/24/2022 23:04:37 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.28125 on epoch=899
04/24/2022 23:04:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/24/2022 23:04:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/24/2022 23:04:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/24/2022 23:04:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/24/2022 23:04:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
04/24/2022 23:04:56 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.28125 on epoch=924
04/24/2022 23:04:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/24/2022 23:05:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/24/2022 23:05:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
04/24/2022 23:05:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/24/2022 23:05:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/24/2022 23:05:14 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.3125 on epoch=949
04/24/2022 23:05:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/24/2022 23:05:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/24/2022 23:05:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/24/2022 23:05:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
04/24/2022 23:05:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/24/2022 23:05:33 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.34375 on epoch=974
04/24/2022 23:05:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
04/24/2022 23:05:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
04/24/2022 23:05:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/24/2022 23:05:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/24/2022 23:05:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/24/2022 23:05:50 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:05:50 - INFO - __main__ - Printing 3 examples
04/24/2022 23:05:50 - INFO - __main__ -  [cosmos_qa] Why was the war game a success ? [SEP] I was very irritated at myself for being exhausted , which is not productive and got people to think that I was pissed at them . Sorry about that . However , mostly spending the day seating and making the best use of the 3 brain cells I had left got me to relax some and insomnia receded . During the Wreath meeting , a war was started and conceded , which I think was very cool . These last few years I ' ve seen too many battles with unequal numbers and it gets old pretty quick . [SEP]  (A) Each side had men and women on it . (B) Neither side would concede . (C) Each side had equal numbers . (D) None of the above choices .
04/24/2022 23:05:50 - INFO - __main__ - ['Each side had equal numbers .']
04/24/2022 23:05:50 - INFO - __main__ -  [cosmos_qa] Why does Wade refuse to believe the narrator ? [SEP] SO FUNNY . Seriously things like this only happen to me which is a nice story . So he still doens't believe me and I said this stupid line that I can never take back . " Whatever Wade , if you do n't beleive me , go " So what was his glorious responce to that ? [SEP]  (A) The narrator is a known cheat . (B) The story is outlandish . (C) None of the above choices . (D) The narrator is known to lie .
04/24/2022 23:05:50 - INFO - __main__ - ['The story is outlandish .']
04/24/2022 23:05:50 - INFO - __main__ -  [cosmos_qa] What happened because of the wedding ? [SEP] Last weekend we were in Gougan Barra , in county Cork in Ireland . We were there for the wedding of Arlene 's best friend Angella . It was the most relaxing wedding we ever attended . [SEP]  (A) They had a very relaxing weekend (B) They did not go to Gougan Barra (C) They did not have a relaxing weekend (D) None of the above choices .
04/24/2022 23:05:50 - INFO - __main__ - ['They had a very relaxing weekend']
04/24/2022 23:05:50 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:05:50 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:05:50 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 23:05:50 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:05:50 - INFO - __main__ - Printing 3 examples
04/24/2022 23:05:50 - INFO - __main__ -  [cosmos_qa] How may I feeling during this post ? [SEP] The Baltimore Sun , like all print newspapers , is going through a restructuring to cut costs . They have already gone to " tabloid size , " have raised their price , and have offered buyouts to " seasoned staff " . It has been leaked that Gregory Kane , whose articles I ' ve occasionally linked , has taken the buy out . That 's bad news for The Baltimore Sun . [SEP]  (A) Sad (B) None of the above choices . (C) Happy (D) Angry
04/24/2022 23:05:50 - INFO - __main__ - ['Sad']
04/24/2022 23:05:50 - INFO - __main__ -  [cosmos_qa] What state is Alison traveling to at the end of August ? [SEP] Last night was a delightful night . When viewed in isolation , it looks just like I ' ve got a full social life here in Philly . As it is , it was a delightful night , but I ' m REALLY looking forward to when Alison gets here in the end of August . [SEP]  (A) None of the above choices . (B) Alison is headed to California at the end of August . (C) Alison is headed to Pennsylvania . (D) Alison is headed to New York at the end of August .
04/24/2022 23:05:50 - INFO - __main__ - ['Alison is headed to Pennsylvania .']
04/24/2022 23:05:50 - INFO - __main__ -  [cosmos_qa] How may the narrator have injured themselves ? [SEP] I was really out of it the whole time because of the pills I had to take for the pain . I 'm gon na go lay down . My arm is killing me.but there you go . a little update- drive safe everyone and wear your seatbelts please ! I ' m so thankful none of us were severely injured . [SEP]  (A) They got injured playing around . (B) They got injured behind the wheel . (C) They got injured horsing around . (D) They got injured playing sports .
04/24/2022 23:05:50 - INFO - __main__ - ['They got injured behind the wheel .']
04/24/2022 23:05:50 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:05:50 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:05:50 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 23:05:51 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.28125 on epoch=999
04/24/2022 23:05:51 - INFO - __main__ - save last model!
04/24/2022 23:05:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/24/2022 23:05:51 - INFO - __main__ - Start tokenizing ... 1000 instances
04/24/2022 23:05:51 - INFO - __main__ - Printing 3 examples
04/24/2022 23:05:51 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/24/2022 23:05:51 - INFO - __main__ - ['He wants to get married to a different person .']
04/24/2022 23:05:51 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/24/2022 23:05:51 - INFO - __main__ - ['He was married before and she might come back one day .']
04/24/2022 23:05:51 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/24/2022 23:05:51 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/24/2022 23:05:51 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:05:52 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:05:53 - INFO - __main__ - Loaded 1000 examples from test data
04/24/2022 23:06:09 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 23:06:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 23:06:10 - INFO - __main__ - Starting training!
04/24/2022 23:07:43 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_100_0.2_8_predictions.txt
04/24/2022 23:07:43 - INFO - __main__ - ACC on test data: 0.2470
04/24/2022 23:07:44 - INFO - __main__ - prefix=cosmos_qa_32_100, lr=0.2, bsz=8, dev_performance=0.34375, test_performance=0.247
04/24/2022 23:07:44 - INFO - __main__ - Running ... prefix=cosmos_qa_32_13, lr=0.5, bsz=8 ...
04/24/2022 23:07:45 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:07:45 - INFO - __main__ - Printing 3 examples
04/24/2022 23:07:45 - INFO - __main__ -  [cosmos_qa] Why was the war game a success ? [SEP] I was very irritated at myself for being exhausted , which is not productive and got people to think that I was pissed at them . Sorry about that . However , mostly spending the day seating and making the best use of the 3 brain cells I had left got me to relax some and insomnia receded . During the Wreath meeting , a war was started and conceded , which I think was very cool . These last few years I ' ve seen too many battles with unequal numbers and it gets old pretty quick . [SEP]  (A) Each side had men and women on it . (B) Neither side would concede . (C) Each side had equal numbers . (D) None of the above choices .
04/24/2022 23:07:45 - INFO - __main__ - ['Each side had equal numbers .']
04/24/2022 23:07:45 - INFO - __main__ -  [cosmos_qa] Why does Wade refuse to believe the narrator ? [SEP] SO FUNNY . Seriously things like this only happen to me which is a nice story . So he still doens't believe me and I said this stupid line that I can never take back . " Whatever Wade , if you do n't beleive me , go " So what was his glorious responce to that ? [SEP]  (A) The narrator is a known cheat . (B) The story is outlandish . (C) None of the above choices . (D) The narrator is known to lie .
04/24/2022 23:07:45 - INFO - __main__ - ['The story is outlandish .']
04/24/2022 23:07:45 - INFO - __main__ -  [cosmos_qa] What happened because of the wedding ? [SEP] Last weekend we were in Gougan Barra , in county Cork in Ireland . We were there for the wedding of Arlene 's best friend Angella . It was the most relaxing wedding we ever attended . [SEP]  (A) They had a very relaxing weekend (B) They did not go to Gougan Barra (C) They did not have a relaxing weekend (D) None of the above choices .
04/24/2022 23:07:45 - INFO - __main__ - ['They had a very relaxing weekend']
04/24/2022 23:07:45 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:07:45 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:07:45 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 23:07:45 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:07:45 - INFO - __main__ - Printing 3 examples
04/24/2022 23:07:45 - INFO - __main__ -  [cosmos_qa] How may I feeling during this post ? [SEP] The Baltimore Sun , like all print newspapers , is going through a restructuring to cut costs . They have already gone to " tabloid size , " have raised their price , and have offered buyouts to " seasoned staff " . It has been leaked that Gregory Kane , whose articles I ' ve occasionally linked , has taken the buy out . That 's bad news for The Baltimore Sun . [SEP]  (A) Sad (B) None of the above choices . (C) Happy (D) Angry
04/24/2022 23:07:45 - INFO - __main__ - ['Sad']
04/24/2022 23:07:45 - INFO - __main__ -  [cosmos_qa] What state is Alison traveling to at the end of August ? [SEP] Last night was a delightful night . When viewed in isolation , it looks just like I ' ve got a full social life here in Philly . As it is , it was a delightful night , but I ' m REALLY looking forward to when Alison gets here in the end of August . [SEP]  (A) None of the above choices . (B) Alison is headed to California at the end of August . (C) Alison is headed to Pennsylvania . (D) Alison is headed to New York at the end of August .
04/24/2022 23:07:45 - INFO - __main__ - ['Alison is headed to Pennsylvania .']
04/24/2022 23:07:45 - INFO - __main__ -  [cosmos_qa] How may the narrator have injured themselves ? [SEP] I was really out of it the whole time because of the pills I had to take for the pain . I 'm gon na go lay down . My arm is killing me.but there you go . a little update- drive safe everyone and wear your seatbelts please ! I ' m so thankful none of us were severely injured . [SEP]  (A) They got injured playing around . (B) They got injured behind the wheel . (C) They got injured horsing around . (D) They got injured playing sports .
04/24/2022 23:07:45 - INFO - __main__ - ['They got injured behind the wheel .']
04/24/2022 23:07:45 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:07:45 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:07:45 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 23:08:00 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 23:08:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 23:08:01 - INFO - __main__ - Starting training!
04/24/2022 23:08:05 - INFO - __main__ - Step 10 Global step 10 Train loss 1.02 on epoch=4
04/24/2022 23:08:08 - INFO - __main__ - Step 20 Global step 20 Train loss 0.58 on epoch=9
04/24/2022 23:08:11 - INFO - __main__ - Step 30 Global step 30 Train loss 0.33 on epoch=14
04/24/2022 23:08:14 - INFO - __main__ - Step 40 Global step 40 Train loss 0.30 on epoch=19
04/24/2022 23:08:16 - INFO - __main__ - Step 50 Global step 50 Train loss 0.28 on epoch=24
04/24/2022 23:08:21 - INFO - __main__ - Global step 50 Train loss 0.50 ACC 0.1875 on epoch=24
04/24/2022 23:08:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
04/24/2022 23:08:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.24 on epoch=29
04/24/2022 23:08:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.27 on epoch=34
04/24/2022 23:08:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.24 on epoch=39
04/24/2022 23:08:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.21 on epoch=44
04/24/2022 23:08:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.26 on epoch=49
04/24/2022 23:08:38 - INFO - __main__ - Global step 100 Train loss 0.24 ACC 0.1875 on epoch=49
04/24/2022 23:08:41 - INFO - __main__ - Step 110 Global step 110 Train loss 1.05 on epoch=54
04/24/2022 23:08:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.26 on epoch=59
04/24/2022 23:08:47 - INFO - __main__ - Step 130 Global step 130 Train loss 1.18 on epoch=64
04/24/2022 23:08:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
04/24/2022 23:08:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.25 on epoch=74
04/24/2022 23:08:56 - INFO - __main__ - Global step 150 Train loss 0.60 ACC 0.1875 on epoch=74
04/24/2022 23:08:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.22 on epoch=79
04/24/2022 23:09:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.22 on epoch=84
04/24/2022 23:09:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.20 on epoch=89
04/24/2022 23:09:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.18 on epoch=94
04/24/2022 23:09:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
04/24/2022 23:09:13 - INFO - __main__ - Global step 200 Train loss 0.21 ACC 0.125 on epoch=99
04/24/2022 23:09:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.18 on epoch=104
04/24/2022 23:09:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.18 on epoch=109
04/24/2022 23:09:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.17 on epoch=114
04/24/2022 23:09:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.19 on epoch=119
04/24/2022 23:09:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=124
04/24/2022 23:09:31 - INFO - __main__ - Global step 250 Train loss 0.18 ACC 0.125 on epoch=124
04/24/2022 23:09:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=129
04/24/2022 23:09:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.17 on epoch=134
04/24/2022 23:09:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.18 on epoch=139
04/24/2022 23:09:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.19 on epoch=144
04/24/2022 23:09:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.19 on epoch=149
04/24/2022 23:09:48 - INFO - __main__ - Global step 300 Train loss 0.19 ACC 0.15625 on epoch=149
04/24/2022 23:09:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=154
04/24/2022 23:09:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.20 on epoch=159
04/24/2022 23:09:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.16 on epoch=164
04/24/2022 23:10:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.16 on epoch=169
04/24/2022 23:10:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.16 on epoch=174
04/24/2022 23:10:06 - INFO - __main__ - Global step 350 Train loss 0.18 ACC 0.125 on epoch=174
04/24/2022 23:10:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=179
04/24/2022 23:10:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=184
04/24/2022 23:10:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.18 on epoch=189
04/24/2022 23:10:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.16 on epoch=194
04/24/2022 23:10:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
04/24/2022 23:10:23 - INFO - __main__ - Global step 400 Train loss 0.18 ACC 0.09375 on epoch=199
04/24/2022 23:10:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.15 on epoch=204
04/24/2022 23:10:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
04/24/2022 23:10:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=214
04/24/2022 23:10:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.17 on epoch=219
04/24/2022 23:10:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.15 on epoch=224
04/24/2022 23:10:41 - INFO - __main__ - Global step 450 Train loss 0.16 ACC 0.15625 on epoch=224
04/24/2022 23:10:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.15 on epoch=229
04/24/2022 23:10:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.18 on epoch=234
04/24/2022 23:10:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.16 on epoch=239
04/24/2022 23:10:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=244
04/24/2022 23:10:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
04/24/2022 23:10:59 - INFO - __main__ - Global step 500 Train loss 0.15 ACC 0.09375 on epoch=249
04/24/2022 23:11:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.13 on epoch=254
04/24/2022 23:11:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=259
04/24/2022 23:11:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=264
04/24/2022 23:11:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=269
04/24/2022 23:11:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=274
04/24/2022 23:11:17 - INFO - __main__ - Global step 550 Train loss 0.15 ACC 0.0625 on epoch=274
04/24/2022 23:11:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.16 on epoch=279
04/24/2022 23:11:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=284
04/24/2022 23:11:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=289
04/24/2022 23:11:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
04/24/2022 23:11:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=299
04/24/2022 23:11:34 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.0625 on epoch=299
04/24/2022 23:11:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=304
04/24/2022 23:11:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=309
04/24/2022 23:11:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=314
04/24/2022 23:11:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=319
04/24/2022 23:11:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=324
04/24/2022 23:11:52 - INFO - __main__ - Global step 650 Train loss 0.13 ACC 0.125 on epoch=324
04/24/2022 23:11:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=329
04/24/2022 23:11:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
04/24/2022 23:12:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=339
04/24/2022 23:12:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=344
04/24/2022 23:12:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
04/24/2022 23:12:09 - INFO - __main__ - Global step 700 Train loss 0.11 ACC 0.125 on epoch=349
04/24/2022 23:12:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=354
04/24/2022 23:12:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=359
04/24/2022 23:12:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=364
04/24/2022 23:12:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=369
04/24/2022 23:12:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
04/24/2022 23:12:27 - INFO - __main__ - Global step 750 Train loss 0.10 ACC 0.0625 on epoch=374
04/24/2022 23:12:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=379
04/24/2022 23:12:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
04/24/2022 23:12:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=389
04/24/2022 23:12:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
04/24/2022 23:12:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.62 on epoch=399
04/24/2022 23:12:45 - INFO - __main__ - Global step 800 Train loss 0.19 ACC 0.0625 on epoch=399
04/24/2022 23:12:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.70 on epoch=404
04/24/2022 23:12:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=409
04/24/2022 23:12:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.64 on epoch=414
04/24/2022 23:12:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=419
04/24/2022 23:12:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=424
04/24/2022 23:13:02 - INFO - __main__ - Global step 850 Train loss 0.49 ACC 0.125 on epoch=424
04/24/2022 23:13:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=429
04/24/2022 23:13:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=434
04/24/2022 23:13:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=439
04/24/2022 23:13:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=444
04/24/2022 23:13:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=449
04/24/2022 23:13:20 - INFO - __main__ - Global step 900 Train loss 0.15 ACC 0.09375 on epoch=449
04/24/2022 23:13:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=454
04/24/2022 23:13:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=459
04/24/2022 23:13:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=464
04/24/2022 23:13:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=469
04/24/2022 23:13:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=474
04/24/2022 23:13:38 - INFO - __main__ - Global step 950 Train loss 0.10 ACC 0.03125 on epoch=474
04/24/2022 23:13:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=479
04/24/2022 23:13:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=484
04/24/2022 23:13:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=489
04/24/2022 23:13:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=494
04/24/2022 23:13:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=499
04/24/2022 23:13:56 - INFO - __main__ - Global step 1000 Train loss 0.11 ACC 0.0625 on epoch=499
04/24/2022 23:13:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=504
04/24/2022 23:14:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=509
04/24/2022 23:14:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=514
04/24/2022 23:14:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=519
04/24/2022 23:14:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=524
04/24/2022 23:14:14 - INFO - __main__ - Global step 1050 Train loss 0.09 ACC 0.0 on epoch=524
04/24/2022 23:14:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=529
04/24/2022 23:14:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=534
04/24/2022 23:14:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=539
04/24/2022 23:14:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=544
04/24/2022 23:14:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=549
04/24/2022 23:14:31 - INFO - __main__ - Global step 1100 Train loss 0.10 ACC 0.03125 on epoch=549
04/24/2022 23:14:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=554
04/24/2022 23:14:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=559
04/24/2022 23:14:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=564
04/24/2022 23:14:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=569
04/24/2022 23:14:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=574
04/24/2022 23:14:49 - INFO - __main__ - Global step 1150 Train loss 0.11 ACC 0.0 on epoch=574
04/24/2022 23:14:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=579
04/24/2022 23:14:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=584
04/24/2022 23:14:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=589
04/24/2022 23:15:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=594
04/24/2022 23:15:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=599
04/24/2022 23:15:07 - INFO - __main__ - Global step 1200 Train loss 0.10 ACC 0.0625 on epoch=599
04/24/2022 23:15:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=604
04/24/2022 23:15:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=609
04/24/2022 23:15:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=614
04/24/2022 23:15:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=619
04/24/2022 23:15:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=624
04/24/2022 23:15:24 - INFO - __main__ - Global step 1250 Train loss 0.09 ACC 0.03125 on epoch=624
04/24/2022 23:15:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=629
04/24/2022 23:15:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=634
04/24/2022 23:15:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=639
04/24/2022 23:15:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=644
04/24/2022 23:15:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=649
04/24/2022 23:15:42 - INFO - __main__ - Global step 1300 Train loss 0.08 ACC 0.0625 on epoch=649
04/24/2022 23:15:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=654
04/24/2022 23:15:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=659
04/24/2022 23:15:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=664
04/24/2022 23:15:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=669
04/24/2022 23:15:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=674
04/24/2022 23:15:59 - INFO - __main__ - Global step 1350 Train loss 0.09 ACC 0.09375 on epoch=674
04/24/2022 23:16:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=679
04/24/2022 23:16:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=684
04/24/2022 23:16:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=689
04/24/2022 23:16:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=694
04/24/2022 23:16:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=699
04/24/2022 23:16:17 - INFO - __main__ - Global step 1400 Train loss 0.09 ACC 0.0625 on epoch=699
04/24/2022 23:16:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=704
04/24/2022 23:16:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=709
04/24/2022 23:16:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=714
04/24/2022 23:16:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=719
04/24/2022 23:16:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=724
04/24/2022 23:16:35 - INFO - __main__ - Global step 1450 Train loss 0.08 ACC 0.0625 on epoch=724
04/24/2022 23:16:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=729
04/24/2022 23:16:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=734
04/24/2022 23:16:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=739
04/24/2022 23:16:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=744
04/24/2022 23:16:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=749
04/24/2022 23:16:52 - INFO - __main__ - Global step 1500 Train loss 0.07 ACC 0.0625 on epoch=749
04/24/2022 23:16:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=754
04/24/2022 23:16:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=759
04/24/2022 23:17:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=764
04/24/2022 23:17:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
04/24/2022 23:17:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=774
04/24/2022 23:17:10 - INFO - __main__ - Global step 1550 Train loss 0.08 ACC 0.09375 on epoch=774
04/24/2022 23:17:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=779
04/24/2022 23:17:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=784
04/24/2022 23:17:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=789
04/24/2022 23:17:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=794
04/24/2022 23:17:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=799
04/24/2022 23:17:27 - INFO - __main__ - Global step 1600 Train loss 0.07 ACC 0.0625 on epoch=799
04/24/2022 23:17:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=804
04/24/2022 23:17:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=809
04/24/2022 23:17:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=814
04/24/2022 23:17:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=819
04/24/2022 23:17:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=824
04/24/2022 23:17:45 - INFO - __main__ - Global step 1650 Train loss 0.09 ACC 0.0625 on epoch=824
04/24/2022 23:17:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=829
04/24/2022 23:17:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=834
04/24/2022 23:17:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=839
04/24/2022 23:17:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=844
04/24/2022 23:18:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=849
04/24/2022 23:18:03 - INFO - __main__ - Global step 1700 Train loss 0.08 ACC 0.03125 on epoch=849
04/24/2022 23:18:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=854
04/24/2022 23:18:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=859
04/24/2022 23:18:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=864
04/24/2022 23:18:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=869
04/24/2022 23:18:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=874
04/24/2022 23:18:21 - INFO - __main__ - Global step 1750 Train loss 0.08 ACC 0.09375 on epoch=874
04/24/2022 23:18:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=879
04/24/2022 23:18:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=884
04/24/2022 23:18:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=889
04/24/2022 23:18:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=894
04/24/2022 23:18:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=899
04/24/2022 23:18:38 - INFO - __main__ - Global step 1800 Train loss 0.07 ACC 0.09375 on epoch=899
04/24/2022 23:18:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=904
04/24/2022 23:18:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=909
04/24/2022 23:18:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=914
04/24/2022 23:18:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
04/24/2022 23:18:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=924
04/24/2022 23:18:56 - INFO - __main__ - Global step 1850 Train loss 0.06 ACC 0.09375 on epoch=924
04/24/2022 23:18:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=929
04/24/2022 23:19:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=934
04/24/2022 23:19:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=939
04/24/2022 23:19:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=944
04/24/2022 23:19:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=949
04/24/2022 23:19:14 - INFO - __main__ - Global step 1900 Train loss 0.06 ACC 0.09375 on epoch=949
04/24/2022 23:19:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=954
04/24/2022 23:19:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=959
04/24/2022 23:19:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=964
04/24/2022 23:19:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=969
04/24/2022 23:19:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=974
04/24/2022 23:19:32 - INFO - __main__ - Global step 1950 Train loss 0.07 ACC 0.09375 on epoch=974
04/24/2022 23:19:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=979
04/24/2022 23:19:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=984
04/24/2022 23:19:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=989
04/24/2022 23:19:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=994
04/24/2022 23:19:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=999
04/24/2022 23:19:49 - INFO - __main__ - Global step 2000 Train loss 0.07 ACC 0.03125 on epoch=999
04/24/2022 23:19:49 - INFO - __main__ - save last model!
04/24/2022 23:19:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/24/2022 23:19:49 - INFO - __main__ - Start tokenizing ... 1000 instances
04/24/2022 23:19:49 - INFO - __main__ - Printing 3 examples
04/24/2022 23:19:49 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/24/2022 23:19:49 - INFO - __main__ - ['He wants to get married to a different person .']
04/24/2022 23:19:49 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/24/2022 23:19:49 - INFO - __main__ - ['He was married before and she might come back one day .']
04/24/2022 23:19:49 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/24/2022 23:19:49 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/24/2022 23:19:49 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:19:50 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:19:50 - INFO - __main__ - Printing 3 examples
04/24/2022 23:19:50 - INFO - __main__ -  [cosmos_qa] Why was the war game a success ? [SEP] I was very irritated at myself for being exhausted , which is not productive and got people to think that I was pissed at them . Sorry about that . However , mostly spending the day seating and making the best use of the 3 brain cells I had left got me to relax some and insomnia receded . During the Wreath meeting , a war was started and conceded , which I think was very cool . These last few years I ' ve seen too many battles with unequal numbers and it gets old pretty quick . [SEP]  (A) Each side had men and women on it . (B) Neither side would concede . (C) Each side had equal numbers . (D) None of the above choices .
04/24/2022 23:19:50 - INFO - __main__ - ['Each side had equal numbers .']
04/24/2022 23:19:50 - INFO - __main__ -  [cosmos_qa] Why does Wade refuse to believe the narrator ? [SEP] SO FUNNY . Seriously things like this only happen to me which is a nice story . So he still doens't believe me and I said this stupid line that I can never take back . " Whatever Wade , if you do n't beleive me , go " So what was his glorious responce to that ? [SEP]  (A) The narrator is a known cheat . (B) The story is outlandish . (C) None of the above choices . (D) The narrator is known to lie .
04/24/2022 23:19:50 - INFO - __main__ - ['The story is outlandish .']
04/24/2022 23:19:50 - INFO - __main__ -  [cosmos_qa] What happened because of the wedding ? [SEP] Last weekend we were in Gougan Barra , in county Cork in Ireland . We were there for the wedding of Arlene 's best friend Angella . It was the most relaxing wedding we ever attended . [SEP]  (A) They had a very relaxing weekend (B) They did not go to Gougan Barra (C) They did not have a relaxing weekend (D) None of the above choices .
04/24/2022 23:19:50 - INFO - __main__ - ['They had a very relaxing weekend']
04/24/2022 23:19:50 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:19:50 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:19:50 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 23:19:50 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:19:50 - INFO - __main__ - Printing 3 examples
04/24/2022 23:19:50 - INFO - __main__ -  [cosmos_qa] How may I feeling during this post ? [SEP] The Baltimore Sun , like all print newspapers , is going through a restructuring to cut costs . They have already gone to " tabloid size , " have raised their price , and have offered buyouts to " seasoned staff " . It has been leaked that Gregory Kane , whose articles I ' ve occasionally linked , has taken the buy out . That 's bad news for The Baltimore Sun . [SEP]  (A) Sad (B) None of the above choices . (C) Happy (D) Angry
04/24/2022 23:19:50 - INFO - __main__ - ['Sad']
04/24/2022 23:19:50 - INFO - __main__ -  [cosmos_qa] What state is Alison traveling to at the end of August ? [SEP] Last night was a delightful night . When viewed in isolation , it looks just like I ' ve got a full social life here in Philly . As it is , it was a delightful night , but I ' m REALLY looking forward to when Alison gets here in the end of August . [SEP]  (A) None of the above choices . (B) Alison is headed to California at the end of August . (C) Alison is headed to Pennsylvania . (D) Alison is headed to New York at the end of August .
04/24/2022 23:19:50 - INFO - __main__ - ['Alison is headed to Pennsylvania .']
04/24/2022 23:19:50 - INFO - __main__ -  [cosmos_qa] How may the narrator have injured themselves ? [SEP] I was really out of it the whole time because of the pills I had to take for the pain . I 'm gon na go lay down . My arm is killing me.but there you go . a little update- drive safe everyone and wear your seatbelts please ! I ' m so thankful none of us were severely injured . [SEP]  (A) They got injured playing around . (B) They got injured behind the wheel . (C) They got injured horsing around . (D) They got injured playing sports .
04/24/2022 23:19:50 - INFO - __main__ - ['They got injured behind the wheel .']
04/24/2022 23:19:50 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:19:50 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:19:50 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 23:19:50 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:19:51 - INFO - __main__ - Loaded 1000 examples from test data
04/24/2022 23:20:05 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 23:20:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 23:20:06 - INFO - __main__ - Starting training!
04/24/2022 23:21:39 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_13_0.5_8_predictions.txt
04/24/2022 23:21:39 - INFO - __main__ - ACC on test data: 0.2100
04/24/2022 23:21:40 - INFO - __main__ - prefix=cosmos_qa_32_13, lr=0.5, bsz=8, dev_performance=0.1875, test_performance=0.21
04/24/2022 23:21:40 - INFO - __main__ - Running ... prefix=cosmos_qa_32_13, lr=0.4, bsz=8 ...
04/24/2022 23:21:41 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:21:41 - INFO - __main__ - Printing 3 examples
04/24/2022 23:21:41 - INFO - __main__ -  [cosmos_qa] Why was the war game a success ? [SEP] I was very irritated at myself for being exhausted , which is not productive and got people to think that I was pissed at them . Sorry about that . However , mostly spending the day seating and making the best use of the 3 brain cells I had left got me to relax some and insomnia receded . During the Wreath meeting , a war was started and conceded , which I think was very cool . These last few years I ' ve seen too many battles with unequal numbers and it gets old pretty quick . [SEP]  (A) Each side had men and women on it . (B) Neither side would concede . (C) Each side had equal numbers . (D) None of the above choices .
04/24/2022 23:21:41 - INFO - __main__ - ['Each side had equal numbers .']
04/24/2022 23:21:41 - INFO - __main__ -  [cosmos_qa] Why does Wade refuse to believe the narrator ? [SEP] SO FUNNY . Seriously things like this only happen to me which is a nice story . So he still doens't believe me and I said this stupid line that I can never take back . " Whatever Wade , if you do n't beleive me , go " So what was his glorious responce to that ? [SEP]  (A) The narrator is a known cheat . (B) The story is outlandish . (C) None of the above choices . (D) The narrator is known to lie .
04/24/2022 23:21:41 - INFO - __main__ - ['The story is outlandish .']
04/24/2022 23:21:41 - INFO - __main__ -  [cosmos_qa] What happened because of the wedding ? [SEP] Last weekend we were in Gougan Barra , in county Cork in Ireland . We were there for the wedding of Arlene 's best friend Angella . It was the most relaxing wedding we ever attended . [SEP]  (A) They had a very relaxing weekend (B) They did not go to Gougan Barra (C) They did not have a relaxing weekend (D) None of the above choices .
04/24/2022 23:21:41 - INFO - __main__ - ['They had a very relaxing weekend']
04/24/2022 23:21:41 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:21:41 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:21:41 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 23:21:41 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:21:41 - INFO - __main__ - Printing 3 examples
04/24/2022 23:21:41 - INFO - __main__ -  [cosmos_qa] How may I feeling during this post ? [SEP] The Baltimore Sun , like all print newspapers , is going through a restructuring to cut costs . They have already gone to " tabloid size , " have raised their price , and have offered buyouts to " seasoned staff " . It has been leaked that Gregory Kane , whose articles I ' ve occasionally linked , has taken the buy out . That 's bad news for The Baltimore Sun . [SEP]  (A) Sad (B) None of the above choices . (C) Happy (D) Angry
04/24/2022 23:21:41 - INFO - __main__ - ['Sad']
04/24/2022 23:21:41 - INFO - __main__ -  [cosmos_qa] What state is Alison traveling to at the end of August ? [SEP] Last night was a delightful night . When viewed in isolation , it looks just like I ' ve got a full social life here in Philly . As it is , it was a delightful night , but I ' m REALLY looking forward to when Alison gets here in the end of August . [SEP]  (A) None of the above choices . (B) Alison is headed to California at the end of August . (C) Alison is headed to Pennsylvania . (D) Alison is headed to New York at the end of August .
04/24/2022 23:21:41 - INFO - __main__ - ['Alison is headed to Pennsylvania .']
04/24/2022 23:21:41 - INFO - __main__ -  [cosmos_qa] How may the narrator have injured themselves ? [SEP] I was really out of it the whole time because of the pills I had to take for the pain . I 'm gon na go lay down . My arm is killing me.but there you go . a little update- drive safe everyone and wear your seatbelts please ! I ' m so thankful none of us were severely injured . [SEP]  (A) They got injured playing around . (B) They got injured behind the wheel . (C) They got injured horsing around . (D) They got injured playing sports .
04/24/2022 23:21:41 - INFO - __main__ - ['They got injured behind the wheel .']
04/24/2022 23:21:41 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:21:41 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:21:41 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 23:21:56 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 23:21:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 23:21:57 - INFO - __main__ - Starting training!
04/24/2022 23:22:00 - INFO - __main__ - Step 10 Global step 10 Train loss 1.00 on epoch=4
04/24/2022 23:22:03 - INFO - __main__ - Step 20 Global step 20 Train loss 0.60 on epoch=9
04/24/2022 23:22:06 - INFO - __main__ - Step 30 Global step 30 Train loss 0.38 on epoch=14
04/24/2022 23:22:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.34 on epoch=19
04/24/2022 23:22:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.30 on epoch=24
04/24/2022 23:22:15 - INFO - __main__ - Global step 50 Train loss 0.53 ACC 0.15625 on epoch=24
04/24/2022 23:22:15 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
04/24/2022 23:22:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.28 on epoch=29
04/24/2022 23:22:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.22 on epoch=34
04/24/2022 23:22:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.22 on epoch=39
04/24/2022 23:22:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.22 on epoch=44
04/24/2022 23:22:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.19 on epoch=49
04/24/2022 23:22:33 - INFO - __main__ - Global step 100 Train loss 0.22 ACC 0.15625 on epoch=49
04/24/2022 23:22:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.31 on epoch=54
04/24/2022 23:22:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=59
04/24/2022 23:22:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.24 on epoch=64
04/24/2022 23:22:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.24 on epoch=69
04/24/2022 23:22:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.23 on epoch=74
04/24/2022 23:22:50 - INFO - __main__ - Global step 150 Train loss 0.33 ACC 0.15625 on epoch=74
04/24/2022 23:22:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=79
04/24/2022 23:22:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.19 on epoch=84
04/24/2022 23:22:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
04/24/2022 23:23:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.18 on epoch=94
04/24/2022 23:23:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
04/24/2022 23:23:08 - INFO - __main__ - Global step 200 Train loss 0.21 ACC 0.15625 on epoch=99
04/24/2022 23:23:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.19 on epoch=104
04/24/2022 23:23:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.21 on epoch=109
04/24/2022 23:23:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.17 on epoch=114
04/24/2022 23:23:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.19 on epoch=119
04/24/2022 23:23:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.16 on epoch=124
04/24/2022 23:23:25 - INFO - __main__ - Global step 250 Train loss 0.18 ACC 0.125 on epoch=124
04/24/2022 23:23:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.15 on epoch=129
04/24/2022 23:23:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.16 on epoch=134
04/24/2022 23:23:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.13 on epoch=139
04/24/2022 23:23:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
04/24/2022 23:23:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.16 on epoch=149
04/24/2022 23:23:43 - INFO - __main__ - Global step 300 Train loss 0.15 ACC 0.125 on epoch=149
04/24/2022 23:23:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.12 on epoch=154
04/24/2022 23:23:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.14 on epoch=159
04/24/2022 23:23:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.15 on epoch=164
04/24/2022 23:23:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.15 on epoch=169
04/24/2022 23:23:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
04/24/2022 23:24:00 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.15625 on epoch=174
04/24/2022 23:24:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
04/24/2022 23:24:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
04/24/2022 23:24:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.15 on epoch=189
04/24/2022 23:24:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.14 on epoch=194
04/24/2022 23:24:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=199
04/24/2022 23:24:18 - INFO - __main__ - Global step 400 Train loss 0.13 ACC 0.125 on epoch=199
04/24/2022 23:24:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.15 on epoch=204
04/24/2022 23:24:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.13 on epoch=209
04/24/2022 23:24:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.11 on epoch=214
04/24/2022 23:24:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.13 on epoch=219
04/24/2022 23:24:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.12 on epoch=224
04/24/2022 23:24:36 - INFO - __main__ - Global step 450 Train loss 0.13 ACC 0.15625 on epoch=224
04/24/2022 23:24:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.08 on epoch=229
04/24/2022 23:24:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=234
04/24/2022 23:24:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.12 on epoch=239
04/24/2022 23:24:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.10 on epoch=244
04/24/2022 23:24:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
04/24/2022 23:24:53 - INFO - __main__ - Global step 500 Train loss 0.10 ACC 0.09375 on epoch=249
04/24/2022 23:24:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.13 on epoch=254
04/24/2022 23:24:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.09 on epoch=259
04/24/2022 23:25:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=264
04/24/2022 23:25:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=269
04/24/2022 23:25:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=274
04/24/2022 23:25:11 - INFO - __main__ - Global step 550 Train loss 0.11 ACC 0.125 on epoch=274
04/24/2022 23:25:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=279
04/24/2022 23:25:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.09 on epoch=284
04/24/2022 23:25:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
04/24/2022 23:25:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.12 on epoch=294
04/24/2022 23:25:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=299
04/24/2022 23:25:29 - INFO - __main__ - Global step 600 Train loss 0.09 ACC 0.125 on epoch=299
04/24/2022 23:25:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.08 on epoch=304
04/24/2022 23:25:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=309
04/24/2022 23:25:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=314
04/24/2022 23:25:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=319
04/24/2022 23:25:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
04/24/2022 23:25:46 - INFO - __main__ - Global step 650 Train loss 0.10 ACC 0.21875 on epoch=324
04/24/2022 23:25:46 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.21875 on epoch=324, global_step=650
04/24/2022 23:25:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.10 on epoch=329
04/24/2022 23:25:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=334
04/24/2022 23:25:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
04/24/2022 23:25:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=344
04/24/2022 23:26:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=349
04/24/2022 23:26:04 - INFO - __main__ - Global step 700 Train loss 0.08 ACC 0.09375 on epoch=349
04/24/2022 23:26:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
04/24/2022 23:26:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=359
04/24/2022 23:26:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=364
04/24/2022 23:26:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
04/24/2022 23:26:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=374
04/24/2022 23:26:22 - INFO - __main__ - Global step 750 Train loss 0.07 ACC 0.21875 on epoch=374
04/24/2022 23:26:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=379
04/24/2022 23:26:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=384
04/24/2022 23:26:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
04/24/2022 23:26:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
04/24/2022 23:26:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
04/24/2022 23:26:41 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.1875 on epoch=399
04/24/2022 23:26:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
04/24/2022 23:26:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
04/24/2022 23:26:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=414
04/24/2022 23:26:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
04/24/2022 23:26:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
04/24/2022 23:26:59 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.1875 on epoch=424
04/24/2022 23:27:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
04/24/2022 23:27:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
04/24/2022 23:27:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=439
04/24/2022 23:27:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=444
04/24/2022 23:27:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
04/24/2022 23:27:16 - INFO - __main__ - Global step 900 Train loss 0.04 ACC 0.15625 on epoch=449
04/24/2022 23:27:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
04/24/2022 23:27:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
04/24/2022 23:27:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
04/24/2022 23:27:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
04/24/2022 23:27:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
04/24/2022 23:27:34 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.15625 on epoch=474
04/24/2022 23:27:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=479
04/24/2022 23:27:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
04/24/2022 23:27:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
04/24/2022 23:27:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
04/24/2022 23:27:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/24/2022 23:27:52 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.21875 on epoch=499
04/24/2022 23:27:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
04/24/2022 23:27:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
04/24/2022 23:28:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
04/24/2022 23:28:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
04/24/2022 23:28:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
04/24/2022 23:28:09 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.15625 on epoch=524
04/24/2022 23:28:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
04/24/2022 23:28:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
04/24/2022 23:28:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/24/2022 23:28:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=544
04/24/2022 23:28:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
04/24/2022 23:28:27 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.21875 on epoch=549
04/24/2022 23:28:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
04/24/2022 23:28:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
04/24/2022 23:28:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/24/2022 23:28:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/24/2022 23:28:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
04/24/2022 23:28:45 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.25 on epoch=574
04/24/2022 23:28:45 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=574, global_step=1150
04/24/2022 23:28:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
04/24/2022 23:28:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
04/24/2022 23:28:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/24/2022 23:28:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
04/24/2022 23:28:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
04/24/2022 23:29:02 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.15625 on epoch=599
04/24/2022 23:29:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/24/2022 23:29:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
04/24/2022 23:29:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/24/2022 23:29:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
04/24/2022 23:29:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
04/24/2022 23:29:20 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.21875 on epoch=624
04/24/2022 23:29:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/24/2022 23:29:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
04/24/2022 23:29:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
04/24/2022 23:29:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/24/2022 23:29:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
04/24/2022 23:29:37 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.25 on epoch=649
04/24/2022 23:29:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/24/2022 23:29:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
04/24/2022 23:29:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
04/24/2022 23:29:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/24/2022 23:29:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
04/24/2022 23:29:55 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.21875 on epoch=674
04/24/2022 23:29:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/24/2022 23:30:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/24/2022 23:30:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/24/2022 23:30:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
04/24/2022 23:30:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
04/24/2022 23:30:13 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.25 on epoch=699
04/24/2022 23:30:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/24/2022 23:30:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
04/24/2022 23:30:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
04/24/2022 23:30:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/24/2022 23:30:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/24/2022 23:30:30 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.21875 on epoch=724
04/24/2022 23:30:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/24/2022 23:30:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/24/2022 23:30:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/24/2022 23:30:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/24/2022 23:30:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/24/2022 23:30:48 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.15625 on epoch=749
04/24/2022 23:30:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/24/2022 23:30:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/24/2022 23:30:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
04/24/2022 23:31:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
04/24/2022 23:31:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/24/2022 23:31:06 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.1875 on epoch=774
04/24/2022 23:31:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/24/2022 23:31:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/24/2022 23:31:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/24/2022 23:31:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/24/2022 23:31:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/24/2022 23:31:24 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.21875 on epoch=799
04/24/2022 23:31:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/24/2022 23:31:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/24/2022 23:31:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/24/2022 23:31:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/24/2022 23:31:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/24/2022 23:31:42 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.15625 on epoch=824
04/24/2022 23:31:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/24/2022 23:31:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/24/2022 23:31:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/24/2022 23:31:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/24/2022 23:31:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/24/2022 23:32:00 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.21875 on epoch=849
04/24/2022 23:32:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/24/2022 23:32:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/24/2022 23:32:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/24/2022 23:32:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/24/2022 23:32:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/24/2022 23:32:17 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.25 on epoch=874
04/24/2022 23:32:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/24/2022 23:32:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/24/2022 23:32:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/24/2022 23:32:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
04/24/2022 23:32:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/24/2022 23:32:35 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.25 on epoch=899
04/24/2022 23:32:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/24/2022 23:32:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/24/2022 23:32:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
04/24/2022 23:32:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/24/2022 23:32:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/24/2022 23:32:52 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.125 on epoch=924
04/24/2022 23:32:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/24/2022 23:32:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/24/2022 23:33:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
04/24/2022 23:33:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/24/2022 23:33:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
04/24/2022 23:33:10 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.15625 on epoch=949
04/24/2022 23:33:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
04/24/2022 23:33:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/24/2022 23:33:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/24/2022 23:33:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
04/24/2022 23:33:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/24/2022 23:33:28 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.25 on epoch=974
04/24/2022 23:33:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
04/24/2022 23:33:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/24/2022 23:33:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/24/2022 23:33:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
04/24/2022 23:33:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/24/2022 23:33:44 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:33:44 - INFO - __main__ - Printing 3 examples
04/24/2022 23:33:44 - INFO - __main__ -  [cosmos_qa] Why was the war game a success ? [SEP] I was very irritated at myself for being exhausted , which is not productive and got people to think that I was pissed at them . Sorry about that . However , mostly spending the day seating and making the best use of the 3 brain cells I had left got me to relax some and insomnia receded . During the Wreath meeting , a war was started and conceded , which I think was very cool . These last few years I ' ve seen too many battles with unequal numbers and it gets old pretty quick . [SEP]  (A) Each side had men and women on it . (B) Neither side would concede . (C) Each side had equal numbers . (D) None of the above choices .
04/24/2022 23:33:44 - INFO - __main__ - ['Each side had equal numbers .']
04/24/2022 23:33:44 - INFO - __main__ -  [cosmos_qa] Why does Wade refuse to believe the narrator ? [SEP] SO FUNNY . Seriously things like this only happen to me which is a nice story . So he still doens't believe me and I said this stupid line that I can never take back . " Whatever Wade , if you do n't beleive me , go " So what was his glorious responce to that ? [SEP]  (A) The narrator is a known cheat . (B) The story is outlandish . (C) None of the above choices . (D) The narrator is known to lie .
04/24/2022 23:33:44 - INFO - __main__ - ['The story is outlandish .']
04/24/2022 23:33:44 - INFO - __main__ -  [cosmos_qa] What happened because of the wedding ? [SEP] Last weekend we were in Gougan Barra , in county Cork in Ireland . We were there for the wedding of Arlene 's best friend Angella . It was the most relaxing wedding we ever attended . [SEP]  (A) They had a very relaxing weekend (B) They did not go to Gougan Barra (C) They did not have a relaxing weekend (D) None of the above choices .
04/24/2022 23:33:44 - INFO - __main__ - ['They had a very relaxing weekend']
04/24/2022 23:33:44 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:33:44 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:33:44 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 23:33:44 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:33:44 - INFO - __main__ - Printing 3 examples
04/24/2022 23:33:44 - INFO - __main__ -  [cosmos_qa] How may I feeling during this post ? [SEP] The Baltimore Sun , like all print newspapers , is going through a restructuring to cut costs . They have already gone to " tabloid size , " have raised their price , and have offered buyouts to " seasoned staff " . It has been leaked that Gregory Kane , whose articles I ' ve occasionally linked , has taken the buy out . That 's bad news for The Baltimore Sun . [SEP]  (A) Sad (B) None of the above choices . (C) Happy (D) Angry
04/24/2022 23:33:44 - INFO - __main__ - ['Sad']
04/24/2022 23:33:44 - INFO - __main__ -  [cosmos_qa] What state is Alison traveling to at the end of August ? [SEP] Last night was a delightful night . When viewed in isolation , it looks just like I ' ve got a full social life here in Philly . As it is , it was a delightful night , but I ' m REALLY looking forward to when Alison gets here in the end of August . [SEP]  (A) None of the above choices . (B) Alison is headed to California at the end of August . (C) Alison is headed to Pennsylvania . (D) Alison is headed to New York at the end of August .
04/24/2022 23:33:44 - INFO - __main__ - ['Alison is headed to Pennsylvania .']
04/24/2022 23:33:44 - INFO - __main__ -  [cosmos_qa] How may the narrator have injured themselves ? [SEP] I was really out of it the whole time because of the pills I had to take for the pain . I 'm gon na go lay down . My arm is killing me.but there you go . a little update- drive safe everyone and wear your seatbelts please ! I ' m so thankful none of us were severely injured . [SEP]  (A) They got injured playing around . (B) They got injured behind the wheel . (C) They got injured horsing around . (D) They got injured playing sports .
04/24/2022 23:33:44 - INFO - __main__ - ['They got injured behind the wheel .']
04/24/2022 23:33:44 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:33:44 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:33:44 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 23:33:46 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.15625 on epoch=999
04/24/2022 23:33:46 - INFO - __main__ - save last model!
04/24/2022 23:33:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/24/2022 23:33:46 - INFO - __main__ - Start tokenizing ... 1000 instances
04/24/2022 23:33:46 - INFO - __main__ - Printing 3 examples
04/24/2022 23:33:46 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/24/2022 23:33:46 - INFO - __main__ - ['He wants to get married to a different person .']
04/24/2022 23:33:46 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/24/2022 23:33:46 - INFO - __main__ - ['He was married before and she might come back one day .']
04/24/2022 23:33:46 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/24/2022 23:33:46 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/24/2022 23:33:46 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:33:46 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:33:47 - INFO - __main__ - Loaded 1000 examples from test data
04/24/2022 23:33:59 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 23:33:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 23:33:59 - INFO - __main__ - Starting training!
04/24/2022 23:35:38 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_13_0.4_8_predictions.txt
04/24/2022 23:35:38 - INFO - __main__ - ACC on test data: 0.2220
04/24/2022 23:35:38 - INFO - __main__ - prefix=cosmos_qa_32_13, lr=0.4, bsz=8, dev_performance=0.25, test_performance=0.222
04/24/2022 23:35:38 - INFO - __main__ - Running ... prefix=cosmos_qa_32_13, lr=0.3, bsz=8 ...
04/24/2022 23:35:39 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:35:39 - INFO - __main__ - Printing 3 examples
04/24/2022 23:35:39 - INFO - __main__ -  [cosmos_qa] Why was the war game a success ? [SEP] I was very irritated at myself for being exhausted , which is not productive and got people to think that I was pissed at them . Sorry about that . However , mostly spending the day seating and making the best use of the 3 brain cells I had left got me to relax some and insomnia receded . During the Wreath meeting , a war was started and conceded , which I think was very cool . These last few years I ' ve seen too many battles with unequal numbers and it gets old pretty quick . [SEP]  (A) Each side had men and women on it . (B) Neither side would concede . (C) Each side had equal numbers . (D) None of the above choices .
04/24/2022 23:35:39 - INFO - __main__ - ['Each side had equal numbers .']
04/24/2022 23:35:39 - INFO - __main__ -  [cosmos_qa] Why does Wade refuse to believe the narrator ? [SEP] SO FUNNY . Seriously things like this only happen to me which is a nice story . So he still doens't believe me and I said this stupid line that I can never take back . " Whatever Wade , if you do n't beleive me , go " So what was his glorious responce to that ? [SEP]  (A) The narrator is a known cheat . (B) The story is outlandish . (C) None of the above choices . (D) The narrator is known to lie .
04/24/2022 23:35:39 - INFO - __main__ - ['The story is outlandish .']
04/24/2022 23:35:39 - INFO - __main__ -  [cosmos_qa] What happened because of the wedding ? [SEP] Last weekend we were in Gougan Barra , in county Cork in Ireland . We were there for the wedding of Arlene 's best friend Angella . It was the most relaxing wedding we ever attended . [SEP]  (A) They had a very relaxing weekend (B) They did not go to Gougan Barra (C) They did not have a relaxing weekend (D) None of the above choices .
04/24/2022 23:35:39 - INFO - __main__ - ['They had a very relaxing weekend']
04/24/2022 23:35:39 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:35:39 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:35:39 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 23:35:39 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:35:39 - INFO - __main__ - Printing 3 examples
04/24/2022 23:35:39 - INFO - __main__ -  [cosmos_qa] How may I feeling during this post ? [SEP] The Baltimore Sun , like all print newspapers , is going through a restructuring to cut costs . They have already gone to " tabloid size , " have raised their price , and have offered buyouts to " seasoned staff " . It has been leaked that Gregory Kane , whose articles I ' ve occasionally linked , has taken the buy out . That 's bad news for The Baltimore Sun . [SEP]  (A) Sad (B) None of the above choices . (C) Happy (D) Angry
04/24/2022 23:35:39 - INFO - __main__ - ['Sad']
04/24/2022 23:35:39 - INFO - __main__ -  [cosmos_qa] What state is Alison traveling to at the end of August ? [SEP] Last night was a delightful night . When viewed in isolation , it looks just like I ' ve got a full social life here in Philly . As it is , it was a delightful night , but I ' m REALLY looking forward to when Alison gets here in the end of August . [SEP]  (A) None of the above choices . (B) Alison is headed to California at the end of August . (C) Alison is headed to Pennsylvania . (D) Alison is headed to New York at the end of August .
04/24/2022 23:35:39 - INFO - __main__ - ['Alison is headed to Pennsylvania .']
04/24/2022 23:35:39 - INFO - __main__ -  [cosmos_qa] How may the narrator have injured themselves ? [SEP] I was really out of it the whole time because of the pills I had to take for the pain . I 'm gon na go lay down . My arm is killing me.but there you go . a little update- drive safe everyone and wear your seatbelts please ! I ' m so thankful none of us were severely injured . [SEP]  (A) They got injured playing around . (B) They got injured behind the wheel . (C) They got injured horsing around . (D) They got injured playing sports .
04/24/2022 23:35:39 - INFO - __main__ - ['They got injured behind the wheel .']
04/24/2022 23:35:39 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:35:39 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:35:39 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 23:35:58 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 23:35:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 23:35:59 - INFO - __main__ - Starting training!
04/24/2022 23:36:02 - INFO - __main__ - Step 10 Global step 10 Train loss 1.10 on epoch=4
04/24/2022 23:36:05 - INFO - __main__ - Step 20 Global step 20 Train loss 0.75 on epoch=9
04/24/2022 23:36:08 - INFO - __main__ - Step 30 Global step 30 Train loss 0.42 on epoch=14
04/24/2022 23:36:11 - INFO - __main__ - Step 40 Global step 40 Train loss 0.37 on epoch=19
04/24/2022 23:36:14 - INFO - __main__ - Step 50 Global step 50 Train loss 0.28 on epoch=24
04/24/2022 23:36:17 - INFO - __main__ - Global step 50 Train loss 0.58 ACC 0.1875 on epoch=24
04/24/2022 23:36:17 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
04/24/2022 23:36:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.27 on epoch=29
04/24/2022 23:36:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.24 on epoch=34
04/24/2022 23:36:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.23 on epoch=39
04/24/2022 23:36:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.24 on epoch=44
04/24/2022 23:36:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.24 on epoch=49
04/24/2022 23:36:35 - INFO - __main__ - Global step 100 Train loss 0.25 ACC 0.125 on epoch=49
04/24/2022 23:36:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.23 on epoch=54
04/24/2022 23:36:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.24 on epoch=59
04/24/2022 23:36:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.23 on epoch=64
04/24/2022 23:36:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.20 on epoch=69
04/24/2022 23:36:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.21 on epoch=74
04/24/2022 23:36:52 - INFO - __main__ - Global step 150 Train loss 0.22 ACC 0.125 on epoch=74
04/24/2022 23:36:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.18 on epoch=79
04/24/2022 23:36:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.17 on epoch=84
04/24/2022 23:37:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.16 on epoch=89
04/24/2022 23:37:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.20 on epoch=94
04/24/2022 23:37:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.15 on epoch=99
04/24/2022 23:37:09 - INFO - __main__ - Global step 200 Train loss 0.17 ACC 0.1875 on epoch=99
04/24/2022 23:37:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.17 on epoch=104
04/24/2022 23:37:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.16 on epoch=109
04/24/2022 23:37:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.14 on epoch=114
04/24/2022 23:37:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.14 on epoch=119
04/24/2022 23:37:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.17 on epoch=124
04/24/2022 23:37:27 - INFO - __main__ - Global step 250 Train loss 0.16 ACC 0.125 on epoch=124
04/24/2022 23:37:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.14 on epoch=129
04/24/2022 23:37:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.11 on epoch=134
04/24/2022 23:37:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.10 on epoch=139
04/24/2022 23:37:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.13 on epoch=144
04/24/2022 23:37:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.11 on epoch=149
04/24/2022 23:37:44 - INFO - __main__ - Global step 300 Train loss 0.12 ACC 0.125 on epoch=149
04/24/2022 23:37:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.11 on epoch=154
04/24/2022 23:37:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.09 on epoch=159
04/24/2022 23:37:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.10 on epoch=164
04/24/2022 23:37:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.12 on epoch=169
04/24/2022 23:37:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
04/24/2022 23:38:02 - INFO - __main__ - Global step 350 Train loss 0.11 ACC 0.125 on epoch=174
04/24/2022 23:38:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.10 on epoch=179
04/24/2022 23:38:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.08 on epoch=184
04/24/2022 23:38:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.10 on epoch=189
04/24/2022 23:38:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.10 on epoch=194
04/24/2022 23:38:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.10 on epoch=199
04/24/2022 23:38:19 - INFO - __main__ - Global step 400 Train loss 0.10 ACC 0.15625 on epoch=199
04/24/2022 23:38:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.06 on epoch=204
04/24/2022 23:38:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=209
04/24/2022 23:38:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.06 on epoch=214
04/24/2022 23:38:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.08 on epoch=219
04/24/2022 23:38:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.06 on epoch=224
04/24/2022 23:38:36 - INFO - __main__ - Global step 450 Train loss 0.07 ACC 0.15625 on epoch=224
04/24/2022 23:38:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.06 on epoch=229
04/24/2022 23:38:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
04/24/2022 23:38:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.05 on epoch=239
04/24/2022 23:38:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.06 on epoch=244
04/24/2022 23:38:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
04/24/2022 23:38:54 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.125 on epoch=249
04/24/2022 23:38:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
04/24/2022 23:38:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
04/24/2022 23:39:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
04/24/2022 23:39:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.04 on epoch=269
04/24/2022 23:39:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
04/24/2022 23:39:11 - INFO - __main__ - Global step 550 Train loss 0.05 ACC 0.125 on epoch=274
04/24/2022 23:39:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
04/24/2022 23:39:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
04/24/2022 23:39:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
04/24/2022 23:39:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
04/24/2022 23:39:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
04/24/2022 23:39:29 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.15625 on epoch=299
04/24/2022 23:39:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
04/24/2022 23:39:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
04/24/2022 23:39:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
04/24/2022 23:39:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
04/24/2022 23:39:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=324
04/24/2022 23:39:46 - INFO - __main__ - Global step 650 Train loss 0.04 ACC 0.15625 on epoch=324
04/24/2022 23:39:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
04/24/2022 23:39:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
04/24/2022 23:39:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
04/24/2022 23:39:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
04/24/2022 23:40:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
04/24/2022 23:40:03 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.15625 on epoch=349
04/24/2022 23:40:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
04/24/2022 23:40:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
04/24/2022 23:40:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
04/24/2022 23:40:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
04/24/2022 23:40:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
04/24/2022 23:40:21 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.125 on epoch=374
04/24/2022 23:40:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
04/24/2022 23:40:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
04/24/2022 23:40:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
04/24/2022 23:40:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
04/24/2022 23:40:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
04/24/2022 23:40:38 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.125 on epoch=399
04/24/2022 23:40:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
04/24/2022 23:40:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
04/24/2022 23:40:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
04/24/2022 23:40:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
04/24/2022 23:40:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
04/24/2022 23:40:56 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.0625 on epoch=424
04/24/2022 23:40:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
04/24/2022 23:41:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
04/24/2022 23:41:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
04/24/2022 23:41:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
04/24/2022 23:41:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
04/24/2022 23:41:13 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.09375 on epoch=449
04/24/2022 23:41:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
04/24/2022 23:41:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
04/24/2022 23:41:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
04/24/2022 23:41:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
04/24/2022 23:41:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
04/24/2022 23:41:30 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.09375 on epoch=474
04/24/2022 23:41:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/24/2022 23:41:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
04/24/2022 23:41:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
04/24/2022 23:41:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
04/24/2022 23:41:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/24/2022 23:41:48 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.15625 on epoch=499
04/24/2022 23:41:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
04/24/2022 23:41:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
04/24/2022 23:41:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
04/24/2022 23:41:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
04/24/2022 23:42:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
04/24/2022 23:42:05 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.25 on epoch=524
04/24/2022 23:42:05 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.25 on epoch=524, global_step=1050
04/24/2022 23:42:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
04/24/2022 23:42:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
04/24/2022 23:42:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/24/2022 23:42:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
04/24/2022 23:42:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
04/24/2022 23:42:23 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.15625 on epoch=549
04/24/2022 23:42:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/24/2022 23:42:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/24/2022 23:42:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
04/24/2022 23:42:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
04/24/2022 23:42:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/24/2022 23:42:40 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.125 on epoch=574
04/24/2022 23:42:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/24/2022 23:42:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
04/24/2022 23:42:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
04/24/2022 23:42:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/24/2022 23:42:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/24/2022 23:42:58 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.125 on epoch=599
04/24/2022 23:43:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
04/24/2022 23:43:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
04/24/2022 23:43:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
04/24/2022 23:43:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
04/24/2022 23:43:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/24/2022 23:43:16 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.125 on epoch=624
04/24/2022 23:43:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
04/24/2022 23:43:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
04/24/2022 23:43:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/24/2022 23:43:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/24/2022 23:43:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/24/2022 23:43:34 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.1875 on epoch=649
04/24/2022 23:43:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/24/2022 23:43:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/24/2022 23:43:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/24/2022 23:43:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/24/2022 23:43:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/24/2022 23:43:52 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.125 on epoch=674
04/24/2022 23:43:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/24/2022 23:43:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/24/2022 23:44:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/24/2022 23:44:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/24/2022 23:44:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
04/24/2022 23:44:09 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.15625 on epoch=699
04/24/2022 23:44:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/24/2022 23:44:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
04/24/2022 23:44:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/24/2022 23:44:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
04/24/2022 23:44:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/24/2022 23:44:27 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.0625 on epoch=724
04/24/2022 23:44:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/24/2022 23:44:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/24/2022 23:44:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/24/2022 23:44:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/24/2022 23:44:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/24/2022 23:44:44 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.09375 on epoch=749
04/24/2022 23:44:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/24/2022 23:44:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/24/2022 23:44:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
04/24/2022 23:44:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
04/24/2022 23:44:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/24/2022 23:45:02 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.15625 on epoch=774
04/24/2022 23:45:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/24/2022 23:45:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/24/2022 23:45:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/24/2022 23:45:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/24/2022 23:45:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/24/2022 23:45:19 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.21875 on epoch=799
04/24/2022 23:45:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/24/2022 23:45:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/24/2022 23:45:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/24/2022 23:45:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
04/24/2022 23:45:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/24/2022 23:45:37 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.125 on epoch=824
04/24/2022 23:45:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
04/24/2022 23:45:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/24/2022 23:45:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/24/2022 23:45:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/24/2022 23:45:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
04/24/2022 23:45:55 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.15625 on epoch=849
04/24/2022 23:45:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/24/2022 23:46:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/24/2022 23:46:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
04/24/2022 23:46:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
04/24/2022 23:46:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/24/2022 23:46:13 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.15625 on epoch=874
04/24/2022 23:46:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
04/24/2022 23:46:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/24/2022 23:46:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/24/2022 23:46:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
04/24/2022 23:46:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/24/2022 23:46:30 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.15625 on epoch=899
04/24/2022 23:46:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/24/2022 23:46:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/24/2022 23:46:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/24/2022 23:46:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/24/2022 23:46:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
04/24/2022 23:46:48 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.125 on epoch=924
04/24/2022 23:46:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/24/2022 23:46:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
04/24/2022 23:46:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/24/2022 23:47:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/24/2022 23:47:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
04/24/2022 23:47:06 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.15625 on epoch=949
04/24/2022 23:47:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/24/2022 23:47:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/24/2022 23:47:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/24/2022 23:47:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
04/24/2022 23:47:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/24/2022 23:47:23 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.125 on epoch=974
04/24/2022 23:47:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/24/2022 23:47:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
04/24/2022 23:47:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/24/2022 23:47:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
04/24/2022 23:47:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/24/2022 23:47:39 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:47:39 - INFO - __main__ - Printing 3 examples
04/24/2022 23:47:39 - INFO - __main__ -  [cosmos_qa] Why was the war game a success ? [SEP] I was very irritated at myself for being exhausted , which is not productive and got people to think that I was pissed at them . Sorry about that . However , mostly spending the day seating and making the best use of the 3 brain cells I had left got me to relax some and insomnia receded . During the Wreath meeting , a war was started and conceded , which I think was very cool . These last few years I ' ve seen too many battles with unequal numbers and it gets old pretty quick . [SEP]  (A) Each side had men and women on it . (B) Neither side would concede . (C) Each side had equal numbers . (D) None of the above choices .
04/24/2022 23:47:39 - INFO - __main__ - ['Each side had equal numbers .']
04/24/2022 23:47:39 - INFO - __main__ -  [cosmos_qa] Why does Wade refuse to believe the narrator ? [SEP] SO FUNNY . Seriously things like this only happen to me which is a nice story . So he still doens't believe me and I said this stupid line that I can never take back . " Whatever Wade , if you do n't beleive me , go " So what was his glorious responce to that ? [SEP]  (A) The narrator is a known cheat . (B) The story is outlandish . (C) None of the above choices . (D) The narrator is known to lie .
04/24/2022 23:47:39 - INFO - __main__ - ['The story is outlandish .']
04/24/2022 23:47:39 - INFO - __main__ -  [cosmos_qa] What happened because of the wedding ? [SEP] Last weekend we were in Gougan Barra , in county Cork in Ireland . We were there for the wedding of Arlene 's best friend Angella . It was the most relaxing wedding we ever attended . [SEP]  (A) They had a very relaxing weekend (B) They did not go to Gougan Barra (C) They did not have a relaxing weekend (D) None of the above choices .
04/24/2022 23:47:39 - INFO - __main__ - ['They had a very relaxing weekend']
04/24/2022 23:47:39 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:47:39 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:47:39 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 23:47:39 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:47:39 - INFO - __main__ - Printing 3 examples
04/24/2022 23:47:39 - INFO - __main__ -  [cosmos_qa] How may I feeling during this post ? [SEP] The Baltimore Sun , like all print newspapers , is going through a restructuring to cut costs . They have already gone to " tabloid size , " have raised their price , and have offered buyouts to " seasoned staff " . It has been leaked that Gregory Kane , whose articles I ' ve occasionally linked , has taken the buy out . That 's bad news for The Baltimore Sun . [SEP]  (A) Sad (B) None of the above choices . (C) Happy (D) Angry
04/24/2022 23:47:39 - INFO - __main__ - ['Sad']
04/24/2022 23:47:39 - INFO - __main__ -  [cosmos_qa] What state is Alison traveling to at the end of August ? [SEP] Last night was a delightful night . When viewed in isolation , it looks just like I ' ve got a full social life here in Philly . As it is , it was a delightful night , but I ' m REALLY looking forward to when Alison gets here in the end of August . [SEP]  (A) None of the above choices . (B) Alison is headed to California at the end of August . (C) Alison is headed to Pennsylvania . (D) Alison is headed to New York at the end of August .
04/24/2022 23:47:39 - INFO - __main__ - ['Alison is headed to Pennsylvania .']
04/24/2022 23:47:39 - INFO - __main__ -  [cosmos_qa] How may the narrator have injured themselves ? [SEP] I was really out of it the whole time because of the pills I had to take for the pain . I 'm gon na go lay down . My arm is killing me.but there you go . a little update- drive safe everyone and wear your seatbelts please ! I ' m so thankful none of us were severely injured . [SEP]  (A) They got injured playing around . (B) They got injured behind the wheel . (C) They got injured horsing around . (D) They got injured playing sports .
04/24/2022 23:47:39 - INFO - __main__ - ['They got injured behind the wheel .']
04/24/2022 23:47:39 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:47:39 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:47:39 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 23:47:41 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.125 on epoch=999
04/24/2022 23:47:41 - INFO - __main__ - save last model!
04/24/2022 23:47:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/24/2022 23:47:41 - INFO - __main__ - Start tokenizing ... 1000 instances
04/24/2022 23:47:41 - INFO - __main__ - Printing 3 examples
04/24/2022 23:47:41 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/24/2022 23:47:41 - INFO - __main__ - ['He wants to get married to a different person .']
04/24/2022 23:47:41 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/24/2022 23:47:41 - INFO - __main__ - ['He was married before and she might come back one day .']
04/24/2022 23:47:41 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/24/2022 23:47:41 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/24/2022 23:47:41 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:47:42 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:47:43 - INFO - __main__ - Loaded 1000 examples from test data
04/24/2022 23:47:58 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 23:47:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 23:47:59 - INFO - __main__ - Starting training!
04/24/2022 23:49:29 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_13_0.3_8_predictions.txt
04/24/2022 23:49:29 - INFO - __main__ - ACC on test data: 0.2160
04/24/2022 23:49:29 - INFO - __main__ - prefix=cosmos_qa_32_13, lr=0.3, bsz=8, dev_performance=0.25, test_performance=0.216
04/24/2022 23:49:29 - INFO - __main__ - Running ... prefix=cosmos_qa_32_13, lr=0.2, bsz=8 ...
04/24/2022 23:49:30 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:49:30 - INFO - __main__ - Printing 3 examples
04/24/2022 23:49:30 - INFO - __main__ -  [cosmos_qa] Why was the war game a success ? [SEP] I was very irritated at myself for being exhausted , which is not productive and got people to think that I was pissed at them . Sorry about that . However , mostly spending the day seating and making the best use of the 3 brain cells I had left got me to relax some and insomnia receded . During the Wreath meeting , a war was started and conceded , which I think was very cool . These last few years I ' ve seen too many battles with unequal numbers and it gets old pretty quick . [SEP]  (A) Each side had men and women on it . (B) Neither side would concede . (C) Each side had equal numbers . (D) None of the above choices .
04/24/2022 23:49:30 - INFO - __main__ - ['Each side had equal numbers .']
04/24/2022 23:49:30 - INFO - __main__ -  [cosmos_qa] Why does Wade refuse to believe the narrator ? [SEP] SO FUNNY . Seriously things like this only happen to me which is a nice story . So he still doens't believe me and I said this stupid line that I can never take back . " Whatever Wade , if you do n't beleive me , go " So what was his glorious responce to that ? [SEP]  (A) The narrator is a known cheat . (B) The story is outlandish . (C) None of the above choices . (D) The narrator is known to lie .
04/24/2022 23:49:30 - INFO - __main__ - ['The story is outlandish .']
04/24/2022 23:49:30 - INFO - __main__ -  [cosmos_qa] What happened because of the wedding ? [SEP] Last weekend we were in Gougan Barra , in county Cork in Ireland . We were there for the wedding of Arlene 's best friend Angella . It was the most relaxing wedding we ever attended . [SEP]  (A) They had a very relaxing weekend (B) They did not go to Gougan Barra (C) They did not have a relaxing weekend (D) None of the above choices .
04/24/2022 23:49:30 - INFO - __main__ - ['They had a very relaxing weekend']
04/24/2022 23:49:30 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:49:30 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:49:30 - INFO - __main__ - Loaded 32 examples from train data
04/24/2022 23:49:30 - INFO - __main__ - Start tokenizing ... 32 instances
04/24/2022 23:49:30 - INFO - __main__ - Printing 3 examples
04/24/2022 23:49:30 - INFO - __main__ -  [cosmos_qa] How may I feeling during this post ? [SEP] The Baltimore Sun , like all print newspapers , is going through a restructuring to cut costs . They have already gone to " tabloid size , " have raised their price , and have offered buyouts to " seasoned staff " . It has been leaked that Gregory Kane , whose articles I ' ve occasionally linked , has taken the buy out . That 's bad news for The Baltimore Sun . [SEP]  (A) Sad (B) None of the above choices . (C) Happy (D) Angry
04/24/2022 23:49:30 - INFO - __main__ - ['Sad']
04/24/2022 23:49:30 - INFO - __main__ -  [cosmos_qa] What state is Alison traveling to at the end of August ? [SEP] Last night was a delightful night . When viewed in isolation , it looks just like I ' ve got a full social life here in Philly . As it is , it was a delightful night , but I ' m REALLY looking forward to when Alison gets here in the end of August . [SEP]  (A) None of the above choices . (B) Alison is headed to California at the end of August . (C) Alison is headed to Pennsylvania . (D) Alison is headed to New York at the end of August .
04/24/2022 23:49:30 - INFO - __main__ - ['Alison is headed to Pennsylvania .']
04/24/2022 23:49:30 - INFO - __main__ -  [cosmos_qa] How may the narrator have injured themselves ? [SEP] I was really out of it the whole time because of the pills I had to take for the pain . I 'm gon na go lay down . My arm is killing me.but there you go . a little update- drive safe everyone and wear your seatbelts please ! I ' m so thankful none of us were severely injured . [SEP]  (A) They got injured playing around . (B) They got injured behind the wheel . (C) They got injured horsing around . (D) They got injured playing sports .
04/24/2022 23:49:30 - INFO - __main__ - ['They got injured behind the wheel .']
04/24/2022 23:49:30 - INFO - __main__ - Tokenizing Input ...
04/24/2022 23:49:30 - INFO - __main__ - Tokenizing Output ...
04/24/2022 23:49:30 - INFO - __main__ - Loaded 32 examples from dev data
04/24/2022 23:49:45 - INFO - __main__ - load prompt embedding from ckpt
04/24/2022 23:49:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/24/2022 23:49:46 - INFO - __main__ - Starting training!
04/24/2022 23:49:52 - INFO - __main__ - Step 10 Global step 10 Train loss 1.12 on epoch=4
04/24/2022 23:49:55 - INFO - __main__ - Step 20 Global step 20 Train loss 0.87 on epoch=9
04/24/2022 23:49:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.67 on epoch=14
04/24/2022 23:50:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=19
04/24/2022 23:50:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.33 on epoch=24
04/24/2022 23:50:07 - INFO - __main__ - Global step 50 Train loss 0.69 ACC 0.15625 on epoch=24
04/24/2022 23:50:07 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
04/24/2022 23:50:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.30 on epoch=29
04/24/2022 23:50:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.29 on epoch=34
04/24/2022 23:50:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.28 on epoch=39
04/24/2022 23:50:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=44
04/24/2022 23:50:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=49
04/24/2022 23:50:24 - INFO - __main__ - Global step 100 Train loss 0.37 ACC 0.1875 on epoch=49
04/24/2022 23:50:24 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=49, global_step=100
04/24/2022 23:50:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.30 on epoch=54
04/24/2022 23:50:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
04/24/2022 23:50:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.22 on epoch=64
04/24/2022 23:50:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.22 on epoch=69
04/24/2022 23:50:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.23 on epoch=74
04/24/2022 23:50:42 - INFO - __main__ - Global step 150 Train loss 0.25 ACC 0.15625 on epoch=74
04/24/2022 23:50:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
04/24/2022 23:50:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.21 on epoch=84
04/24/2022 23:50:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=89
04/24/2022 23:50:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=94
04/24/2022 23:50:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.23 on epoch=99
04/24/2022 23:50:59 - INFO - __main__ - Global step 200 Train loss 0.24 ACC 0.21875 on epoch=99
04/24/2022 23:50:59 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=99, global_step=200
04/24/2022 23:51:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=104
04/24/2022 23:51:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.19 on epoch=109
04/24/2022 23:51:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.21 on epoch=114
04/24/2022 23:51:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=119
04/24/2022 23:51:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.18 on epoch=124
04/24/2022 23:51:17 - INFO - __main__ - Global step 250 Train loss 0.20 ACC 0.1875 on epoch=124
04/24/2022 23:51:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.20 on epoch=129
04/24/2022 23:51:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.20 on epoch=134
04/24/2022 23:51:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=139
04/24/2022 23:51:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=144
04/24/2022 23:51:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.20 on epoch=149
04/24/2022 23:51:34 - INFO - __main__ - Global step 300 Train loss 0.20 ACC 0.15625 on epoch=149
04/24/2022 23:51:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
04/24/2022 23:51:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=159
04/24/2022 23:51:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.18 on epoch=164
04/24/2022 23:51:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.19 on epoch=169
04/24/2022 23:51:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.20 on epoch=174
04/24/2022 23:51:52 - INFO - __main__ - Global step 350 Train loss 0.19 ACC 0.15625 on epoch=174
04/24/2022 23:51:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.17 on epoch=179
04/24/2022 23:51:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.16 on epoch=184
04/24/2022 23:52:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.17 on epoch=189
04/24/2022 23:52:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.16 on epoch=194
04/24/2022 23:52:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.14 on epoch=199
04/24/2022 23:52:09 - INFO - __main__ - Global step 400 Train loss 0.16 ACC 0.1875 on epoch=199
04/24/2022 23:52:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.15 on epoch=204
04/24/2022 23:52:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
04/24/2022 23:52:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=214
04/24/2022 23:52:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.13 on epoch=219
04/24/2022 23:52:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.14 on epoch=224
04/24/2022 23:52:27 - INFO - __main__ - Global step 450 Train loss 0.15 ACC 0.1875 on epoch=224
04/24/2022 23:52:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=229
04/24/2022 23:52:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.14 on epoch=234
04/24/2022 23:52:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=239
04/24/2022 23:52:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
04/24/2022 23:52:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
04/24/2022 23:52:45 - INFO - __main__ - Global step 500 Train loss 0.13 ACC 0.125 on epoch=249
04/24/2022 23:52:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.12 on epoch=254
04/24/2022 23:52:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=259
04/24/2022 23:52:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.14 on epoch=264
04/24/2022 23:52:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.12 on epoch=269
04/24/2022 23:53:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=274
04/24/2022 23:53:03 - INFO - __main__ - Global step 550 Train loss 0.13 ACC 0.0625 on epoch=274
04/24/2022 23:53:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.16 on epoch=279
04/24/2022 23:53:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=284
04/24/2022 23:53:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.12 on epoch=289
04/24/2022 23:53:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.12 on epoch=294
04/24/2022 23:53:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
04/24/2022 23:53:20 - INFO - __main__ - Global step 600 Train loss 0.13 ACC 0.09375 on epoch=299
04/24/2022 23:53:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=304
04/24/2022 23:53:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=309
04/24/2022 23:53:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=314
04/24/2022 23:53:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
04/24/2022 23:53:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=324
04/24/2022 23:53:38 - INFO - __main__ - Global step 650 Train loss 0.12 ACC 0.15625 on epoch=324
04/24/2022 23:53:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.10 on epoch=329
04/24/2022 23:53:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=334
04/24/2022 23:53:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=339
04/24/2022 23:53:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=344
04/24/2022 23:53:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=349
04/24/2022 23:53:56 - INFO - __main__ - Global step 700 Train loss 0.11 ACC 0.09375 on epoch=349
04/24/2022 23:53:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=354
04/24/2022 23:54:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=359
04/24/2022 23:54:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=364
04/24/2022 23:54:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=369
04/24/2022 23:54:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
04/24/2022 23:54:13 - INFO - __main__ - Global step 750 Train loss 0.10 ACC 0.125 on epoch=374
04/24/2022 23:54:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=379
04/24/2022 23:54:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=384
04/24/2022 23:54:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=389
04/24/2022 23:54:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=394
04/24/2022 23:54:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=399
04/24/2022 23:54:31 - INFO - __main__ - Global step 800 Train loss 0.10 ACC 0.0625 on epoch=399
04/24/2022 23:54:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.13 on epoch=404
04/24/2022 23:54:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=409
04/24/2022 23:54:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.13 on epoch=414
04/24/2022 23:54:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=419
04/24/2022 23:54:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=424
04/24/2022 23:54:48 - INFO - __main__ - Global step 850 Train loss 0.10 ACC 0.09375 on epoch=424
04/24/2022 23:54:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=429
04/24/2022 23:54:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=434
04/24/2022 23:54:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=439
04/24/2022 23:55:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=444
04/24/2022 23:55:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=449
04/24/2022 23:55:06 - INFO - __main__ - Global step 900 Train loss 0.09 ACC 0.0625 on epoch=449
04/24/2022 23:55:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=454
04/24/2022 23:55:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
04/24/2022 23:55:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
04/24/2022 23:55:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=469
04/24/2022 23:55:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=474
04/24/2022 23:55:24 - INFO - __main__ - Global step 950 Train loss 0.08 ACC 0.09375 on epoch=474
04/24/2022 23:55:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=479
04/24/2022 23:55:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=484
04/24/2022 23:55:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=489
04/24/2022 23:55:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=494
04/24/2022 23:55:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=499
04/24/2022 23:55:41 - INFO - __main__ - Global step 1000 Train loss 0.08 ACC 0.0625 on epoch=499
04/24/2022 23:55:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=504
04/24/2022 23:55:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=509
04/24/2022 23:55:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
04/24/2022 23:55:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=519
04/24/2022 23:55:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
04/24/2022 23:55:59 - INFO - __main__ - Global step 1050 Train loss 0.07 ACC 0.15625 on epoch=524
04/24/2022 23:56:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=529
04/24/2022 23:56:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=534
04/24/2022 23:56:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=539
04/24/2022 23:56:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
04/24/2022 23:56:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=549
04/24/2022 23:56:16 - INFO - __main__ - Global step 1100 Train loss 0.07 ACC 0.15625 on epoch=549
04/24/2022 23:56:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=554
04/24/2022 23:56:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=559
04/24/2022 23:56:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=564
04/24/2022 23:56:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=569
04/24/2022 23:56:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=574
04/24/2022 23:56:34 - INFO - __main__ - Global step 1150 Train loss 0.07 ACC 0.09375 on epoch=574
04/24/2022 23:56:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=579
04/24/2022 23:56:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=584
04/24/2022 23:56:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=589
04/24/2022 23:56:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=594
04/24/2022 23:56:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=599
04/24/2022 23:56:52 - INFO - __main__ - Global step 1200 Train loss 0.07 ACC 0.125 on epoch=599
04/24/2022 23:56:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
04/24/2022 23:56:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
04/24/2022 23:57:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
04/24/2022 23:57:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
04/24/2022 23:57:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=624
04/24/2022 23:57:09 - INFO - __main__ - Global step 1250 Train loss 0.06 ACC 0.15625 on epoch=624
04/24/2022 23:57:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
04/24/2022 23:57:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=634
04/24/2022 23:57:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=639
04/24/2022 23:57:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=644
04/24/2022 23:57:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
04/24/2022 23:57:27 - INFO - __main__ - Global step 1300 Train loss 0.06 ACC 0.15625 on epoch=649
04/24/2022 23:57:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=654
04/24/2022 23:57:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=659
04/24/2022 23:57:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=664
04/24/2022 23:57:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
04/24/2022 23:57:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
04/24/2022 23:57:45 - INFO - __main__ - Global step 1350 Train loss 0.05 ACC 0.1875 on epoch=674
04/24/2022 23:57:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=679
04/24/2022 23:57:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=684
04/24/2022 23:57:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
04/24/2022 23:57:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=694
04/24/2022 23:58:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
04/24/2022 23:58:03 - INFO - __main__ - Global step 1400 Train loss 0.05 ACC 0.125 on epoch=699
04/24/2022 23:58:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=704
04/24/2022 23:58:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=709
04/24/2022 23:58:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=714
04/24/2022 23:58:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=719
04/24/2022 23:58:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=724
04/24/2022 23:58:21 - INFO - __main__ - Global step 1450 Train loss 0.05 ACC 0.1875 on epoch=724
04/24/2022 23:58:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
04/24/2022 23:58:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=734
04/24/2022 23:58:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
04/24/2022 23:58:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
04/24/2022 23:58:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
04/24/2022 23:58:38 - INFO - __main__ - Global step 1500 Train loss 0.04 ACC 0.09375 on epoch=749
04/24/2022 23:58:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
04/24/2022 23:58:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
04/24/2022 23:58:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
04/24/2022 23:58:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
04/24/2022 23:58:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
04/24/2022 23:58:56 - INFO - __main__ - Global step 1550 Train loss 0.03 ACC 0.1875 on epoch=774
04/24/2022 23:58:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=779
04/24/2022 23:59:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=784
04/24/2022 23:59:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=789
04/24/2022 23:59:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
04/24/2022 23:59:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=799
04/24/2022 23:59:14 - INFO - __main__ - Global step 1600 Train loss 0.05 ACC 0.25 on epoch=799
04/24/2022 23:59:14 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=799, global_step=1600
04/24/2022 23:59:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=804
04/24/2022 23:59:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
04/24/2022 23:59:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=814
04/24/2022 23:59:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
04/24/2022 23:59:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
04/24/2022 23:59:32 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.21875 on epoch=824
04/24/2022 23:59:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=829
04/24/2022 23:59:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=834
04/24/2022 23:59:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
04/24/2022 23:59:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=844
04/24/2022 23:59:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=849
04/24/2022 23:59:50 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.15625 on epoch=849
04/24/2022 23:59:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
04/24/2022 23:59:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=859
04/24/2022 23:59:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
04/25/2022 00:00:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
04/25/2022 00:00:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=874
04/25/2022 00:00:08 - INFO - __main__ - Global step 1750 Train loss 0.03 ACC 0.1875 on epoch=874
04/25/2022 00:00:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/25/2022 00:00:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=884
04/25/2022 00:00:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
04/25/2022 00:00:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
04/25/2022 00:00:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
04/25/2022 00:00:26 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.09375 on epoch=899
04/25/2022 00:00:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
04/25/2022 00:00:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=909
04/25/2022 00:00:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
04/25/2022 00:00:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=919
04/25/2022 00:00:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=924
04/25/2022 00:00:43 - INFO - __main__ - Global step 1850 Train loss 0.04 ACC 0.09375 on epoch=924
04/25/2022 00:00:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=929
04/25/2022 00:00:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
04/25/2022 00:00:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=939
04/25/2022 00:00:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 00:00:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
04/25/2022 00:01:01 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.15625 on epoch=949
04/25/2022 00:01:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
04/25/2022 00:01:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
04/25/2022 00:01:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=964
04/25/2022 00:01:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
04/25/2022 00:01:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
04/25/2022 00:01:19 - INFO - __main__ - Global step 1950 Train loss 0.03 ACC 0.09375 on epoch=974
04/25/2022 00:01:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
04/25/2022 00:01:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 00:01:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=989
04/25/2022 00:01:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
04/25/2022 00:01:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 00:01:35 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:01:35 - INFO - __main__ - Printing 3 examples
04/25/2022 00:01:35 - INFO - __main__ -  [cosmos_qa] What may happen after the event has concluded ? [SEP] After it was all said and done , we surpassed our fundraising target of $ 25,000 ! First and foremost , this meant a good donation to the Rickshaw Bank , which was able to start a new outfit in Allahabad . But the group accomplished something else this year - the establishment of our team . [SEP]  (A) They will find a way to pay back the $ 25,000 . (B) The earnings will be put to charitable use . (C) They will hope to surpass the fundraising target . (D) They will invest their donation from the Rickshaw Bank .
04/25/2022 00:01:35 - INFO - __main__ - ['The earnings will be put to charitable use .']
04/25/2022 00:01:35 - INFO - __main__ -  [cosmos_qa] What can you conclude about Bri and whipped cream ? [SEP] We hosted dinner for a young couple who just moved to Eugene ( daughter & her partner of Chicago friend . ) Bri expressed amazement when Bob announced he was going to make the whipped cream for the dessert . " You 're making the whipped cream ? ! " I guess she thought it was some complicated process , and was amazed further to find that all you do it put heavy cream in a bowl and whip it . [SEP]  (A) Bri has never made cream dessert in her life . (B) Bri has never made whipped cream in her life . (C) None of the above choices . (D) Bri has never made heavy cream in her life .
04/25/2022 00:01:35 - INFO - __main__ - ['Bri has never made whipped cream in her life .']
04/25/2022 00:01:35 - INFO - __main__ -  [cosmos_qa] Why is the narrator down on USC ? [SEP] When one looks at the rankings of CF for the first four weeks , you see USC in the # 1 spot . In fact , in weeks 3 and 4 they nearly got all of the # 1 votes . WHY ? [SEP]  (A) They feel they let them down . (B) They feel they are mean . (C) They feel they are a bad football team . (D) They feel they are not as good as others
04/25/2022 00:01:35 - INFO - __main__ - ['They feel they are not as good as others']
04/25/2022 00:01:35 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:01:35 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:01:35 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 00:01:35 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:01:35 - INFO - __main__ - Printing 3 examples
04/25/2022 00:01:35 - INFO - __main__ -  [cosmos_qa] Why is Roger 's face so red and flushed ? [SEP] He took a deep breath and unlocked the door . Roger pulled out the door and simply stepped in and locked it again . John was surprised , Roger did n't seem to be angry with him . " I 'm so sorry , I do n't know why I did that , I must be out of my mind and ... " John was interrupted by Roger 's tongue in his mouth , curiously seeking around in there . John felt both surprised and pleased at the same time , and answered Roger 's kiss . When they broke the contact , John could n't say anything except a simple " .. why ? " Roger blushed and shrugged his shoulders . [SEP]  (A) Roger just got dumped . (B) Roger is fearful for his job . (C) Roger got kissed by John . (D) Roger is worried about something .
04/25/2022 00:01:35 - INFO - __main__ - ['Roger got kissed by John .']
04/25/2022 00:01:35 - INFO - __main__ -  [cosmos_qa] What is the speaker trying to do ? [SEP] If someone points out something substantive I can fix in the analysis , I will try to clean it up . First , read this . This page gives a good discussion of a liquidity crisis in corporate finance . A macroeconomic " high finance " liquidity crisis is essentially the same thing , only happening with large banks . As in everything in high finance , high - finance liquidity crises are more complex . One key reason high - finance liquidity crises are much more complex and potentially disastrous than regular corporate liquidity crises is the meshed nature of the problem . [SEP]  (A) They are teaching their coworkers about liquidation . (B) They are trying to give a presentation at work . (C) They are trying to teach others about macroeconomics . (D) None of the above choices .
04/25/2022 00:01:35 - INFO - __main__ - ['They are trying to teach others about macroeconomics .']
04/25/2022 00:01:35 - INFO - __main__ -  [cosmos_qa] What may happen if i was more sociable ? [SEP] I do n't think I did the greatest job at networking , but I tried a bit . I ' m just not naturally a social person and it 's rather difficult for me to talk with people I do n't know very well . ( I could have used The Alternative Scientist 's Guide To Networking , but , alas , it was mostly posted while I was away . ) [SEP]  (A) I would feel like an outcast . (B) None of the above choices . (C) I would be disliked . (D) I would be criticized .
04/25/2022 00:01:35 - INFO - __main__ - ['None of the above choices .']
04/25/2022 00:01:35 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:01:35 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:01:35 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 00:01:37 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.125 on epoch=999
04/25/2022 00:01:37 - INFO - __main__ - save last model!
04/25/2022 00:01:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 00:01:37 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 00:01:37 - INFO - __main__ - Printing 3 examples
04/25/2022 00:01:37 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 00:01:37 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 00:01:37 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 00:01:37 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 00:01:37 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 00:01:37 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 00:01:37 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:01:38 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:01:39 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 00:01:50 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 00:01:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 00:01:51 - INFO - __main__ - Starting training!
04/25/2022 00:03:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_13_0.2_8_predictions.txt
04/25/2022 00:03:28 - INFO - __main__ - ACC on test data: 0.2210
04/25/2022 00:03:28 - INFO - __main__ - prefix=cosmos_qa_32_13, lr=0.2, bsz=8, dev_performance=0.25, test_performance=0.221
04/25/2022 00:03:28 - INFO - __main__ - Running ... prefix=cosmos_qa_32_21, lr=0.5, bsz=8 ...
04/25/2022 00:03:29 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:03:29 - INFO - __main__ - Printing 3 examples
04/25/2022 00:03:29 - INFO - __main__ -  [cosmos_qa] What may happen after the event has concluded ? [SEP] After it was all said and done , we surpassed our fundraising target of $ 25,000 ! First and foremost , this meant a good donation to the Rickshaw Bank , which was able to start a new outfit in Allahabad . But the group accomplished something else this year - the establishment of our team . [SEP]  (A) They will find a way to pay back the $ 25,000 . (B) The earnings will be put to charitable use . (C) They will hope to surpass the fundraising target . (D) They will invest their donation from the Rickshaw Bank .
04/25/2022 00:03:29 - INFO - __main__ - ['The earnings will be put to charitable use .']
04/25/2022 00:03:29 - INFO - __main__ -  [cosmos_qa] What can you conclude about Bri and whipped cream ? [SEP] We hosted dinner for a young couple who just moved to Eugene ( daughter & her partner of Chicago friend . ) Bri expressed amazement when Bob announced he was going to make the whipped cream for the dessert . " You 're making the whipped cream ? ! " I guess she thought it was some complicated process , and was amazed further to find that all you do it put heavy cream in a bowl and whip it . [SEP]  (A) Bri has never made cream dessert in her life . (B) Bri has never made whipped cream in her life . (C) None of the above choices . (D) Bri has never made heavy cream in her life .
04/25/2022 00:03:29 - INFO - __main__ - ['Bri has never made whipped cream in her life .']
04/25/2022 00:03:29 - INFO - __main__ -  [cosmos_qa] Why is the narrator down on USC ? [SEP] When one looks at the rankings of CF for the first four weeks , you see USC in the # 1 spot . In fact , in weeks 3 and 4 they nearly got all of the # 1 votes . WHY ? [SEP]  (A) They feel they let them down . (B) They feel they are mean . (C) They feel they are a bad football team . (D) They feel they are not as good as others
04/25/2022 00:03:29 - INFO - __main__ - ['They feel they are not as good as others']
04/25/2022 00:03:29 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:03:29 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:03:29 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 00:03:29 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:03:29 - INFO - __main__ - Printing 3 examples
04/25/2022 00:03:29 - INFO - __main__ -  [cosmos_qa] Why is Roger 's face so red and flushed ? [SEP] He took a deep breath and unlocked the door . Roger pulled out the door and simply stepped in and locked it again . John was surprised , Roger did n't seem to be angry with him . " I 'm so sorry , I do n't know why I did that , I must be out of my mind and ... " John was interrupted by Roger 's tongue in his mouth , curiously seeking around in there . John felt both surprised and pleased at the same time , and answered Roger 's kiss . When they broke the contact , John could n't say anything except a simple " .. why ? " Roger blushed and shrugged his shoulders . [SEP]  (A) Roger just got dumped . (B) Roger is fearful for his job . (C) Roger got kissed by John . (D) Roger is worried about something .
04/25/2022 00:03:29 - INFO - __main__ - ['Roger got kissed by John .']
04/25/2022 00:03:29 - INFO - __main__ -  [cosmos_qa] What is the speaker trying to do ? [SEP] If someone points out something substantive I can fix in the analysis , I will try to clean it up . First , read this . This page gives a good discussion of a liquidity crisis in corporate finance . A macroeconomic " high finance " liquidity crisis is essentially the same thing , only happening with large banks . As in everything in high finance , high - finance liquidity crises are more complex . One key reason high - finance liquidity crises are much more complex and potentially disastrous than regular corporate liquidity crises is the meshed nature of the problem . [SEP]  (A) They are teaching their coworkers about liquidation . (B) They are trying to give a presentation at work . (C) They are trying to teach others about macroeconomics . (D) None of the above choices .
04/25/2022 00:03:29 - INFO - __main__ - ['They are trying to teach others about macroeconomics .']
04/25/2022 00:03:29 - INFO - __main__ -  [cosmos_qa] What may happen if i was more sociable ? [SEP] I do n't think I did the greatest job at networking , but I tried a bit . I ' m just not naturally a social person and it 's rather difficult for me to talk with people I do n't know very well . ( I could have used The Alternative Scientist 's Guide To Networking , but , alas , it was mostly posted while I was away . ) [SEP]  (A) I would feel like an outcast . (B) None of the above choices . (C) I would be disliked . (D) I would be criticized .
04/25/2022 00:03:29 - INFO - __main__ - ['None of the above choices .']
04/25/2022 00:03:29 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:03:29 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:03:29 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 00:03:44 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 00:03:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 00:03:45 - INFO - __main__ - Starting training!
04/25/2022 00:03:52 - INFO - __main__ - Step 10 Global step 10 Train loss 0.97 on epoch=4
04/25/2022 00:03:55 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=9
04/25/2022 00:03:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.33 on epoch=14
04/25/2022 00:04:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.30 on epoch=19
04/25/2022 00:04:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.29 on epoch=24
04/25/2022 00:04:07 - INFO - __main__ - Global step 50 Train loss 0.49 ACC 0.125 on epoch=24
04/25/2022 00:04:07 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.125 on epoch=24, global_step=50
04/25/2022 00:04:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.24 on epoch=29
04/25/2022 00:04:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.25 on epoch=34
04/25/2022 00:04:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.22 on epoch=39
04/25/2022 00:04:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.22 on epoch=44
04/25/2022 00:04:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.19 on epoch=49
04/25/2022 00:04:26 - INFO - __main__ - Global step 100 Train loss 0.23 ACC 0.15625 on epoch=49
04/25/2022 00:04:26 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.15625 on epoch=49, global_step=100
04/25/2022 00:04:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.21 on epoch=54
04/25/2022 00:04:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.18 on epoch=59
04/25/2022 00:04:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.18 on epoch=64
04/25/2022 00:04:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.15 on epoch=69
04/25/2022 00:04:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.17 on epoch=74
04/25/2022 00:04:45 - INFO - __main__ - Global step 150 Train loss 0.18 ACC 0.15625 on epoch=74
04/25/2022 00:04:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.13 on epoch=79
04/25/2022 00:04:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.17 on epoch=84
04/25/2022 00:04:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.14 on epoch=89
04/25/2022 00:04:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.11 on epoch=94
04/25/2022 00:05:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.13 on epoch=99
04/25/2022 00:05:03 - INFO - __main__ - Global step 200 Train loss 0.13 ACC 0.125 on epoch=99
04/25/2022 00:05:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.10 on epoch=104
04/25/2022 00:05:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.11 on epoch=109
04/25/2022 00:05:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.12 on epoch=114
04/25/2022 00:05:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.09 on epoch=119
04/25/2022 00:05:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.11 on epoch=124
04/25/2022 00:05:23 - INFO - __main__ - Global step 250 Train loss 0.11 ACC 0.1875 on epoch=124
04/25/2022 00:05:23 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=124, global_step=250
04/25/2022 00:05:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.08 on epoch=129
04/25/2022 00:05:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.08 on epoch=134
04/25/2022 00:05:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.08 on epoch=139
04/25/2022 00:05:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.09 on epoch=144
04/25/2022 00:05:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.08 on epoch=149
04/25/2022 00:05:42 - INFO - __main__ - Global step 300 Train loss 0.08 ACC 0.15625 on epoch=149
04/25/2022 00:05:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.07 on epoch=154
04/25/2022 00:05:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.07 on epoch=159
04/25/2022 00:05:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.06 on epoch=164
04/25/2022 00:05:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.05 on epoch=169
04/25/2022 00:05:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.05 on epoch=174
04/25/2022 00:06:01 - INFO - __main__ - Global step 350 Train loss 0.06 ACC 0.15625 on epoch=174
04/25/2022 00:06:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.04 on epoch=179
04/25/2022 00:06:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.06 on epoch=184
04/25/2022 00:06:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.04 on epoch=189
04/25/2022 00:06:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.04 on epoch=194
04/25/2022 00:06:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.04 on epoch=199
04/25/2022 00:06:21 - INFO - __main__ - Global step 400 Train loss 0.04 ACC 0.125 on epoch=199
04/25/2022 00:06:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.04 on epoch=204
04/25/2022 00:06:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.05 on epoch=209
04/25/2022 00:06:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
04/25/2022 00:06:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.05 on epoch=219
04/25/2022 00:06:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.06 on epoch=224
04/25/2022 00:06:40 - INFO - __main__ - Global step 450 Train loss 0.05 ACC 0.125 on epoch=224
04/25/2022 00:06:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
04/25/2022 00:06:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
04/25/2022 00:06:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.06 on epoch=239
04/25/2022 00:06:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
04/25/2022 00:06:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.03 on epoch=249
04/25/2022 00:06:59 - INFO - __main__ - Global step 500 Train loss 0.05 ACC 0.1875 on epoch=249
04/25/2022 00:07:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.04 on epoch=254
04/25/2022 00:07:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
04/25/2022 00:07:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=264
04/25/2022 00:07:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.04 on epoch=269
04/25/2022 00:07:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
04/25/2022 00:07:19 - INFO - __main__ - Global step 550 Train loss 0.04 ACC 0.15625 on epoch=274
04/25/2022 00:07:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
04/25/2022 00:07:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
04/25/2022 00:07:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=289
04/25/2022 00:07:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
04/25/2022 00:07:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
04/25/2022 00:07:38 - INFO - __main__ - Global step 600 Train loss 0.02 ACC 0.125 on epoch=299
04/25/2022 00:07:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
04/25/2022 00:07:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
04/25/2022 00:07:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
04/25/2022 00:07:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
04/25/2022 00:07:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=324
04/25/2022 00:07:57 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.21875 on epoch=324
04/25/2022 00:07:57 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=324, global_step=650
04/25/2022 00:08:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
04/25/2022 00:08:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
04/25/2022 00:08:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
04/25/2022 00:08:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
04/25/2022 00:08:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
04/25/2022 00:08:17 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.09375 on epoch=349
04/25/2022 00:08:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
04/25/2022 00:08:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
04/25/2022 00:08:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
04/25/2022 00:08:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
04/25/2022 00:08:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
04/25/2022 00:08:36 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.125 on epoch=374
04/25/2022 00:08:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
04/25/2022 00:08:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
04/25/2022 00:08:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
04/25/2022 00:08:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
04/25/2022 00:08:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
04/25/2022 00:08:56 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.15625 on epoch=399
04/25/2022 00:08:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
04/25/2022 00:09:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
04/25/2022 00:09:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
04/25/2022 00:09:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
04/25/2022 00:09:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
04/25/2022 00:09:15 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.1875 on epoch=424
04/25/2022 00:09:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
04/25/2022 00:09:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
04/25/2022 00:09:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
04/25/2022 00:09:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
04/25/2022 00:09:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
04/25/2022 00:09:35 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.09375 on epoch=449
04/25/2022 00:09:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
04/25/2022 00:09:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
04/25/2022 00:09:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
04/25/2022 00:09:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
04/25/2022 00:09:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
04/25/2022 00:09:54 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.0625 on epoch=474
04/25/2022 00:09:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/25/2022 00:10:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
04/25/2022 00:10:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
04/25/2022 00:10:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
04/25/2022 00:10:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
04/25/2022 00:10:13 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.1875 on epoch=499
04/25/2022 00:10:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
04/25/2022 00:10:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
04/25/2022 00:10:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
04/25/2022 00:10:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
04/25/2022 00:10:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/25/2022 00:10:33 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.03125 on epoch=524
04/25/2022 00:10:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
04/25/2022 00:10:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
04/25/2022 00:10:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
04/25/2022 00:10:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
04/25/2022 00:10:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
04/25/2022 00:10:52 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.15625 on epoch=549
04/25/2022 00:10:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/25/2022 00:10:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/25/2022 00:11:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
04/25/2022 00:11:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/25/2022 00:11:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
04/25/2022 00:11:12 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.15625 on epoch=574
04/25/2022 00:11:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 00:11:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
04/25/2022 00:11:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
04/25/2022 00:11:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 00:11:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/25/2022 00:11:31 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.15625 on epoch=599
04/25/2022 00:11:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
04/25/2022 00:11:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 00:11:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
04/25/2022 00:11:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
04/25/2022 00:11:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
04/25/2022 00:11:51 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.1875 on epoch=624
04/25/2022 00:11:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 00:11:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
04/25/2022 00:12:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/25/2022 00:12:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
04/25/2022 00:12:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
04/25/2022 00:12:11 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.15625 on epoch=649
04/25/2022 00:12:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 00:12:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 00:12:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
04/25/2022 00:12:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/25/2022 00:12:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
04/25/2022 00:12:30 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.1875 on epoch=674
04/25/2022 00:12:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 00:12:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=684
04/25/2022 00:12:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/25/2022 00:12:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 00:12:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 00:12:50 - INFO - __main__ - Global step 1400 Train loss 0.02 ACC 0.1875 on epoch=699
04/25/2022 00:12:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 00:12:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 00:12:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
04/25/2022 00:13:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 00:13:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/25/2022 00:13:09 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.125 on epoch=724
04/25/2022 00:13:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 00:13:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
04/25/2022 00:13:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 00:13:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 00:13:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
04/25/2022 00:13:29 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.09375 on epoch=749
04/25/2022 00:13:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 00:13:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
04/25/2022 00:13:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
04/25/2022 00:13:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
04/25/2022 00:13:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/25/2022 00:13:48 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.09375 on epoch=774
04/25/2022 00:13:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 00:13:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
04/25/2022 00:13:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
04/25/2022 00:14:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
04/25/2022 00:14:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
04/25/2022 00:14:08 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.09375 on epoch=799
04/25/2022 00:14:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
04/25/2022 00:14:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 00:14:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
04/25/2022 00:14:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 00:14:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
04/25/2022 00:14:27 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.09375 on epoch=824
04/25/2022 00:14:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/25/2022 00:14:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
04/25/2022 00:14:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 00:14:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
04/25/2022 00:14:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
04/25/2022 00:14:46 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.21875 on epoch=849
04/25/2022 00:14:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
04/25/2022 00:14:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
04/25/2022 00:14:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 00:14:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 00:15:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 00:15:06 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.21875 on epoch=874
04/25/2022 00:15:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
04/25/2022 00:15:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
04/25/2022 00:15:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 00:15:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
04/25/2022 00:15:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
04/25/2022 00:15:25 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.1875 on epoch=899
04/25/2022 00:15:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
04/25/2022 00:15:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
04/25/2022 00:15:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 00:15:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
04/25/2022 00:15:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
04/25/2022 00:15:45 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.21875 on epoch=924
04/25/2022 00:15:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 00:15:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
04/25/2022 00:15:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
04/25/2022 00:15:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 00:16:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
04/25/2022 00:16:06 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.1875 on epoch=949
04/25/2022 00:16:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 00:16:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
04/25/2022 00:16:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
04/25/2022 00:16:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
04/25/2022 00:16:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
04/25/2022 00:16:25 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.15625 on epoch=974
04/25/2022 00:16:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
04/25/2022 00:16:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
04/25/2022 00:16:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
04/25/2022 00:16:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
04/25/2022 00:16:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
04/25/2022 00:16:42 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:16:42 - INFO - __main__ - Printing 3 examples
04/25/2022 00:16:42 - INFO - __main__ -  [cosmos_qa] What may happen after the event has concluded ? [SEP] After it was all said and done , we surpassed our fundraising target of $ 25,000 ! First and foremost , this meant a good donation to the Rickshaw Bank , which was able to start a new outfit in Allahabad . But the group accomplished something else this year - the establishment of our team . [SEP]  (A) They will find a way to pay back the $ 25,000 . (B) The earnings will be put to charitable use . (C) They will hope to surpass the fundraising target . (D) They will invest their donation from the Rickshaw Bank .
04/25/2022 00:16:42 - INFO - __main__ - ['The earnings will be put to charitable use .']
04/25/2022 00:16:42 - INFO - __main__ -  [cosmos_qa] What can you conclude about Bri and whipped cream ? [SEP] We hosted dinner for a young couple who just moved to Eugene ( daughter & her partner of Chicago friend . ) Bri expressed amazement when Bob announced he was going to make the whipped cream for the dessert . " You 're making the whipped cream ? ! " I guess she thought it was some complicated process , and was amazed further to find that all you do it put heavy cream in a bowl and whip it . [SEP]  (A) Bri has never made cream dessert in her life . (B) Bri has never made whipped cream in her life . (C) None of the above choices . (D) Bri has never made heavy cream in her life .
04/25/2022 00:16:42 - INFO - __main__ - ['Bri has never made whipped cream in her life .']
04/25/2022 00:16:42 - INFO - __main__ -  [cosmos_qa] Why is the narrator down on USC ? [SEP] When one looks at the rankings of CF for the first four weeks , you see USC in the # 1 spot . In fact , in weeks 3 and 4 they nearly got all of the # 1 votes . WHY ? [SEP]  (A) They feel they let them down . (B) They feel they are mean . (C) They feel they are a bad football team . (D) They feel they are not as good as others
04/25/2022 00:16:42 - INFO - __main__ - ['They feel they are not as good as others']
04/25/2022 00:16:42 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:16:42 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:16:42 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 00:16:42 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:16:42 - INFO - __main__ - Printing 3 examples
04/25/2022 00:16:42 - INFO - __main__ -  [cosmos_qa] Why is Roger 's face so red and flushed ? [SEP] He took a deep breath and unlocked the door . Roger pulled out the door and simply stepped in and locked it again . John was surprised , Roger did n't seem to be angry with him . " I 'm so sorry , I do n't know why I did that , I must be out of my mind and ... " John was interrupted by Roger 's tongue in his mouth , curiously seeking around in there . John felt both surprised and pleased at the same time , and answered Roger 's kiss . When they broke the contact , John could n't say anything except a simple " .. why ? " Roger blushed and shrugged his shoulders . [SEP]  (A) Roger just got dumped . (B) Roger is fearful for his job . (C) Roger got kissed by John . (D) Roger is worried about something .
04/25/2022 00:16:42 - INFO - __main__ - ['Roger got kissed by John .']
04/25/2022 00:16:42 - INFO - __main__ -  [cosmos_qa] What is the speaker trying to do ? [SEP] If someone points out something substantive I can fix in the analysis , I will try to clean it up . First , read this . This page gives a good discussion of a liquidity crisis in corporate finance . A macroeconomic " high finance " liquidity crisis is essentially the same thing , only happening with large banks . As in everything in high finance , high - finance liquidity crises are more complex . One key reason high - finance liquidity crises are much more complex and potentially disastrous than regular corporate liquidity crises is the meshed nature of the problem . [SEP]  (A) They are teaching their coworkers about liquidation . (B) They are trying to give a presentation at work . (C) They are trying to teach others about macroeconomics . (D) None of the above choices .
04/25/2022 00:16:42 - INFO - __main__ - ['They are trying to teach others about macroeconomics .']
04/25/2022 00:16:42 - INFO - __main__ -  [cosmos_qa] What may happen if i was more sociable ? [SEP] I do n't think I did the greatest job at networking , but I tried a bit . I ' m just not naturally a social person and it 's rather difficult for me to talk with people I do n't know very well . ( I could have used The Alternative Scientist 's Guide To Networking , but , alas , it was mostly posted while I was away . ) [SEP]  (A) I would feel like an outcast . (B) None of the above choices . (C) I would be disliked . (D) I would be criticized .
04/25/2022 00:16:42 - INFO - __main__ - ['None of the above choices .']
04/25/2022 00:16:42 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:16:42 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:16:42 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 00:16:45 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.21875 on epoch=999
04/25/2022 00:16:45 - INFO - __main__ - save last model!
04/25/2022 00:16:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 00:16:45 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 00:16:45 - INFO - __main__ - Printing 3 examples
04/25/2022 00:16:45 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 00:16:45 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 00:16:45 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 00:16:45 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 00:16:45 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 00:16:45 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 00:16:45 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:16:45 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:16:46 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 00:16:57 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 00:16:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 00:16:58 - INFO - __main__ - Starting training!
04/25/2022 00:19:06 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_21_0.5_8_predictions.txt
04/25/2022 00:19:06 - INFO - __main__ - ACC on test data: 0.3010
04/25/2022 00:19:07 - INFO - __main__ - prefix=cosmos_qa_32_21, lr=0.5, bsz=8, dev_performance=0.21875, test_performance=0.301
04/25/2022 00:19:07 - INFO - __main__ - Running ... prefix=cosmos_qa_32_21, lr=0.4, bsz=8 ...
04/25/2022 00:19:08 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:19:08 - INFO - __main__ - Printing 3 examples
04/25/2022 00:19:08 - INFO - __main__ -  [cosmos_qa] What may happen after the event has concluded ? [SEP] After it was all said and done , we surpassed our fundraising target of $ 25,000 ! First and foremost , this meant a good donation to the Rickshaw Bank , which was able to start a new outfit in Allahabad . But the group accomplished something else this year - the establishment of our team . [SEP]  (A) They will find a way to pay back the $ 25,000 . (B) The earnings will be put to charitable use . (C) They will hope to surpass the fundraising target . (D) They will invest their donation from the Rickshaw Bank .
04/25/2022 00:19:08 - INFO - __main__ - ['The earnings will be put to charitable use .']
04/25/2022 00:19:08 - INFO - __main__ -  [cosmos_qa] What can you conclude about Bri and whipped cream ? [SEP] We hosted dinner for a young couple who just moved to Eugene ( daughter & her partner of Chicago friend . ) Bri expressed amazement when Bob announced he was going to make the whipped cream for the dessert . " You 're making the whipped cream ? ! " I guess she thought it was some complicated process , and was amazed further to find that all you do it put heavy cream in a bowl and whip it . [SEP]  (A) Bri has never made cream dessert in her life . (B) Bri has never made whipped cream in her life . (C) None of the above choices . (D) Bri has never made heavy cream in her life .
04/25/2022 00:19:08 - INFO - __main__ - ['Bri has never made whipped cream in her life .']
04/25/2022 00:19:08 - INFO - __main__ -  [cosmos_qa] Why is the narrator down on USC ? [SEP] When one looks at the rankings of CF for the first four weeks , you see USC in the # 1 spot . In fact , in weeks 3 and 4 they nearly got all of the # 1 votes . WHY ? [SEP]  (A) They feel they let them down . (B) They feel they are mean . (C) They feel they are a bad football team . (D) They feel they are not as good as others
04/25/2022 00:19:08 - INFO - __main__ - ['They feel they are not as good as others']
04/25/2022 00:19:08 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:19:08 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:19:08 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 00:19:08 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:19:08 - INFO - __main__ - Printing 3 examples
04/25/2022 00:19:08 - INFO - __main__ -  [cosmos_qa] Why is Roger 's face so red and flushed ? [SEP] He took a deep breath and unlocked the door . Roger pulled out the door and simply stepped in and locked it again . John was surprised , Roger did n't seem to be angry with him . " I 'm so sorry , I do n't know why I did that , I must be out of my mind and ... " John was interrupted by Roger 's tongue in his mouth , curiously seeking around in there . John felt both surprised and pleased at the same time , and answered Roger 's kiss . When they broke the contact , John could n't say anything except a simple " .. why ? " Roger blushed and shrugged his shoulders . [SEP]  (A) Roger just got dumped . (B) Roger is fearful for his job . (C) Roger got kissed by John . (D) Roger is worried about something .
04/25/2022 00:19:08 - INFO - __main__ - ['Roger got kissed by John .']
04/25/2022 00:19:08 - INFO - __main__ -  [cosmos_qa] What is the speaker trying to do ? [SEP] If someone points out something substantive I can fix in the analysis , I will try to clean it up . First , read this . This page gives a good discussion of a liquidity crisis in corporate finance . A macroeconomic " high finance " liquidity crisis is essentially the same thing , only happening with large banks . As in everything in high finance , high - finance liquidity crises are more complex . One key reason high - finance liquidity crises are much more complex and potentially disastrous than regular corporate liquidity crises is the meshed nature of the problem . [SEP]  (A) They are teaching their coworkers about liquidation . (B) They are trying to give a presentation at work . (C) They are trying to teach others about macroeconomics . (D) None of the above choices .
04/25/2022 00:19:08 - INFO - __main__ - ['They are trying to teach others about macroeconomics .']
04/25/2022 00:19:08 - INFO - __main__ -  [cosmos_qa] What may happen if i was more sociable ? [SEP] I do n't think I did the greatest job at networking , but I tried a bit . I ' m just not naturally a social person and it 's rather difficult for me to talk with people I do n't know very well . ( I could have used The Alternative Scientist 's Guide To Networking , but , alas , it was mostly posted while I was away . ) [SEP]  (A) I would feel like an outcast . (B) None of the above choices . (C) I would be disliked . (D) I would be criticized .
04/25/2022 00:19:08 - INFO - __main__ - ['None of the above choices .']
04/25/2022 00:19:08 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:19:08 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:19:08 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 00:19:27 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 00:19:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 00:19:27 - INFO - __main__ - Starting training!
04/25/2022 00:19:31 - INFO - __main__ - Step 10 Global step 10 Train loss 0.94 on epoch=4
04/25/2022 00:19:34 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=9
04/25/2022 00:19:37 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=14
04/25/2022 00:19:40 - INFO - __main__ - Step 40 Global step 40 Train loss 0.35 on epoch=19
04/25/2022 00:19:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.31 on epoch=24
04/25/2022 00:19:47 - INFO - __main__ - Global step 50 Train loss 0.53 ACC 0.15625 on epoch=24
04/25/2022 00:19:47 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
04/25/2022 00:19:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.27 on epoch=29
04/25/2022 00:19:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.23 on epoch=34
04/25/2022 00:19:56 - INFO - __main__ - Step 80 Global step 80 Train loss 0.28 on epoch=39
04/25/2022 00:19:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.87 on epoch=44
04/25/2022 00:20:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.89 on epoch=49
04/25/2022 00:20:05 - INFO - __main__ - Global step 100 Train loss 0.51 ACC 0.125 on epoch=49
04/25/2022 00:20:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.92 on epoch=54
04/25/2022 00:20:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.86 on epoch=59
04/25/2022 00:20:15 - INFO - __main__ - Step 130 Global step 130 Train loss 1.06 on epoch=64
04/25/2022 00:20:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.78 on epoch=69
04/25/2022 00:20:21 - INFO - __main__ - Step 150 Global step 150 Train loss 1.11 on epoch=74
04/25/2022 00:20:24 - INFO - __main__ - Global step 150 Train loss 0.94 ACC 0.125 on epoch=74
04/25/2022 00:20:27 - INFO - __main__ - Step 160 Global step 160 Train loss 1.95 on epoch=79
04/25/2022 00:20:30 - INFO - __main__ - Step 170 Global step 170 Train loss 1.63 on epoch=84
04/25/2022 00:20:33 - INFO - __main__ - Step 180 Global step 180 Train loss 1.72 on epoch=89
04/25/2022 00:20:36 - INFO - __main__ - Step 190 Global step 190 Train loss 1.52 on epoch=94
04/25/2022 00:20:40 - INFO - __main__ - Step 200 Global step 200 Train loss 1.45 on epoch=99
04/25/2022 00:20:44 - INFO - __main__ - Global step 200 Train loss 1.65 ACC 0.0 on epoch=99
04/25/2022 00:20:47 - INFO - __main__ - Step 210 Global step 210 Train loss 1.38 on epoch=104
04/25/2022 00:20:51 - INFO - __main__ - Step 220 Global step 220 Train loss 1.00 on epoch=109
04/25/2022 00:20:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.82 on epoch=114
04/25/2022 00:20:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.71 on epoch=119
04/25/2022 00:21:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.75 on epoch=124
04/25/2022 00:21:03 - INFO - __main__ - Global step 250 Train loss 0.93 ACC 0.1875 on epoch=124
04/25/2022 00:21:03 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=124, global_step=250
04/25/2022 00:21:06 - INFO - __main__ - Step 260 Global step 260 Train loss 1.13 on epoch=129
04/25/2022 00:21:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=134
04/25/2022 00:21:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.99 on epoch=139
04/25/2022 00:21:16 - INFO - __main__ - Step 290 Global step 290 Train loss 1.07 on epoch=144
04/25/2022 00:21:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.86 on epoch=149
04/25/2022 00:21:23 - INFO - __main__ - Global step 300 Train loss 0.93 ACC 0.1875 on epoch=149
04/25/2022 00:21:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.69 on epoch=154
04/25/2022 00:21:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.65 on epoch=159
04/25/2022 00:21:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=164
04/25/2022 00:21:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.54 on epoch=169
04/25/2022 00:21:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=174
04/25/2022 00:21:42 - INFO - __main__ - Global step 350 Train loss 0.56 ACC 0.0625 on epoch=174
04/25/2022 00:21:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=179
04/25/2022 00:21:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=184
04/25/2022 00:21:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.33 on epoch=189
04/25/2022 00:21:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=194
04/25/2022 00:21:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=199
04/25/2022 00:22:01 - INFO - __main__ - Global step 400 Train loss 0.40 ACC 0.125 on epoch=199
04/25/2022 00:22:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=204
04/25/2022 00:22:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.32 on epoch=209
04/25/2022 00:22:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=214
04/25/2022 00:22:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=219
04/25/2022 00:22:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=224
04/25/2022 00:22:19 - INFO - __main__ - Global step 450 Train loss 0.32 ACC 0.15625 on epoch=224
04/25/2022 00:22:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=229
04/25/2022 00:22:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
04/25/2022 00:22:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=239
04/25/2022 00:22:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=244
04/25/2022 00:22:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=249
04/25/2022 00:22:38 - INFO - __main__ - Global step 500 Train loss 0.33 ACC 0.09375 on epoch=249
04/25/2022 00:22:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=254
04/25/2022 00:22:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.31 on epoch=259
04/25/2022 00:22:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=264
04/25/2022 00:22:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=269
04/25/2022 00:22:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=274
04/25/2022 00:22:56 - INFO - __main__ - Global step 550 Train loss 0.29 ACC 0.125 on epoch=274
04/25/2022 00:22:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.28 on epoch=279
04/25/2022 00:23:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=284
04/25/2022 00:23:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
04/25/2022 00:23:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=294
04/25/2022 00:23:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=299
04/25/2022 00:23:15 - INFO - __main__ - Global step 600 Train loss 0.27 ACC 0.15625 on epoch=299
04/25/2022 00:23:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=304
04/25/2022 00:23:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=309
04/25/2022 00:23:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=314
04/25/2022 00:23:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=319
04/25/2022 00:23:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=324
04/25/2022 00:23:33 - INFO - __main__ - Global step 650 Train loss 0.24 ACC 0.15625 on epoch=324
04/25/2022 00:23:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=329
04/25/2022 00:23:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=334
04/25/2022 00:23:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=339
04/25/2022 00:23:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=344
04/25/2022 00:23:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
04/25/2022 00:23:52 - INFO - __main__ - Global step 700 Train loss 0.25 ACC 0.125 on epoch=349
04/25/2022 00:23:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=354
04/25/2022 00:23:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=359
04/25/2022 00:24:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
04/25/2022 00:24:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=369
04/25/2022 00:24:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=374
04/25/2022 00:24:11 - INFO - __main__ - Global step 750 Train loss 0.24 ACC 0.125 on epoch=374
04/25/2022 00:24:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
04/25/2022 00:24:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=384
04/25/2022 00:24:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=389
04/25/2022 00:24:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=394
04/25/2022 00:24:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=399
04/25/2022 00:24:30 - INFO - __main__ - Global step 800 Train loss 0.24 ACC 0.15625 on epoch=399
04/25/2022 00:24:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=404
04/25/2022 00:24:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=409
04/25/2022 00:24:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=414
04/25/2022 00:24:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=419
04/25/2022 00:24:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=424
04/25/2022 00:24:48 - INFO - __main__ - Global step 850 Train loss 0.22 ACC 0.125 on epoch=424
04/25/2022 00:24:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=429
04/25/2022 00:24:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=434
04/25/2022 00:24:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=439
04/25/2022 00:25:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=444
04/25/2022 00:25:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=449
04/25/2022 00:25:07 - INFO - __main__ - Global step 900 Train loss 0.20 ACC 0.21875 on epoch=449
04/25/2022 00:25:07 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=449, global_step=900
04/25/2022 00:25:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=454
04/25/2022 00:25:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=459
04/25/2022 00:25:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=464
04/25/2022 00:25:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=469
04/25/2022 00:25:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=474
04/25/2022 00:25:26 - INFO - __main__ - Global step 950 Train loss 0.21 ACC 0.21875 on epoch=474
04/25/2022 00:25:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=479
04/25/2022 00:25:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=484
04/25/2022 00:25:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=489
04/25/2022 00:25:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=494
04/25/2022 00:25:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=499
04/25/2022 00:25:45 - INFO - __main__ - Global step 1000 Train loss 0.21 ACC 0.1875 on epoch=499
04/25/2022 00:25:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=504
04/25/2022 00:25:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=509
04/25/2022 00:25:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=514
04/25/2022 00:25:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=519
04/25/2022 00:26:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=524
04/25/2022 00:26:03 - INFO - __main__ - Global step 1050 Train loss 0.19 ACC 0.15625 on epoch=524
04/25/2022 00:26:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=529
04/25/2022 00:26:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.17 on epoch=534
04/25/2022 00:26:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=539
04/25/2022 00:26:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=544
04/25/2022 00:26:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=549
04/25/2022 00:26:22 - INFO - __main__ - Global step 1100 Train loss 0.18 ACC 0.21875 on epoch=549
04/25/2022 00:26:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=554
04/25/2022 00:26:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=559
04/25/2022 00:26:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=564
04/25/2022 00:26:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=569
04/25/2022 00:26:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=574
04/25/2022 00:26:41 - INFO - __main__ - Global step 1150 Train loss 0.19 ACC 0.21875 on epoch=574
04/25/2022 00:26:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=579
04/25/2022 00:26:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=584
04/25/2022 00:26:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=589
04/25/2022 00:26:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=594
04/25/2022 00:26:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=599
04/25/2022 00:27:00 - INFO - __main__ - Global step 1200 Train loss 0.18 ACC 0.21875 on epoch=599
04/25/2022 00:27:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=604
04/25/2022 00:27:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=609
04/25/2022 00:27:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=614
04/25/2022 00:27:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=619
04/25/2022 00:27:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=624
04/25/2022 00:27:19 - INFO - __main__ - Global step 1250 Train loss 0.17 ACC 0.15625 on epoch=624
04/25/2022 00:27:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=629
04/25/2022 00:27:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=634
04/25/2022 00:27:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=639
04/25/2022 00:27:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=644
04/25/2022 00:27:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=649
04/25/2022 00:27:38 - INFO - __main__ - Global step 1300 Train loss 0.15 ACC 0.1875 on epoch=649
04/25/2022 00:27:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=654
04/25/2022 00:27:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=659
04/25/2022 00:27:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=664
04/25/2022 00:27:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=669
04/25/2022 00:27:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.17 on epoch=674
04/25/2022 00:27:57 - INFO - __main__ - Global step 1350 Train loss 0.15 ACC 0.28125 on epoch=674
04/25/2022 00:27:57 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.28125 on epoch=674, global_step=1350
04/25/2022 00:28:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=679
04/25/2022 00:28:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=684
04/25/2022 00:28:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=689
04/25/2022 00:28:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.14 on epoch=694
04/25/2022 00:28:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=699
04/25/2022 00:28:16 - INFO - __main__ - Global step 1400 Train loss 0.16 ACC 0.125 on epoch=699
04/25/2022 00:28:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=704
04/25/2022 00:28:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=709
04/25/2022 00:28:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=714
04/25/2022 00:28:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=719
04/25/2022 00:28:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=724
04/25/2022 00:28:34 - INFO - __main__ - Global step 1450 Train loss 0.13 ACC 0.21875 on epoch=724
04/25/2022 00:28:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=729
04/25/2022 00:28:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=734
04/25/2022 00:28:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=739
04/25/2022 00:28:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=744
04/25/2022 00:28:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=749
04/25/2022 00:28:54 - INFO - __main__ - Global step 1500 Train loss 0.12 ACC 0.15625 on epoch=749
04/25/2022 00:28:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=754
04/25/2022 00:29:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=759
04/25/2022 00:29:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=764
04/25/2022 00:29:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=769
04/25/2022 00:29:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=774
04/25/2022 00:29:13 - INFO - __main__ - Global step 1550 Train loss 0.13 ACC 0.25 on epoch=774
04/25/2022 00:29:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=779
04/25/2022 00:29:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=784
04/25/2022 00:29:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=789
04/25/2022 00:29:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=794
04/25/2022 00:29:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=799
04/25/2022 00:29:32 - INFO - __main__ - Global step 1600 Train loss 0.12 ACC 0.1875 on epoch=799
04/25/2022 00:29:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=804
04/25/2022 00:29:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=809
04/25/2022 00:29:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=814
04/25/2022 00:29:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=819
04/25/2022 00:29:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=824
04/25/2022 00:29:52 - INFO - __main__ - Global step 1650 Train loss 0.10 ACC 0.125 on epoch=824
04/25/2022 00:29:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=829
04/25/2022 00:29:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=834
04/25/2022 00:30:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=839
04/25/2022 00:30:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=844
04/25/2022 00:30:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=849
04/25/2022 00:30:11 - INFO - __main__ - Global step 1700 Train loss 0.11 ACC 0.28125 on epoch=849
04/25/2022 00:30:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=854
04/25/2022 00:30:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=859
04/25/2022 00:30:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=864
04/25/2022 00:30:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=869
04/25/2022 00:30:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=874
04/25/2022 00:30:31 - INFO - __main__ - Global step 1750 Train loss 0.10 ACC 0.34375 on epoch=874
04/25/2022 00:30:31 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.34375 on epoch=874, global_step=1750
04/25/2022 00:30:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=879
04/25/2022 00:30:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=884
04/25/2022 00:30:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=889
04/25/2022 00:30:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=894
04/25/2022 00:30:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=899
04/25/2022 00:30:50 - INFO - __main__ - Global step 1800 Train loss 0.11 ACC 0.25 on epoch=899
04/25/2022 00:30:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=904
04/25/2022 00:30:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=909
04/25/2022 00:30:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=914
04/25/2022 00:31:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=919
04/25/2022 00:31:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=924
04/25/2022 00:31:09 - INFO - __main__ - Global step 1850 Train loss 0.10 ACC 0.34375 on epoch=924
04/25/2022 00:31:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=929
04/25/2022 00:31:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=934
04/25/2022 00:31:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=939
04/25/2022 00:31:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=944
04/25/2022 00:31:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=949
04/25/2022 00:31:29 - INFO - __main__ - Global step 1900 Train loss 0.08 ACC 0.28125 on epoch=949
04/25/2022 00:31:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=954
04/25/2022 00:31:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=959
04/25/2022 00:31:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=964
04/25/2022 00:31:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=969
04/25/2022 00:31:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=974
04/25/2022 00:31:49 - INFO - __main__ - Global step 1950 Train loss 0.08 ACC 0.25 on epoch=974
04/25/2022 00:31:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=979
04/25/2022 00:31:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=984
04/25/2022 00:31:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=989
04/25/2022 00:32:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=994
04/25/2022 00:32:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=999
04/25/2022 00:32:05 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:32:05 - INFO - __main__ - Printing 3 examples
04/25/2022 00:32:05 - INFO - __main__ -  [cosmos_qa] What may happen after the event has concluded ? [SEP] After it was all said and done , we surpassed our fundraising target of $ 25,000 ! First and foremost , this meant a good donation to the Rickshaw Bank , which was able to start a new outfit in Allahabad . But the group accomplished something else this year - the establishment of our team . [SEP]  (A) They will find a way to pay back the $ 25,000 . (B) The earnings will be put to charitable use . (C) They will hope to surpass the fundraising target . (D) They will invest their donation from the Rickshaw Bank .
04/25/2022 00:32:05 - INFO - __main__ - ['The earnings will be put to charitable use .']
04/25/2022 00:32:05 - INFO - __main__ -  [cosmos_qa] What can you conclude about Bri and whipped cream ? [SEP] We hosted dinner for a young couple who just moved to Eugene ( daughter & her partner of Chicago friend . ) Bri expressed amazement when Bob announced he was going to make the whipped cream for the dessert . " You 're making the whipped cream ? ! " I guess she thought it was some complicated process , and was amazed further to find that all you do it put heavy cream in a bowl and whip it . [SEP]  (A) Bri has never made cream dessert in her life . (B) Bri has never made whipped cream in her life . (C) None of the above choices . (D) Bri has never made heavy cream in her life .
04/25/2022 00:32:05 - INFO - __main__ - ['Bri has never made whipped cream in her life .']
04/25/2022 00:32:05 - INFO - __main__ -  [cosmos_qa] Why is the narrator down on USC ? [SEP] When one looks at the rankings of CF for the first four weeks , you see USC in the # 1 spot . In fact , in weeks 3 and 4 they nearly got all of the # 1 votes . WHY ? [SEP]  (A) They feel they let them down . (B) They feel they are mean . (C) They feel they are a bad football team . (D) They feel they are not as good as others
04/25/2022 00:32:05 - INFO - __main__ - ['They feel they are not as good as others']
04/25/2022 00:32:05 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:32:05 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:32:06 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 00:32:06 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:32:06 - INFO - __main__ - Printing 3 examples
04/25/2022 00:32:06 - INFO - __main__ -  [cosmos_qa] Why is Roger 's face so red and flushed ? [SEP] He took a deep breath and unlocked the door . Roger pulled out the door and simply stepped in and locked it again . John was surprised , Roger did n't seem to be angry with him . " I 'm so sorry , I do n't know why I did that , I must be out of my mind and ... " John was interrupted by Roger 's tongue in his mouth , curiously seeking around in there . John felt both surprised and pleased at the same time , and answered Roger 's kiss . When they broke the contact , John could n't say anything except a simple " .. why ? " Roger blushed and shrugged his shoulders . [SEP]  (A) Roger just got dumped . (B) Roger is fearful for his job . (C) Roger got kissed by John . (D) Roger is worried about something .
04/25/2022 00:32:06 - INFO - __main__ - ['Roger got kissed by John .']
04/25/2022 00:32:06 - INFO - __main__ -  [cosmos_qa] What is the speaker trying to do ? [SEP] If someone points out something substantive I can fix in the analysis , I will try to clean it up . First , read this . This page gives a good discussion of a liquidity crisis in corporate finance . A macroeconomic " high finance " liquidity crisis is essentially the same thing , only happening with large banks . As in everything in high finance , high - finance liquidity crises are more complex . One key reason high - finance liquidity crises are much more complex and potentially disastrous than regular corporate liquidity crises is the meshed nature of the problem . [SEP]  (A) They are teaching their coworkers about liquidation . (B) They are trying to give a presentation at work . (C) They are trying to teach others about macroeconomics . (D) None of the above choices .
04/25/2022 00:32:06 - INFO - __main__ - ['They are trying to teach others about macroeconomics .']
04/25/2022 00:32:06 - INFO - __main__ -  [cosmos_qa] What may happen if i was more sociable ? [SEP] I do n't think I did the greatest job at networking , but I tried a bit . I ' m just not naturally a social person and it 's rather difficult for me to talk with people I do n't know very well . ( I could have used The Alternative Scientist 's Guide To Networking , but , alas , it was mostly posted while I was away . ) [SEP]  (A) I would feel like an outcast . (B) None of the above choices . (C) I would be disliked . (D) I would be criticized .
04/25/2022 00:32:06 - INFO - __main__ - ['None of the above choices .']
04/25/2022 00:32:06 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:32:06 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:32:06 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 00:32:08 - INFO - __main__ - Global step 2000 Train loss 0.07 ACC 0.28125 on epoch=999
04/25/2022 00:32:08 - INFO - __main__ - save last model!
04/25/2022 00:32:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 00:32:08 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 00:32:08 - INFO - __main__ - Printing 3 examples
04/25/2022 00:32:08 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 00:32:08 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 00:32:08 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 00:32:08 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 00:32:08 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 00:32:08 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 00:32:08 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:32:09 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:32:10 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 00:32:21 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 00:32:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 00:32:22 - INFO - __main__ - Starting training!
04/25/2022 00:34:15 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_21_0.4_8_predictions.txt
04/25/2022 00:34:15 - INFO - __main__ - ACC on test data: 0.2220
04/25/2022 00:34:16 - INFO - __main__ - prefix=cosmos_qa_32_21, lr=0.4, bsz=8, dev_performance=0.34375, test_performance=0.222
04/25/2022 00:34:16 - INFO - __main__ - Running ... prefix=cosmos_qa_32_21, lr=0.3, bsz=8 ...
04/25/2022 00:34:17 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:34:17 - INFO - __main__ - Printing 3 examples
04/25/2022 00:34:17 - INFO - __main__ -  [cosmos_qa] What may happen after the event has concluded ? [SEP] After it was all said and done , we surpassed our fundraising target of $ 25,000 ! First and foremost , this meant a good donation to the Rickshaw Bank , which was able to start a new outfit in Allahabad . But the group accomplished something else this year - the establishment of our team . [SEP]  (A) They will find a way to pay back the $ 25,000 . (B) The earnings will be put to charitable use . (C) They will hope to surpass the fundraising target . (D) They will invest their donation from the Rickshaw Bank .
04/25/2022 00:34:17 - INFO - __main__ - ['The earnings will be put to charitable use .']
04/25/2022 00:34:17 - INFO - __main__ -  [cosmos_qa] What can you conclude about Bri and whipped cream ? [SEP] We hosted dinner for a young couple who just moved to Eugene ( daughter & her partner of Chicago friend . ) Bri expressed amazement when Bob announced he was going to make the whipped cream for the dessert . " You 're making the whipped cream ? ! " I guess she thought it was some complicated process , and was amazed further to find that all you do it put heavy cream in a bowl and whip it . [SEP]  (A) Bri has never made cream dessert in her life . (B) Bri has never made whipped cream in her life . (C) None of the above choices . (D) Bri has never made heavy cream in her life .
04/25/2022 00:34:17 - INFO - __main__ - ['Bri has never made whipped cream in her life .']
04/25/2022 00:34:17 - INFO - __main__ -  [cosmos_qa] Why is the narrator down on USC ? [SEP] When one looks at the rankings of CF for the first four weeks , you see USC in the # 1 spot . In fact , in weeks 3 and 4 they nearly got all of the # 1 votes . WHY ? [SEP]  (A) They feel they let them down . (B) They feel they are mean . (C) They feel they are a bad football team . (D) They feel they are not as good as others
04/25/2022 00:34:17 - INFO - __main__ - ['They feel they are not as good as others']
04/25/2022 00:34:17 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:34:17 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:34:17 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 00:34:17 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:34:17 - INFO - __main__ - Printing 3 examples
04/25/2022 00:34:17 - INFO - __main__ -  [cosmos_qa] Why is Roger 's face so red and flushed ? [SEP] He took a deep breath and unlocked the door . Roger pulled out the door and simply stepped in and locked it again . John was surprised , Roger did n't seem to be angry with him . " I 'm so sorry , I do n't know why I did that , I must be out of my mind and ... " John was interrupted by Roger 's tongue in his mouth , curiously seeking around in there . John felt both surprised and pleased at the same time , and answered Roger 's kiss . When they broke the contact , John could n't say anything except a simple " .. why ? " Roger blushed and shrugged his shoulders . [SEP]  (A) Roger just got dumped . (B) Roger is fearful for his job . (C) Roger got kissed by John . (D) Roger is worried about something .
04/25/2022 00:34:17 - INFO - __main__ - ['Roger got kissed by John .']
04/25/2022 00:34:17 - INFO - __main__ -  [cosmos_qa] What is the speaker trying to do ? [SEP] If someone points out something substantive I can fix in the analysis , I will try to clean it up . First , read this . This page gives a good discussion of a liquidity crisis in corporate finance . A macroeconomic " high finance " liquidity crisis is essentially the same thing , only happening with large banks . As in everything in high finance , high - finance liquidity crises are more complex . One key reason high - finance liquidity crises are much more complex and potentially disastrous than regular corporate liquidity crises is the meshed nature of the problem . [SEP]  (A) They are teaching their coworkers about liquidation . (B) They are trying to give a presentation at work . (C) They are trying to teach others about macroeconomics . (D) None of the above choices .
04/25/2022 00:34:17 - INFO - __main__ - ['They are trying to teach others about macroeconomics .']
04/25/2022 00:34:17 - INFO - __main__ -  [cosmos_qa] What may happen if i was more sociable ? [SEP] I do n't think I did the greatest job at networking , but I tried a bit . I ' m just not naturally a social person and it 's rather difficult for me to talk with people I do n't know very well . ( I could have used The Alternative Scientist 's Guide To Networking , but , alas , it was mostly posted while I was away . ) [SEP]  (A) I would feel like an outcast . (B) None of the above choices . (C) I would be disliked . (D) I would be criticized .
04/25/2022 00:34:17 - INFO - __main__ - ['None of the above choices .']
04/25/2022 00:34:17 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:34:17 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:34:17 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 00:34:36 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 00:34:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 00:34:37 - INFO - __main__ - Starting training!
04/25/2022 00:34:40 - INFO - __main__ - Step 10 Global step 10 Train loss 0.98 on epoch=4
04/25/2022 00:34:43 - INFO - __main__ - Step 20 Global step 20 Train loss 0.73 on epoch=9
04/25/2022 00:34:46 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=14
04/25/2022 00:34:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.39 on epoch=19
04/25/2022 00:34:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.35 on epoch=24
04/25/2022 00:34:56 - INFO - __main__ - Global step 50 Train loss 0.59 ACC 0.15625 on epoch=24
04/25/2022 00:34:56 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
04/25/2022 00:34:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.34 on epoch=29
04/25/2022 00:35:03 - INFO - __main__ - Step 70 Global step 70 Train loss 0.28 on epoch=34
04/25/2022 00:35:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.27 on epoch=39
04/25/2022 00:35:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.25 on epoch=44
04/25/2022 00:35:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.26 on epoch=49
04/25/2022 00:35:15 - INFO - __main__ - Global step 100 Train loss 0.28 ACC 0.15625 on epoch=49
04/25/2022 00:35:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.23 on epoch=54
04/25/2022 00:35:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.22 on epoch=59
04/25/2022 00:35:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.23 on epoch=64
04/25/2022 00:35:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.24 on epoch=69
04/25/2022 00:35:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.21 on epoch=74
04/25/2022 00:35:34 - INFO - __main__ - Global step 150 Train loss 0.23 ACC 0.25 on epoch=74
04/25/2022 00:35:34 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.25 on epoch=74, global_step=150
04/25/2022 00:35:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.20 on epoch=79
04/25/2022 00:35:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.18 on epoch=84
04/25/2022 00:35:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=89
04/25/2022 00:35:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.18 on epoch=94
04/25/2022 00:35:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.17 on epoch=99
04/25/2022 00:35:52 - INFO - __main__ - Global step 200 Train loss 0.19 ACC 0.1875 on epoch=99
04/25/2022 00:35:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.16 on epoch=104
04/25/2022 00:35:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.15 on epoch=109
04/25/2022 00:36:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.15 on epoch=114
04/25/2022 00:36:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=119
04/25/2022 00:36:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.12 on epoch=124
04/25/2022 00:36:11 - INFO - __main__ - Global step 250 Train loss 0.16 ACC 0.3125 on epoch=124
04/25/2022 00:36:11 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.3125 on epoch=124, global_step=250
04/25/2022 00:36:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.16 on epoch=129
04/25/2022 00:36:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.12 on epoch=134
04/25/2022 00:36:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.13 on epoch=139
04/25/2022 00:36:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.11 on epoch=144
04/25/2022 00:36:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.11 on epoch=149
04/25/2022 00:36:31 - INFO - __main__ - Global step 300 Train loss 0.13 ACC 0.28125 on epoch=149
04/25/2022 00:36:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.13 on epoch=154
04/25/2022 00:36:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.14 on epoch=159
04/25/2022 00:36:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
04/25/2022 00:36:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.09 on epoch=169
04/25/2022 00:36:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
04/25/2022 00:36:50 - INFO - __main__ - Global step 350 Train loss 0.11 ACC 0.1875 on epoch=174
04/25/2022 00:36:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.11 on epoch=179
04/25/2022 00:36:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.10 on epoch=184
04/25/2022 00:36:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.11 on epoch=189
04/25/2022 00:37:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.11 on epoch=194
04/25/2022 00:37:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.09 on epoch=199
04/25/2022 00:37:09 - INFO - __main__ - Global step 400 Train loss 0.10 ACC 0.21875 on epoch=199
04/25/2022 00:37:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
04/25/2022 00:37:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.10 on epoch=209
04/25/2022 00:37:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
04/25/2022 00:37:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=219
04/25/2022 00:37:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.11 on epoch=224
04/25/2022 00:37:28 - INFO - __main__ - Global step 450 Train loss 0.10 ACC 0.25 on epoch=224
04/25/2022 00:37:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
04/25/2022 00:37:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
04/25/2022 00:37:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.09 on epoch=239
04/25/2022 00:37:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
04/25/2022 00:37:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
04/25/2022 00:37:47 - INFO - __main__ - Global step 500 Train loss 0.07 ACC 0.15625 on epoch=249
04/25/2022 00:37:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
04/25/2022 00:37:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
04/25/2022 00:37:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
04/25/2022 00:37:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
04/25/2022 00:38:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=274
04/25/2022 00:38:06 - INFO - __main__ - Global step 550 Train loss 0.08 ACC 0.15625 on epoch=274
04/25/2022 00:38:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
04/25/2022 00:38:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=284
04/25/2022 00:38:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
04/25/2022 00:38:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=294
04/25/2022 00:38:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
04/25/2022 00:38:26 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.125 on epoch=299
04/25/2022 00:38:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
04/25/2022 00:38:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=309
04/25/2022 00:38:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=314
04/25/2022 00:38:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=319
04/25/2022 00:38:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
04/25/2022 00:38:45 - INFO - __main__ - Global step 650 Train loss 0.06 ACC 0.21875 on epoch=324
04/25/2022 00:38:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.07 on epoch=329
04/25/2022 00:38:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
04/25/2022 00:38:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=339
04/25/2022 00:38:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
04/25/2022 00:39:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=349
04/25/2022 00:39:05 - INFO - __main__ - Global step 700 Train loss 0.06 ACC 0.15625 on epoch=349
04/25/2022 00:39:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
04/25/2022 00:39:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=359
04/25/2022 00:39:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
04/25/2022 00:39:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
04/25/2022 00:39:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
04/25/2022 00:39:24 - INFO - __main__ - Global step 750 Train loss 0.05 ACC 0.125 on epoch=374
04/25/2022 00:39:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=379
04/25/2022 00:39:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
04/25/2022 00:39:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
04/25/2022 00:39:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
04/25/2022 00:39:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
04/25/2022 00:39:44 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.15625 on epoch=399
04/25/2022 00:39:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
04/25/2022 00:39:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=409
04/25/2022 00:39:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
04/25/2022 00:39:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=419
04/25/2022 00:39:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
04/25/2022 00:40:03 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.125 on epoch=424
04/25/2022 00:40:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
04/25/2022 00:40:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
04/25/2022 00:40:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
04/25/2022 00:40:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=444
04/25/2022 00:40:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
04/25/2022 00:40:23 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.125 on epoch=449
04/25/2022 00:40:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
04/25/2022 00:40:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
04/25/2022 00:40:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
04/25/2022 00:40:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
04/25/2022 00:40:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
04/25/2022 00:40:42 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.28125 on epoch=474
04/25/2022 00:40:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
04/25/2022 00:40:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
04/25/2022 00:40:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
04/25/2022 00:40:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
04/25/2022 00:40:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
04/25/2022 00:41:01 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.0625 on epoch=499
04/25/2022 00:41:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
04/25/2022 00:41:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
04/25/2022 00:41:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
04/25/2022 00:41:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
04/25/2022 00:41:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
04/25/2022 00:41:20 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.125 on epoch=524
04/25/2022 00:41:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
04/25/2022 00:41:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
04/25/2022 00:41:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
04/25/2022 00:41:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
04/25/2022 00:41:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/25/2022 00:41:39 - INFO - __main__ - Global step 1100 Train loss 0.03 ACC 0.125 on epoch=549
04/25/2022 00:41:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
04/25/2022 00:41:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
04/25/2022 00:41:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
04/25/2022 00:41:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
04/25/2022 00:41:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
04/25/2022 00:41:59 - INFO - __main__ - Global step 1150 Train loss 0.03 ACC 0.125 on epoch=574
04/25/2022 00:42:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/25/2022 00:42:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
04/25/2022 00:42:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/25/2022 00:42:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 00:42:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/25/2022 00:42:19 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.125 on epoch=599
04/25/2022 00:42:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
04/25/2022 00:42:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 00:42:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 00:42:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
04/25/2022 00:42:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
04/25/2022 00:42:38 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.21875 on epoch=624
04/25/2022 00:42:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
04/25/2022 00:42:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
04/25/2022 00:42:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/25/2022 00:42:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/25/2022 00:42:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/25/2022 00:42:58 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.09375 on epoch=649
04/25/2022 00:43:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 00:43:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 00:43:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/25/2022 00:43:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/25/2022 00:43:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 00:43:17 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.09375 on epoch=674
04/25/2022 00:43:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/25/2022 00:43:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 00:43:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/25/2022 00:43:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 00:43:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
04/25/2022 00:43:37 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.09375 on epoch=699
04/25/2022 00:43:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 00:43:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 00:43:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
04/25/2022 00:43:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 00:43:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
04/25/2022 00:43:56 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.0625 on epoch=724
04/25/2022 00:43:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 00:44:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/25/2022 00:44:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 00:44:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
04/25/2022 00:44:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 00:44:15 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.125 on epoch=749
04/25/2022 00:44:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 00:44:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 00:44:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
04/25/2022 00:44:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
04/25/2022 00:44:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/25/2022 00:44:35 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.09375 on epoch=774
04/25/2022 00:44:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 00:44:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 00:44:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 00:44:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 00:44:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/25/2022 00:44:54 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.09375 on epoch=799
04/25/2022 00:44:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/25/2022 00:45:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/25/2022 00:45:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/25/2022 00:45:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
04/25/2022 00:45:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 00:45:13 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.0625 on epoch=824
04/25/2022 00:45:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
04/25/2022 00:45:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 00:45:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 00:45:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 00:45:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 00:45:33 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.125 on epoch=849
04/25/2022 00:45:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 00:45:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
04/25/2022 00:45:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 00:45:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 00:45:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 00:45:52 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.0625 on epoch=874
04/25/2022 00:45:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/25/2022 00:45:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 00:46:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
04/25/2022 00:46:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
04/25/2022 00:46:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 00:46:11 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.09375 on epoch=899
04/25/2022 00:46:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
04/25/2022 00:46:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/25/2022 00:46:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 00:46:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 00:46:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 00:46:31 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.125 on epoch=924
04/25/2022 00:46:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 00:46:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 00:46:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 00:46:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
04/25/2022 00:46:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 00:46:49 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.15625 on epoch=949
04/25/2022 00:46:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
04/25/2022 00:46:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
04/25/2022 00:46:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 00:47:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
04/25/2022 00:47:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 00:47:09 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.09375 on epoch=974
04/25/2022 00:47:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
04/25/2022 00:47:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 00:47:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
04/25/2022 00:47:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
04/25/2022 00:47:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 00:47:26 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:47:26 - INFO - __main__ - Printing 3 examples
04/25/2022 00:47:26 - INFO - __main__ -  [cosmos_qa] What may happen after the event has concluded ? [SEP] After it was all said and done , we surpassed our fundraising target of $ 25,000 ! First and foremost , this meant a good donation to the Rickshaw Bank , which was able to start a new outfit in Allahabad . But the group accomplished something else this year - the establishment of our team . [SEP]  (A) They will find a way to pay back the $ 25,000 . (B) The earnings will be put to charitable use . (C) They will hope to surpass the fundraising target . (D) They will invest their donation from the Rickshaw Bank .
04/25/2022 00:47:26 - INFO - __main__ - ['The earnings will be put to charitable use .']
04/25/2022 00:47:26 - INFO - __main__ -  [cosmos_qa] What can you conclude about Bri and whipped cream ? [SEP] We hosted dinner for a young couple who just moved to Eugene ( daughter & her partner of Chicago friend . ) Bri expressed amazement when Bob announced he was going to make the whipped cream for the dessert . " You 're making the whipped cream ? ! " I guess she thought it was some complicated process , and was amazed further to find that all you do it put heavy cream in a bowl and whip it . [SEP]  (A) Bri has never made cream dessert in her life . (B) Bri has never made whipped cream in her life . (C) None of the above choices . (D) Bri has never made heavy cream in her life .
04/25/2022 00:47:26 - INFO - __main__ - ['Bri has never made whipped cream in her life .']
04/25/2022 00:47:26 - INFO - __main__ -  [cosmos_qa] Why is the narrator down on USC ? [SEP] When one looks at the rankings of CF for the first four weeks , you see USC in the # 1 spot . In fact , in weeks 3 and 4 they nearly got all of the # 1 votes . WHY ? [SEP]  (A) They feel they let them down . (B) They feel they are mean . (C) They feel they are a bad football team . (D) They feel they are not as good as others
04/25/2022 00:47:26 - INFO - __main__ - ['They feel they are not as good as others']
04/25/2022 00:47:26 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:47:26 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:47:26 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 00:47:26 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:47:26 - INFO - __main__ - Printing 3 examples
04/25/2022 00:47:26 - INFO - __main__ -  [cosmos_qa] Why is Roger 's face so red and flushed ? [SEP] He took a deep breath and unlocked the door . Roger pulled out the door and simply stepped in and locked it again . John was surprised , Roger did n't seem to be angry with him . " I 'm so sorry , I do n't know why I did that , I must be out of my mind and ... " John was interrupted by Roger 's tongue in his mouth , curiously seeking around in there . John felt both surprised and pleased at the same time , and answered Roger 's kiss . When they broke the contact , John could n't say anything except a simple " .. why ? " Roger blushed and shrugged his shoulders . [SEP]  (A) Roger just got dumped . (B) Roger is fearful for his job . (C) Roger got kissed by John . (D) Roger is worried about something .
04/25/2022 00:47:26 - INFO - __main__ - ['Roger got kissed by John .']
04/25/2022 00:47:26 - INFO - __main__ -  [cosmos_qa] What is the speaker trying to do ? [SEP] If someone points out something substantive I can fix in the analysis , I will try to clean it up . First , read this . This page gives a good discussion of a liquidity crisis in corporate finance . A macroeconomic " high finance " liquidity crisis is essentially the same thing , only happening with large banks . As in everything in high finance , high - finance liquidity crises are more complex . One key reason high - finance liquidity crises are much more complex and potentially disastrous than regular corporate liquidity crises is the meshed nature of the problem . [SEP]  (A) They are teaching their coworkers about liquidation . (B) They are trying to give a presentation at work . (C) They are trying to teach others about macroeconomics . (D) None of the above choices .
04/25/2022 00:47:26 - INFO - __main__ - ['They are trying to teach others about macroeconomics .']
04/25/2022 00:47:26 - INFO - __main__ -  [cosmos_qa] What may happen if i was more sociable ? [SEP] I do n't think I did the greatest job at networking , but I tried a bit . I ' m just not naturally a social person and it 's rather difficult for me to talk with people I do n't know very well . ( I could have used The Alternative Scientist 's Guide To Networking , but , alas , it was mostly posted while I was away . ) [SEP]  (A) I would feel like an outcast . (B) None of the above choices . (C) I would be disliked . (D) I would be criticized .
04/25/2022 00:47:26 - INFO - __main__ - ['None of the above choices .']
04/25/2022 00:47:26 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:47:26 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:47:26 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 00:47:29 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.09375 on epoch=999
04/25/2022 00:47:29 - INFO - __main__ - save last model!
04/25/2022 00:47:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 00:47:29 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 00:47:29 - INFO - __main__ - Printing 3 examples
04/25/2022 00:47:29 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 00:47:29 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 00:47:29 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 00:47:29 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 00:47:29 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 00:47:29 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 00:47:29 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:47:30 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:47:31 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 00:47:45 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 00:47:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 00:47:46 - INFO - __main__ - Starting training!
04/25/2022 00:49:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_21_0.3_8_predictions.txt
04/25/2022 00:49:30 - INFO - __main__ - ACC on test data: 0.2450
04/25/2022 00:49:30 - INFO - __main__ - prefix=cosmos_qa_32_21, lr=0.3, bsz=8, dev_performance=0.3125, test_performance=0.245
04/25/2022 00:49:30 - INFO - __main__ - Running ... prefix=cosmos_qa_32_21, lr=0.2, bsz=8 ...
04/25/2022 00:49:31 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:49:31 - INFO - __main__ - Printing 3 examples
04/25/2022 00:49:31 - INFO - __main__ -  [cosmos_qa] What may happen after the event has concluded ? [SEP] After it was all said and done , we surpassed our fundraising target of $ 25,000 ! First and foremost , this meant a good donation to the Rickshaw Bank , which was able to start a new outfit in Allahabad . But the group accomplished something else this year - the establishment of our team . [SEP]  (A) They will find a way to pay back the $ 25,000 . (B) The earnings will be put to charitable use . (C) They will hope to surpass the fundraising target . (D) They will invest their donation from the Rickshaw Bank .
04/25/2022 00:49:31 - INFO - __main__ - ['The earnings will be put to charitable use .']
04/25/2022 00:49:31 - INFO - __main__ -  [cosmos_qa] What can you conclude about Bri and whipped cream ? [SEP] We hosted dinner for a young couple who just moved to Eugene ( daughter & her partner of Chicago friend . ) Bri expressed amazement when Bob announced he was going to make the whipped cream for the dessert . " You 're making the whipped cream ? ! " I guess she thought it was some complicated process , and was amazed further to find that all you do it put heavy cream in a bowl and whip it . [SEP]  (A) Bri has never made cream dessert in her life . (B) Bri has never made whipped cream in her life . (C) None of the above choices . (D) Bri has never made heavy cream in her life .
04/25/2022 00:49:31 - INFO - __main__ - ['Bri has never made whipped cream in her life .']
04/25/2022 00:49:31 - INFO - __main__ -  [cosmos_qa] Why is the narrator down on USC ? [SEP] When one looks at the rankings of CF for the first four weeks , you see USC in the # 1 spot . In fact , in weeks 3 and 4 they nearly got all of the # 1 votes . WHY ? [SEP]  (A) They feel they let them down . (B) They feel they are mean . (C) They feel they are a bad football team . (D) They feel they are not as good as others
04/25/2022 00:49:31 - INFO - __main__ - ['They feel they are not as good as others']
04/25/2022 00:49:31 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:49:31 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:49:31 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 00:49:31 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 00:49:31 - INFO - __main__ - Printing 3 examples
04/25/2022 00:49:31 - INFO - __main__ -  [cosmos_qa] Why is Roger 's face so red and flushed ? [SEP] He took a deep breath and unlocked the door . Roger pulled out the door and simply stepped in and locked it again . John was surprised , Roger did n't seem to be angry with him . " I 'm so sorry , I do n't know why I did that , I must be out of my mind and ... " John was interrupted by Roger 's tongue in his mouth , curiously seeking around in there . John felt both surprised and pleased at the same time , and answered Roger 's kiss . When they broke the contact , John could n't say anything except a simple " .. why ? " Roger blushed and shrugged his shoulders . [SEP]  (A) Roger just got dumped . (B) Roger is fearful for his job . (C) Roger got kissed by John . (D) Roger is worried about something .
04/25/2022 00:49:31 - INFO - __main__ - ['Roger got kissed by John .']
04/25/2022 00:49:31 - INFO - __main__ -  [cosmos_qa] What is the speaker trying to do ? [SEP] If someone points out something substantive I can fix in the analysis , I will try to clean it up . First , read this . This page gives a good discussion of a liquidity crisis in corporate finance . A macroeconomic " high finance " liquidity crisis is essentially the same thing , only happening with large banks . As in everything in high finance , high - finance liquidity crises are more complex . One key reason high - finance liquidity crises are much more complex and potentially disastrous than regular corporate liquidity crises is the meshed nature of the problem . [SEP]  (A) They are teaching their coworkers about liquidation . (B) They are trying to give a presentation at work . (C) They are trying to teach others about macroeconomics . (D) None of the above choices .
04/25/2022 00:49:31 - INFO - __main__ - ['They are trying to teach others about macroeconomics .']
04/25/2022 00:49:31 - INFO - __main__ -  [cosmos_qa] What may happen if i was more sociable ? [SEP] I do n't think I did the greatest job at networking , but I tried a bit . I ' m just not naturally a social person and it 's rather difficult for me to talk with people I do n't know very well . ( I could have used The Alternative Scientist 's Guide To Networking , but , alas , it was mostly posted while I was away . ) [SEP]  (A) I would feel like an outcast . (B) None of the above choices . (C) I would be disliked . (D) I would be criticized .
04/25/2022 00:49:31 - INFO - __main__ - ['None of the above choices .']
04/25/2022 00:49:31 - INFO - __main__ - Tokenizing Input ...
04/25/2022 00:49:31 - INFO - __main__ - Tokenizing Output ...
04/25/2022 00:49:31 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 00:49:50 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 00:49:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 00:49:50 - INFO - __main__ - Starting training!
04/25/2022 00:49:54 - INFO - __main__ - Step 10 Global step 10 Train loss 1.10 on epoch=4
04/25/2022 00:49:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.82 on epoch=9
04/25/2022 00:50:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=14
04/25/2022 00:50:04 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=19
04/25/2022 00:50:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.40 on epoch=24
04/25/2022 00:50:11 - INFO - __main__ - Global step 50 Train loss 0.68 ACC 0.0625 on epoch=24
04/25/2022 00:50:11 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0625 on epoch=24, global_step=50
04/25/2022 00:50:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.33 on epoch=29
04/25/2022 00:50:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.35 on epoch=34
04/25/2022 00:50:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.29 on epoch=39
04/25/2022 00:50:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.30 on epoch=44
04/25/2022 00:50:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
04/25/2022 00:50:30 - INFO - __main__ - Global step 100 Train loss 0.31 ACC 0.1875 on epoch=49
04/25/2022 00:50:30 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.1875 on epoch=49, global_step=100
04/25/2022 00:50:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.29 on epoch=54
04/25/2022 00:50:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
04/25/2022 00:50:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.26 on epoch=64
04/25/2022 00:50:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.21 on epoch=69
04/25/2022 00:50:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=74
04/25/2022 00:50:49 - INFO - __main__ - Global step 150 Train loss 0.26 ACC 0.1875 on epoch=74
04/25/2022 00:50:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=79
04/25/2022 00:50:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=84
04/25/2022 00:50:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
04/25/2022 00:51:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.21 on epoch=94
04/25/2022 00:51:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
04/25/2022 00:51:07 - INFO - __main__ - Global step 200 Train loss 0.23 ACC 0.15625 on epoch=99
04/25/2022 00:51:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=104
04/25/2022 00:51:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.23 on epoch=109
04/25/2022 00:51:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.20 on epoch=114
04/25/2022 00:51:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.19 on epoch=119
04/25/2022 00:51:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=124
04/25/2022 00:51:26 - INFO - __main__ - Global step 250 Train loss 0.20 ACC 0.1875 on epoch=124
04/25/2022 00:51:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.17 on epoch=129
04/25/2022 00:51:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.16 on epoch=134
04/25/2022 00:51:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.18 on epoch=139
04/25/2022 00:51:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
04/25/2022 00:51:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.16 on epoch=149
04/25/2022 00:51:45 - INFO - __main__ - Global step 300 Train loss 0.17 ACC 0.125 on epoch=149
04/25/2022 00:51:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.17 on epoch=154
04/25/2022 00:51:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.14 on epoch=159
04/25/2022 00:51:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.12 on epoch=164
04/25/2022 00:51:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
04/25/2022 00:52:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.15 on epoch=174
04/25/2022 00:52:03 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.125 on epoch=174
04/25/2022 00:52:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.12 on epoch=179
04/25/2022 00:52:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
04/25/2022 00:52:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.12 on epoch=189
04/25/2022 00:52:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.11 on epoch=194
04/25/2022 00:52:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.09 on epoch=199
04/25/2022 00:52:23 - INFO - __main__ - Global step 400 Train loss 0.12 ACC 0.1875 on epoch=199
04/25/2022 00:52:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.12 on epoch=204
04/25/2022 00:52:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=209
04/25/2022 00:52:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.11 on epoch=214
04/25/2022 00:52:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
04/25/2022 00:52:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.15 on epoch=224
04/25/2022 00:52:43 - INFO - __main__ - Global step 450 Train loss 0.11 ACC 0.21875 on epoch=224
04/25/2022 00:52:43 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=224, global_step=450
04/25/2022 00:52:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=229
04/25/2022 00:52:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
04/25/2022 00:52:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.10 on epoch=239
04/25/2022 00:52:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=244
04/25/2022 00:52:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.11 on epoch=249
04/25/2022 00:53:01 - INFO - __main__ - Global step 500 Train loss 0.11 ACC 0.15625 on epoch=249
04/25/2022 00:53:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
04/25/2022 00:53:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=259
04/25/2022 00:53:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
04/25/2022 00:53:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=269
04/25/2022 00:53:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=274
04/25/2022 00:53:20 - INFO - __main__ - Global step 550 Train loss 0.10 ACC 0.21875 on epoch=274
04/25/2022 00:53:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=279
04/25/2022 00:53:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=284
04/25/2022 00:53:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.11 on epoch=289
04/25/2022 00:53:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
04/25/2022 00:53:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
04/25/2022 00:53:39 - INFO - __main__ - Global step 600 Train loss 0.11 ACC 0.25 on epoch=299
04/25/2022 00:53:39 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=299, global_step=600
04/25/2022 00:53:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=304
04/25/2022 00:53:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=309
04/25/2022 00:53:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=314
04/25/2022 00:53:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=319
04/25/2022 00:53:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=324
04/25/2022 00:53:58 - INFO - __main__ - Global step 650 Train loss 0.09 ACC 0.09375 on epoch=324
04/25/2022 00:54:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
04/25/2022 00:54:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=334
04/25/2022 00:54:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=339
04/25/2022 00:54:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.07 on epoch=344
04/25/2022 00:54:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
04/25/2022 00:54:22 - INFO - __main__ - Global step 700 Train loss 0.08 ACC 0.0625 on epoch=349
04/25/2022 00:54:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=354
04/25/2022 00:54:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=359
04/25/2022 00:54:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=364
04/25/2022 00:54:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=369
04/25/2022 00:54:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
04/25/2022 00:54:41 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.09375 on epoch=374
04/25/2022 00:54:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=379
04/25/2022 00:54:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
04/25/2022 00:54:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=389
04/25/2022 00:54:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
04/25/2022 00:54:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
04/25/2022 00:55:00 - INFO - __main__ - Global step 800 Train loss 0.07 ACC 0.15625 on epoch=399
04/25/2022 00:55:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
04/25/2022 00:55:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
04/25/2022 00:55:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=414
04/25/2022 00:55:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
04/25/2022 00:55:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=424
04/25/2022 00:55:20 - INFO - __main__ - Global step 850 Train loss 0.05 ACC 0.09375 on epoch=424
04/25/2022 00:55:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
04/25/2022 00:55:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
04/25/2022 00:55:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=439
04/25/2022 00:55:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
04/25/2022 00:55:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=449
04/25/2022 00:55:41 - INFO - __main__ - Global step 900 Train loss 0.05 ACC 0.15625 on epoch=449
04/25/2022 00:55:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=454
04/25/2022 00:55:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
04/25/2022 00:55:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
04/25/2022 00:55:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
04/25/2022 00:55:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
04/25/2022 00:56:00 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.125 on epoch=474
04/25/2022 00:56:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
04/25/2022 00:56:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
04/25/2022 00:56:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
04/25/2022 00:56:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
04/25/2022 00:56:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/25/2022 00:56:20 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.1875 on epoch=499
04/25/2022 00:56:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
04/25/2022 00:56:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
04/25/2022 00:56:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
04/25/2022 00:56:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
04/25/2022 00:56:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
04/25/2022 00:56:40 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.15625 on epoch=524
04/25/2022 00:56:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=529
04/25/2022 00:56:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
04/25/2022 00:56:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=539
04/25/2022 00:56:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
04/25/2022 00:56:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/25/2022 00:56:59 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.15625 on epoch=549
04/25/2022 00:57:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
04/25/2022 00:57:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
04/25/2022 00:57:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/25/2022 00:57:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
04/25/2022 00:57:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 00:57:19 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.1875 on epoch=574
04/25/2022 00:57:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 00:57:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
04/25/2022 00:57:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/25/2022 00:57:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
04/25/2022 00:57:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
04/25/2022 00:57:38 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.15625 on epoch=599
04/25/2022 00:57:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
04/25/2022 00:57:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
04/25/2022 00:57:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
04/25/2022 00:57:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
04/25/2022 00:57:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/25/2022 00:57:58 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.1875 on epoch=624
04/25/2022 00:58:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
04/25/2022 00:58:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 00:58:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/25/2022 00:58:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/25/2022 00:58:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
04/25/2022 00:58:18 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.15625 on epoch=649
04/25/2022 00:58:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
04/25/2022 00:58:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 00:58:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
04/25/2022 00:58:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
04/25/2022 00:58:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
04/25/2022 00:58:40 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.125 on epoch=674
04/25/2022 00:58:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/25/2022 00:58:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=684
04/25/2022 00:58:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=689
04/25/2022 00:58:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=694
04/25/2022 00:58:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 00:59:00 - INFO - __main__ - Global step 1400 Train loss 0.11 ACC 0.125 on epoch=699
04/25/2022 00:59:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/25/2022 00:59:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
04/25/2022 00:59:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
04/25/2022 00:59:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 00:59:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
04/25/2022 00:59:19 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.125 on epoch=724
04/25/2022 00:59:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/25/2022 00:59:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/25/2022 00:59:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 00:59:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 00:59:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
04/25/2022 00:59:39 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.125 on epoch=749
04/25/2022 00:59:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
04/25/2022 00:59:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 00:59:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
04/25/2022 00:59:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
04/25/2022 00:59:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/25/2022 00:59:58 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.125 on epoch=774
04/25/2022 01:00:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 01:00:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 01:00:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 01:00:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
04/25/2022 01:00:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
04/25/2022 01:00:18 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.125 on epoch=799
04/25/2022 01:00:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 01:00:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 01:00:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 01:00:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
04/25/2022 01:00:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
04/25/2022 01:00:38 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.15625 on epoch=824
04/25/2022 01:00:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
04/25/2022 01:00:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/25/2022 01:00:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/25/2022 01:00:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=844
04/25/2022 01:00:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 01:00:57 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.125 on epoch=849
04/25/2022 01:01:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 01:01:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
04/25/2022 01:01:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
04/25/2022 01:01:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 01:01:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/25/2022 01:01:17 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.125 on epoch=874
04/25/2022 01:01:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/25/2022 01:01:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 01:01:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
04/25/2022 01:01:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
04/25/2022 01:01:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
04/25/2022 01:01:39 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.125 on epoch=899
04/25/2022 01:01:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 01:01:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/25/2022 01:01:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 01:01:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 01:01:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 01:01:58 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.125 on epoch=924
04/25/2022 01:02:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 01:02:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
04/25/2022 01:02:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 01:02:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 01:02:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
04/25/2022 01:02:18 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.125 on epoch=949
04/25/2022 01:02:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
04/25/2022 01:02:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
04/25/2022 01:02:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 01:02:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
04/25/2022 01:02:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 01:02:37 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.125 on epoch=974
04/25/2022 01:02:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 01:02:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 01:02:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
04/25/2022 01:02:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 01:02:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 01:02:54 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:02:54 - INFO - __main__ - Printing 3 examples
04/25/2022 01:02:54 - INFO - __main__ -  [cosmos_qa] What happened before going to the church ? [SEP] He even extended his hand to the pastor after the service , it was so funny . My daughter was baptized in this church 29 years too , so being able to see him baptized here as well made the day even more special . We ended the day with a delicious brunch after the service with a few family members and friends . The weather was beautiful . [SEP]  (A) None of the above choices . (B) Everyone enjoyed the weather (C) They drove in the car to get to the church (D) Everyone went to the park
04/25/2022 01:02:54 - INFO - __main__ - ['They drove in the car to get to the church']
04/25/2022 01:02:54 - INFO - __main__ -  [cosmos_qa] Why are working class women activists rendered invisible and anonymous by history ? [SEP] harpymarx said : I saw Her Naked Skin last weekend ( and wrote my own review on my blog ) and thought it was very good but I have to say that I did n't think Eve was as a well developed character like Celia . We saw interactions between Celia and husband but what about Eve .... In some ways I saw Eve as a parallel with the whole of the Suffragette movement where the privileged Pankhursts ' along with other privileged women are remembered and at the forefront of the history but what about many of the working class women activists who have been rendered invisible and anonymous by history ? Very little trace is left . [SEP]  (A) Working class women activists are rendered invisible and anonymous by history because they are unimportant in the suffragette movement . (B) Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society . (C) Working class women activists are rendered invisible and anonymous by history because historians deem them to have minor impact on history . (D) Working class women activists are rendered invisible and anonymous by history because the wealthy elite try to downplay their role in suffrage .
04/25/2022 01:02:54 - INFO - __main__ - ['Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society .']
04/25/2022 01:02:54 - INFO - __main__ -  [cosmos_qa] What may be your reason for going to the doctors on Monday ? [SEP] The jar went up my arms into my back and neck and left me with a headache all evening and I am now sporting very sore and bruised arms and both knees are sore . So I am going to take things easy for a while . Another thing that is concerning me is MLA has developed a twitch of shaking his head , he is even doing it in his sleep . So I am taking him to the doctors on Monday as he is getting so tired because of it . [SEP]  (A) I have a headache that wo n't go away . (B) Someone I know is starting to have an odd spasm . (C) I 'm starting to have an odd spasm . (D) My arms are bruised and I need to go to the doctors .
04/25/2022 01:02:54 - INFO - __main__ - ['Someone I know is starting to have an odd spasm .']
04/25/2022 01:02:54 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:02:54 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:02:54 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 01:02:54 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:02:54 - INFO - __main__ - Printing 3 examples
04/25/2022 01:02:54 - INFO - __main__ -  [cosmos_qa] What may happen after they put up the clock ? [SEP] The only thing on the wall yet is our Auburn clock that Hannah gave us . I was worried that it would get messed up , so as soon as I found my drill and a screw , it went up . Hopefully tomorrow while Josh is at work I can get some more stuff up . I have to start on the guest room too . [SEP]  (A) They will get more clocks for their house . (B) They will continue remodeling their house . (C) They will decorate the rest of their house . (D) They will get more wall decorations .
04/25/2022 01:02:54 - INFO - __main__ - ['They will decorate the rest of their house .']
04/25/2022 01:02:54 - INFO - __main__ -  [cosmos_qa] What is the woman to do to try to tell these two apart ? [SEP] They really were stunningly similar , with a spattering of freckles and a puff of pale hair on their heads . If it was n't for the cut and the expressions on their faces , she would n't have been able to tell them apart . She stood back up . [SEP]  (A) There is no way to distinguish these two individuals . (B) These two individuals are exactly the same in every way . (C) They will be distinguished by the differences in their freckles . (D) She will look to distinguish them by the difference in facial expressions .
04/25/2022 01:02:54 - INFO - __main__ - ['She will look to distinguish them by the difference in facial expressions .']
04/25/2022 01:02:54 - INFO - __main__ -  [cosmos_qa] Why has the narrator decided to stop drinking soda ? [SEP] I do n't want to be ultra thin , I still got ta keep the booty ! Haha . I 've already cut soda out of my diet completely , unless it 's diet soda . So if you guys can give me any pointers or any help at all . That would be great ! [SEP]  (A) None of the above choices . (B) They are not a fan of the new taste . (C) They are allergic . (D) They are dieting .
04/25/2022 01:02:54 - INFO - __main__ - ['They are dieting .']
04/25/2022 01:02:54 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:02:54 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:02:54 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 01:02:58 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.15625 on epoch=999
04/25/2022 01:02:58 - INFO - __main__ - save last model!
04/25/2022 01:02:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 01:02:58 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 01:02:58 - INFO - __main__ - Printing 3 examples
04/25/2022 01:02:58 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 01:02:58 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 01:02:58 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 01:02:58 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 01:02:58 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 01:02:58 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 01:02:58 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:02:59 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:03:00 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 01:03:13 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 01:03:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 01:03:14 - INFO - __main__ - Starting training!
04/25/2022 01:05:04 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_21_0.2_8_predictions.txt
04/25/2022 01:05:04 - INFO - __main__ - ACC on test data: 0.2520
04/25/2022 01:05:04 - INFO - __main__ - prefix=cosmos_qa_32_21, lr=0.2, bsz=8, dev_performance=0.25, test_performance=0.252
04/25/2022 01:05:04 - INFO - __main__ - Running ... prefix=cosmos_qa_32_42, lr=0.5, bsz=8 ...
04/25/2022 01:05:05 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:05:05 - INFO - __main__ - Printing 3 examples
04/25/2022 01:05:05 - INFO - __main__ -  [cosmos_qa] What happened before going to the church ? [SEP] He even extended his hand to the pastor after the service , it was so funny . My daughter was baptized in this church 29 years too , so being able to see him baptized here as well made the day even more special . We ended the day with a delicious brunch after the service with a few family members and friends . The weather was beautiful . [SEP]  (A) None of the above choices . (B) Everyone enjoyed the weather (C) They drove in the car to get to the church (D) Everyone went to the park
04/25/2022 01:05:05 - INFO - __main__ - ['They drove in the car to get to the church']
04/25/2022 01:05:05 - INFO - __main__ -  [cosmos_qa] Why are working class women activists rendered invisible and anonymous by history ? [SEP] harpymarx said : I saw Her Naked Skin last weekend ( and wrote my own review on my blog ) and thought it was very good but I have to say that I did n't think Eve was as a well developed character like Celia . We saw interactions between Celia and husband but what about Eve .... In some ways I saw Eve as a parallel with the whole of the Suffragette movement where the privileged Pankhursts ' along with other privileged women are remembered and at the forefront of the history but what about many of the working class women activists who have been rendered invisible and anonymous by history ? Very little trace is left . [SEP]  (A) Working class women activists are rendered invisible and anonymous by history because they are unimportant in the suffragette movement . (B) Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society . (C) Working class women activists are rendered invisible and anonymous by history because historians deem them to have minor impact on history . (D) Working class women activists are rendered invisible and anonymous by history because the wealthy elite try to downplay their role in suffrage .
04/25/2022 01:05:05 - INFO - __main__ - ['Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society .']
04/25/2022 01:05:05 - INFO - __main__ -  [cosmos_qa] What may be your reason for going to the doctors on Monday ? [SEP] The jar went up my arms into my back and neck and left me with a headache all evening and I am now sporting very sore and bruised arms and both knees are sore . So I am going to take things easy for a while . Another thing that is concerning me is MLA has developed a twitch of shaking his head , he is even doing it in his sleep . So I am taking him to the doctors on Monday as he is getting so tired because of it . [SEP]  (A) I have a headache that wo n't go away . (B) Someone I know is starting to have an odd spasm . (C) I 'm starting to have an odd spasm . (D) My arms are bruised and I need to go to the doctors .
04/25/2022 01:05:05 - INFO - __main__ - ['Someone I know is starting to have an odd spasm .']
04/25/2022 01:05:05 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:05:05 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:05:05 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 01:05:05 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:05:05 - INFO - __main__ - Printing 3 examples
04/25/2022 01:05:05 - INFO - __main__ -  [cosmos_qa] What may happen after they put up the clock ? [SEP] The only thing on the wall yet is our Auburn clock that Hannah gave us . I was worried that it would get messed up , so as soon as I found my drill and a screw , it went up . Hopefully tomorrow while Josh is at work I can get some more stuff up . I have to start on the guest room too . [SEP]  (A) They will get more clocks for their house . (B) They will continue remodeling their house . (C) They will decorate the rest of their house . (D) They will get more wall decorations .
04/25/2022 01:05:05 - INFO - __main__ - ['They will decorate the rest of their house .']
04/25/2022 01:05:05 - INFO - __main__ -  [cosmos_qa] What is the woman to do to try to tell these two apart ? [SEP] They really were stunningly similar , with a spattering of freckles and a puff of pale hair on their heads . If it was n't for the cut and the expressions on their faces , she would n't have been able to tell them apart . She stood back up . [SEP]  (A) There is no way to distinguish these two individuals . (B) These two individuals are exactly the same in every way . (C) They will be distinguished by the differences in their freckles . (D) She will look to distinguish them by the difference in facial expressions .
04/25/2022 01:05:05 - INFO - __main__ - ['She will look to distinguish them by the difference in facial expressions .']
04/25/2022 01:05:05 - INFO - __main__ -  [cosmos_qa] Why has the narrator decided to stop drinking soda ? [SEP] I do n't want to be ultra thin , I still got ta keep the booty ! Haha . I 've already cut soda out of my diet completely , unless it 's diet soda . So if you guys can give me any pointers or any help at all . That would be great ! [SEP]  (A) None of the above choices . (B) They are not a fan of the new taste . (C) They are allergic . (D) They are dieting .
04/25/2022 01:05:05 - INFO - __main__ - ['They are dieting .']
04/25/2022 01:05:05 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:05:05 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:05:05 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 01:05:21 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 01:05:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 01:05:22 - INFO - __main__ - Starting training!
04/25/2022 01:05:26 - INFO - __main__ - Step 10 Global step 10 Train loss 0.94 on epoch=4
04/25/2022 01:05:29 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=9
04/25/2022 01:05:32 - INFO - __main__ - Step 30 Global step 30 Train loss 0.39 on epoch=14
04/25/2022 01:05:35 - INFO - __main__ - Step 40 Global step 40 Train loss 0.31 on epoch=19
04/25/2022 01:05:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.26 on epoch=24
04/25/2022 01:05:43 - INFO - __main__ - Global step 50 Train loss 0.48 ACC 0.25 on epoch=24
04/25/2022 01:05:43 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.25 on epoch=24, global_step=50
04/25/2022 01:05:46 - INFO - __main__ - Step 60 Global step 60 Train loss 0.22 on epoch=29
04/25/2022 01:05:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.24 on epoch=34
04/25/2022 01:05:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.22 on epoch=39
04/25/2022 01:05:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.19 on epoch=44
04/25/2022 01:05:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.18 on epoch=49
04/25/2022 01:06:01 - INFO - __main__ - Global step 100 Train loss 0.21 ACC 0.34375 on epoch=49
04/25/2022 01:06:02 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.34375 on epoch=49, global_step=100
04/25/2022 01:06:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.16 on epoch=54
04/25/2022 01:06:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.15 on epoch=59
04/25/2022 01:06:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.15 on epoch=64
04/25/2022 01:06:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.11 on epoch=69
04/25/2022 01:06:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.12 on epoch=74
04/25/2022 01:06:20 - INFO - __main__ - Global step 150 Train loss 0.14 ACC 0.28125 on epoch=74
04/25/2022 01:06:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.11 on epoch=79
04/25/2022 01:06:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.11 on epoch=84
04/25/2022 01:06:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.10 on epoch=89
04/25/2022 01:06:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.10 on epoch=94
04/25/2022 01:06:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.10 on epoch=99
04/25/2022 01:06:38 - INFO - __main__ - Global step 200 Train loss 0.10 ACC 0.3125 on epoch=99
04/25/2022 01:06:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.10 on epoch=104
04/25/2022 01:06:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.08 on epoch=109
04/25/2022 01:06:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.06 on epoch=114
04/25/2022 01:06:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.09 on epoch=119
04/25/2022 01:06:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.07 on epoch=124
04/25/2022 01:06:56 - INFO - __main__ - Global step 250 Train loss 0.08 ACC 0.21875 on epoch=124
04/25/2022 01:06:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.07 on epoch=129
04/25/2022 01:07:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.06 on epoch=134
04/25/2022 01:07:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.05 on epoch=139
04/25/2022 01:07:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.06 on epoch=144
04/25/2022 01:07:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.06 on epoch=149
04/25/2022 01:07:15 - INFO - __main__ - Global step 300 Train loss 0.06 ACC 0.25 on epoch=149
04/25/2022 01:07:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.05 on epoch=154
04/25/2022 01:07:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.04 on epoch=159
04/25/2022 01:07:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.04 on epoch=164
04/25/2022 01:07:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.02 on epoch=169
04/25/2022 01:07:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.04 on epoch=174
04/25/2022 01:07:33 - INFO - __main__ - Global step 350 Train loss 0.04 ACC 0.375 on epoch=174
04/25/2022 01:07:33 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=174, global_step=350
04/25/2022 01:07:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.03 on epoch=179
04/25/2022 01:07:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.03 on epoch=184
04/25/2022 01:07:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.05 on epoch=189
04/25/2022 01:07:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.03 on epoch=194
04/25/2022 01:07:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.03 on epoch=199
04/25/2022 01:07:52 - INFO - __main__ - Global step 400 Train loss 0.04 ACC 0.1875 on epoch=199
04/25/2022 01:07:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.03 on epoch=204
04/25/2022 01:07:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.02 on epoch=209
04/25/2022 01:08:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.03 on epoch=214
04/25/2022 01:08:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.02 on epoch=219
04/25/2022 01:08:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.15 on epoch=224
04/25/2022 01:08:10 - INFO - __main__ - Global step 450 Train loss 0.05 ACC 0.21875 on epoch=224
04/25/2022 01:08:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
04/25/2022 01:08:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.03 on epoch=234
04/25/2022 01:08:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.04 on epoch=239
04/25/2022 01:08:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.03 on epoch=244
04/25/2022 01:08:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.03 on epoch=249
04/25/2022 01:08:29 - INFO - __main__ - Global step 500 Train loss 0.03 ACC 0.25 on epoch=249
04/25/2022 01:08:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
04/25/2022 01:08:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
04/25/2022 01:08:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
04/25/2022 01:08:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
04/25/2022 01:08:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
04/25/2022 01:08:48 - INFO - __main__ - Global step 550 Train loss 0.03 ACC 0.21875 on epoch=274
04/25/2022 01:08:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
04/25/2022 01:08:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
04/25/2022 01:08:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
04/25/2022 01:09:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
04/25/2022 01:09:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
04/25/2022 01:09:06 - INFO - __main__ - Global step 600 Train loss 0.02 ACC 0.28125 on epoch=299
04/25/2022 01:09:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
04/25/2022 01:09:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
04/25/2022 01:09:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
04/25/2022 01:09:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
04/25/2022 01:09:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
04/25/2022 01:09:25 - INFO - __main__ - Global step 650 Train loss 0.01 ACC 0.21875 on epoch=324
04/25/2022 01:09:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
04/25/2022 01:09:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
04/25/2022 01:09:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
04/25/2022 01:09:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
04/25/2022 01:09:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
04/25/2022 01:09:43 - INFO - __main__ - Global step 700 Train loss 0.02 ACC 0.21875 on epoch=349
04/25/2022 01:09:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
04/25/2022 01:09:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
04/25/2022 01:09:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
04/25/2022 01:09:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
04/25/2022 01:09:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
04/25/2022 01:10:02 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.21875 on epoch=374
04/25/2022 01:10:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
04/25/2022 01:10:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
04/25/2022 01:10:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
04/25/2022 01:10:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
04/25/2022 01:10:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
04/25/2022 01:10:21 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.21875 on epoch=399
04/25/2022 01:10:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
04/25/2022 01:10:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
04/25/2022 01:10:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
04/25/2022 01:10:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
04/25/2022 01:10:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
04/25/2022 01:10:39 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.1875 on epoch=424
04/25/2022 01:10:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
04/25/2022 01:10:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
04/25/2022 01:10:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
04/25/2022 01:10:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
04/25/2022 01:10:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
04/25/2022 01:10:58 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.15625 on epoch=449
04/25/2022 01:11:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
04/25/2022 01:11:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
04/25/2022 01:11:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
04/25/2022 01:11:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
04/25/2022 01:11:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
04/25/2022 01:11:16 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.28125 on epoch=474
04/25/2022 01:11:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/25/2022 01:11:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
04/25/2022 01:11:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
04/25/2022 01:11:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
04/25/2022 01:11:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
04/25/2022 01:11:35 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.21875 on epoch=499
04/25/2022 01:11:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
04/25/2022 01:11:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
04/25/2022 01:11:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
04/25/2022 01:11:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
04/25/2022 01:11:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/25/2022 01:11:53 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.15625 on epoch=524
04/25/2022 01:11:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
04/25/2022 01:11:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
04/25/2022 01:12:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
04/25/2022 01:12:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
04/25/2022 01:12:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
04/25/2022 01:12:12 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.21875 on epoch=549
04/25/2022 01:12:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/25/2022 01:12:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/25/2022 01:12:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
04/25/2022 01:12:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
04/25/2022 01:12:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 01:12:31 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.1875 on epoch=574
04/25/2022 01:12:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
04/25/2022 01:12:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/25/2022 01:12:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/25/2022 01:12:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
04/25/2022 01:12:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
04/25/2022 01:12:49 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.15625 on epoch=599
04/25/2022 01:12:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
04/25/2022 01:12:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 01:12:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
04/25/2022 01:13:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
04/25/2022 01:13:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
04/25/2022 01:13:07 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.1875 on epoch=624
04/25/2022 01:13:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 01:13:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 01:13:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/25/2022 01:13:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/25/2022 01:13:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
04/25/2022 01:13:26 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.25 on epoch=649
04/25/2022 01:13:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 01:13:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 01:13:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
04/25/2022 01:13:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
04/25/2022 01:13:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 01:13:44 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.21875 on epoch=674
04/25/2022 01:13:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
04/25/2022 01:13:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
04/25/2022 01:13:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/25/2022 01:13:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
04/25/2022 01:13:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
04/25/2022 01:14:03 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.125 on epoch=699
04/25/2022 01:14:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 01:14:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
04/25/2022 01:14:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
04/25/2022 01:14:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
04/25/2022 01:14:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
04/25/2022 01:14:21 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.1875 on epoch=724
04/25/2022 01:14:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
04/25/2022 01:14:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
04/25/2022 01:14:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
04/25/2022 01:14:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
04/25/2022 01:14:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 01:14:40 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.21875 on epoch=749
04/25/2022 01:14:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
04/25/2022 01:14:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 01:14:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
04/25/2022 01:14:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
04/25/2022 01:14:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/25/2022 01:14:58 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.21875 on epoch=774
04/25/2022 01:15:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
04/25/2022 01:15:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
04/25/2022 01:15:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 01:15:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 01:15:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
04/25/2022 01:15:17 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.21875 on epoch=799
04/25/2022 01:15:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
04/25/2022 01:15:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/25/2022 01:15:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 01:15:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
04/25/2022 01:15:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 01:15:35 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.21875 on epoch=824
04/25/2022 01:15:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/25/2022 01:15:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 01:15:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
04/25/2022 01:15:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
04/25/2022 01:15:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 01:15:54 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.3125 on epoch=849
04/25/2022 01:15:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
04/25/2022 01:16:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
04/25/2022 01:16:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 01:16:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
04/25/2022 01:16:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 01:16:13 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.28125 on epoch=874
04/25/2022 01:16:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
04/25/2022 01:16:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 01:16:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
04/25/2022 01:16:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
04/25/2022 01:16:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
04/25/2022 01:16:31 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.1875 on epoch=899
04/25/2022 01:16:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
04/25/2022 01:16:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/25/2022 01:16:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
04/25/2022 01:16:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
04/25/2022 01:16:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
04/25/2022 01:16:50 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.25 on epoch=924
04/25/2022 01:16:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
04/25/2022 01:16:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 01:16:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
04/25/2022 01:17:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 01:17:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
04/25/2022 01:17:09 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.1875 on epoch=949
04/25/2022 01:17:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
04/25/2022 01:17:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
04/25/2022 01:17:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
04/25/2022 01:17:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
04/25/2022 01:17:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
04/25/2022 01:17:27 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.25 on epoch=974
04/25/2022 01:17:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
04/25/2022 01:17:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 01:17:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
04/25/2022 01:17:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 01:17:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
04/25/2022 01:17:43 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:17:43 - INFO - __main__ - Printing 3 examples
04/25/2022 01:17:43 - INFO - __main__ -  [cosmos_qa] What happened before going to the church ? [SEP] He even extended his hand to the pastor after the service , it was so funny . My daughter was baptized in this church 29 years too , so being able to see him baptized here as well made the day even more special . We ended the day with a delicious brunch after the service with a few family members and friends . The weather was beautiful . [SEP]  (A) None of the above choices . (B) Everyone enjoyed the weather (C) They drove in the car to get to the church (D) Everyone went to the park
04/25/2022 01:17:43 - INFO - __main__ - ['They drove in the car to get to the church']
04/25/2022 01:17:43 - INFO - __main__ -  [cosmos_qa] Why are working class women activists rendered invisible and anonymous by history ? [SEP] harpymarx said : I saw Her Naked Skin last weekend ( and wrote my own review on my blog ) and thought it was very good but I have to say that I did n't think Eve was as a well developed character like Celia . We saw interactions between Celia and husband but what about Eve .... In some ways I saw Eve as a parallel with the whole of the Suffragette movement where the privileged Pankhursts ' along with other privileged women are remembered and at the forefront of the history but what about many of the working class women activists who have been rendered invisible and anonymous by history ? Very little trace is left . [SEP]  (A) Working class women activists are rendered invisible and anonymous by history because they are unimportant in the suffragette movement . (B) Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society . (C) Working class women activists are rendered invisible and anonymous by history because historians deem them to have minor impact on history . (D) Working class women activists are rendered invisible and anonymous by history because the wealthy elite try to downplay their role in suffrage .
04/25/2022 01:17:43 - INFO - __main__ - ['Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society .']
04/25/2022 01:17:43 - INFO - __main__ -  [cosmos_qa] What may be your reason for going to the doctors on Monday ? [SEP] The jar went up my arms into my back and neck and left me with a headache all evening and I am now sporting very sore and bruised arms and both knees are sore . So I am going to take things easy for a while . Another thing that is concerning me is MLA has developed a twitch of shaking his head , he is even doing it in his sleep . So I am taking him to the doctors on Monday as he is getting so tired because of it . [SEP]  (A) I have a headache that wo n't go away . (B) Someone I know is starting to have an odd spasm . (C) I 'm starting to have an odd spasm . (D) My arms are bruised and I need to go to the doctors .
04/25/2022 01:17:43 - INFO - __main__ - ['Someone I know is starting to have an odd spasm .']
04/25/2022 01:17:43 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:17:43 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:17:43 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 01:17:43 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:17:43 - INFO - __main__ - Printing 3 examples
04/25/2022 01:17:43 - INFO - __main__ -  [cosmos_qa] What may happen after they put up the clock ? [SEP] The only thing on the wall yet is our Auburn clock that Hannah gave us . I was worried that it would get messed up , so as soon as I found my drill and a screw , it went up . Hopefully tomorrow while Josh is at work I can get some more stuff up . I have to start on the guest room too . [SEP]  (A) They will get more clocks for their house . (B) They will continue remodeling their house . (C) They will decorate the rest of their house . (D) They will get more wall decorations .
04/25/2022 01:17:43 - INFO - __main__ - ['They will decorate the rest of their house .']
04/25/2022 01:17:43 - INFO - __main__ -  [cosmos_qa] What is the woman to do to try to tell these two apart ? [SEP] They really were stunningly similar , with a spattering of freckles and a puff of pale hair on their heads . If it was n't for the cut and the expressions on their faces , she would n't have been able to tell them apart . She stood back up . [SEP]  (A) There is no way to distinguish these two individuals . (B) These two individuals are exactly the same in every way . (C) They will be distinguished by the differences in their freckles . (D) She will look to distinguish them by the difference in facial expressions .
04/25/2022 01:17:43 - INFO - __main__ - ['She will look to distinguish them by the difference in facial expressions .']
04/25/2022 01:17:43 - INFO - __main__ -  [cosmos_qa] Why has the narrator decided to stop drinking soda ? [SEP] I do n't want to be ultra thin , I still got ta keep the booty ! Haha . I 've already cut soda out of my diet completely , unless it 's diet soda . So if you guys can give me any pointers or any help at all . That would be great ! [SEP]  (A) None of the above choices . (B) They are not a fan of the new taste . (C) They are allergic . (D) They are dieting .
04/25/2022 01:17:43 - INFO - __main__ - ['They are dieting .']
04/25/2022 01:17:43 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:17:44 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:17:44 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 01:17:46 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.25 on epoch=999
04/25/2022 01:17:46 - INFO - __main__ - save last model!
04/25/2022 01:17:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 01:17:46 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 01:17:46 - INFO - __main__ - Printing 3 examples
04/25/2022 01:17:46 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 01:17:46 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 01:17:46 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 01:17:46 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 01:17:46 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 01:17:46 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 01:17:46 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:17:47 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:17:48 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 01:18:00 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 01:18:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 01:18:00 - INFO - __main__ - Starting training!
04/25/2022 01:20:06 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_42_0.5_8_predictions.txt
04/25/2022 01:20:06 - INFO - __main__ - ACC on test data: 0.3100
04/25/2022 01:20:06 - INFO - __main__ - prefix=cosmos_qa_32_42, lr=0.5, bsz=8, dev_performance=0.375, test_performance=0.31
04/25/2022 01:20:06 - INFO - __main__ - Running ... prefix=cosmos_qa_32_42, lr=0.4, bsz=8 ...
04/25/2022 01:20:07 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:20:07 - INFO - __main__ - Printing 3 examples
04/25/2022 01:20:07 - INFO - __main__ -  [cosmos_qa] What happened before going to the church ? [SEP] He even extended his hand to the pastor after the service , it was so funny . My daughter was baptized in this church 29 years too , so being able to see him baptized here as well made the day even more special . We ended the day with a delicious brunch after the service with a few family members and friends . The weather was beautiful . [SEP]  (A) None of the above choices . (B) Everyone enjoyed the weather (C) They drove in the car to get to the church (D) Everyone went to the park
04/25/2022 01:20:07 - INFO - __main__ - ['They drove in the car to get to the church']
04/25/2022 01:20:07 - INFO - __main__ -  [cosmos_qa] Why are working class women activists rendered invisible and anonymous by history ? [SEP] harpymarx said : I saw Her Naked Skin last weekend ( and wrote my own review on my blog ) and thought it was very good but I have to say that I did n't think Eve was as a well developed character like Celia . We saw interactions between Celia and husband but what about Eve .... In some ways I saw Eve as a parallel with the whole of the Suffragette movement where the privileged Pankhursts ' along with other privileged women are remembered and at the forefront of the history but what about many of the working class women activists who have been rendered invisible and anonymous by history ? Very little trace is left . [SEP]  (A) Working class women activists are rendered invisible and anonymous by history because they are unimportant in the suffragette movement . (B) Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society . (C) Working class women activists are rendered invisible and anonymous by history because historians deem them to have minor impact on history . (D) Working class women activists are rendered invisible and anonymous by history because the wealthy elite try to downplay their role in suffrage .
04/25/2022 01:20:07 - INFO - __main__ - ['Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society .']
04/25/2022 01:20:07 - INFO - __main__ -  [cosmos_qa] What may be your reason for going to the doctors on Monday ? [SEP] The jar went up my arms into my back and neck and left me with a headache all evening and I am now sporting very sore and bruised arms and both knees are sore . So I am going to take things easy for a while . Another thing that is concerning me is MLA has developed a twitch of shaking his head , he is even doing it in his sleep . So I am taking him to the doctors on Monday as he is getting so tired because of it . [SEP]  (A) I have a headache that wo n't go away . (B) Someone I know is starting to have an odd spasm . (C) I 'm starting to have an odd spasm . (D) My arms are bruised and I need to go to the doctors .
04/25/2022 01:20:07 - INFO - __main__ - ['Someone I know is starting to have an odd spasm .']
04/25/2022 01:20:07 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:20:07 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:20:07 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 01:20:07 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:20:07 - INFO - __main__ - Printing 3 examples
04/25/2022 01:20:07 - INFO - __main__ -  [cosmos_qa] What may happen after they put up the clock ? [SEP] The only thing on the wall yet is our Auburn clock that Hannah gave us . I was worried that it would get messed up , so as soon as I found my drill and a screw , it went up . Hopefully tomorrow while Josh is at work I can get some more stuff up . I have to start on the guest room too . [SEP]  (A) They will get more clocks for their house . (B) They will continue remodeling their house . (C) They will decorate the rest of their house . (D) They will get more wall decorations .
04/25/2022 01:20:07 - INFO - __main__ - ['They will decorate the rest of their house .']
04/25/2022 01:20:07 - INFO - __main__ -  [cosmos_qa] What is the woman to do to try to tell these two apart ? [SEP] They really were stunningly similar , with a spattering of freckles and a puff of pale hair on their heads . If it was n't for the cut and the expressions on their faces , she would n't have been able to tell them apart . She stood back up . [SEP]  (A) There is no way to distinguish these two individuals . (B) These two individuals are exactly the same in every way . (C) They will be distinguished by the differences in their freckles . (D) She will look to distinguish them by the difference in facial expressions .
04/25/2022 01:20:07 - INFO - __main__ - ['She will look to distinguish them by the difference in facial expressions .']
04/25/2022 01:20:07 - INFO - __main__ -  [cosmos_qa] Why has the narrator decided to stop drinking soda ? [SEP] I do n't want to be ultra thin , I still got ta keep the booty ! Haha . I 've already cut soda out of my diet completely , unless it 's diet soda . So if you guys can give me any pointers or any help at all . That would be great ! [SEP]  (A) None of the above choices . (B) They are not a fan of the new taste . (C) They are allergic . (D) They are dieting .
04/25/2022 01:20:07 - INFO - __main__ - ['They are dieting .']
04/25/2022 01:20:07 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:20:07 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:20:07 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 01:20:22 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 01:20:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 01:20:23 - INFO - __main__ - Starting training!
04/25/2022 01:20:27 - INFO - __main__ - Step 10 Global step 10 Train loss 0.96 on epoch=4
04/25/2022 01:20:30 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=9
04/25/2022 01:20:33 - INFO - __main__ - Step 30 Global step 30 Train loss 0.32 on epoch=14
04/25/2022 01:20:36 - INFO - __main__ - Step 40 Global step 40 Train loss 0.33 on epoch=19
04/25/2022 01:20:39 - INFO - __main__ - Step 50 Global step 50 Train loss 0.26 on epoch=24
04/25/2022 01:20:45 - INFO - __main__ - Global step 50 Train loss 0.49 ACC 0.28125 on epoch=24
04/25/2022 01:20:45 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.28125 on epoch=24, global_step=50
04/25/2022 01:20:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.25 on epoch=29
04/25/2022 01:20:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.22 on epoch=34
04/25/2022 01:20:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.25 on epoch=39
04/25/2022 01:20:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.24 on epoch=44
04/25/2022 01:21:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.21 on epoch=49
04/25/2022 01:21:06 - INFO - __main__ - Global step 100 Train loss 0.24 ACC 0.3125 on epoch=49
04/25/2022 01:21:06 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.3125 on epoch=49, global_step=100
04/25/2022 01:21:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.19 on epoch=54
04/25/2022 01:21:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.17 on epoch=59
04/25/2022 01:21:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.19 on epoch=64
04/25/2022 01:21:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.15 on epoch=69
04/25/2022 01:21:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.36 on epoch=74
04/25/2022 01:21:24 - INFO - __main__ - Global step 150 Train loss 0.21 ACC 0.21875 on epoch=74
04/25/2022 01:21:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=79
04/25/2022 01:21:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.14 on epoch=84
04/25/2022 01:21:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.16 on epoch=89
04/25/2022 01:21:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.14 on epoch=94
04/25/2022 01:21:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.11 on epoch=99
04/25/2022 01:21:43 - INFO - __main__ - Global step 200 Train loss 0.16 ACC 0.25 on epoch=99
04/25/2022 01:21:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.15 on epoch=104
04/25/2022 01:21:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.11 on epoch=109
04/25/2022 01:21:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.09 on epoch=114
04/25/2022 01:21:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.11 on epoch=119
04/25/2022 01:21:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.09 on epoch=124
04/25/2022 01:22:01 - INFO - __main__ - Global step 250 Train loss 0.11 ACC 0.46875 on epoch=124
04/25/2022 01:22:01 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.46875 on epoch=124, global_step=250
04/25/2022 01:22:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.09 on epoch=129
04/25/2022 01:22:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.08 on epoch=134
04/25/2022 01:22:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.08 on epoch=139
04/25/2022 01:22:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.09 on epoch=144
04/25/2022 01:22:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.08 on epoch=149
04/25/2022 01:22:20 - INFO - __main__ - Global step 300 Train loss 0.09 ACC 0.28125 on epoch=149
04/25/2022 01:22:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.10 on epoch=154
04/25/2022 01:22:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.09 on epoch=159
04/25/2022 01:22:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.10 on epoch=164
04/25/2022 01:22:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.07 on epoch=169
04/25/2022 01:22:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.09 on epoch=174
04/25/2022 01:22:39 - INFO - __main__ - Global step 350 Train loss 0.09 ACC 0.28125 on epoch=174
04/25/2022 01:22:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.10 on epoch=179
04/25/2022 01:22:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.06 on epoch=184
04/25/2022 01:22:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.06 on epoch=189
04/25/2022 01:22:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.05 on epoch=194
04/25/2022 01:22:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.05 on epoch=199
04/25/2022 01:22:57 - INFO - __main__ - Global step 400 Train loss 0.07 ACC 0.1875 on epoch=199
04/25/2022 01:23:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.06 on epoch=204
04/25/2022 01:23:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
04/25/2022 01:23:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.03 on epoch=214
04/25/2022 01:23:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.05 on epoch=219
04/25/2022 01:23:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.04 on epoch=224
04/25/2022 01:23:16 - INFO - __main__ - Global step 450 Train loss 0.05 ACC 0.21875 on epoch=224
04/25/2022 01:23:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.05 on epoch=229
04/25/2022 01:23:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.04 on epoch=234
04/25/2022 01:23:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.05 on epoch=239
04/25/2022 01:23:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.04 on epoch=244
04/25/2022 01:23:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
04/25/2022 01:23:35 - INFO - __main__ - Global step 500 Train loss 0.05 ACC 0.125 on epoch=249
04/25/2022 01:23:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
04/25/2022 01:23:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=259
04/25/2022 01:23:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
04/25/2022 01:23:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.02 on epoch=269
04/25/2022 01:23:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
04/25/2022 01:23:53 - INFO - __main__ - Global step 550 Train loss 0.04 ACC 0.1875 on epoch=274
04/25/2022 01:23:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
04/25/2022 01:23:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
04/25/2022 01:24:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.03 on epoch=289
04/25/2022 01:24:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
04/25/2022 01:24:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
04/25/2022 01:24:14 - INFO - __main__ - Global step 600 Train loss 0.03 ACC 0.21875 on epoch=299
04/25/2022 01:24:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
04/25/2022 01:24:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
04/25/2022 01:24:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
04/25/2022 01:24:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
04/25/2022 01:24:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
04/25/2022 01:24:33 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.15625 on epoch=324
04/25/2022 01:24:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=329
04/25/2022 01:24:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
04/25/2022 01:24:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
04/25/2022 01:24:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
04/25/2022 01:24:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
04/25/2022 01:24:51 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.125 on epoch=349
04/25/2022 01:24:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
04/25/2022 01:24:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
04/25/2022 01:25:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
04/25/2022 01:25:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
04/25/2022 01:25:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
04/25/2022 01:25:10 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.21875 on epoch=374
04/25/2022 01:25:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
04/25/2022 01:25:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
04/25/2022 01:25:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
04/25/2022 01:25:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
04/25/2022 01:25:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
04/25/2022 01:25:28 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.125 on epoch=399
04/25/2022 01:25:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
04/25/2022 01:25:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
04/25/2022 01:25:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
04/25/2022 01:25:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
04/25/2022 01:25:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
04/25/2022 01:25:47 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.21875 on epoch=424
04/25/2022 01:25:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
04/25/2022 01:25:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
04/25/2022 01:25:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
04/25/2022 01:25:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
04/25/2022 01:26:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
04/25/2022 01:26:05 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.1875 on epoch=449
04/25/2022 01:26:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
04/25/2022 01:26:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
04/25/2022 01:26:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
04/25/2022 01:26:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
04/25/2022 01:26:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
04/25/2022 01:26:24 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.125 on epoch=474
04/25/2022 01:26:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/25/2022 01:26:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
04/25/2022 01:26:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
04/25/2022 01:26:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
04/25/2022 01:26:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
04/25/2022 01:26:42 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.1875 on epoch=499
04/25/2022 01:26:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
04/25/2022 01:26:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
04/25/2022 01:26:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
04/25/2022 01:26:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
04/25/2022 01:26:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/25/2022 01:27:01 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.15625 on epoch=524
04/25/2022 01:27:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
04/25/2022 01:27:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
04/25/2022 01:27:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/25/2022 01:27:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
04/25/2022 01:27:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
04/25/2022 01:27:19 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.15625 on epoch=549
04/25/2022 01:27:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/25/2022 01:27:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/25/2022 01:27:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
04/25/2022 01:27:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/25/2022 01:27:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 01:27:38 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.1875 on epoch=574
04/25/2022 01:27:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 01:27:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/25/2022 01:27:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/25/2022 01:27:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 01:27:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/25/2022 01:27:56 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.15625 on epoch=599
04/25/2022 01:27:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/25/2022 01:28:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 01:28:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
04/25/2022 01:28:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
04/25/2022 01:28:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
04/25/2022 01:28:15 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.1875 on epoch=624
04/25/2022 01:28:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 01:28:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 01:28:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=639
04/25/2022 01:28:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/25/2022 01:28:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/25/2022 01:28:33 - INFO - __main__ - Global step 1300 Train loss 0.04 ACC 0.15625 on epoch=649
04/25/2022 01:28:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 01:28:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 01:28:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/25/2022 01:28:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/25/2022 01:28:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
04/25/2022 01:28:52 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.1875 on epoch=674
04/25/2022 01:28:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 01:28:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 01:29:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/25/2022 01:29:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 01:29:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 01:29:10 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.15625 on epoch=699
04/25/2022 01:29:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 01:29:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 01:29:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/25/2022 01:29:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/25/2022 01:29:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/25/2022 01:29:29 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.15625 on epoch=724
04/25/2022 01:29:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 01:29:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/25/2022 01:29:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 01:29:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
04/25/2022 01:29:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 01:29:48 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.125 on epoch=749
04/25/2022 01:29:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
04/25/2022 01:29:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 01:29:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 01:30:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
04/25/2022 01:30:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
04/25/2022 01:30:06 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.15625 on epoch=774
04/25/2022 01:30:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 01:30:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
04/25/2022 01:30:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 01:30:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 01:30:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
04/25/2022 01:30:25 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.15625 on epoch=799
04/25/2022 01:30:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 01:30:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 01:30:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 01:30:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 01:30:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 01:30:43 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.15625 on epoch=824
04/25/2022 01:30:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/25/2022 01:30:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 01:30:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
04/25/2022 01:30:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 01:30:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 01:31:02 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.21875 on epoch=849
04/25/2022 01:31:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 01:31:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 01:31:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 01:31:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
04/25/2022 01:31:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
04/25/2022 01:31:20 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.125 on epoch=874
04/25/2022 01:31:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/25/2022 01:31:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 01:31:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 01:31:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
04/25/2022 01:31:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
04/25/2022 01:31:39 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.15625 on epoch=899
04/25/2022 01:31:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
04/25/2022 01:31:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
04/25/2022 01:31:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 01:31:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
04/25/2022 01:31:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
04/25/2022 01:31:58 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.09375 on epoch=924
04/25/2022 01:32:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 01:32:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 01:32:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
04/25/2022 01:32:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 01:32:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
04/25/2022 01:32:16 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.125 on epoch=949
04/25/2022 01:32:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 01:32:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 01:32:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
04/25/2022 01:32:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
04/25/2022 01:32:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 01:32:35 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.125 on epoch=974
04/25/2022 01:32:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
04/25/2022 01:32:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 01:32:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
04/25/2022 01:32:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
04/25/2022 01:32:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 01:32:52 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:32:52 - INFO - __main__ - Printing 3 examples
04/25/2022 01:32:52 - INFO - __main__ -  [cosmos_qa] What happened before going to the church ? [SEP] He even extended his hand to the pastor after the service , it was so funny . My daughter was baptized in this church 29 years too , so being able to see him baptized here as well made the day even more special . We ended the day with a delicious brunch after the service with a few family members and friends . The weather was beautiful . [SEP]  (A) None of the above choices . (B) Everyone enjoyed the weather (C) They drove in the car to get to the church (D) Everyone went to the park
04/25/2022 01:32:52 - INFO - __main__ - ['They drove in the car to get to the church']
04/25/2022 01:32:52 - INFO - __main__ -  [cosmos_qa] Why are working class women activists rendered invisible and anonymous by history ? [SEP] harpymarx said : I saw Her Naked Skin last weekend ( and wrote my own review on my blog ) and thought it was very good but I have to say that I did n't think Eve was as a well developed character like Celia . We saw interactions between Celia and husband but what about Eve .... In some ways I saw Eve as a parallel with the whole of the Suffragette movement where the privileged Pankhursts ' along with other privileged women are remembered and at the forefront of the history but what about many of the working class women activists who have been rendered invisible and anonymous by history ? Very little trace is left . [SEP]  (A) Working class women activists are rendered invisible and anonymous by history because they are unimportant in the suffragette movement . (B) Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society . (C) Working class women activists are rendered invisible and anonymous by history because historians deem them to have minor impact on history . (D) Working class women activists are rendered invisible and anonymous by history because the wealthy elite try to downplay their role in suffrage .
04/25/2022 01:32:52 - INFO - __main__ - ['Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society .']
04/25/2022 01:32:52 - INFO - __main__ -  [cosmos_qa] What may be your reason for going to the doctors on Monday ? [SEP] The jar went up my arms into my back and neck and left me with a headache all evening and I am now sporting very sore and bruised arms and both knees are sore . So I am going to take things easy for a while . Another thing that is concerning me is MLA has developed a twitch of shaking his head , he is even doing it in his sleep . So I am taking him to the doctors on Monday as he is getting so tired because of it . [SEP]  (A) I have a headache that wo n't go away . (B) Someone I know is starting to have an odd spasm . (C) I 'm starting to have an odd spasm . (D) My arms are bruised and I need to go to the doctors .
04/25/2022 01:32:52 - INFO - __main__ - ['Someone I know is starting to have an odd spasm .']
04/25/2022 01:32:52 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:32:52 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:32:52 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 01:32:52 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:32:52 - INFO - __main__ - Printing 3 examples
04/25/2022 01:32:52 - INFO - __main__ -  [cosmos_qa] What may happen after they put up the clock ? [SEP] The only thing on the wall yet is our Auburn clock that Hannah gave us . I was worried that it would get messed up , so as soon as I found my drill and a screw , it went up . Hopefully tomorrow while Josh is at work I can get some more stuff up . I have to start on the guest room too . [SEP]  (A) They will get more clocks for their house . (B) They will continue remodeling their house . (C) They will decorate the rest of their house . (D) They will get more wall decorations .
04/25/2022 01:32:52 - INFO - __main__ - ['They will decorate the rest of their house .']
04/25/2022 01:32:52 - INFO - __main__ -  [cosmos_qa] What is the woman to do to try to tell these two apart ? [SEP] They really were stunningly similar , with a spattering of freckles and a puff of pale hair on their heads . If it was n't for the cut and the expressions on their faces , she would n't have been able to tell them apart . She stood back up . [SEP]  (A) There is no way to distinguish these two individuals . (B) These two individuals are exactly the same in every way . (C) They will be distinguished by the differences in their freckles . (D) She will look to distinguish them by the difference in facial expressions .
04/25/2022 01:32:52 - INFO - __main__ - ['She will look to distinguish them by the difference in facial expressions .']
04/25/2022 01:32:52 - INFO - __main__ -  [cosmos_qa] Why has the narrator decided to stop drinking soda ? [SEP] I do n't want to be ultra thin , I still got ta keep the booty ! Haha . I 've already cut soda out of my diet completely , unless it 's diet soda . So if you guys can give me any pointers or any help at all . That would be great ! [SEP]  (A) None of the above choices . (B) They are not a fan of the new taste . (C) They are allergic . (D) They are dieting .
04/25/2022 01:32:52 - INFO - __main__ - ['They are dieting .']
04/25/2022 01:32:52 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:32:52 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:32:52 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 01:32:54 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.15625 on epoch=999
04/25/2022 01:32:54 - INFO - __main__ - save last model!
04/25/2022 01:32:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 01:32:54 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 01:32:54 - INFO - __main__ - Printing 3 examples
04/25/2022 01:32:54 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 01:32:54 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 01:32:54 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 01:32:54 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 01:32:54 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 01:32:54 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 01:32:54 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:32:55 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:32:56 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 01:33:10 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 01:33:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 01:33:11 - INFO - __main__ - Starting training!
04/25/2022 01:35:08 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_42_0.4_8_predictions.txt
04/25/2022 01:35:08 - INFO - __main__ - ACC on test data: 0.2970
04/25/2022 01:35:08 - INFO - __main__ - prefix=cosmos_qa_32_42, lr=0.4, bsz=8, dev_performance=0.46875, test_performance=0.297
04/25/2022 01:35:08 - INFO - __main__ - Running ... prefix=cosmos_qa_32_42, lr=0.3, bsz=8 ...
04/25/2022 01:35:09 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:35:09 - INFO - __main__ - Printing 3 examples
04/25/2022 01:35:09 - INFO - __main__ -  [cosmos_qa] What happened before going to the church ? [SEP] He even extended his hand to the pastor after the service , it was so funny . My daughter was baptized in this church 29 years too , so being able to see him baptized here as well made the day even more special . We ended the day with a delicious brunch after the service with a few family members and friends . The weather was beautiful . [SEP]  (A) None of the above choices . (B) Everyone enjoyed the weather (C) They drove in the car to get to the church (D) Everyone went to the park
04/25/2022 01:35:09 - INFO - __main__ - ['They drove in the car to get to the church']
04/25/2022 01:35:09 - INFO - __main__ -  [cosmos_qa] Why are working class women activists rendered invisible and anonymous by history ? [SEP] harpymarx said : I saw Her Naked Skin last weekend ( and wrote my own review on my blog ) and thought it was very good but I have to say that I did n't think Eve was as a well developed character like Celia . We saw interactions between Celia and husband but what about Eve .... In some ways I saw Eve as a parallel with the whole of the Suffragette movement where the privileged Pankhursts ' along with other privileged women are remembered and at the forefront of the history but what about many of the working class women activists who have been rendered invisible and anonymous by history ? Very little trace is left . [SEP]  (A) Working class women activists are rendered invisible and anonymous by history because they are unimportant in the suffragette movement . (B) Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society . (C) Working class women activists are rendered invisible and anonymous by history because historians deem them to have minor impact on history . (D) Working class women activists are rendered invisible and anonymous by history because the wealthy elite try to downplay their role in suffrage .
04/25/2022 01:35:09 - INFO - __main__ - ['Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society .']
04/25/2022 01:35:09 - INFO - __main__ -  [cosmos_qa] What may be your reason for going to the doctors on Monday ? [SEP] The jar went up my arms into my back and neck and left me with a headache all evening and I am now sporting very sore and bruised arms and both knees are sore . So I am going to take things easy for a while . Another thing that is concerning me is MLA has developed a twitch of shaking his head , he is even doing it in his sleep . So I am taking him to the doctors on Monday as he is getting so tired because of it . [SEP]  (A) I have a headache that wo n't go away . (B) Someone I know is starting to have an odd spasm . (C) I 'm starting to have an odd spasm . (D) My arms are bruised and I need to go to the doctors .
04/25/2022 01:35:09 - INFO - __main__ - ['Someone I know is starting to have an odd spasm .']
04/25/2022 01:35:09 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:35:09 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:35:09 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 01:35:09 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:35:09 - INFO - __main__ - Printing 3 examples
04/25/2022 01:35:09 - INFO - __main__ -  [cosmos_qa] What may happen after they put up the clock ? [SEP] The only thing on the wall yet is our Auburn clock that Hannah gave us . I was worried that it would get messed up , so as soon as I found my drill and a screw , it went up . Hopefully tomorrow while Josh is at work I can get some more stuff up . I have to start on the guest room too . [SEP]  (A) They will get more clocks for their house . (B) They will continue remodeling their house . (C) They will decorate the rest of their house . (D) They will get more wall decorations .
04/25/2022 01:35:09 - INFO - __main__ - ['They will decorate the rest of their house .']
04/25/2022 01:35:09 - INFO - __main__ -  [cosmos_qa] What is the woman to do to try to tell these two apart ? [SEP] They really were stunningly similar , with a spattering of freckles and a puff of pale hair on their heads . If it was n't for the cut and the expressions on their faces , she would n't have been able to tell them apart . She stood back up . [SEP]  (A) There is no way to distinguish these two individuals . (B) These two individuals are exactly the same in every way . (C) They will be distinguished by the differences in their freckles . (D) She will look to distinguish them by the difference in facial expressions .
04/25/2022 01:35:09 - INFO - __main__ - ['She will look to distinguish them by the difference in facial expressions .']
04/25/2022 01:35:09 - INFO - __main__ -  [cosmos_qa] Why has the narrator decided to stop drinking soda ? [SEP] I do n't want to be ultra thin , I still got ta keep the booty ! Haha . I 've already cut soda out of my diet completely , unless it 's diet soda . So if you guys can give me any pointers or any help at all . That would be great ! [SEP]  (A) None of the above choices . (B) They are not a fan of the new taste . (C) They are allergic . (D) They are dieting .
04/25/2022 01:35:09 - INFO - __main__ - ['They are dieting .']
04/25/2022 01:35:09 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:35:09 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:35:09 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 01:35:24 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 01:35:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 01:35:25 - INFO - __main__ - Starting training!
04/25/2022 01:35:29 - INFO - __main__ - Step 10 Global step 10 Train loss 0.98 on epoch=4
04/25/2022 01:35:32 - INFO - __main__ - Step 20 Global step 20 Train loss 0.64 on epoch=9
04/25/2022 01:35:35 - INFO - __main__ - Step 30 Global step 30 Train loss 0.42 on epoch=14
04/25/2022 01:35:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.36 on epoch=19
04/25/2022 01:35:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.31 on epoch=24
04/25/2022 01:35:45 - INFO - __main__ - Global step 50 Train loss 0.54 ACC 0.3125 on epoch=24
04/25/2022 01:35:45 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.3125 on epoch=24, global_step=50
04/25/2022 01:35:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.27 on epoch=29
04/25/2022 01:35:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.26 on epoch=34
04/25/2022 01:35:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.24 on epoch=39
04/25/2022 01:35:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.22 on epoch=44
04/25/2022 01:36:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.21 on epoch=49
04/25/2022 01:36:03 - INFO - __main__ - Global step 100 Train loss 0.24 ACC 0.21875 on epoch=49
04/25/2022 01:36:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.21 on epoch=54
04/25/2022 01:36:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.21 on epoch=59
04/25/2022 01:36:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.19 on epoch=64
04/25/2022 01:36:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.18 on epoch=69
04/25/2022 01:36:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.18 on epoch=74
04/25/2022 01:36:22 - INFO - __main__ - Global step 150 Train loss 0.19 ACC 0.21875 on epoch=74
04/25/2022 01:36:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.17 on epoch=79
04/25/2022 01:36:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.15 on epoch=84
04/25/2022 01:36:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.13 on epoch=89
04/25/2022 01:36:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.13 on epoch=94
04/25/2022 01:36:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.13 on epoch=99
04/25/2022 01:36:40 - INFO - __main__ - Global step 200 Train loss 0.14 ACC 0.1875 on epoch=99
04/25/2022 01:36:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.11 on epoch=104
04/25/2022 01:36:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.11 on epoch=109
04/25/2022 01:36:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.12 on epoch=114
04/25/2022 01:36:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.12 on epoch=119
04/25/2022 01:36:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.11 on epoch=124
04/25/2022 01:36:59 - INFO - __main__ - Global step 250 Train loss 0.11 ACC 0.25 on epoch=124
04/25/2022 01:37:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.13 on epoch=129
04/25/2022 01:37:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.10 on epoch=134
04/25/2022 01:37:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.10 on epoch=139
04/25/2022 01:37:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.12 on epoch=144
04/25/2022 01:37:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.11 on epoch=149
04/25/2022 01:37:18 - INFO - __main__ - Global step 300 Train loss 0.11 ACC 0.125 on epoch=149
04/25/2022 01:37:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.05 on epoch=154
04/25/2022 01:37:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.07 on epoch=159
04/25/2022 01:37:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.07 on epoch=164
04/25/2022 01:37:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.08 on epoch=169
04/25/2022 01:37:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.07 on epoch=174
04/25/2022 01:37:36 - INFO - __main__ - Global step 350 Train loss 0.07 ACC 0.25 on epoch=174
04/25/2022 01:37:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.10 on epoch=179
04/25/2022 01:37:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.06 on epoch=184
04/25/2022 01:37:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.08 on epoch=189
04/25/2022 01:37:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.05 on epoch=194
04/25/2022 01:37:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.05 on epoch=199
04/25/2022 01:37:54 - INFO - __main__ - Global step 400 Train loss 0.07 ACC 0.3125 on epoch=199
04/25/2022 01:37:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.06 on epoch=204
04/25/2022 01:38:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
04/25/2022 01:38:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.05 on epoch=214
04/25/2022 01:38:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.05 on epoch=219
04/25/2022 01:38:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.04 on epoch=224
04/25/2022 01:38:13 - INFO - __main__ - Global step 450 Train loss 0.05 ACC 0.25 on epoch=224
04/25/2022 01:38:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
04/25/2022 01:38:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
04/25/2022 01:38:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.04 on epoch=239
04/25/2022 01:38:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.04 on epoch=244
04/25/2022 01:38:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
04/25/2022 01:38:31 - INFO - __main__ - Global step 500 Train loss 0.05 ACC 0.25 on epoch=249
04/25/2022 01:38:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
04/25/2022 01:38:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.03 on epoch=259
04/25/2022 01:38:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
04/25/2022 01:38:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
04/25/2022 01:38:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
04/25/2022 01:38:49 - INFO - __main__ - Global step 550 Train loss 0.04 ACC 0.3125 on epoch=274
04/25/2022 01:38:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
04/25/2022 01:38:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
04/25/2022 01:38:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
04/25/2022 01:39:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
04/25/2022 01:39:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
04/25/2022 01:39:07 - INFO - __main__ - Global step 600 Train loss 0.04 ACC 0.21875 on epoch=299
04/25/2022 01:39:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
04/25/2022 01:39:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
04/25/2022 01:39:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
04/25/2022 01:39:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
04/25/2022 01:39:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
04/25/2022 01:39:26 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.1875 on epoch=324
04/25/2022 01:39:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
04/25/2022 01:39:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
04/25/2022 01:39:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
04/25/2022 01:39:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
04/25/2022 01:39:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
04/25/2022 01:39:44 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.25 on epoch=349
04/25/2022 01:39:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
04/25/2022 01:39:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=359
04/25/2022 01:39:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
04/25/2022 01:39:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
04/25/2022 01:39:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
04/25/2022 01:40:02 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.125 on epoch=374
04/25/2022 01:40:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=379
04/25/2022 01:40:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
04/25/2022 01:40:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
04/25/2022 01:40:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
04/25/2022 01:40:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
04/25/2022 01:40:20 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.1875 on epoch=399
04/25/2022 01:40:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
04/25/2022 01:40:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
04/25/2022 01:40:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
04/25/2022 01:40:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
04/25/2022 01:40:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
04/25/2022 01:40:39 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.21875 on epoch=424
04/25/2022 01:40:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
04/25/2022 01:40:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
04/25/2022 01:40:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
04/25/2022 01:40:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
04/25/2022 01:40:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
04/25/2022 01:40:57 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.1875 on epoch=449
04/25/2022 01:41:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
04/25/2022 01:41:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
04/25/2022 01:41:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
04/25/2022 01:41:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
04/25/2022 01:41:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
04/25/2022 01:41:15 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.3125 on epoch=474
04/25/2022 01:41:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
04/25/2022 01:41:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
04/25/2022 01:41:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
04/25/2022 01:41:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
04/25/2022 01:41:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
04/25/2022 01:41:34 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.03125 on epoch=499
04/25/2022 01:41:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
04/25/2022 01:41:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
04/25/2022 01:41:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
04/25/2022 01:41:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
04/25/2022 01:41:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
04/25/2022 01:41:52 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.28125 on epoch=524
04/25/2022 01:41:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
04/25/2022 01:41:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
04/25/2022 01:42:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/25/2022 01:42:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
04/25/2022 01:42:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
04/25/2022 01:42:10 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.15625 on epoch=549
04/25/2022 01:42:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/25/2022 01:42:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/25/2022 01:42:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
04/25/2022 01:42:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
04/25/2022 01:42:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 01:42:28 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.125 on epoch=574
04/25/2022 01:42:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
04/25/2022 01:42:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
04/25/2022 01:42:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
04/25/2022 01:42:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 01:42:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/25/2022 01:42:46 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.125 on epoch=599
04/25/2022 01:42:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
04/25/2022 01:42:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
04/25/2022 01:42:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
04/25/2022 01:42:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
04/25/2022 01:43:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/25/2022 01:43:04 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.25 on epoch=624
04/25/2022 01:43:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 01:43:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 01:43:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
04/25/2022 01:43:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/25/2022 01:43:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/25/2022 01:43:22 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.34375 on epoch=649
04/25/2022 01:43:22 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.34375 on epoch=649, global_step=1300
04/25/2022 01:43:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 01:43:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 01:43:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
04/25/2022 01:43:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/25/2022 01:43:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
04/25/2022 01:43:40 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.28125 on epoch=674
04/25/2022 01:43:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 01:43:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
04/25/2022 01:43:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/25/2022 01:43:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 01:43:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 01:43:59 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.21875 on epoch=699
04/25/2022 01:44:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 01:44:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 01:44:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/25/2022 01:44:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/25/2022 01:44:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/25/2022 01:44:17 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.15625 on epoch=724
04/25/2022 01:44:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/25/2022 01:44:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/25/2022 01:44:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 01:44:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 01:44:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 01:44:35 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.21875 on epoch=749
04/25/2022 01:44:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
04/25/2022 01:44:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
04/25/2022 01:44:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 01:44:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 01:44:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
04/25/2022 01:44:53 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.3125 on epoch=774
04/25/2022 01:44:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 01:44:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 01:45:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 01:45:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 01:45:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/25/2022 01:45:12 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.21875 on epoch=799
04/25/2022 01:45:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 01:45:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/25/2022 01:45:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 01:45:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 01:45:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
04/25/2022 01:45:30 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.25 on epoch=824
04/25/2022 01:45:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/25/2022 01:45:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
04/25/2022 01:45:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/25/2022 01:45:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 01:45:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
04/25/2022 01:45:48 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.25 on epoch=849
04/25/2022 01:45:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 01:45:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 01:45:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
04/25/2022 01:46:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
04/25/2022 01:46:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 01:46:06 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.25 on epoch=874
04/25/2022 01:46:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
04/25/2022 01:46:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
04/25/2022 01:46:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 01:46:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
04/25/2022 01:46:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
04/25/2022 01:46:24 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.25 on epoch=899
04/25/2022 01:46:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 01:46:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
04/25/2022 01:46:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
04/25/2022 01:46:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 01:46:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
04/25/2022 01:46:42 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.3125 on epoch=924
04/25/2022 01:46:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
04/25/2022 01:46:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 01:46:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 01:46:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
04/25/2022 01:46:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 01:47:00 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.28125 on epoch=949
04/25/2022 01:47:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 01:47:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 01:47:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 01:47:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 01:47:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
04/25/2022 01:47:19 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.25 on epoch=974
04/25/2022 01:47:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
04/25/2022 01:47:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
04/25/2022 01:47:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
04/25/2022 01:47:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 01:47:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 01:47:35 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:47:35 - INFO - __main__ - Printing 3 examples
04/25/2022 01:47:35 - INFO - __main__ -  [cosmos_qa] What happened before going to the church ? [SEP] He even extended his hand to the pastor after the service , it was so funny . My daughter was baptized in this church 29 years too , so being able to see him baptized here as well made the day even more special . We ended the day with a delicious brunch after the service with a few family members and friends . The weather was beautiful . [SEP]  (A) None of the above choices . (B) Everyone enjoyed the weather (C) They drove in the car to get to the church (D) Everyone went to the park
04/25/2022 01:47:35 - INFO - __main__ - ['They drove in the car to get to the church']
04/25/2022 01:47:35 - INFO - __main__ -  [cosmos_qa] Why are working class women activists rendered invisible and anonymous by history ? [SEP] harpymarx said : I saw Her Naked Skin last weekend ( and wrote my own review on my blog ) and thought it was very good but I have to say that I did n't think Eve was as a well developed character like Celia . We saw interactions between Celia and husband but what about Eve .... In some ways I saw Eve as a parallel with the whole of the Suffragette movement where the privileged Pankhursts ' along with other privileged women are remembered and at the forefront of the history but what about many of the working class women activists who have been rendered invisible and anonymous by history ? Very little trace is left . [SEP]  (A) Working class women activists are rendered invisible and anonymous by history because they are unimportant in the suffragette movement . (B) Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society . (C) Working class women activists are rendered invisible and anonymous by history because historians deem them to have minor impact on history . (D) Working class women activists are rendered invisible and anonymous by history because the wealthy elite try to downplay their role in suffrage .
04/25/2022 01:47:35 - INFO - __main__ - ['Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society .']
04/25/2022 01:47:35 - INFO - __main__ -  [cosmos_qa] What may be your reason for going to the doctors on Monday ? [SEP] The jar went up my arms into my back and neck and left me with a headache all evening and I am now sporting very sore and bruised arms and both knees are sore . So I am going to take things easy for a while . Another thing that is concerning me is MLA has developed a twitch of shaking his head , he is even doing it in his sleep . So I am taking him to the doctors on Monday as he is getting so tired because of it . [SEP]  (A) I have a headache that wo n't go away . (B) Someone I know is starting to have an odd spasm . (C) I 'm starting to have an odd spasm . (D) My arms are bruised and I need to go to the doctors .
04/25/2022 01:47:35 - INFO - __main__ - ['Someone I know is starting to have an odd spasm .']
04/25/2022 01:47:35 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:47:35 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:47:35 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 01:47:35 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:47:35 - INFO - __main__ - Printing 3 examples
04/25/2022 01:47:35 - INFO - __main__ -  [cosmos_qa] What may happen after they put up the clock ? [SEP] The only thing on the wall yet is our Auburn clock that Hannah gave us . I was worried that it would get messed up , so as soon as I found my drill and a screw , it went up . Hopefully tomorrow while Josh is at work I can get some more stuff up . I have to start on the guest room too . [SEP]  (A) They will get more clocks for their house . (B) They will continue remodeling their house . (C) They will decorate the rest of their house . (D) They will get more wall decorations .
04/25/2022 01:47:35 - INFO - __main__ - ['They will decorate the rest of their house .']
04/25/2022 01:47:35 - INFO - __main__ -  [cosmos_qa] What is the woman to do to try to tell these two apart ? [SEP] They really were stunningly similar , with a spattering of freckles and a puff of pale hair on their heads . If it was n't for the cut and the expressions on their faces , she would n't have been able to tell them apart . She stood back up . [SEP]  (A) There is no way to distinguish these two individuals . (B) These two individuals are exactly the same in every way . (C) They will be distinguished by the differences in their freckles . (D) She will look to distinguish them by the difference in facial expressions .
04/25/2022 01:47:35 - INFO - __main__ - ['She will look to distinguish them by the difference in facial expressions .']
04/25/2022 01:47:35 - INFO - __main__ -  [cosmos_qa] Why has the narrator decided to stop drinking soda ? [SEP] I do n't want to be ultra thin , I still got ta keep the booty ! Haha . I 've already cut soda out of my diet completely , unless it 's diet soda . So if you guys can give me any pointers or any help at all . That would be great ! [SEP]  (A) None of the above choices . (B) They are not a fan of the new taste . (C) They are allergic . (D) They are dieting .
04/25/2022 01:47:35 - INFO - __main__ - ['They are dieting .']
04/25/2022 01:47:35 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:47:35 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:47:35 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 01:47:37 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.28125 on epoch=999
04/25/2022 01:47:37 - INFO - __main__ - save last model!
04/25/2022 01:47:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 01:47:37 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 01:47:37 - INFO - __main__ - Printing 3 examples
04/25/2022 01:47:37 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 01:47:37 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 01:47:37 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 01:47:37 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 01:47:37 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 01:47:37 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 01:47:37 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:47:38 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:47:39 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 01:47:51 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 01:47:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 01:47:52 - INFO - __main__ - Starting training!
04/25/2022 01:49:44 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_42_0.3_8_predictions.txt
04/25/2022 01:49:44 - INFO - __main__ - ACC on test data: 0.2650
04/25/2022 01:49:44 - INFO - __main__ - prefix=cosmos_qa_32_42, lr=0.3, bsz=8, dev_performance=0.34375, test_performance=0.265
04/25/2022 01:49:44 - INFO - __main__ - Running ... prefix=cosmos_qa_32_42, lr=0.2, bsz=8 ...
04/25/2022 01:49:45 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:49:45 - INFO - __main__ - Printing 3 examples
04/25/2022 01:49:45 - INFO - __main__ -  [cosmos_qa] What happened before going to the church ? [SEP] He even extended his hand to the pastor after the service , it was so funny . My daughter was baptized in this church 29 years too , so being able to see him baptized here as well made the day even more special . We ended the day with a delicious brunch after the service with a few family members and friends . The weather was beautiful . [SEP]  (A) None of the above choices . (B) Everyone enjoyed the weather (C) They drove in the car to get to the church (D) Everyone went to the park
04/25/2022 01:49:45 - INFO - __main__ - ['They drove in the car to get to the church']
04/25/2022 01:49:45 - INFO - __main__ -  [cosmos_qa] Why are working class women activists rendered invisible and anonymous by history ? [SEP] harpymarx said : I saw Her Naked Skin last weekend ( and wrote my own review on my blog ) and thought it was very good but I have to say that I did n't think Eve was as a well developed character like Celia . We saw interactions between Celia and husband but what about Eve .... In some ways I saw Eve as a parallel with the whole of the Suffragette movement where the privileged Pankhursts ' along with other privileged women are remembered and at the forefront of the history but what about many of the working class women activists who have been rendered invisible and anonymous by history ? Very little trace is left . [SEP]  (A) Working class women activists are rendered invisible and anonymous by history because they are unimportant in the suffragette movement . (B) Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society . (C) Working class women activists are rendered invisible and anonymous by history because historians deem them to have minor impact on history . (D) Working class women activists are rendered invisible and anonymous by history because the wealthy elite try to downplay their role in suffrage .
04/25/2022 01:49:45 - INFO - __main__ - ['Working class women activists are rendered invisible and anonymous by history because they are not wealthy or distinguished enough by society .']
04/25/2022 01:49:45 - INFO - __main__ -  [cosmos_qa] What may be your reason for going to the doctors on Monday ? [SEP] The jar went up my arms into my back and neck and left me with a headache all evening and I am now sporting very sore and bruised arms and both knees are sore . So I am going to take things easy for a while . Another thing that is concerning me is MLA has developed a twitch of shaking his head , he is even doing it in his sleep . So I am taking him to the doctors on Monday as he is getting so tired because of it . [SEP]  (A) I have a headache that wo n't go away . (B) Someone I know is starting to have an odd spasm . (C) I 'm starting to have an odd spasm . (D) My arms are bruised and I need to go to the doctors .
04/25/2022 01:49:45 - INFO - __main__ - ['Someone I know is starting to have an odd spasm .']
04/25/2022 01:49:45 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:49:46 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:49:46 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 01:49:46 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 01:49:46 - INFO - __main__ - Printing 3 examples
04/25/2022 01:49:46 - INFO - __main__ -  [cosmos_qa] What may happen after they put up the clock ? [SEP] The only thing on the wall yet is our Auburn clock that Hannah gave us . I was worried that it would get messed up , so as soon as I found my drill and a screw , it went up . Hopefully tomorrow while Josh is at work I can get some more stuff up . I have to start on the guest room too . [SEP]  (A) They will get more clocks for their house . (B) They will continue remodeling their house . (C) They will decorate the rest of their house . (D) They will get more wall decorations .
04/25/2022 01:49:46 - INFO - __main__ - ['They will decorate the rest of their house .']
04/25/2022 01:49:46 - INFO - __main__ -  [cosmos_qa] What is the woman to do to try to tell these two apart ? [SEP] They really were stunningly similar , with a spattering of freckles and a puff of pale hair on their heads . If it was n't for the cut and the expressions on their faces , she would n't have been able to tell them apart . She stood back up . [SEP]  (A) There is no way to distinguish these two individuals . (B) These two individuals are exactly the same in every way . (C) They will be distinguished by the differences in their freckles . (D) She will look to distinguish them by the difference in facial expressions .
04/25/2022 01:49:46 - INFO - __main__ - ['She will look to distinguish them by the difference in facial expressions .']
04/25/2022 01:49:46 - INFO - __main__ -  [cosmos_qa] Why has the narrator decided to stop drinking soda ? [SEP] I do n't want to be ultra thin , I still got ta keep the booty ! Haha . I 've already cut soda out of my diet completely , unless it 's diet soda . So if you guys can give me any pointers or any help at all . That would be great ! [SEP]  (A) None of the above choices . (B) They are not a fan of the new taste . (C) They are allergic . (D) They are dieting .
04/25/2022 01:49:46 - INFO - __main__ - ['They are dieting .']
04/25/2022 01:49:46 - INFO - __main__ - Tokenizing Input ...
04/25/2022 01:49:46 - INFO - __main__ - Tokenizing Output ...
04/25/2022 01:49:46 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 01:50:04 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 01:50:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 01:50:05 - INFO - __main__ - Starting training!
04/25/2022 01:50:09 - INFO - __main__ - Step 10 Global step 10 Train loss 1.03 on epoch=4
04/25/2022 01:50:12 - INFO - __main__ - Step 20 Global step 20 Train loss 0.72 on epoch=9
04/25/2022 01:50:15 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=14
04/25/2022 01:50:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=19
04/25/2022 01:50:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.36 on epoch=24
04/25/2022 01:50:28 - INFO - __main__ - Global step 50 Train loss 0.62 ACC 0.3125 on epoch=24
04/25/2022 01:50:28 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.3125 on epoch=24, global_step=50
04/25/2022 01:50:31 - INFO - __main__ - Step 60 Global step 60 Train loss 0.30 on epoch=29
04/25/2022 01:50:34 - INFO - __main__ - Step 70 Global step 70 Train loss 0.34 on epoch=34
04/25/2022 01:50:37 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=39
04/25/2022 01:50:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.28 on epoch=44
04/25/2022 01:50:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.25 on epoch=49
04/25/2022 01:50:46 - INFO - __main__ - Global step 100 Train loss 0.30 ACC 0.125 on epoch=49
04/25/2022 01:50:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.24 on epoch=54
04/25/2022 01:50:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.25 on epoch=59
04/25/2022 01:50:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.26 on epoch=64
04/25/2022 01:50:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.24 on epoch=69
04/25/2022 01:51:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.21 on epoch=74
04/25/2022 01:51:04 - INFO - __main__ - Global step 150 Train loss 0.24 ACC 0.25 on epoch=74
04/25/2022 01:51:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.22 on epoch=79
04/25/2022 01:51:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.20 on epoch=84
04/25/2022 01:51:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.20 on epoch=89
04/25/2022 01:51:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.17 on epoch=94
04/25/2022 01:51:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.18 on epoch=99
04/25/2022 01:51:23 - INFO - __main__ - Global step 200 Train loss 0.19 ACC 0.40625 on epoch=99
04/25/2022 01:51:23 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.40625 on epoch=99, global_step=200
04/25/2022 01:51:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.18 on epoch=104
04/25/2022 01:51:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.15 on epoch=109
04/25/2022 01:51:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.13 on epoch=114
04/25/2022 01:51:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.13 on epoch=119
04/25/2022 01:51:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.16 on epoch=124
04/25/2022 01:51:41 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.34375 on epoch=124
04/25/2022 01:51:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.14 on epoch=129
04/25/2022 01:51:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.14 on epoch=134
04/25/2022 01:51:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.15 on epoch=139
04/25/2022 01:51:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.11 on epoch=144
04/25/2022 01:51:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.14 on epoch=149
04/25/2022 01:52:00 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.34375 on epoch=149
04/25/2022 01:52:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
04/25/2022 01:52:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.13 on epoch=159
04/25/2022 01:52:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
04/25/2022 01:52:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.12 on epoch=169
04/25/2022 01:52:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
04/25/2022 01:52:18 - INFO - __main__ - Global step 350 Train loss 0.12 ACC 0.3125 on epoch=174
04/25/2022 01:52:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.08 on epoch=179
04/25/2022 01:52:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.09 on epoch=184
04/25/2022 01:52:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.09 on epoch=189
04/25/2022 01:52:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.11 on epoch=194
04/25/2022 01:52:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.09 on epoch=199
04/25/2022 01:52:37 - INFO - __main__ - Global step 400 Train loss 0.09 ACC 0.28125 on epoch=199
04/25/2022 01:52:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
04/25/2022 01:52:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.08 on epoch=209
04/25/2022 01:52:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.09 on epoch=214
04/25/2022 01:52:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.08 on epoch=219
04/25/2022 01:52:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
04/25/2022 01:52:56 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.21875 on epoch=224
04/25/2022 01:52:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.08 on epoch=229
04/25/2022 01:53:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
04/25/2022 01:53:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
04/25/2022 01:53:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
04/25/2022 01:53:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
04/25/2022 01:53:14 - INFO - __main__ - Global step 500 Train loss 0.07 ACC 0.28125 on epoch=249
04/25/2022 01:53:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
04/25/2022 01:53:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=259
04/25/2022 01:53:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
04/25/2022 01:53:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.07 on epoch=269
04/25/2022 01:53:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.05 on epoch=274
04/25/2022 01:53:33 - INFO - __main__ - Global step 550 Train loss 0.06 ACC 0.25 on epoch=274
04/25/2022 01:53:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
04/25/2022 01:53:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
04/25/2022 01:53:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=289
04/25/2022 01:53:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
04/25/2022 01:53:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
04/25/2022 01:53:52 - INFO - __main__ - Global step 600 Train loss 0.05 ACC 0.21875 on epoch=299
04/25/2022 01:53:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
04/25/2022 01:53:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=309
04/25/2022 01:54:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
04/25/2022 01:54:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
04/25/2022 01:54:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
04/25/2022 01:54:10 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.21875 on epoch=324
04/25/2022 01:54:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
04/25/2022 01:54:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=334
04/25/2022 01:54:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
04/25/2022 01:54:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
04/25/2022 01:54:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
04/25/2022 01:54:29 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.28125 on epoch=349
04/25/2022 01:54:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
04/25/2022 01:54:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
04/25/2022 01:54:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
04/25/2022 01:54:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
04/25/2022 01:54:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
04/25/2022 01:54:48 - INFO - __main__ - Global step 750 Train loss 0.04 ACC 0.21875 on epoch=374
04/25/2022 01:54:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=379
04/25/2022 01:54:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
04/25/2022 01:54:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
04/25/2022 01:55:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
04/25/2022 01:55:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
04/25/2022 01:55:07 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.21875 on epoch=399
04/25/2022 01:55:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
04/25/2022 01:55:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
04/25/2022 01:55:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
04/25/2022 01:55:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
04/25/2022 01:55:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
04/25/2022 01:55:25 - INFO - __main__ - Global step 850 Train loss 0.04 ACC 0.21875 on epoch=424
04/25/2022 01:55:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
04/25/2022 01:55:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
04/25/2022 01:55:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
04/25/2022 01:55:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
04/25/2022 01:55:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
04/25/2022 01:55:44 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.21875 on epoch=449
04/25/2022 01:55:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
04/25/2022 01:55:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
04/25/2022 01:55:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
04/25/2022 01:55:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
04/25/2022 01:55:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=474
04/25/2022 01:56:03 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 0.28125 on epoch=474
04/25/2022 01:56:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=479
04/25/2022 01:56:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
04/25/2022 01:56:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
04/25/2022 01:56:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
04/25/2022 01:56:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
04/25/2022 01:56:22 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.21875 on epoch=499
04/25/2022 01:56:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
04/25/2022 01:56:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
04/25/2022 01:56:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
04/25/2022 01:56:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
04/25/2022 01:56:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
04/25/2022 01:56:40 - INFO - __main__ - Global step 1050 Train loss 0.03 ACC 0.25 on epoch=524
04/25/2022 01:56:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
04/25/2022 01:56:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
04/25/2022 01:56:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/25/2022 01:56:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
04/25/2022 01:56:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/25/2022 01:56:59 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.28125 on epoch=549
04/25/2022 01:57:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
04/25/2022 01:57:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
04/25/2022 01:57:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/25/2022 01:57:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
04/25/2022 01:57:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 01:57:18 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.25 on epoch=574
04/25/2022 01:57:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/25/2022 01:57:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
04/25/2022 01:57:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/25/2022 01:57:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 01:57:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
04/25/2022 01:57:37 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.25 on epoch=599
04/25/2022 01:57:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/25/2022 01:57:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 01:57:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
04/25/2022 01:57:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
04/25/2022 01:57:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/25/2022 01:57:56 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.21875 on epoch=624
04/25/2022 01:57:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
04/25/2022 01:58:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
04/25/2022 01:58:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/25/2022 01:58:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
04/25/2022 01:58:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/25/2022 01:58:15 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.1875 on epoch=649
04/25/2022 01:58:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 01:58:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 01:58:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/25/2022 01:58:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/25/2022 01:58:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 01:58:34 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.28125 on epoch=674
04/25/2022 01:58:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/25/2022 01:58:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/25/2022 01:58:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/25/2022 01:58:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 01:58:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 01:58:53 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.25 on epoch=699
04/25/2022 01:58:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 01:58:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 01:59:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 01:59:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/25/2022 01:59:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/25/2022 01:59:11 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.25 on epoch=724
04/25/2022 01:59:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 01:59:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/25/2022 01:59:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
04/25/2022 01:59:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 01:59:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 01:59:30 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.21875 on epoch=749
04/25/2022 01:59:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
04/25/2022 01:59:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
04/25/2022 01:59:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
04/25/2022 01:59:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 01:59:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/25/2022 01:59:48 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.25 on epoch=774
04/25/2022 01:59:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/25/2022 01:59:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 01:59:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 02:00:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 02:00:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
04/25/2022 02:00:07 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.25 on epoch=799
04/25/2022 02:00:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/25/2022 02:00:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
04/25/2022 02:00:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 02:00:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 02:00:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 02:00:26 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.1875 on epoch=824
04/25/2022 02:00:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/25/2022 02:00:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/25/2022 02:00:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 02:00:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
04/25/2022 02:00:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 02:00:45 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.21875 on epoch=849
04/25/2022 02:00:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 02:00:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 02:00:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
04/25/2022 02:00:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
04/25/2022 02:01:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 02:01:03 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.28125 on epoch=874
04/25/2022 02:01:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/25/2022 02:01:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
04/25/2022 02:01:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 02:01:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
04/25/2022 02:01:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 02:01:22 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.21875 on epoch=899
04/25/2022 02:01:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 02:01:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/25/2022 02:01:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 02:01:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 02:01:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 02:01:41 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.21875 on epoch=924
04/25/2022 02:01:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 02:01:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
04/25/2022 02:01:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 02:01:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 02:01:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 02:02:00 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.1875 on epoch=949
04/25/2022 02:02:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 02:02:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 02:02:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
04/25/2022 02:02:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 02:02:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 02:02:18 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.25 on epoch=974
04/25/2022 02:02:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
04/25/2022 02:02:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
04/25/2022 02:02:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
04/25/2022 02:02:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 02:02:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 02:02:35 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:02:35 - INFO - __main__ - Printing 3 examples
04/25/2022 02:02:35 - INFO - __main__ -  [cosmos_qa] Why did they buy those products in specific ? [SEP] This morning I went to Wal - Mart to buy deodorant and soap . Funny that I should simultaneously run out of those things . While I was there , " Who Let the Dogs Out " was playing . It was playing when I came in , and I swear , it went on for like 6 minutes . [SEP]  (A) They wanted to spend as little money as possible . (B) They did n't like the products they already had . (C) They need them to keep themselves clean . (D) None of the above choices .
04/25/2022 02:02:35 - INFO - __main__ - ['They need them to keep themselves clean .']
04/25/2022 02:02:35 - INFO - __main__ -  [cosmos_qa] Why might the writer be agitated ? [SEP] I ' ve been awake for an hour and i ' ve already smoked 3 cigarettes . my computer is going to die soon , and by die i mean fucking die forever and it wo nt ever come back . i m scared . [SEP]  (A) The writer may feel guilty for smoking . (B) The writer might need more sleep . (C) The writer might be lonely . (D) None of the above choices .
04/25/2022 02:02:35 - INFO - __main__ - ['None of the above choices .']
04/25/2022 02:02:35 - INFO - __main__ -  [cosmos_qa] Why did the narrator feel they needed to go to their room ? [SEP] It was really hard to breath and I thought I was going to faint in the hallway in my towel . I got back to my room and took a zantac , and I was a little better . In bed though , I felt faint again but not nearly as severely , and my hands were shaking . [SEP]  (A) They were wanting a nap . (B) None of the above choices . (C) They needed sleep . (D) They needed to take medicine .
04/25/2022 02:02:35 - INFO - __main__ - ['They needed to take medicine .']
04/25/2022 02:02:35 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:02:35 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:02:35 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 02:02:35 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:02:35 - INFO - __main__ - Printing 3 examples
04/25/2022 02:02:35 - INFO - __main__ -  [cosmos_qa] What may be the reason for their situation ? [SEP] Well , after reading around , I came to the conclusion that it 's merely media influence . In almost everything I ' ve seen , the light represents good and the darkness is evil . Of course , I was pretty much guilty in putting that in most of my works . [SEP]  (A) They want to influence the public on purpose . (B) They have to publish new content quickly . (C) It is their job to influence the public . (D) None of the above choices .
04/25/2022 02:02:35 - INFO - __main__ - ['It is their job to influence the public .']
04/25/2022 02:02:35 - INFO - __main__ -  [cosmos_qa] What happened to the speakers mother ? [SEP] I do n't really have too much to write about today . Worked at white spot for six hours . It was pretty busy but nothing I could nt handle . Brent was a little chocked about me not showing up on monday but I could n't not go spread my mommas ashes and he did n't give me too much fuss because he knows usually i m an excellent worker . [SEP]  (A) The speakers mother has passed on . (B) The speakers mother is choked up . (C) None of the above choices . (D) The speakers mother wrote about today .
04/25/2022 02:02:35 - INFO - __main__ - ['The speakers mother has passed on .']
04/25/2022 02:02:35 - INFO - __main__ -  [cosmos_qa] What did the kids enjoy at the EIS cafe ? [SEP] Will had fun riding his little bike as well . After about an hour of bike riding , the kids were treated to an ice cream cone from " Homie " . He drives around post selling the best ice cream cones EVER from EIS Cafe , the ice cream cafe in town . They each got a strawberry cone and enjoyed them immensely . [SEP]  (A) They enjoyed the strawberry ice cream cone . (B) They loved the cone filled with chocolate . (C) They loved the vanilla cone . (D) None of the above choices .
04/25/2022 02:02:35 - INFO - __main__ - ['They enjoyed the strawberry ice cream cone .']
04/25/2022 02:02:35 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:02:35 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:02:35 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 02:02:36 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.25 on epoch=999
04/25/2022 02:02:36 - INFO - __main__ - save last model!
04/25/2022 02:02:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 02:02:36 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 02:02:36 - INFO - __main__ - Printing 3 examples
04/25/2022 02:02:36 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 02:02:36 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 02:02:36 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 02:02:36 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 02:02:36 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 02:02:36 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 02:02:36 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:02:37 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:02:38 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 02:02:54 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 02:02:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 02:02:55 - INFO - __main__ - Starting training!
04/25/2022 02:04:43 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_42_0.2_8_predictions.txt
04/25/2022 02:04:44 - INFO - __main__ - ACC on test data: 0.2770
04/25/2022 02:04:45 - INFO - __main__ - prefix=cosmos_qa_32_42, lr=0.2, bsz=8, dev_performance=0.40625, test_performance=0.277
04/25/2022 02:04:45 - INFO - __main__ - Running ... prefix=cosmos_qa_32_87, lr=0.5, bsz=8 ...
04/25/2022 02:04:46 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:04:46 - INFO - __main__ - Printing 3 examples
04/25/2022 02:04:46 - INFO - __main__ -  [cosmos_qa] Why did they buy those products in specific ? [SEP] This morning I went to Wal - Mart to buy deodorant and soap . Funny that I should simultaneously run out of those things . While I was there , " Who Let the Dogs Out " was playing . It was playing when I came in , and I swear , it went on for like 6 minutes . [SEP]  (A) They wanted to spend as little money as possible . (B) They did n't like the products they already had . (C) They need them to keep themselves clean . (D) None of the above choices .
04/25/2022 02:04:46 - INFO - __main__ - ['They need them to keep themselves clean .']
04/25/2022 02:04:46 - INFO - __main__ -  [cosmos_qa] Why might the writer be agitated ? [SEP] I ' ve been awake for an hour and i ' ve already smoked 3 cigarettes . my computer is going to die soon , and by die i mean fucking die forever and it wo nt ever come back . i m scared . [SEP]  (A) The writer may feel guilty for smoking . (B) The writer might need more sleep . (C) The writer might be lonely . (D) None of the above choices .
04/25/2022 02:04:46 - INFO - __main__ - ['None of the above choices .']
04/25/2022 02:04:46 - INFO - __main__ -  [cosmos_qa] Why did the narrator feel they needed to go to their room ? [SEP] It was really hard to breath and I thought I was going to faint in the hallway in my towel . I got back to my room and took a zantac , and I was a little better . In bed though , I felt faint again but not nearly as severely , and my hands were shaking . [SEP]  (A) They were wanting a nap . (B) None of the above choices . (C) They needed sleep . (D) They needed to take medicine .
04/25/2022 02:04:46 - INFO - __main__ - ['They needed to take medicine .']
04/25/2022 02:04:46 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:04:46 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:04:46 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 02:04:46 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:04:46 - INFO - __main__ - Printing 3 examples
04/25/2022 02:04:46 - INFO - __main__ -  [cosmos_qa] What may be the reason for their situation ? [SEP] Well , after reading around , I came to the conclusion that it 's merely media influence . In almost everything I ' ve seen , the light represents good and the darkness is evil . Of course , I was pretty much guilty in putting that in most of my works . [SEP]  (A) They want to influence the public on purpose . (B) They have to publish new content quickly . (C) It is their job to influence the public . (D) None of the above choices .
04/25/2022 02:04:46 - INFO - __main__ - ['It is their job to influence the public .']
04/25/2022 02:04:46 - INFO - __main__ -  [cosmos_qa] What happened to the speakers mother ? [SEP] I do n't really have too much to write about today . Worked at white spot for six hours . It was pretty busy but nothing I could nt handle . Brent was a little chocked about me not showing up on monday but I could n't not go spread my mommas ashes and he did n't give me too much fuss because he knows usually i m an excellent worker . [SEP]  (A) The speakers mother has passed on . (B) The speakers mother is choked up . (C) None of the above choices . (D) The speakers mother wrote about today .
04/25/2022 02:04:46 - INFO - __main__ - ['The speakers mother has passed on .']
04/25/2022 02:04:46 - INFO - __main__ -  [cosmos_qa] What did the kids enjoy at the EIS cafe ? [SEP] Will had fun riding his little bike as well . After about an hour of bike riding , the kids were treated to an ice cream cone from " Homie " . He drives around post selling the best ice cream cones EVER from EIS Cafe , the ice cream cafe in town . They each got a strawberry cone and enjoyed them immensely . [SEP]  (A) They enjoyed the strawberry ice cream cone . (B) They loved the cone filled with chocolate . (C) They loved the vanilla cone . (D) None of the above choices .
04/25/2022 02:04:46 - INFO - __main__ - ['They enjoyed the strawberry ice cream cone .']
04/25/2022 02:04:46 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:04:46 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:04:46 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 02:05:01 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 02:05:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 02:05:02 - INFO - __main__ - Starting training!
04/25/2022 02:05:06 - INFO - __main__ - Step 10 Global step 10 Train loss 0.83 on epoch=4
04/25/2022 02:05:09 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=9
04/25/2022 02:05:13 - INFO - __main__ - Step 30 Global step 30 Train loss 0.38 on epoch=14
04/25/2022 02:05:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.31 on epoch=19
04/25/2022 02:05:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.32 on epoch=24
04/25/2022 02:05:22 - INFO - __main__ - Global step 50 Train loss 0.47 ACC 0.125 on epoch=24
04/25/2022 02:05:22 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.125 on epoch=24, global_step=50
04/25/2022 02:05:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.29 on epoch=29
04/25/2022 02:05:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.27 on epoch=34
04/25/2022 02:05:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=39
04/25/2022 02:05:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.25 on epoch=44
04/25/2022 02:05:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.21 on epoch=49
04/25/2022 02:05:40 - INFO - __main__ - Global step 100 Train loss 0.26 ACC 0.1875 on epoch=49
04/25/2022 02:05:40 - INFO - __main__ - Saving model with best ACC: 0.125 -> 0.1875 on epoch=49, global_step=100
04/25/2022 02:05:44 - INFO - __main__ - Step 110 Global step 110 Train loss 0.22 on epoch=54
04/25/2022 02:05:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=59
04/25/2022 02:05:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.23 on epoch=64
04/25/2022 02:05:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.19 on epoch=69
04/25/2022 02:05:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.18 on epoch=74
04/25/2022 02:05:59 - INFO - __main__ - Global step 150 Train loss 0.24 ACC 0.15625 on epoch=74
04/25/2022 02:06:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.19 on epoch=79
04/25/2022 02:06:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=84
04/25/2022 02:06:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.19 on epoch=89
04/25/2022 02:06:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=94
04/25/2022 02:06:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.17 on epoch=99
04/25/2022 02:06:17 - INFO - __main__ - Global step 200 Train loss 0.20 ACC 0.125 on epoch=99
04/25/2022 02:06:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.16 on epoch=104
04/25/2022 02:06:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.14 on epoch=109
04/25/2022 02:06:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.12 on epoch=114
04/25/2022 02:06:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.16 on epoch=119
04/25/2022 02:06:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.11 on epoch=124
04/25/2022 02:06:35 - INFO - __main__ - Global step 250 Train loss 0.14 ACC 0.15625 on epoch=124
04/25/2022 02:06:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.10 on epoch=129
04/25/2022 02:06:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.13 on epoch=134
04/25/2022 02:06:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.11 on epoch=139
04/25/2022 02:06:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.13 on epoch=144
04/25/2022 02:06:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.12 on epoch=149
04/25/2022 02:06:54 - INFO - __main__ - Global step 300 Train loss 0.12 ACC 0.15625 on epoch=149
04/25/2022 02:06:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.10 on epoch=154
04/25/2022 02:07:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.08 on epoch=159
04/25/2022 02:07:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.09 on epoch=164
04/25/2022 02:07:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.09 on epoch=169
04/25/2022 02:07:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.08 on epoch=174
04/25/2022 02:07:13 - INFO - __main__ - Global step 350 Train loss 0.09 ACC 0.15625 on epoch=174
04/25/2022 02:07:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.08 on epoch=179
04/25/2022 02:07:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.09 on epoch=184
04/25/2022 02:07:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.09 on epoch=189
04/25/2022 02:07:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.07 on epoch=194
04/25/2022 02:07:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.08 on epoch=199
04/25/2022 02:07:31 - INFO - __main__ - Global step 400 Train loss 0.08 ACC 0.15625 on epoch=199
04/25/2022 02:07:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.07 on epoch=204
04/25/2022 02:07:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.07 on epoch=209
04/25/2022 02:07:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.09 on epoch=214
04/25/2022 02:07:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.06 on epoch=219
04/25/2022 02:07:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.05 on epoch=224
04/25/2022 02:07:50 - INFO - __main__ - Global step 450 Train loss 0.07 ACC 0.125 on epoch=224
04/25/2022 02:07:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.05 on epoch=229
04/25/2022 02:07:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
04/25/2022 02:07:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
04/25/2022 02:08:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
04/25/2022 02:08:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=249
04/25/2022 02:08:09 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.15625 on epoch=249
04/25/2022 02:08:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.06 on epoch=254
04/25/2022 02:08:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
04/25/2022 02:08:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=264
04/25/2022 02:08:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
04/25/2022 02:08:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
04/25/2022 02:08:27 - INFO - __main__ - Global step 550 Train loss 0.05 ACC 0.15625 on epoch=274
04/25/2022 02:08:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
04/25/2022 02:08:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.04 on epoch=284
04/25/2022 02:08:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
04/25/2022 02:08:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
04/25/2022 02:08:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
04/25/2022 02:08:45 - INFO - __main__ - Global step 600 Train loss 0.04 ACC 0.15625 on epoch=299
04/25/2022 02:08:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
04/25/2022 02:08:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
04/25/2022 02:08:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
04/25/2022 02:08:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
04/25/2022 02:09:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=324
04/25/2022 02:09:04 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.1875 on epoch=324
04/25/2022 02:09:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
04/25/2022 02:09:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
04/25/2022 02:09:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
04/25/2022 02:09:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
04/25/2022 02:09:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
04/25/2022 02:09:22 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.1875 on epoch=349
04/25/2022 02:09:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
04/25/2022 02:09:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
04/25/2022 02:09:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
04/25/2022 02:09:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
04/25/2022 02:09:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
04/25/2022 02:09:40 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.125 on epoch=374
04/25/2022 02:09:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
04/25/2022 02:09:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
04/25/2022 02:09:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
04/25/2022 02:09:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
04/25/2022 02:09:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
04/25/2022 02:09:58 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.1875 on epoch=399
04/25/2022 02:10:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
04/25/2022 02:10:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
04/25/2022 02:10:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
04/25/2022 02:10:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
04/25/2022 02:10:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
04/25/2022 02:10:17 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.125 on epoch=424
04/25/2022 02:10:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
04/25/2022 02:10:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
04/25/2022 02:10:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
04/25/2022 02:10:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
04/25/2022 02:10:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
04/25/2022 02:10:35 - INFO - __main__ - Global step 900 Train loss 0.03 ACC 0.15625 on epoch=449
04/25/2022 02:10:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
04/25/2022 02:10:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
04/25/2022 02:10:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
04/25/2022 02:10:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
04/25/2022 02:10:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
04/25/2022 02:10:53 - INFO - __main__ - Global step 950 Train loss 0.03 ACC 0.15625 on epoch=474
04/25/2022 02:10:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
04/25/2022 02:11:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
04/25/2022 02:11:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
04/25/2022 02:11:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
04/25/2022 02:11:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/25/2022 02:11:12 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.125 on epoch=499
04/25/2022 02:11:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
04/25/2022 02:11:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
04/25/2022 02:11:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
04/25/2022 02:11:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
04/25/2022 02:11:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/25/2022 02:11:30 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.09375 on epoch=524
04/25/2022 02:11:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
04/25/2022 02:11:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
04/25/2022 02:11:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
04/25/2022 02:11:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
04/25/2022 02:11:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
04/25/2022 02:11:49 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.1875 on epoch=549
04/25/2022 02:11:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
04/25/2022 02:11:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
04/25/2022 02:11:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/25/2022 02:12:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
04/25/2022 02:12:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
04/25/2022 02:12:08 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.25 on epoch=574
04/25/2022 02:12:08 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.25 on epoch=574, global_step=1150
04/25/2022 02:12:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/25/2022 02:12:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/25/2022 02:12:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
04/25/2022 02:12:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
04/25/2022 02:12:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
04/25/2022 02:12:27 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.25 on epoch=599
04/25/2022 02:12:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
04/25/2022 02:12:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
04/25/2022 02:12:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
04/25/2022 02:12:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
04/25/2022 02:12:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
04/25/2022 02:12:46 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.15625 on epoch=624
04/25/2022 02:12:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
04/25/2022 02:12:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
04/25/2022 02:12:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
04/25/2022 02:12:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/25/2022 02:13:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/25/2022 02:13:04 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.15625 on epoch=649
04/25/2022 02:13:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 02:13:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 02:13:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
04/25/2022 02:13:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
04/25/2022 02:13:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 02:13:23 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.21875 on epoch=674
04/25/2022 02:13:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 02:13:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 02:13:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
04/25/2022 02:13:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
04/25/2022 02:13:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 02:13:42 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.1875 on epoch=699
04/25/2022 02:13:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/25/2022 02:13:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 02:13:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 02:13:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
04/25/2022 02:13:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
04/25/2022 02:14:00 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.21875 on epoch=724
04/25/2022 02:14:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 02:14:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/25/2022 02:14:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
04/25/2022 02:14:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 02:14:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
04/25/2022 02:14:19 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.21875 on epoch=749
04/25/2022 02:14:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 02:14:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 02:14:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
04/25/2022 02:14:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
04/25/2022 02:14:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/25/2022 02:14:37 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.25 on epoch=774
04/25/2022 02:14:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 02:14:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 02:14:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 02:14:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/25/2022 02:14:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/25/2022 02:14:56 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.21875 on epoch=799
04/25/2022 02:14:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 02:15:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
04/25/2022 02:15:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/25/2022 02:15:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
04/25/2022 02:15:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
04/25/2022 02:15:15 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.1875 on epoch=824
04/25/2022 02:15:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/25/2022 02:15:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
04/25/2022 02:15:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/25/2022 02:15:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
04/25/2022 02:15:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 02:15:33 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.21875 on epoch=849
04/25/2022 02:15:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 02:15:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 02:15:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 02:15:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
04/25/2022 02:15:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
04/25/2022 02:15:52 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.21875 on epoch=874
04/25/2022 02:15:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
04/25/2022 02:15:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
04/25/2022 02:16:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 02:16:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
04/25/2022 02:16:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
04/25/2022 02:16:10 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.21875 on epoch=899
04/25/2022 02:16:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
04/25/2022 02:16:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/25/2022 02:16:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 02:16:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 02:16:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 02:16:29 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.28125 on epoch=924
04/25/2022 02:16:29 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.28125 on epoch=924, global_step=1850
04/25/2022 02:16:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 02:16:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
04/25/2022 02:16:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 02:16:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
04/25/2022 02:16:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
04/25/2022 02:16:47 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.21875 on epoch=949
04/25/2022 02:16:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 02:16:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 02:16:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 02:17:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 02:17:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 02:17:06 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.21875 on epoch=974
04/25/2022 02:17:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 02:17:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 02:17:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/25/2022 02:17:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 02:17:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 02:17:24 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:17:24 - INFO - __main__ - Printing 3 examples
04/25/2022 02:17:24 - INFO - __main__ -  [cosmos_qa] Why did they buy those products in specific ? [SEP] This morning I went to Wal - Mart to buy deodorant and soap . Funny that I should simultaneously run out of those things . While I was there , " Who Let the Dogs Out " was playing . It was playing when I came in , and I swear , it went on for like 6 minutes . [SEP]  (A) They wanted to spend as little money as possible . (B) They did n't like the products they already had . (C) They need them to keep themselves clean . (D) None of the above choices .
04/25/2022 02:17:24 - INFO - __main__ - ['They need them to keep themselves clean .']
04/25/2022 02:17:24 - INFO - __main__ -  [cosmos_qa] Why might the writer be agitated ? [SEP] I ' ve been awake for an hour and i ' ve already smoked 3 cigarettes . my computer is going to die soon , and by die i mean fucking die forever and it wo nt ever come back . i m scared . [SEP]  (A) The writer may feel guilty for smoking . (B) The writer might need more sleep . (C) The writer might be lonely . (D) None of the above choices .
04/25/2022 02:17:24 - INFO - __main__ - ['None of the above choices .']
04/25/2022 02:17:24 - INFO - __main__ -  [cosmos_qa] Why did the narrator feel they needed to go to their room ? [SEP] It was really hard to breath and I thought I was going to faint in the hallway in my towel . I got back to my room and took a zantac , and I was a little better . In bed though , I felt faint again but not nearly as severely , and my hands were shaking . [SEP]  (A) They were wanting a nap . (B) None of the above choices . (C) They needed sleep . (D) They needed to take medicine .
04/25/2022 02:17:24 - INFO - __main__ - ['They needed to take medicine .']
04/25/2022 02:17:24 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:17:24 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:17:24 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 02:17:24 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:17:24 - INFO - __main__ - Printing 3 examples
04/25/2022 02:17:24 - INFO - __main__ -  [cosmos_qa] What may be the reason for their situation ? [SEP] Well , after reading around , I came to the conclusion that it 's merely media influence . In almost everything I ' ve seen , the light represents good and the darkness is evil . Of course , I was pretty much guilty in putting that in most of my works . [SEP]  (A) They want to influence the public on purpose . (B) They have to publish new content quickly . (C) It is their job to influence the public . (D) None of the above choices .
04/25/2022 02:17:24 - INFO - __main__ - ['It is their job to influence the public .']
04/25/2022 02:17:24 - INFO - __main__ -  [cosmos_qa] What happened to the speakers mother ? [SEP] I do n't really have too much to write about today . Worked at white spot for six hours . It was pretty busy but nothing I could nt handle . Brent was a little chocked about me not showing up on monday but I could n't not go spread my mommas ashes and he did n't give me too much fuss because he knows usually i m an excellent worker . [SEP]  (A) The speakers mother has passed on . (B) The speakers mother is choked up . (C) None of the above choices . (D) The speakers mother wrote about today .
04/25/2022 02:17:24 - INFO - __main__ - ['The speakers mother has passed on .']
04/25/2022 02:17:24 - INFO - __main__ -  [cosmos_qa] What did the kids enjoy at the EIS cafe ? [SEP] Will had fun riding his little bike as well . After about an hour of bike riding , the kids were treated to an ice cream cone from " Homie " . He drives around post selling the best ice cream cones EVER from EIS Cafe , the ice cream cafe in town . They each got a strawberry cone and enjoyed them immensely . [SEP]  (A) They enjoyed the strawberry ice cream cone . (B) They loved the cone filled with chocolate . (C) They loved the vanilla cone . (D) None of the above choices .
04/25/2022 02:17:24 - INFO - __main__ - ['They enjoyed the strawberry ice cream cone .']
04/25/2022 02:17:24 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:17:24 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:17:24 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 02:17:25 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.1875 on epoch=999
04/25/2022 02:17:25 - INFO - __main__ - save last model!
04/25/2022 02:17:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 02:17:25 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 02:17:25 - INFO - __main__ - Printing 3 examples
04/25/2022 02:17:25 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 02:17:25 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 02:17:25 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 02:17:25 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 02:17:25 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 02:17:25 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 02:17:25 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:17:26 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:17:27 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 02:17:40 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 02:17:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 02:17:41 - INFO - __main__ - Starting training!
04/25/2022 02:19:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_87_0.5_8_predictions.txt
04/25/2022 02:19:28 - INFO - __main__ - ACC on test data: 0.2730
04/25/2022 02:19:29 - INFO - __main__ - prefix=cosmos_qa_32_87, lr=0.5, bsz=8, dev_performance=0.28125, test_performance=0.273
04/25/2022 02:19:29 - INFO - __main__ - Running ... prefix=cosmos_qa_32_87, lr=0.4, bsz=8 ...
04/25/2022 02:19:30 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:19:30 - INFO - __main__ - Printing 3 examples
04/25/2022 02:19:30 - INFO - __main__ -  [cosmos_qa] Why did they buy those products in specific ? [SEP] This morning I went to Wal - Mart to buy deodorant and soap . Funny that I should simultaneously run out of those things . While I was there , " Who Let the Dogs Out " was playing . It was playing when I came in , and I swear , it went on for like 6 minutes . [SEP]  (A) They wanted to spend as little money as possible . (B) They did n't like the products they already had . (C) They need them to keep themselves clean . (D) None of the above choices .
04/25/2022 02:19:30 - INFO - __main__ - ['They need them to keep themselves clean .']
04/25/2022 02:19:30 - INFO - __main__ -  [cosmos_qa] Why might the writer be agitated ? [SEP] I ' ve been awake for an hour and i ' ve already smoked 3 cigarettes . my computer is going to die soon , and by die i mean fucking die forever and it wo nt ever come back . i m scared . [SEP]  (A) The writer may feel guilty for smoking . (B) The writer might need more sleep . (C) The writer might be lonely . (D) None of the above choices .
04/25/2022 02:19:30 - INFO - __main__ - ['None of the above choices .']
04/25/2022 02:19:30 - INFO - __main__ -  [cosmos_qa] Why did the narrator feel they needed to go to their room ? [SEP] It was really hard to breath and I thought I was going to faint in the hallway in my towel . I got back to my room and took a zantac , and I was a little better . In bed though , I felt faint again but not nearly as severely , and my hands were shaking . [SEP]  (A) They were wanting a nap . (B) None of the above choices . (C) They needed sleep . (D) They needed to take medicine .
04/25/2022 02:19:30 - INFO - __main__ - ['They needed to take medicine .']
04/25/2022 02:19:30 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:19:30 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:19:30 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 02:19:30 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:19:30 - INFO - __main__ - Printing 3 examples
04/25/2022 02:19:30 - INFO - __main__ -  [cosmos_qa] What may be the reason for their situation ? [SEP] Well , after reading around , I came to the conclusion that it 's merely media influence . In almost everything I ' ve seen , the light represents good and the darkness is evil . Of course , I was pretty much guilty in putting that in most of my works . [SEP]  (A) They want to influence the public on purpose . (B) They have to publish new content quickly . (C) It is their job to influence the public . (D) None of the above choices .
04/25/2022 02:19:30 - INFO - __main__ - ['It is their job to influence the public .']
04/25/2022 02:19:30 - INFO - __main__ -  [cosmos_qa] What happened to the speakers mother ? [SEP] I do n't really have too much to write about today . Worked at white spot for six hours . It was pretty busy but nothing I could nt handle . Brent was a little chocked about me not showing up on monday but I could n't not go spread my mommas ashes and he did n't give me too much fuss because he knows usually i m an excellent worker . [SEP]  (A) The speakers mother has passed on . (B) The speakers mother is choked up . (C) None of the above choices . (D) The speakers mother wrote about today .
04/25/2022 02:19:30 - INFO - __main__ - ['The speakers mother has passed on .']
04/25/2022 02:19:30 - INFO - __main__ -  [cosmos_qa] What did the kids enjoy at the EIS cafe ? [SEP] Will had fun riding his little bike as well . After about an hour of bike riding , the kids were treated to an ice cream cone from " Homie " . He drives around post selling the best ice cream cones EVER from EIS Cafe , the ice cream cafe in town . They each got a strawberry cone and enjoyed them immensely . [SEP]  (A) They enjoyed the strawberry ice cream cone . (B) They loved the cone filled with chocolate . (C) They loved the vanilla cone . (D) None of the above choices .
04/25/2022 02:19:30 - INFO - __main__ - ['They enjoyed the strawberry ice cream cone .']
04/25/2022 02:19:30 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:19:30 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:19:30 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 02:19:48 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 02:19:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 02:19:49 - INFO - __main__ - Starting training!
04/25/2022 02:19:53 - INFO - __main__ - Step 10 Global step 10 Train loss 0.88 on epoch=4
04/25/2022 02:19:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.55 on epoch=9
04/25/2022 02:19:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=14
04/25/2022 02:20:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.39 on epoch=19
04/25/2022 02:20:06 - INFO - __main__ - Step 50 Global step 50 Train loss 0.34 on epoch=24
04/25/2022 02:20:08 - INFO - __main__ - Global step 50 Train loss 0.51 ACC 0.15625 on epoch=24
04/25/2022 02:20:08 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
04/25/2022 02:20:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.60 on epoch=29
04/25/2022 02:20:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=34
04/25/2022 02:20:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=39
04/25/2022 02:20:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=44
04/25/2022 02:20:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.31 on epoch=49
04/25/2022 02:20:26 - INFO - __main__ - Global step 100 Train loss 0.43 ACC 0.15625 on epoch=49
04/25/2022 02:20:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.31 on epoch=54
04/25/2022 02:20:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.30 on epoch=59
04/25/2022 02:20:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.31 on epoch=64
04/25/2022 02:20:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=69
04/25/2022 02:20:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=74
04/25/2022 02:20:45 - INFO - __main__ - Global step 150 Train loss 0.29 ACC 0.1875 on epoch=74
04/25/2022 02:20:45 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.1875 on epoch=74, global_step=150
04/25/2022 02:20:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.29 on epoch=79
04/25/2022 02:20:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=84
04/25/2022 02:20:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=89
04/25/2022 02:20:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=94
04/25/2022 02:21:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
04/25/2022 02:21:04 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.21875 on epoch=99
04/25/2022 02:21:04 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=99, global_step=200
04/25/2022 02:21:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
04/25/2022 02:21:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.23 on epoch=109
04/25/2022 02:21:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.29 on epoch=114
04/25/2022 02:21:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.28 on epoch=119
04/25/2022 02:21:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
04/25/2022 02:21:22 - INFO - __main__ - Global step 250 Train loss 0.26 ACC 0.15625 on epoch=124
04/25/2022 02:21:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
04/25/2022 02:21:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
04/25/2022 02:21:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
04/25/2022 02:21:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=144
04/25/2022 02:21:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=149
04/25/2022 02:21:41 - INFO - __main__ - Global step 300 Train loss 0.24 ACC 0.15625 on epoch=149
04/25/2022 02:21:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=154
04/25/2022 02:21:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.20 on epoch=159
04/25/2022 02:21:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=164
04/25/2022 02:21:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.20 on epoch=169
04/25/2022 02:21:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=174
04/25/2022 02:21:59 - INFO - __main__ - Global step 350 Train loss 0.21 ACC 0.0625 on epoch=174
04/25/2022 02:22:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.20 on epoch=179
04/25/2022 02:22:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.19 on epoch=184
04/25/2022 02:22:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.18 on epoch=189
04/25/2022 02:22:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=194
04/25/2022 02:22:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.16 on epoch=199
04/25/2022 02:22:17 - INFO - __main__ - Global step 400 Train loss 0.19 ACC 0.125 on epoch=199
04/25/2022 02:22:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.15 on epoch=204
04/25/2022 02:22:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.15 on epoch=209
04/25/2022 02:22:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.18 on epoch=214
04/25/2022 02:22:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.16 on epoch=219
04/25/2022 02:22:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.16 on epoch=224
04/25/2022 02:22:35 - INFO - __main__ - Global step 450 Train loss 0.16 ACC 0.15625 on epoch=224
04/25/2022 02:22:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=229
04/25/2022 02:22:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.16 on epoch=234
04/25/2022 02:22:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=239
04/25/2022 02:22:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=244
04/25/2022 02:22:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=249
04/25/2022 02:22:54 - INFO - __main__ - Global step 500 Train loss 0.14 ACC 0.1875 on epoch=249
04/25/2022 02:22:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.13 on epoch=254
04/25/2022 02:23:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.15 on epoch=259
04/25/2022 02:23:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
04/25/2022 02:23:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.13 on epoch=269
04/25/2022 02:23:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.10 on epoch=274
04/25/2022 02:23:12 - INFO - __main__ - Global step 550 Train loss 0.13 ACC 0.15625 on epoch=274
04/25/2022 02:23:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=279
04/25/2022 02:23:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=284
04/25/2022 02:23:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=289
04/25/2022 02:23:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.10 on epoch=294
04/25/2022 02:23:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=299
04/25/2022 02:23:30 - INFO - __main__ - Global step 600 Train loss 0.11 ACC 0.125 on epoch=299
04/25/2022 02:23:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.10 on epoch=304
04/25/2022 02:23:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=309
04/25/2022 02:23:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=314
04/25/2022 02:23:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=319
04/25/2022 02:23:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.08 on epoch=324
04/25/2022 02:23:49 - INFO - __main__ - Global step 650 Train loss 0.11 ACC 0.09375 on epoch=324
04/25/2022 02:23:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
04/25/2022 02:23:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=334
04/25/2022 02:23:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=339
04/25/2022 02:24:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.08 on epoch=344
04/25/2022 02:24:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
04/25/2022 02:24:07 - INFO - __main__ - Global step 700 Train loss 0.09 ACC 0.15625 on epoch=349
04/25/2022 02:24:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
04/25/2022 02:24:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=359
04/25/2022 02:24:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=364
04/25/2022 02:24:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=369
04/25/2022 02:24:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=374
04/25/2022 02:24:25 - INFO - __main__ - Global step 750 Train loss 0.08 ACC 0.0625 on epoch=374
04/25/2022 02:24:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=379
04/25/2022 02:24:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
04/25/2022 02:24:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
04/25/2022 02:24:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
04/25/2022 02:24:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
04/25/2022 02:24:44 - INFO - __main__ - Global step 800 Train loss 0.08 ACC 0.125 on epoch=399
04/25/2022 02:24:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
04/25/2022 02:24:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=409
04/25/2022 02:24:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=414
04/25/2022 02:24:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
04/25/2022 02:24:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=424
04/25/2022 02:25:02 - INFO - __main__ - Global step 850 Train loss 0.06 ACC 0.0625 on epoch=424
04/25/2022 02:25:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
04/25/2022 02:25:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=434
04/25/2022 02:25:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=439
04/25/2022 02:25:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
04/25/2022 02:25:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=449
04/25/2022 02:25:20 - INFO - __main__ - Global step 900 Train loss 0.05 ACC 0.1875 on epoch=449
04/25/2022 02:25:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
04/25/2022 02:25:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
04/25/2022 02:25:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
04/25/2022 02:25:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=469
04/25/2022 02:25:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
04/25/2022 02:25:38 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.1875 on epoch=474
04/25/2022 02:25:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=479
04/25/2022 02:25:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
04/25/2022 02:25:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
04/25/2022 02:25:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
04/25/2022 02:25:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
04/25/2022 02:25:56 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.125 on epoch=499
04/25/2022 02:26:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
04/25/2022 02:26:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
04/25/2022 02:26:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
04/25/2022 02:26:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
04/25/2022 02:26:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
04/25/2022 02:26:15 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.15625 on epoch=524
04/25/2022 02:26:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
04/25/2022 02:26:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
04/25/2022 02:26:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
04/25/2022 02:26:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
04/25/2022 02:26:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=549
04/25/2022 02:26:33 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.15625 on epoch=549
04/25/2022 02:26:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
04/25/2022 02:26:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
04/25/2022 02:26:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
04/25/2022 02:26:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
04/25/2022 02:26:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
04/25/2022 02:26:51 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.15625 on epoch=574
04/25/2022 02:26:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
04/25/2022 02:26:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=584
04/25/2022 02:27:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
04/25/2022 02:27:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
04/25/2022 02:27:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
04/25/2022 02:27:09 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.125 on epoch=599
04/25/2022 02:27:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
04/25/2022 02:27:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
04/25/2022 02:27:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 02:27:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
04/25/2022 02:27:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/25/2022 02:27:27 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.125 on epoch=624
04/25/2022 02:27:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
04/25/2022 02:27:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
04/25/2022 02:27:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
04/25/2022 02:27:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/25/2022 02:27:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/25/2022 02:27:46 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.125 on epoch=649
04/25/2022 02:27:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
04/25/2022 02:27:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 02:27:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=664
04/25/2022 02:27:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/25/2022 02:28:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
04/25/2022 02:28:04 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.15625 on epoch=674
04/25/2022 02:28:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
04/25/2022 02:28:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/25/2022 02:28:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/25/2022 02:28:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
04/25/2022 02:28:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
04/25/2022 02:28:22 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.21875 on epoch=699
04/25/2022 02:28:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
04/25/2022 02:28:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=709
04/25/2022 02:28:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 02:28:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
04/25/2022 02:28:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
04/25/2022 02:28:40 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.3125 on epoch=724
04/25/2022 02:28:40 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.3125 on epoch=724, global_step=1450
04/25/2022 02:28:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 02:28:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
04/25/2022 02:28:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
04/25/2022 02:28:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 02:28:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
04/25/2022 02:28:58 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.15625 on epoch=749
04/25/2022 02:29:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
04/25/2022 02:29:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
04/25/2022 02:29:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
04/25/2022 02:29:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 02:29:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/25/2022 02:29:16 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.21875 on epoch=774
04/25/2022 02:29:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
04/25/2022 02:29:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 02:29:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
04/25/2022 02:29:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/25/2022 02:29:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
04/25/2022 02:29:35 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.1875 on epoch=799
04/25/2022 02:29:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 02:29:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 02:29:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
04/25/2022 02:29:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
04/25/2022 02:29:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 02:29:53 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.3125 on epoch=824
04/25/2022 02:29:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
04/25/2022 02:29:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
04/25/2022 02:30:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/25/2022 02:30:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/25/2022 02:30:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
04/25/2022 02:30:11 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.28125 on epoch=849
04/25/2022 02:30:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
04/25/2022 02:30:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 02:30:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
04/25/2022 02:30:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
04/25/2022 02:30:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/25/2022 02:30:29 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.25 on epoch=874
04/25/2022 02:30:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/25/2022 02:30:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
04/25/2022 02:30:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
04/25/2022 02:30:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 02:30:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 02:30:48 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.1875 on epoch=899
04/25/2022 02:30:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 02:30:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
04/25/2022 02:30:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
04/25/2022 02:31:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 02:31:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 02:31:06 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.21875 on epoch=924
04/25/2022 02:31:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 02:31:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
04/25/2022 02:31:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
04/25/2022 02:31:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 02:31:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
04/25/2022 02:31:24 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.1875 on epoch=949
04/25/2022 02:31:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 02:31:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 02:31:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 02:31:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 02:31:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 02:31:42 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.21875 on epoch=974
04/25/2022 02:31:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
04/25/2022 02:31:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 02:31:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
04/25/2022 02:31:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 02:31:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 02:32:00 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:32:00 - INFO - __main__ - Printing 3 examples
04/25/2022 02:32:00 - INFO - __main__ -  [cosmos_qa] Why did they buy those products in specific ? [SEP] This morning I went to Wal - Mart to buy deodorant and soap . Funny that I should simultaneously run out of those things . While I was there , " Who Let the Dogs Out " was playing . It was playing when I came in , and I swear , it went on for like 6 minutes . [SEP]  (A) They wanted to spend as little money as possible . (B) They did n't like the products they already had . (C) They need them to keep themselves clean . (D) None of the above choices .
04/25/2022 02:32:00 - INFO - __main__ - ['They need them to keep themselves clean .']
04/25/2022 02:32:00 - INFO - __main__ -  [cosmos_qa] Why might the writer be agitated ? [SEP] I ' ve been awake for an hour and i ' ve already smoked 3 cigarettes . my computer is going to die soon , and by die i mean fucking die forever and it wo nt ever come back . i m scared . [SEP]  (A) The writer may feel guilty for smoking . (B) The writer might need more sleep . (C) The writer might be lonely . (D) None of the above choices .
04/25/2022 02:32:00 - INFO - __main__ - ['None of the above choices .']
04/25/2022 02:32:00 - INFO - __main__ -  [cosmos_qa] Why did the narrator feel they needed to go to their room ? [SEP] It was really hard to breath and I thought I was going to faint in the hallway in my towel . I got back to my room and took a zantac , and I was a little better . In bed though , I felt faint again but not nearly as severely , and my hands were shaking . [SEP]  (A) They were wanting a nap . (B) None of the above choices . (C) They needed sleep . (D) They needed to take medicine .
04/25/2022 02:32:00 - INFO - __main__ - ['They needed to take medicine .']
04/25/2022 02:32:00 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:32:00 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:32:00 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 02:32:00 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:32:00 - INFO - __main__ - Printing 3 examples
04/25/2022 02:32:00 - INFO - __main__ -  [cosmos_qa] What may be the reason for their situation ? [SEP] Well , after reading around , I came to the conclusion that it 's merely media influence . In almost everything I ' ve seen , the light represents good and the darkness is evil . Of course , I was pretty much guilty in putting that in most of my works . [SEP]  (A) They want to influence the public on purpose . (B) They have to publish new content quickly . (C) It is their job to influence the public . (D) None of the above choices .
04/25/2022 02:32:00 - INFO - __main__ - ['It is their job to influence the public .']
04/25/2022 02:32:00 - INFO - __main__ -  [cosmos_qa] What happened to the speakers mother ? [SEP] I do n't really have too much to write about today . Worked at white spot for six hours . It was pretty busy but nothing I could nt handle . Brent was a little chocked about me not showing up on monday but I could n't not go spread my mommas ashes and he did n't give me too much fuss because he knows usually i m an excellent worker . [SEP]  (A) The speakers mother has passed on . (B) The speakers mother is choked up . (C) None of the above choices . (D) The speakers mother wrote about today .
04/25/2022 02:32:00 - INFO - __main__ - ['The speakers mother has passed on .']
04/25/2022 02:32:00 - INFO - __main__ -  [cosmos_qa] What did the kids enjoy at the EIS cafe ? [SEP] Will had fun riding his little bike as well . After about an hour of bike riding , the kids were treated to an ice cream cone from " Homie " . He drives around post selling the best ice cream cones EVER from EIS Cafe , the ice cream cafe in town . They each got a strawberry cone and enjoyed them immensely . [SEP]  (A) They enjoyed the strawberry ice cream cone . (B) They loved the cone filled with chocolate . (C) They loved the vanilla cone . (D) None of the above choices .
04/25/2022 02:32:00 - INFO - __main__ - ['They enjoyed the strawberry ice cream cone .']
04/25/2022 02:32:00 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:32:00 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:32:00 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 02:32:01 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.1875 on epoch=999
04/25/2022 02:32:01 - INFO - __main__ - save last model!
04/25/2022 02:32:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 02:32:01 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 02:32:01 - INFO - __main__ - Printing 3 examples
04/25/2022 02:32:01 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 02:32:01 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 02:32:01 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 02:32:01 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 02:32:01 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 02:32:01 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 02:32:01 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:32:02 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:32:03 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 02:32:16 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 02:32:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 02:32:16 - INFO - __main__ - Starting training!
04/25/2022 02:34:08 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_87_0.4_8_predictions.txt
04/25/2022 02:34:08 - INFO - __main__ - ACC on test data: 0.2860
04/25/2022 02:34:08 - INFO - __main__ - prefix=cosmos_qa_32_87, lr=0.4, bsz=8, dev_performance=0.3125, test_performance=0.286
04/25/2022 02:34:08 - INFO - __main__ - Running ... prefix=cosmos_qa_32_87, lr=0.3, bsz=8 ...
04/25/2022 02:34:09 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:34:09 - INFO - __main__ - Printing 3 examples
04/25/2022 02:34:09 - INFO - __main__ -  [cosmos_qa] Why did they buy those products in specific ? [SEP] This morning I went to Wal - Mart to buy deodorant and soap . Funny that I should simultaneously run out of those things . While I was there , " Who Let the Dogs Out " was playing . It was playing when I came in , and I swear , it went on for like 6 minutes . [SEP]  (A) They wanted to spend as little money as possible . (B) They did n't like the products they already had . (C) They need them to keep themselves clean . (D) None of the above choices .
04/25/2022 02:34:09 - INFO - __main__ - ['They need them to keep themselves clean .']
04/25/2022 02:34:09 - INFO - __main__ -  [cosmos_qa] Why might the writer be agitated ? [SEP] I ' ve been awake for an hour and i ' ve already smoked 3 cigarettes . my computer is going to die soon , and by die i mean fucking die forever and it wo nt ever come back . i m scared . [SEP]  (A) The writer may feel guilty for smoking . (B) The writer might need more sleep . (C) The writer might be lonely . (D) None of the above choices .
04/25/2022 02:34:09 - INFO - __main__ - ['None of the above choices .']
04/25/2022 02:34:09 - INFO - __main__ -  [cosmos_qa] Why did the narrator feel they needed to go to their room ? [SEP] It was really hard to breath and I thought I was going to faint in the hallway in my towel . I got back to my room and took a zantac , and I was a little better . In bed though , I felt faint again but not nearly as severely , and my hands were shaking . [SEP]  (A) They were wanting a nap . (B) None of the above choices . (C) They needed sleep . (D) They needed to take medicine .
04/25/2022 02:34:09 - INFO - __main__ - ['They needed to take medicine .']
04/25/2022 02:34:09 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:34:09 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:34:09 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 02:34:09 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:34:09 - INFO - __main__ - Printing 3 examples
04/25/2022 02:34:09 - INFO - __main__ -  [cosmos_qa] What may be the reason for their situation ? [SEP] Well , after reading around , I came to the conclusion that it 's merely media influence . In almost everything I ' ve seen , the light represents good and the darkness is evil . Of course , I was pretty much guilty in putting that in most of my works . [SEP]  (A) They want to influence the public on purpose . (B) They have to publish new content quickly . (C) It is their job to influence the public . (D) None of the above choices .
04/25/2022 02:34:09 - INFO - __main__ - ['It is their job to influence the public .']
04/25/2022 02:34:09 - INFO - __main__ -  [cosmos_qa] What happened to the speakers mother ? [SEP] I do n't really have too much to write about today . Worked at white spot for six hours . It was pretty busy but nothing I could nt handle . Brent was a little chocked about me not showing up on monday but I could n't not go spread my mommas ashes and he did n't give me too much fuss because he knows usually i m an excellent worker . [SEP]  (A) The speakers mother has passed on . (B) The speakers mother is choked up . (C) None of the above choices . (D) The speakers mother wrote about today .
04/25/2022 02:34:09 - INFO - __main__ - ['The speakers mother has passed on .']
04/25/2022 02:34:09 - INFO - __main__ -  [cosmos_qa] What did the kids enjoy at the EIS cafe ? [SEP] Will had fun riding his little bike as well . After about an hour of bike riding , the kids were treated to an ice cream cone from " Homie " . He drives around post selling the best ice cream cones EVER from EIS Cafe , the ice cream cafe in town . They each got a strawberry cone and enjoyed them immensely . [SEP]  (A) They enjoyed the strawberry ice cream cone . (B) They loved the cone filled with chocolate . (C) They loved the vanilla cone . (D) None of the above choices .
04/25/2022 02:34:09 - INFO - __main__ - ['They enjoyed the strawberry ice cream cone .']
04/25/2022 02:34:09 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:34:09 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:34:09 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 02:34:28 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 02:34:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 02:34:29 - INFO - __main__ - Starting training!
04/25/2022 02:34:33 - INFO - __main__ - Step 10 Global step 10 Train loss 0.86 on epoch=4
04/25/2022 02:34:36 - INFO - __main__ - Step 20 Global step 20 Train loss 0.58 on epoch=9
04/25/2022 02:34:39 - INFO - __main__ - Step 30 Global step 30 Train loss 0.45 on epoch=14
04/25/2022 02:34:42 - INFO - __main__ - Step 40 Global step 40 Train loss 0.41 on epoch=19
04/25/2022 02:34:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.36 on epoch=24
04/25/2022 02:34:49 - INFO - __main__ - Global step 50 Train loss 0.53 ACC 0.1875 on epoch=24
04/25/2022 02:34:49 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=24, global_step=50
04/25/2022 02:34:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.34 on epoch=29
04/25/2022 02:34:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.31 on epoch=34
04/25/2022 02:34:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.34 on epoch=39
04/25/2022 02:35:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=44
04/25/2022 02:35:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
04/25/2022 02:35:07 - INFO - __main__ - Global step 100 Train loss 0.32 ACC 0.15625 on epoch=49
04/25/2022 02:35:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=54
04/25/2022 02:35:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
04/25/2022 02:35:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=64
04/25/2022 02:35:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=69
04/25/2022 02:35:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=74
04/25/2022 02:35:27 - INFO - __main__ - Global step 150 Train loss 0.26 ACC 0.15625 on epoch=74
04/25/2022 02:35:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.22 on epoch=79
04/25/2022 02:35:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=84
04/25/2022 02:35:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.20 on epoch=89
04/25/2022 02:35:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.21 on epoch=94
04/25/2022 02:35:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
04/25/2022 02:35:46 - INFO - __main__ - Global step 200 Train loss 0.21 ACC 0.125 on epoch=99
04/25/2022 02:35:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=104
04/25/2022 02:35:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.20 on epoch=109
04/25/2022 02:35:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.20 on epoch=114
04/25/2022 02:35:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.18 on epoch=119
04/25/2022 02:36:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.17 on epoch=124
04/25/2022 02:36:04 - INFO - __main__ - Global step 250 Train loss 0.19 ACC 0.09375 on epoch=124
04/25/2022 02:36:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.16 on epoch=129
04/25/2022 02:36:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.17 on epoch=134
04/25/2022 02:36:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.16 on epoch=139
04/25/2022 02:36:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.13 on epoch=144
04/25/2022 02:36:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.11 on epoch=149
04/25/2022 02:36:23 - INFO - __main__ - Global step 300 Train loss 0.15 ACC 0.09375 on epoch=149
04/25/2022 02:36:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.13 on epoch=154
04/25/2022 02:36:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.16 on epoch=159
04/25/2022 02:36:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=164
04/25/2022 02:36:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.13 on epoch=169
04/25/2022 02:36:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.10 on epoch=174
04/25/2022 02:36:42 - INFO - __main__ - Global step 350 Train loss 0.13 ACC 0.09375 on epoch=174
04/25/2022 02:36:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
04/25/2022 02:36:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.11 on epoch=184
04/25/2022 02:36:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.12 on epoch=189
04/25/2022 02:36:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.11 on epoch=194
04/25/2022 02:36:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.10 on epoch=199
04/25/2022 02:37:00 - INFO - __main__ - Global step 400 Train loss 0.11 ACC 0.125 on epoch=199
04/25/2022 02:37:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
04/25/2022 02:37:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.10 on epoch=209
04/25/2022 02:37:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
04/25/2022 02:37:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.08 on epoch=219
04/25/2022 02:37:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
04/25/2022 02:37:19 - INFO - __main__ - Global step 450 Train loss 0.09 ACC 0.21875 on epoch=224
04/25/2022 02:37:19 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.21875 on epoch=224, global_step=450
04/25/2022 02:37:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.08 on epoch=229
04/25/2022 02:37:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
04/25/2022 02:37:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.08 on epoch=239
04/25/2022 02:37:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=244
04/25/2022 02:37:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
04/25/2022 02:37:37 - INFO - __main__ - Global step 500 Train loss 0.08 ACC 0.15625 on epoch=249
04/25/2022 02:37:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
04/25/2022 02:37:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=259
04/25/2022 02:37:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
04/25/2022 02:37:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
04/25/2022 02:37:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
04/25/2022 02:37:56 - INFO - __main__ - Global step 550 Train loss 0.06 ACC 0.1875 on epoch=274
04/25/2022 02:37:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
04/25/2022 02:38:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
04/25/2022 02:38:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
04/25/2022 02:38:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=294
04/25/2022 02:38:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
04/25/2022 02:38:15 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.15625 on epoch=299
04/25/2022 02:38:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
04/25/2022 02:38:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=309
04/25/2022 02:38:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
04/25/2022 02:38:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
04/25/2022 02:38:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
04/25/2022 02:38:33 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.1875 on epoch=324
04/25/2022 02:38:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
04/25/2022 02:38:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=334
04/25/2022 02:38:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=339
04/25/2022 02:38:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
04/25/2022 02:38:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
04/25/2022 02:38:52 - INFO - __main__ - Global step 700 Train loss 0.05 ACC 0.15625 on epoch=349
04/25/2022 02:38:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
04/25/2022 02:38:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
04/25/2022 02:39:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
04/25/2022 02:39:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
04/25/2022 02:39:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
04/25/2022 02:39:10 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.25 on epoch=374
04/25/2022 02:39:10 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=374, global_step=750
04/25/2022 02:39:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
04/25/2022 02:39:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
04/25/2022 02:39:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
04/25/2022 02:39:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
04/25/2022 02:39:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
04/25/2022 02:39:29 - INFO - __main__ - Global step 800 Train loss 0.03 ACC 0.15625 on epoch=399
04/25/2022 02:39:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
04/25/2022 02:39:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
04/25/2022 02:39:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
04/25/2022 02:39:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
04/25/2022 02:39:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
04/25/2022 02:39:48 - INFO - __main__ - Global step 850 Train loss 0.03 ACC 0.125 on epoch=424
04/25/2022 02:39:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
04/25/2022 02:39:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
04/25/2022 02:39:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
04/25/2022 02:40:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
04/25/2022 02:40:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
04/25/2022 02:40:06 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.1875 on epoch=449
04/25/2022 02:40:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
04/25/2022 02:40:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
04/25/2022 02:40:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
04/25/2022 02:40:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
04/25/2022 02:40:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
04/25/2022 02:40:25 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.1875 on epoch=474
04/25/2022 02:40:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
04/25/2022 02:40:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
04/25/2022 02:40:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
04/25/2022 02:40:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
04/25/2022 02:40:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/25/2022 02:40:43 - INFO - __main__ - Global step 1000 Train loss 0.03 ACC 0.25 on epoch=499
04/25/2022 02:40:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
04/25/2022 02:40:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
04/25/2022 02:40:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
04/25/2022 02:40:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
04/25/2022 02:40:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
04/25/2022 02:41:03 - INFO - __main__ - Global step 1050 Train loss 0.02 ACC 0.25 on epoch=524
04/25/2022 02:41:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
04/25/2022 02:41:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
04/25/2022 02:41:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
04/25/2022 02:41:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
04/25/2022 02:41:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
04/25/2022 02:41:21 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.25 on epoch=549
04/25/2022 02:41:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
04/25/2022 02:41:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
04/25/2022 02:41:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
04/25/2022 02:41:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
04/25/2022 02:41:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
04/25/2022 02:41:40 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.1875 on epoch=574
04/25/2022 02:41:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
04/25/2022 02:41:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/25/2022 02:41:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
04/25/2022 02:41:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
04/25/2022 02:41:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
04/25/2022 02:41:58 - INFO - __main__ - Global step 1200 Train loss 0.02 ACC 0.21875 on epoch=599
04/25/2022 02:42:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
04/25/2022 02:42:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
04/25/2022 02:42:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 02:42:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
04/25/2022 02:42:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
04/25/2022 02:42:17 - INFO - __main__ - Global step 1250 Train loss 0.02 ACC 0.25 on epoch=624
04/25/2022 02:42:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
04/25/2022 02:42:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
04/25/2022 02:42:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
04/25/2022 02:42:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
04/25/2022 02:42:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
04/25/2022 02:42:36 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.21875 on epoch=649
04/25/2022 02:42:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
04/25/2022 02:42:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
04/25/2022 02:42:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
04/25/2022 02:42:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/25/2022 02:42:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
04/25/2022 02:42:54 - INFO - __main__ - Global step 1350 Train loss 0.02 ACC 0.15625 on epoch=674
04/25/2022 02:42:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
04/25/2022 02:43:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
04/25/2022 02:43:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
04/25/2022 02:43:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
04/25/2022 02:43:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
04/25/2022 02:43:13 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.25 on epoch=699
04/25/2022 02:43:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
04/25/2022 02:43:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
04/25/2022 02:43:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
04/25/2022 02:43:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 02:43:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
04/25/2022 02:43:31 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.21875 on epoch=724
04/25/2022 02:43:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
04/25/2022 02:43:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
04/25/2022 02:43:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
04/25/2022 02:43:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
04/25/2022 02:43:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
04/25/2022 02:43:50 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.21875 on epoch=749
04/25/2022 02:43:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 02:43:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
04/25/2022 02:43:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
04/25/2022 02:44:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 02:44:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
04/25/2022 02:44:09 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.21875 on epoch=774
04/25/2022 02:44:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
04/25/2022 02:44:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
04/25/2022 02:44:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
04/25/2022 02:44:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
04/25/2022 02:44:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
04/25/2022 02:44:27 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.15625 on epoch=799
04/25/2022 02:44:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
04/25/2022 02:44:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
04/25/2022 02:44:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/25/2022 02:44:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
04/25/2022 02:44:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
04/25/2022 02:44:45 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.1875 on epoch=824
04/25/2022 02:44:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
04/25/2022 02:44:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
04/25/2022 02:44:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
04/25/2022 02:44:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/25/2022 02:45:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
04/25/2022 02:45:03 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.1875 on epoch=849
04/25/2022 02:45:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
04/25/2022 02:45:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
04/25/2022 02:45:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
04/25/2022 02:45:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
04/25/2022 02:45:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
04/25/2022 02:45:21 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.21875 on epoch=874
04/25/2022 02:45:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
04/25/2022 02:45:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 02:45:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 02:45:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 02:45:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 02:45:40 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.21875 on epoch=899
04/25/2022 02:45:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
04/25/2022 02:45:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/25/2022 02:45:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 02:45:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
04/25/2022 02:45:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
04/25/2022 02:45:58 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.21875 on epoch=924
04/25/2022 02:46:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 02:46:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
04/25/2022 02:46:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 02:46:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 02:46:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 02:46:16 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.1875 on epoch=949
04/25/2022 02:46:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
04/25/2022 02:46:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
04/25/2022 02:46:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 02:46:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 02:46:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 02:46:34 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.15625 on epoch=974
04/25/2022 02:46:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
04/25/2022 02:46:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
04/25/2022 02:46:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
04/25/2022 02:46:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
04/25/2022 02:46:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
04/25/2022 02:46:52 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:46:52 - INFO - __main__ - Printing 3 examples
04/25/2022 02:46:52 - INFO - __main__ -  [cosmos_qa] Why did they buy those products in specific ? [SEP] This morning I went to Wal - Mart to buy deodorant and soap . Funny that I should simultaneously run out of those things . While I was there , " Who Let the Dogs Out " was playing . It was playing when I came in , and I swear , it went on for like 6 minutes . [SEP]  (A) They wanted to spend as little money as possible . (B) They did n't like the products they already had . (C) They need them to keep themselves clean . (D) None of the above choices .
04/25/2022 02:46:52 - INFO - __main__ - ['They need them to keep themselves clean .']
04/25/2022 02:46:52 - INFO - __main__ -  [cosmos_qa] Why might the writer be agitated ? [SEP] I ' ve been awake for an hour and i ' ve already smoked 3 cigarettes . my computer is going to die soon , and by die i mean fucking die forever and it wo nt ever come back . i m scared . [SEP]  (A) The writer may feel guilty for smoking . (B) The writer might need more sleep . (C) The writer might be lonely . (D) None of the above choices .
04/25/2022 02:46:52 - INFO - __main__ - ['None of the above choices .']
04/25/2022 02:46:52 - INFO - __main__ -  [cosmos_qa] Why did the narrator feel they needed to go to their room ? [SEP] It was really hard to breath and I thought I was going to faint in the hallway in my towel . I got back to my room and took a zantac , and I was a little better . In bed though , I felt faint again but not nearly as severely , and my hands were shaking . [SEP]  (A) They were wanting a nap . (B) None of the above choices . (C) They needed sleep . (D) They needed to take medicine .
04/25/2022 02:46:52 - INFO - __main__ - ['They needed to take medicine .']
04/25/2022 02:46:52 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:46:52 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:46:52 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 02:46:52 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:46:52 - INFO - __main__ - Printing 3 examples
04/25/2022 02:46:52 - INFO - __main__ -  [cosmos_qa] What may be the reason for their situation ? [SEP] Well , after reading around , I came to the conclusion that it 's merely media influence . In almost everything I ' ve seen , the light represents good and the darkness is evil . Of course , I was pretty much guilty in putting that in most of my works . [SEP]  (A) They want to influence the public on purpose . (B) They have to publish new content quickly . (C) It is their job to influence the public . (D) None of the above choices .
04/25/2022 02:46:52 - INFO - __main__ - ['It is their job to influence the public .']
04/25/2022 02:46:52 - INFO - __main__ -  [cosmos_qa] What happened to the speakers mother ? [SEP] I do n't really have too much to write about today . Worked at white spot for six hours . It was pretty busy but nothing I could nt handle . Brent was a little chocked about me not showing up on monday but I could n't not go spread my mommas ashes and he did n't give me too much fuss because he knows usually i m an excellent worker . [SEP]  (A) The speakers mother has passed on . (B) The speakers mother is choked up . (C) None of the above choices . (D) The speakers mother wrote about today .
04/25/2022 02:46:52 - INFO - __main__ - ['The speakers mother has passed on .']
04/25/2022 02:46:52 - INFO - __main__ -  [cosmos_qa] What did the kids enjoy at the EIS cafe ? [SEP] Will had fun riding his little bike as well . After about an hour of bike riding , the kids were treated to an ice cream cone from " Homie " . He drives around post selling the best ice cream cones EVER from EIS Cafe , the ice cream cafe in town . They each got a strawberry cone and enjoyed them immensely . [SEP]  (A) They enjoyed the strawberry ice cream cone . (B) They loved the cone filled with chocolate . (C) They loved the vanilla cone . (D) None of the above choices .
04/25/2022 02:46:52 - INFO - __main__ - ['They enjoyed the strawberry ice cream cone .']
04/25/2022 02:46:52 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:46:52 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:46:52 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 02:46:53 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.25 on epoch=999
04/25/2022 02:46:53 - INFO - __main__ - save last model!
04/25/2022 02:46:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 02:46:53 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 02:46:53 - INFO - __main__ - Printing 3 examples
04/25/2022 02:46:53 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 02:46:53 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 02:46:53 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 02:46:53 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 02:46:53 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 02:46:53 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 02:46:53 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:46:53 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:46:54 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 02:47:08 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 02:47:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 02:47:09 - INFO - __main__ - Starting training!
04/25/2022 02:49:04 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_87_0.3_8_predictions.txt
04/25/2022 02:49:04 - INFO - __main__ - ACC on test data: 0.3320
04/25/2022 02:49:04 - INFO - __main__ - prefix=cosmos_qa_32_87, lr=0.3, bsz=8, dev_performance=0.25, test_performance=0.332
04/25/2022 02:49:04 - INFO - __main__ - Running ... prefix=cosmos_qa_32_87, lr=0.2, bsz=8 ...
04/25/2022 02:49:05 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:49:05 - INFO - __main__ - Printing 3 examples
04/25/2022 02:49:05 - INFO - __main__ -  [cosmos_qa] Why did they buy those products in specific ? [SEP] This morning I went to Wal - Mart to buy deodorant and soap . Funny that I should simultaneously run out of those things . While I was there , " Who Let the Dogs Out " was playing . It was playing when I came in , and I swear , it went on for like 6 minutes . [SEP]  (A) They wanted to spend as little money as possible . (B) They did n't like the products they already had . (C) They need them to keep themselves clean . (D) None of the above choices .
04/25/2022 02:49:05 - INFO - __main__ - ['They need them to keep themselves clean .']
04/25/2022 02:49:05 - INFO - __main__ -  [cosmos_qa] Why might the writer be agitated ? [SEP] I ' ve been awake for an hour and i ' ve already smoked 3 cigarettes . my computer is going to die soon , and by die i mean fucking die forever and it wo nt ever come back . i m scared . [SEP]  (A) The writer may feel guilty for smoking . (B) The writer might need more sleep . (C) The writer might be lonely . (D) None of the above choices .
04/25/2022 02:49:05 - INFO - __main__ - ['None of the above choices .']
04/25/2022 02:49:05 - INFO - __main__ -  [cosmos_qa] Why did the narrator feel they needed to go to their room ? [SEP] It was really hard to breath and I thought I was going to faint in the hallway in my towel . I got back to my room and took a zantac , and I was a little better . In bed though , I felt faint again but not nearly as severely , and my hands were shaking . [SEP]  (A) They were wanting a nap . (B) None of the above choices . (C) They needed sleep . (D) They needed to take medicine .
04/25/2022 02:49:05 - INFO - __main__ - ['They needed to take medicine .']
04/25/2022 02:49:05 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:49:05 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:49:05 - INFO - __main__ - Loaded 32 examples from train data
04/25/2022 02:49:05 - INFO - __main__ - Start tokenizing ... 32 instances
04/25/2022 02:49:05 - INFO - __main__ - Printing 3 examples
04/25/2022 02:49:05 - INFO - __main__ -  [cosmos_qa] What may be the reason for their situation ? [SEP] Well , after reading around , I came to the conclusion that it 's merely media influence . In almost everything I ' ve seen , the light represents good and the darkness is evil . Of course , I was pretty much guilty in putting that in most of my works . [SEP]  (A) They want to influence the public on purpose . (B) They have to publish new content quickly . (C) It is their job to influence the public . (D) None of the above choices .
04/25/2022 02:49:05 - INFO - __main__ - ['It is their job to influence the public .']
04/25/2022 02:49:05 - INFO - __main__ -  [cosmos_qa] What happened to the speakers mother ? [SEP] I do n't really have too much to write about today . Worked at white spot for six hours . It was pretty busy but nothing I could nt handle . Brent was a little chocked about me not showing up on monday but I could n't not go spread my mommas ashes and he did n't give me too much fuss because he knows usually i m an excellent worker . [SEP]  (A) The speakers mother has passed on . (B) The speakers mother is choked up . (C) None of the above choices . (D) The speakers mother wrote about today .
04/25/2022 02:49:05 - INFO - __main__ - ['The speakers mother has passed on .']
04/25/2022 02:49:05 - INFO - __main__ -  [cosmos_qa] What did the kids enjoy at the EIS cafe ? [SEP] Will had fun riding his little bike as well . After about an hour of bike riding , the kids were treated to an ice cream cone from " Homie " . He drives around post selling the best ice cream cones EVER from EIS Cafe , the ice cream cafe in town . They each got a strawberry cone and enjoyed them immensely . [SEP]  (A) They enjoyed the strawberry ice cream cone . (B) They loved the cone filled with chocolate . (C) They loved the vanilla cone . (D) None of the above choices .
04/25/2022 02:49:05 - INFO - __main__ - ['They enjoyed the strawberry ice cream cone .']
04/25/2022 02:49:05 - INFO - __main__ - Tokenizing Input ...
04/25/2022 02:49:05 - INFO - __main__ - Tokenizing Output ...
04/25/2022 02:49:05 - INFO - __main__ - Loaded 32 examples from dev data
04/25/2022 02:49:24 - INFO - __main__ - load prompt embedding from ckpt
04/25/2022 02:49:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
04/25/2022 02:49:25 - INFO - __main__ - Starting training!
04/25/2022 02:49:29 - INFO - __main__ - Step 10 Global step 10 Train loss 0.93 on epoch=4
04/25/2022 02:49:32 - INFO - __main__ - Step 20 Global step 20 Train loss 0.76 on epoch=9
04/25/2022 02:49:35 - INFO - __main__ - Step 30 Global step 30 Train loss 0.57 on epoch=14
04/25/2022 02:49:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=19
04/25/2022 02:49:42 - INFO - __main__ - Step 50 Global step 50 Train loss 0.39 on epoch=24
04/25/2022 02:49:45 - INFO - __main__ - Global step 50 Train loss 0.62 ACC 0.15625 on epoch=24
04/25/2022 02:49:45 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=24, global_step=50
04/25/2022 02:49:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.34 on epoch=29
04/25/2022 02:49:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.31 on epoch=34
04/25/2022 02:49:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=39
04/25/2022 02:49:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.34 on epoch=44
04/25/2022 02:50:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.31 on epoch=49
04/25/2022 02:50:03 - INFO - __main__ - Global step 100 Train loss 0.32 ACC 0.125 on epoch=49
04/25/2022 02:50:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.29 on epoch=54
04/25/2022 02:50:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.30 on epoch=59
04/25/2022 02:50:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=64
04/25/2022 02:50:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
04/25/2022 02:50:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=74
04/25/2022 02:50:22 - INFO - __main__ - Global step 150 Train loss 0.28 ACC 0.09375 on epoch=74
04/25/2022 02:50:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=79
04/25/2022 02:50:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.26 on epoch=84
04/25/2022 02:50:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=89
04/25/2022 02:50:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=94
04/25/2022 02:50:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
04/25/2022 02:50:41 - INFO - __main__ - Global step 200 Train loss 0.23 ACC 0.15625 on epoch=99
04/25/2022 02:50:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=104
04/25/2022 02:50:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.21 on epoch=109
04/25/2022 02:50:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.19 on epoch=114
04/25/2022 02:50:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=119
04/25/2022 02:50:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.18 on epoch=124
04/25/2022 02:51:00 - INFO - __main__ - Global step 250 Train loss 0.20 ACC 0.15625 on epoch=124
04/25/2022 02:51:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.19 on epoch=129
04/25/2022 02:51:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.18 on epoch=134
04/25/2022 02:51:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=139
04/25/2022 02:51:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.19 on epoch=144
04/25/2022 02:51:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.18 on epoch=149
04/25/2022 02:51:19 - INFO - __main__ - Global step 300 Train loss 0.19 ACC 0.15625 on epoch=149
04/25/2022 02:51:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.16 on epoch=154
04/25/2022 02:51:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.17 on epoch=159
04/25/2022 02:51:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.19 on epoch=164
04/25/2022 02:51:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.19 on epoch=169
04/25/2022 02:51:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=174
04/25/2022 02:51:37 - INFO - __main__ - Global step 350 Train loss 0.17 ACC 0.21875 on epoch=174
04/25/2022 02:51:37 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.21875 on epoch=174, global_step=350
04/25/2022 02:51:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.12 on epoch=179
04/25/2022 02:51:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.13 on epoch=184
04/25/2022 02:51:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
04/25/2022 02:51:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.13 on epoch=194
04/25/2022 02:51:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=199
04/25/2022 02:51:56 - INFO - __main__ - Global step 400 Train loss 0.14 ACC 0.15625 on epoch=199
04/25/2022 02:51:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.15 on epoch=204
04/25/2022 02:52:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.13 on epoch=209
04/25/2022 02:52:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=214
04/25/2022 02:52:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=219
04/25/2022 02:52:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.14 on epoch=224
04/25/2022 02:52:14 - INFO - __main__ - Global step 450 Train loss 0.14 ACC 0.09375 on epoch=224
04/25/2022 02:52:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.12 on epoch=229
04/25/2022 02:52:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.10 on epoch=234
04/25/2022 02:52:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=239
04/25/2022 02:52:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=244
04/25/2022 02:52:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=249
04/25/2022 02:52:33 - INFO - __main__ - Global step 500 Train loss 0.12 ACC 0.15625 on epoch=249
04/25/2022 02:52:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.12 on epoch=254
04/25/2022 02:52:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=259
04/25/2022 02:52:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
04/25/2022 02:52:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=269
04/25/2022 02:52:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.10 on epoch=274
04/25/2022 02:52:51 - INFO - __main__ - Global step 550 Train loss 0.11 ACC 0.125 on epoch=274
04/25/2022 02:52:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.10 on epoch=279
04/25/2022 02:52:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.11 on epoch=284
04/25/2022 02:53:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.09 on epoch=289
04/25/2022 02:53:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
04/25/2022 02:53:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=299
04/25/2022 02:53:09 - INFO - __main__ - Global step 600 Train loss 0.09 ACC 0.125 on epoch=299
04/25/2022 02:53:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=304
04/25/2022 02:53:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=309
04/25/2022 02:53:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=314
04/25/2022 02:53:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=319
04/25/2022 02:53:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=324
04/25/2022 02:53:28 - INFO - __main__ - Global step 650 Train loss 0.09 ACC 0.1875 on epoch=324
04/25/2022 02:53:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
04/25/2022 02:53:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=334
04/25/2022 02:53:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=339
04/25/2022 02:53:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.08 on epoch=344
04/25/2022 02:53:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
04/25/2022 02:53:46 - INFO - __main__ - Global step 700 Train loss 0.08 ACC 0.21875 on epoch=349
04/25/2022 02:53:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
04/25/2022 02:53:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=359
04/25/2022 02:53:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=364
04/25/2022 02:53:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
04/25/2022 02:54:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=374
04/25/2022 02:54:05 - INFO - __main__ - Global step 750 Train loss 0.06 ACC 0.15625 on epoch=374
04/25/2022 02:54:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=379
04/25/2022 02:54:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
04/25/2022 02:54:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
04/25/2022 02:54:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
04/25/2022 02:54:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
04/25/2022 02:54:24 - INFO - __main__ - Global step 800 Train loss 0.07 ACC 0.125 on epoch=399
04/25/2022 02:54:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
04/25/2022 02:54:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
04/25/2022 02:54:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=414
04/25/2022 02:54:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=419
04/25/2022 02:54:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=424
04/25/2022 02:54:42 - INFO - __main__ - Global step 850 Train loss 0.06 ACC 0.21875 on epoch=424
04/25/2022 02:54:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
04/25/2022 02:54:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=434
04/25/2022 02:54:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=439
04/25/2022 02:54:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=444
04/25/2022 02:54:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=449
04/25/2022 02:55:01 - INFO - __main__ - Global step 900 Train loss 0.06 ACC 0.125 on epoch=449
04/25/2022 02:55:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
04/25/2022 02:55:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
04/25/2022 02:55:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
04/25/2022 02:55:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
04/25/2022 02:55:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
04/25/2022 02:55:20 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.125 on epoch=474
04/25/2022 02:55:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
04/25/2022 02:55:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
04/25/2022 02:55:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
04/25/2022 02:55:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
04/25/2022 02:55:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
04/25/2022 02:55:38 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.1875 on epoch=499
04/25/2022 02:55:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
04/25/2022 02:55:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
04/25/2022 02:55:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
04/25/2022 02:55:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
04/25/2022 02:55:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
04/25/2022 02:55:57 - INFO - __main__ - Global step 1050 Train loss 0.05 ACC 0.21875 on epoch=524
04/25/2022 02:56:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
04/25/2022 02:56:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
04/25/2022 02:56:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
04/25/2022 02:56:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
04/25/2022 02:56:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
04/25/2022 02:56:15 - INFO - __main__ - Global step 1100 Train loss 0.04 ACC 0.09375 on epoch=549
04/25/2022 02:56:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
04/25/2022 02:56:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
04/25/2022 02:56:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
04/25/2022 02:56:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=569
04/25/2022 02:56:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
04/25/2022 02:56:34 - INFO - __main__ - Global step 1150 Train loss 0.04 ACC 0.15625 on epoch=574
04/25/2022 02:56:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
04/25/2022 02:56:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
04/25/2022 02:56:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
04/25/2022 02:56:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
04/25/2022 02:56:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
04/25/2022 02:56:52 - INFO - __main__ - Global step 1200 Train loss 0.03 ACC 0.1875 on epoch=599
04/25/2022 02:56:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
04/25/2022 02:56:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
04/25/2022 02:57:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
04/25/2022 02:57:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
04/25/2022 02:57:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
04/25/2022 02:57:11 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.15625 on epoch=624
04/25/2022 02:57:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
04/25/2022 02:57:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
04/25/2022 02:57:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
04/25/2022 02:57:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
04/25/2022 02:57:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
04/25/2022 02:57:29 - INFO - __main__ - Global step 1300 Train loss 0.03 ACC 0.15625 on epoch=649
04/25/2022 02:57:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
04/25/2022 02:57:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
04/25/2022 02:57:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
04/25/2022 02:57:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
04/25/2022 02:57:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=674
04/25/2022 02:57:48 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.21875 on epoch=674
04/25/2022 02:57:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
04/25/2022 02:57:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
04/25/2022 02:57:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
04/25/2022 02:58:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
04/25/2022 02:58:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
04/25/2022 02:58:07 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.15625 on epoch=699
04/25/2022 02:58:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
04/25/2022 02:58:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
04/25/2022 02:58:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
04/25/2022 02:58:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
04/25/2022 02:58:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
04/25/2022 02:58:25 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.09375 on epoch=724
04/25/2022 02:58:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
04/25/2022 02:58:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=734
04/25/2022 02:58:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
04/25/2022 02:58:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
04/25/2022 02:58:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
04/25/2022 02:58:44 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 0.21875 on epoch=749
04/25/2022 02:58:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
04/25/2022 02:58:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
04/25/2022 02:58:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
04/25/2022 02:58:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
04/25/2022 02:59:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
04/25/2022 02:59:02 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.125 on epoch=774
04/25/2022 02:59:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
04/25/2022 02:59:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
04/25/2022 02:59:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
04/25/2022 02:59:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
04/25/2022 02:59:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
04/25/2022 02:59:21 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.15625 on epoch=799
04/25/2022 02:59:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
04/25/2022 02:59:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
04/25/2022 02:59:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
04/25/2022 02:59:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
04/25/2022 02:59:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
04/25/2022 02:59:39 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 0.1875 on epoch=824
04/25/2022 02:59:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
04/25/2022 02:59:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=834
04/25/2022 02:59:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
04/25/2022 02:59:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
04/25/2022 02:59:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
04/25/2022 02:59:58 - INFO - __main__ - Global step 1700 Train loss 0.03 ACC 0.125 on epoch=849
04/25/2022 03:00:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
04/25/2022 03:00:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
04/25/2022 03:00:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
04/25/2022 03:00:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
04/25/2022 03:00:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
04/25/2022 03:00:18 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.21875 on epoch=874
04/25/2022 03:00:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
04/25/2022 03:00:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
04/25/2022 03:00:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
04/25/2022 03:00:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
04/25/2022 03:00:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
04/25/2022 03:00:37 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.25 on epoch=899
04/25/2022 03:00:37 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.25 on epoch=899, global_step=1800
04/25/2022 03:00:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
04/25/2022 03:00:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
04/25/2022 03:00:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
04/25/2022 03:00:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
04/25/2022 03:00:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
04/25/2022 03:00:55 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.15625 on epoch=924
04/25/2022 03:00:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
04/25/2022 03:01:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
04/25/2022 03:01:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
04/25/2022 03:01:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
04/25/2022 03:01:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
04/25/2022 03:01:14 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.15625 on epoch=949
04/25/2022 03:01:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
04/25/2022 03:01:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
04/25/2022 03:01:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
04/25/2022 03:01:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
04/25/2022 03:01:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
04/25/2022 03:01:33 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.1875 on epoch=974
04/25/2022 03:01:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
04/25/2022 03:01:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
04/25/2022 03:01:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
04/25/2022 03:01:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
04/25/2022 03:01:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
04/25/2022 03:01:51 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.21875 on epoch=999
04/25/2022 03:01:51 - INFO - __main__ - save last model!
04/25/2022 03:01:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
04/25/2022 03:01:51 - INFO - __main__ - Start tokenizing ... 1000 instances
04/25/2022 03:01:51 - INFO - __main__ - Printing 3 examples
04/25/2022 03:01:51 - INFO - __main__ -  [cosmos_qa] Why is this person asking about divorce ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) If he gets married in the church he wo nt have to get a divorce . (B) He wants to get married to a different person . (C) He wants to know if he does nt like this girl can he divorce her ? (D) None of the above choices .
04/25/2022 03:01:51 - INFO - __main__ - ['He wants to get married to a different person .']
04/25/2022 03:01:51 - INFO - __main__ -  [cosmos_qa] Why is he worried about getting married ? [SEP] Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ? [SEP]  (A) He was married before and she might come back one day . (B) He wants the girl he is going to marry get in trouble . (C) He knows that he will be committing polygamy . (D) None of the above choices .
04/25/2022 03:01:51 - INFO - __main__ - ['He was married before and she might come back one day .']
04/25/2022 03:01:51 - INFO - __main__ -  [cosmos_qa] What may happen after the young man makes his call ? [SEP] I was walking home from the store , when I saw an old man laying on the sidewalk , bleeding . The right side of his face was all covered in blood . He was conscious but seemed dazed and probably intoxicated . Nearby there was a young man dialing his cell phone . [SEP]  (A) An ambulance would likely come to the scene (B) The taxi would pick up the young man (C) None of the above choices . (D) The bus would arrive at the stop soon
04/25/2022 03:01:51 - INFO - __main__ - ['An ambulance would likely come to the scene']
04/25/2022 03:01:51 - INFO - __main__ - Tokenizing Input ...
04/25/2022 03:01:52 - INFO - __main__ - Tokenizing Output ...
04/25/2022 03:01:53 - INFO - __main__ - Loaded 1000 examples from test data
04/25/2022 03:03:54 - INFO - __main__ - Saved prediction in models/T5-large-maml-noqa2qa-3e-5-2-5000-5e-1/singletask-cosmos_qa/cosmos_qa_32_87_0.2_8_predictions.txt
04/25/2022 03:03:54 - INFO - __main__ - ACC on test data: 0.2940
04/25/2022 03:03:55 - INFO - __main__ - prefix=cosmos_qa_32_87, lr=0.2, bsz=8, dev_performance=0.25, test_performance=0.294
