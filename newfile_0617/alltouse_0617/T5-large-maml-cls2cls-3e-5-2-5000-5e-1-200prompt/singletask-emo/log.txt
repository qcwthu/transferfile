05/25/2022 22:32:25 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-200prompt-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=200, cuda='0,1')
05/25/2022 22:32:25 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo
05/25/2022 22:32:25 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-200prompt-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=200, cuda='0,1')
05/25/2022 22:32:25 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo
05/25/2022 22:32:27 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/25/2022 22:32:27 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/25/2022 22:32:27 - INFO - __main__ - args.device: cuda:1
05/25/2022 22:32:27 - INFO - __main__ - args.device: cuda:0
05/25/2022 22:32:27 - INFO - __main__ - Using 2 gpus
05/25/2022 22:32:27 - INFO - __main__ - Using 2 gpus
05/25/2022 22:32:27 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/25/2022 22:32:27 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/25/2022 22:32:31 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
05/25/2022 22:32:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 22:32:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 22:32:32 - INFO - __main__ - Printing 3 examples
05/25/2022 22:32:32 - INFO - __main__ - Printing 3 examples
05/25/2022 22:32:32 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 22:32:32 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 22:32:32 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 22:32:32 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:32:32 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:32:32 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:32:32 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:32:32 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 22:32:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 22:32:32 - INFO - __main__ - Printing 3 examples
05/25/2022 22:32:32 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:32:32 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 22:32:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 22:32:32 - INFO - __main__ - Printing 3 examples
05/25/2022 22:32:32 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/25/2022 22:32:32 - INFO - __main__ - ['others']
05/25/2022 22:32:32 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:32:32 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:32:32 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:32:32 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 22:32:32 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 22:32:50 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 22:32:50 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 22:32:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 22:32:51 - INFO - __main__ - Starting training!
05/25/2022 22:32:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 22:32:56 - INFO - __main__ - Starting training!
05/25/2022 22:33:00 - INFO - __main__ - Step 10 Global step 10 Train loss 2.77 on epoch=2
05/25/2022 22:33:02 - INFO - __main__ - Step 20 Global step 20 Train loss 1.16 on epoch=4
05/25/2022 22:33:05 - INFO - __main__ - Step 30 Global step 30 Train loss 1.03 on epoch=7
05/25/2022 22:33:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.93 on epoch=9
05/25/2022 22:33:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.85 on epoch=12
05/25/2022 22:33:11 - INFO - __main__ - Global step 50 Train loss 1.35 Classification-F1 0.1 on epoch=12
05/25/2022 22:33:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/25/2022 22:33:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.86 on epoch=14
05/25/2022 22:33:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.84 on epoch=17
05/25/2022 22:33:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.85 on epoch=19
05/25/2022 22:33:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=22
05/25/2022 22:33:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.87 on epoch=24
05/25/2022 22:33:25 - INFO - __main__ - Global step 100 Train loss 0.85 Classification-F1 0.4241543340380549 on epoch=24
05/25/2022 22:33:25 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.4241543340380549 on epoch=24, global_step=100
05/25/2022 22:33:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.77 on epoch=27
05/25/2022 22:33:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.76 on epoch=29
05/25/2022 22:33:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.73 on epoch=32
05/25/2022 22:33:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.77 on epoch=34
05/25/2022 22:33:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.72 on epoch=37
05/25/2022 22:33:39 - INFO - __main__ - Global step 150 Train loss 0.75 Classification-F1 0.41631944444444446 on epoch=37
05/25/2022 22:33:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.71 on epoch=39
05/25/2022 22:33:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.77 on epoch=42
05/25/2022 22:33:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.62 on epoch=44
05/25/2022 22:33:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.66 on epoch=47
05/25/2022 22:33:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=49
05/25/2022 22:33:52 - INFO - __main__ - Global step 200 Train loss 0.66 Classification-F1 0.5362701814314716 on epoch=49
05/25/2022 22:33:52 - INFO - __main__ - Saving model with best Classification-F1: 0.4241543340380549 -> 0.5362701814314716 on epoch=49, global_step=200
05/25/2022 22:33:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.61 on epoch=52
05/25/2022 22:33:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=54
05/25/2022 22:34:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=57
05/25/2022 22:34:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=59
05/25/2022 22:34:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.59 on epoch=62
05/25/2022 22:34:06 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.6564372119815668 on epoch=62
05/25/2022 22:34:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5362701814314716 -> 0.6564372119815668 on epoch=62, global_step=250
05/25/2022 22:34:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=64
05/25/2022 22:34:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=67
05/25/2022 22:34:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=69
05/25/2022 22:34:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.57 on epoch=72
05/25/2022 22:34:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=74
05/25/2022 22:34:20 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.5888278388278388 on epoch=74
05/25/2022 22:34:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=77
05/25/2022 22:34:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=79
05/25/2022 22:34:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=82
05/25/2022 22:34:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=84
05/25/2022 22:34:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=87
05/25/2022 22:34:33 - INFO - __main__ - Global step 350 Train loss 0.34 Classification-F1 0.687055265654649 on epoch=87
05/25/2022 22:34:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6564372119815668 -> 0.687055265654649 on epoch=87, global_step=350
05/25/2022 22:34:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=89
05/25/2022 22:34:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=92
05/25/2022 22:34:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=94
05/25/2022 22:34:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=97
05/25/2022 22:34:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=99
05/25/2022 22:34:47 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.568613676372297 on epoch=99
05/25/2022 22:34:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.31 on epoch=102
05/25/2022 22:34:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=104
05/25/2022 22:34:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=107
05/25/2022 22:34:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.15 on epoch=109
05/25/2022 22:35:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=112
05/25/2022 22:35:01 - INFO - __main__ - Global step 450 Train loss 0.26 Classification-F1 0.7183823529411765 on epoch=112
05/25/2022 22:35:01 - INFO - __main__ - Saving model with best Classification-F1: 0.687055265654649 -> 0.7183823529411765 on epoch=112, global_step=450
05/25/2022 22:35:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=114
05/25/2022 22:35:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=117
05/25/2022 22:35:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.16 on epoch=119
05/25/2022 22:35:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=122
05/25/2022 22:35:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.11 on epoch=124
05/25/2022 22:35:14 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.684726174826286 on epoch=124
05/25/2022 22:35:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=127
05/25/2022 22:35:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.16 on epoch=129
05/25/2022 22:35:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=132
05/25/2022 22:35:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=134
05/25/2022 22:35:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=137
05/25/2022 22:35:28 - INFO - __main__ - Global step 550 Train loss 0.17 Classification-F1 0.6957264957264956 on epoch=137
05/25/2022 22:35:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=139
05/25/2022 22:35:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=142
05/25/2022 22:35:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.12 on epoch=144
05/25/2022 22:35:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=147
05/25/2022 22:35:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=149
05/25/2022 22:35:42 - INFO - __main__ - Global step 600 Train loss 0.14 Classification-F1 0.6989233193277311 on epoch=149
05/25/2022 22:35:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=152
05/25/2022 22:35:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=154
05/25/2022 22:35:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=157
05/25/2022 22:35:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=159
05/25/2022 22:35:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=162
05/25/2022 22:35:55 - INFO - __main__ - Global step 650 Train loss 0.13 Classification-F1 0.7207792207792207 on epoch=162
05/25/2022 22:35:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7183823529411765 -> 0.7207792207792207 on epoch=162, global_step=650
05/25/2022 22:35:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=164
05/25/2022 22:36:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=167
05/25/2022 22:36:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=169
05/25/2022 22:36:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=172
05/25/2022 22:36:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=174
05/25/2022 22:36:09 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.6980498120300752 on epoch=174
05/25/2022 22:36:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=177
05/25/2022 22:36:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=179
05/25/2022 22:36:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=182
05/25/2022 22:36:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=184
05/25/2022 22:36:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=187
05/25/2022 22:36:23 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.7287031799899447 on epoch=187
05/25/2022 22:36:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7207792207792207 -> 0.7287031799899447 on epoch=187, global_step=750
05/25/2022 22:36:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=189
05/25/2022 22:36:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=192
05/25/2022 22:36:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=194
05/25/2022 22:36:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=197
05/25/2022 22:36:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=199
05/25/2022 22:36:37 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.6802452236719478 on epoch=199
05/25/2022 22:36:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=202
05/25/2022 22:36:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=204
05/25/2022 22:36:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=207
05/25/2022 22:36:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=209
05/25/2022 22:36:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=212
05/25/2022 22:36:50 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.6903044871794872 on epoch=212
05/25/2022 22:36:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=214
05/25/2022 22:36:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=217
05/25/2022 22:36:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=219
05/25/2022 22:37:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=222
05/25/2022 22:37:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=224
05/25/2022 22:37:04 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.7425920688788336 on epoch=224
05/25/2022 22:37:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7287031799899447 -> 0.7425920688788336 on epoch=224, global_step=900
05/25/2022 22:37:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=227
05/25/2022 22:37:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=229
05/25/2022 22:37:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
05/25/2022 22:37:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
05/25/2022 22:37:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=237
05/25/2022 22:37:18 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.6806617692497836 on epoch=237
05/25/2022 22:37:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=239
05/25/2022 22:37:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
05/25/2022 22:37:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=244
05/25/2022 22:37:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
05/25/2022 22:37:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
05/25/2022 22:37:32 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.7136292016806723 on epoch=249
05/25/2022 22:37:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=252
05/25/2022 22:37:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
05/25/2022 22:37:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
05/25/2022 22:37:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
05/25/2022 22:37:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
05/25/2022 22:37:46 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.729520173453997 on epoch=262
05/25/2022 22:37:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=264
05/25/2022 22:37:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
05/25/2022 22:37:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
05/25/2022 22:37:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/25/2022 22:37:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=274
05/25/2022 22:38:00 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.7283068783068782 on epoch=274
05/25/2022 22:38:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
05/25/2022 22:38:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=279
05/25/2022 22:38:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/25/2022 22:38:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/25/2022 22:38:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=287
05/25/2022 22:38:14 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.6974972943722944 on epoch=287
05/25/2022 22:38:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
05/25/2022 22:38:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
05/25/2022 22:38:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=294
05/25/2022 22:38:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
05/25/2022 22:38:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
05/25/2022 22:38:28 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.6406899100338965 on epoch=299
05/25/2022 22:38:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
05/25/2022 22:38:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
05/25/2022 22:38:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
05/25/2022 22:38:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/25/2022 22:38:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=312
05/25/2022 22:38:42 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.7125490196078432 on epoch=312
05/25/2022 22:38:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/25/2022 22:38:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/25/2022 22:38:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
05/25/2022 22:38:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/25/2022 22:38:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/25/2022 22:38:56 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6840065960228714 on epoch=324
05/25/2022 22:38:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/25/2022 22:39:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=329
05/25/2022 22:39:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/25/2022 22:39:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/25/2022 22:39:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
05/25/2022 22:39:10 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.6520135566188198 on epoch=337
05/25/2022 22:39:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
05/25/2022 22:39:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/25/2022 22:39:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/25/2022 22:39:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/25/2022 22:39:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
05/25/2022 22:39:24 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.6106951871657755 on epoch=349
05/25/2022 22:39:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/25/2022 22:39:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/25/2022 22:39:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/25/2022 22:39:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/25/2022 22:39:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/25/2022 22:39:38 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.6980498120300752 on epoch=362
05/25/2022 22:39:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/25/2022 22:39:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
05/25/2022 22:39:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/25/2022 22:39:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/25/2022 22:39:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=374
05/25/2022 22:39:51 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7304478297421845 on epoch=374
05/25/2022 22:39:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/25/2022 22:39:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
05/25/2022 22:39:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/25/2022 22:40:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/25/2022 22:40:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/25/2022 22:40:05 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6798809523809524 on epoch=387
05/25/2022 22:40:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
05/25/2022 22:40:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=392
05/25/2022 22:40:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
05/25/2022 22:40:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/25/2022 22:40:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/25/2022 22:40:19 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.679040964694615 on epoch=399
05/25/2022 22:40:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
05/25/2022 22:40:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/25/2022 22:40:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
05/25/2022 22:40:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
05/25/2022 22:40:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/25/2022 22:40:33 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6883982683982683 on epoch=412
05/25/2022 22:40:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/25/2022 22:40:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/25/2022 22:40:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/25/2022 22:40:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/25/2022 22:40:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/25/2022 22:40:47 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.664823717948718 on epoch=424
05/25/2022 22:40:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
05/25/2022 22:40:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
05/25/2022 22:40:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
05/25/2022 22:40:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/25/2022 22:41:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/25/2022 22:41:01 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.714032180818193 on epoch=437
05/25/2022 22:41:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/25/2022 22:41:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/25/2022 22:41:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/25/2022 22:41:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/25/2022 22:41:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/25/2022 22:41:15 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6973060344827586 on epoch=449
05/25/2022 22:41:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/25/2022 22:41:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=454
05/25/2022 22:41:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/25/2022 22:41:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/25/2022 22:41:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=462
05/25/2022 22:41:29 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.7337029569892473 on epoch=462
05/25/2022 22:41:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/25/2022 22:41:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/25/2022 22:41:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/25/2022 22:41:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/25/2022 22:41:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/25/2022 22:41:43 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6809027777777777 on epoch=474
05/25/2022 22:41:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/25/2022 22:41:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/25/2022 22:41:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/25/2022 22:41:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/25/2022 22:41:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
05/25/2022 22:41:57 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6631944444444444 on epoch=487
05/25/2022 22:42:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=489
05/25/2022 22:42:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/25/2022 22:42:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/25/2022 22:42:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/25/2022 22:42:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/25/2022 22:42:11 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6638823574307445 on epoch=499
05/25/2022 22:42:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
05/25/2022 22:42:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/25/2022 22:42:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/25/2022 22:42:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/25/2022 22:42:24 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/25/2022 22:42:25 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6595959595959596 on epoch=512
05/25/2022 22:42:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/25/2022 22:42:30 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/25/2022 22:42:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/25/2022 22:42:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/25/2022 22:42:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/25/2022 22:42:39 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.7034076054664291 on epoch=524
05/25/2022 22:42:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/25/2022 22:42:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/25/2022 22:42:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=532
05/25/2022 22:42:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/25/2022 22:42:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/25/2022 22:42:53 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.696859335839599 on epoch=537
05/25/2022 22:42:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/25/2022 22:42:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=542
05/25/2022 22:43:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/25/2022 22:43:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
05/25/2022 22:43:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
05/25/2022 22:43:08 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.6786492374727668 on epoch=549
05/25/2022 22:43:10 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/25/2022 22:43:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/25/2022 22:43:15 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/25/2022 22:43:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/25/2022 22:43:20 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/25/2022 22:43:22 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7144153225806451 on epoch=562
05/25/2022 22:43:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
05/25/2022 22:43:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/25/2022 22:43:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/25/2022 22:43:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
05/25/2022 22:43:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/25/2022 22:43:36 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6095734126984127 on epoch=574
05/25/2022 22:43:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/25/2022 22:43:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/25/2022 22:43:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/25/2022 22:43:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/25/2022 22:43:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/25/2022 22:43:50 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.6541125541125541 on epoch=587
05/25/2022 22:43:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/25/2022 22:43:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/25/2022 22:43:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/25/2022 22:44:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/25/2022 22:44:03 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/25/2022 22:44:04 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6015085061137693 on epoch=599
05/25/2022 22:44:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/25/2022 22:44:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/25/2022 22:44:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=607
05/25/2022 22:44:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/25/2022 22:44:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/25/2022 22:44:18 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6806617692497836 on epoch=612
05/25/2022 22:44:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=614
05/25/2022 22:44:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/25/2022 22:44:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/25/2022 22:44:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/25/2022 22:44:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/25/2022 22:44:32 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6897916666666666 on epoch=624
05/25/2022 22:44:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/25/2022 22:44:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/25/2022 22:44:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/25/2022 22:44:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/25/2022 22:44:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=637
05/25/2022 22:44:46 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6371360665478312 on epoch=637
05/25/2022 22:44:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/25/2022 22:44:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/25/2022 22:44:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/25/2022 22:44:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/25/2022 22:44:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/25/2022 22:45:00 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6945193355119825 on epoch=649
05/25/2022 22:45:02 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/25/2022 22:45:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/25/2022 22:45:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/25/2022 22:45:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/25/2022 22:45:13 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/25/2022 22:45:14 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6506190882149961 on epoch=662
05/25/2022 22:45:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/25/2022 22:45:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/25/2022 22:45:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/25/2022 22:45:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/25/2022 22:45:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
05/25/2022 22:45:28 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.709168956043956 on epoch=674
05/25/2022 22:45:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/25/2022 22:45:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/25/2022 22:45:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/25/2022 22:45:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/25/2022 22:45:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/25/2022 22:45:42 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.709168956043956 on epoch=687
05/25/2022 22:45:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/25/2022 22:45:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/25/2022 22:45:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/25/2022 22:45:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/25/2022 22:45:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/25/2022 22:45:56 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6945193355119825 on epoch=699
05/25/2022 22:45:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/25/2022 22:46:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/25/2022 22:46:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/25/2022 22:46:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/25/2022 22:46:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/25/2022 22:46:10 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7144153225806451 on epoch=712
05/25/2022 22:46:13 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/25/2022 22:46:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/25/2022 22:46:18 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/25/2022 22:46:20 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
05/25/2022 22:46:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/25/2022 22:46:24 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7117018398268399 on epoch=724
05/25/2022 22:46:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/25/2022 22:46:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/25/2022 22:46:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/25/2022 22:46:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/25/2022 22:46:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
05/25/2022 22:46:38 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6973060344827586 on epoch=737
05/25/2022 22:46:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/25/2022 22:46:43 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/25/2022 22:46:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/25/2022 22:46:48 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/25/2022 22:46:51 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/25/2022 22:46:52 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 22:46:52 - INFO - __main__ - Printing 3 examples
05/25/2022 22:46:52 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 22:46:52 - INFO - __main__ - ['others']
05/25/2022 22:46:52 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 22:46:52 - INFO - __main__ - ['others']
05/25/2022 22:46:52 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 22:46:52 - INFO - __main__ - ['others']
05/25/2022 22:46:52 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:46:52 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:46:52 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.678219696969697 on epoch=749
05/25/2022 22:46:52 - INFO - __main__ - save last model!
05/25/2022 22:46:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 22:46:52 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 22:46:52 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 22:46:52 - INFO - __main__ - Printing 3 examples
05/25/2022 22:46:52 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/25/2022 22:46:52 - INFO - __main__ - ['others']
05/25/2022 22:46:52 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/25/2022 22:46:52 - INFO - __main__ - ['others']
05/25/2022 22:46:52 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/25/2022 22:46:52 - INFO - __main__ - ['others']
05/25/2022 22:46:52 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:46:52 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 22:46:52 - INFO - __main__ - Printing 3 examples
05/25/2022 22:46:52 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 22:46:52 - INFO - __main__ - ['others']
05/25/2022 22:46:52 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 22:46:52 - INFO - __main__ - ['others']
05/25/2022 22:46:52 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 22:46:52 - INFO - __main__ - ['others']
05/25/2022 22:46:52 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:46:52 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:46:52 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 22:46:54 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:47:00 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 22:47:11 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 22:47:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 22:47:12 - INFO - __main__ - Starting training!
05/25/2022 22:48:50 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_100_0.5_8_predictions.txt
05/25/2022 22:48:50 - INFO - __main__ - Classification-F1 on test data: 0.1257
05/25/2022 22:48:50 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.7425920688788336, test_performance=0.1257489402240343
05/25/2022 22:48:50 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
05/25/2022 22:48:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 22:48:51 - INFO - __main__ - Printing 3 examples
05/25/2022 22:48:51 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 22:48:51 - INFO - __main__ - ['others']
05/25/2022 22:48:51 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 22:48:51 - INFO - __main__ - ['others']
05/25/2022 22:48:51 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 22:48:51 - INFO - __main__ - ['others']
05/25/2022 22:48:51 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:48:51 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:48:51 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 22:48:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 22:48:51 - INFO - __main__ - Printing 3 examples
05/25/2022 22:48:51 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/25/2022 22:48:51 - INFO - __main__ - ['others']
05/25/2022 22:48:51 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/25/2022 22:48:51 - INFO - __main__ - ['others']
05/25/2022 22:48:51 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/25/2022 22:48:51 - INFO - __main__ - ['others']
05/25/2022 22:48:51 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:48:51 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:48:51 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 22:49:06 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 22:49:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 22:49:07 - INFO - __main__ - Starting training!
05/25/2022 22:49:10 - INFO - __main__ - Step 10 Global step 10 Train loss 2.64 on epoch=2
05/25/2022 22:49:13 - INFO - __main__ - Step 20 Global step 20 Train loss 1.22 on epoch=4
05/25/2022 22:49:15 - INFO - __main__ - Step 30 Global step 30 Train loss 1.07 on epoch=7
05/25/2022 22:49:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.99 on epoch=9
05/25/2022 22:49:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.90 on epoch=12
05/25/2022 22:49:21 - INFO - __main__ - Global step 50 Train loss 1.36 Classification-F1 0.1 on epoch=12
05/25/2022 22:49:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/25/2022 22:49:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.86 on epoch=14
05/25/2022 22:49:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.87 on epoch=17
05/25/2022 22:49:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=19
05/25/2022 22:49:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.81 on epoch=22
05/25/2022 22:49:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.76 on epoch=24
05/25/2022 22:49:36 - INFO - __main__ - Global step 100 Train loss 0.84 Classification-F1 0.24673489278752436 on epoch=24
05/25/2022 22:49:36 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.24673489278752436 on epoch=24, global_step=100
05/25/2022 22:49:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.76 on epoch=27
05/25/2022 22:49:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=29
05/25/2022 22:49:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.77 on epoch=32
05/25/2022 22:49:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.79 on epoch=34
05/25/2022 22:49:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.83 on epoch=37
05/25/2022 22:49:50 - INFO - __main__ - Global step 150 Train loss 0.79 Classification-F1 0.4682247992863515 on epoch=37
05/25/2022 22:49:50 - INFO - __main__ - Saving model with best Classification-F1: 0.24673489278752436 -> 0.4682247992863515 on epoch=37, global_step=150
05/25/2022 22:49:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.77 on epoch=39
05/25/2022 22:49:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=42
05/25/2022 22:49:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.75 on epoch=44
05/25/2022 22:50:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.72 on epoch=47
05/25/2022 22:50:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.65 on epoch=49
05/25/2022 22:50:04 - INFO - __main__ - Global step 200 Train loss 0.74 Classification-F1 0.5705603080812536 on epoch=49
05/25/2022 22:50:04 - INFO - __main__ - Saving model with best Classification-F1: 0.4682247992863515 -> 0.5705603080812536 on epoch=49, global_step=200
05/25/2022 22:50:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.68 on epoch=52
05/25/2022 22:50:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.67 on epoch=54
05/25/2022 22:50:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.62 on epoch=57
05/25/2022 22:50:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.62 on epoch=59
05/25/2022 22:50:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=62
05/25/2022 22:50:19 - INFO - __main__ - Global step 250 Train loss 0.62 Classification-F1 0.5621947496947497 on epoch=62
05/25/2022 22:50:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.61 on epoch=64
05/25/2022 22:50:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=67
05/25/2022 22:50:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=69
05/25/2022 22:50:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.58 on epoch=72
05/25/2022 22:50:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=74
05/25/2022 22:50:32 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.5133800621056992 on epoch=74
05/25/2022 22:50:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=77
05/25/2022 22:50:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=79
05/25/2022 22:50:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=82
05/25/2022 22:50:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=84
05/25/2022 22:50:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=87
05/25/2022 22:50:46 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.6609228734228734 on epoch=87
05/25/2022 22:50:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5705603080812536 -> 0.6609228734228734 on epoch=87, global_step=350
05/25/2022 22:50:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=89
05/25/2022 22:50:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=92
05/25/2022 22:50:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=94
05/25/2022 22:50:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=97
05/25/2022 22:50:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.32 on epoch=99
05/25/2022 22:51:00 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.5292663043478261 on epoch=99
05/25/2022 22:51:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=102
05/25/2022 22:51:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=104
05/25/2022 22:51:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=107
05/25/2022 22:51:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=109
05/25/2022 22:51:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=112
05/25/2022 22:51:14 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.5839224394958156 on epoch=112
05/25/2022 22:51:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=114
05/25/2022 22:51:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=117
05/25/2022 22:51:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=119
05/25/2022 22:51:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=122
05/25/2022 22:51:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=124
05/25/2022 22:51:28 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.6238095238095238 on epoch=124
05/25/2022 22:51:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=127
05/25/2022 22:51:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=129
05/25/2022 22:51:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=132
05/25/2022 22:51:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=134
05/25/2022 22:51:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=137
05/25/2022 22:51:41 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.6358630952380953 on epoch=137
05/25/2022 22:51:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=139
05/25/2022 22:51:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=142
05/25/2022 22:51:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=144
05/25/2022 22:51:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=147
05/25/2022 22:51:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=149
05/25/2022 22:51:55 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.6011904761904763 on epoch=149
05/25/2022 22:51:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=152
05/25/2022 22:52:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=154
05/25/2022 22:52:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=157
05/25/2022 22:52:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=159
05/25/2022 22:52:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=162
05/25/2022 22:52:10 - INFO - __main__ - Global step 650 Train loss 0.15 Classification-F1 0.6514182920389316 on epoch=162
05/25/2022 22:52:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=164
05/25/2022 22:52:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.17 on epoch=167
05/25/2022 22:52:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=169
05/25/2022 22:52:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=172
05/25/2022 22:52:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.10 on epoch=174
05/25/2022 22:52:24 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.6973060344827586 on epoch=174
05/25/2022 22:52:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6609228734228734 -> 0.6973060344827586 on epoch=174, global_step=700
05/25/2022 22:52:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=177
05/25/2022 22:52:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=179
05/25/2022 22:52:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=182
05/25/2022 22:52:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=184
05/25/2022 22:52:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=187
05/25/2022 22:52:39 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.6543859649122807 on epoch=187
05/25/2022 22:52:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=189
05/25/2022 22:52:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=192
05/25/2022 22:52:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=194
05/25/2022 22:52:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=197
05/25/2022 22:52:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=199
05/25/2022 22:52:53 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.6165864081717741 on epoch=199
05/25/2022 22:52:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
05/25/2022 22:52:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=204
05/25/2022 22:53:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=207
05/25/2022 22:53:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=209
05/25/2022 22:53:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=212
05/25/2022 22:53:07 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.6678297755883963 on epoch=212
05/25/2022 22:53:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=214
05/25/2022 22:53:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=217
05/25/2022 22:53:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=219
05/25/2022 22:53:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=222
05/25/2022 22:53:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=224
05/25/2022 22:53:22 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.6878033205619414 on epoch=224
05/25/2022 22:53:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=227
05/25/2022 22:53:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=229
05/25/2022 22:53:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
05/25/2022 22:53:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
05/25/2022 22:53:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=237
05/25/2022 22:53:37 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.6712029569892473 on epoch=237
05/25/2022 22:53:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
05/25/2022 22:53:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
05/25/2022 22:53:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
05/25/2022 22:53:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=247
05/25/2022 22:53:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
05/25/2022 22:53:51 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.6838845215505558 on epoch=249
05/25/2022 22:53:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
05/25/2022 22:53:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=254
05/25/2022 22:53:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=257
05/25/2022 22:54:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/25/2022 22:54:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=262
05/25/2022 22:54:05 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.7023460410557185 on epoch=262
05/25/2022 22:54:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6973060344827586 -> 0.7023460410557185 on epoch=262, global_step=1050
05/25/2022 22:54:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=264
05/25/2022 22:54:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
05/25/2022 22:54:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=269
05/25/2022 22:54:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
05/25/2022 22:54:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=274
05/25/2022 22:54:20 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.6690972222222222 on epoch=274
05/25/2022 22:54:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
05/25/2022 22:54:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=279
05/25/2022 22:54:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
05/25/2022 22:54:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
05/25/2022 22:54:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=287
05/25/2022 22:54:34 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.6674182139699381 on epoch=287
05/25/2022 22:54:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
05/25/2022 22:54:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
05/25/2022 22:54:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=294
05/25/2022 22:54:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
05/25/2022 22:54:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/25/2022 22:54:49 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6832752106945654 on epoch=299
05/25/2022 22:54:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/25/2022 22:54:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
05/25/2022 22:54:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/25/2022 22:55:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/25/2022 22:55:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
05/25/2022 22:55:03 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.6826164874551971 on epoch=312
05/25/2022 22:55:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
05/25/2022 22:55:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=317
05/25/2022 22:55:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
05/25/2022 22:55:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
05/25/2022 22:55:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/25/2022 22:55:18 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6986030695708115 on epoch=324
05/25/2022 22:55:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
05/25/2022 22:55:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/25/2022 22:55:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=332
05/25/2022 22:55:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/25/2022 22:55:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/25/2022 22:55:33 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.7123015873015873 on epoch=337
05/25/2022 22:55:33 - INFO - __main__ - Saving model with best Classification-F1: 0.7023460410557185 -> 0.7123015873015873 on epoch=337, global_step=1350
05/25/2022 22:55:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=339
05/25/2022 22:55:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
05/25/2022 22:55:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/25/2022 22:55:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/25/2022 22:55:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/25/2022 22:55:48 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.7152777777777777 on epoch=349
05/25/2022 22:55:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7123015873015873 -> 0.7152777777777777 on epoch=349, global_step=1400
05/25/2022 22:55:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
05/25/2022 22:55:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/25/2022 22:55:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/25/2022 22:55:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
05/25/2022 22:56:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
05/25/2022 22:56:02 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.7046401515151515 on epoch=362
05/25/2022 22:56:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/25/2022 22:56:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/25/2022 22:56:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/25/2022 22:56:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/25/2022 22:56:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
05/25/2022 22:56:17 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.6963048294717986 on epoch=374
05/25/2022 22:56:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/25/2022 22:56:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
05/25/2022 22:56:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
05/25/2022 22:56:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/25/2022 22:56:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/25/2022 22:56:31 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.7099248120300753 on epoch=387
05/25/2022 22:56:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
05/25/2022 22:56:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/25/2022 22:56:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/25/2022 22:56:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/25/2022 22:56:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/25/2022 22:56:46 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.7295528982820418 on epoch=399
05/25/2022 22:56:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7152777777777777 -> 0.7295528982820418 on epoch=399, global_step=1600
05/25/2022 22:56:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/25/2022 22:56:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
05/25/2022 22:56:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/25/2022 22:56:56 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/25/2022 22:56:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/25/2022 22:57:00 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7275357744107743 on epoch=412
05/25/2022 22:57:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/25/2022 22:57:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/25/2022 22:57:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/25/2022 22:57:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/25/2022 22:57:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/25/2022 22:57:15 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7279623373373373 on epoch=424
05/25/2022 22:57:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
05/25/2022 22:57:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/25/2022 22:57:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/25/2022 22:57:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
05/25/2022 22:57:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/25/2022 22:57:30 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7086554172951232 on epoch=437
05/25/2022 22:57:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/25/2022 22:57:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=442
05/25/2022 22:57:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/25/2022 22:57:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/25/2022 22:57:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/25/2022 22:57:44 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.6990118891120003 on epoch=449
05/25/2022 22:57:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/25/2022 22:57:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/25/2022 22:57:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/25/2022 22:57:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
05/25/2022 22:57:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/25/2022 22:57:59 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7279265873015873 on epoch=462
05/25/2022 22:58:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/25/2022 22:58:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/25/2022 22:58:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/25/2022 22:58:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=472
05/25/2022 22:58:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/25/2022 22:58:13 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7279265873015873 on epoch=474
05/25/2022 22:58:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/25/2022 22:58:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/25/2022 22:58:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=482
05/25/2022 22:58:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
05/25/2022 22:58:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/25/2022 22:58:28 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6986453201970444 on epoch=487
05/25/2022 22:58:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/25/2022 22:58:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/25/2022 22:58:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/25/2022 22:58:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/25/2022 22:58:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/25/2022 22:58:43 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7003195080781288 on epoch=499
05/25/2022 22:58:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/25/2022 22:58:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/25/2022 22:58:51 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/25/2022 22:58:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/25/2022 22:58:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
05/25/2022 22:58:57 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6979894338118022 on epoch=512
05/25/2022 22:59:00 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/25/2022 22:59:02 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=517
05/25/2022 22:59:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/25/2022 22:59:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/25/2022 22:59:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/25/2022 22:59:12 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7171190692881461 on epoch=524
05/25/2022 22:59:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/25/2022 22:59:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/25/2022 22:59:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=532
05/25/2022 22:59:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/25/2022 22:59:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/25/2022 22:59:26 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7022827899488242 on epoch=537
05/25/2022 22:59:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/25/2022 22:59:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
05/25/2022 22:59:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/25/2022 22:59:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/25/2022 22:59:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/25/2022 22:59:41 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7309518620002491 on epoch=549
05/25/2022 22:59:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7295528982820418 -> 0.7309518620002491 on epoch=549, global_step=2200
05/25/2022 22:59:43 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/25/2022 22:59:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/25/2022 22:59:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/25/2022 22:59:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/25/2022 22:59:54 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/25/2022 22:59:55 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7462622549019607 on epoch=562
05/25/2022 22:59:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7309518620002491 -> 0.7462622549019607 on epoch=562, global_step=2250
05/25/2022 22:59:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/25/2022 23:00:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/25/2022 23:00:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
05/25/2022 23:00:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/25/2022 23:00:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/25/2022 23:00:10 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7013915243516762 on epoch=574
05/25/2022 23:00:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/25/2022 23:00:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/25/2022 23:00:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/25/2022 23:00:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=584
05/25/2022 23:00:23 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=587
05/25/2022 23:00:25 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6834656084656086 on epoch=587
05/25/2022 23:00:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=589
05/25/2022 23:00:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/25/2022 23:00:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/25/2022 23:00:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/25/2022 23:00:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/25/2022 23:00:39 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6715686274509804 on epoch=599
05/25/2022 23:00:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/25/2022 23:00:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/25/2022 23:00:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/25/2022 23:00:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/25/2022 23:00:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/25/2022 23:00:54 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7003126597119923 on epoch=612
05/25/2022 23:00:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/25/2022 23:00:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/25/2022 23:01:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/25/2022 23:01:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/25/2022 23:01:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/25/2022 23:01:08 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6836843336843337 on epoch=624
05/25/2022 23:01:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/25/2022 23:01:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/25/2022 23:01:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/25/2022 23:01:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=634
05/25/2022 23:01:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/25/2022 23:01:23 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.6651709401709401 on epoch=637
05/25/2022 23:01:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
05/25/2022 23:01:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/25/2022 23:01:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/25/2022 23:01:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/25/2022 23:01:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=649
05/25/2022 23:01:37 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6784372236958444 on epoch=649
05/25/2022 23:01:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/25/2022 23:01:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/25/2022 23:01:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/25/2022 23:01:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/25/2022 23:01:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/25/2022 23:01:52 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6930148555148555 on epoch=662
05/25/2022 23:01:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/25/2022 23:01:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/25/2022 23:02:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/25/2022 23:02:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
05/25/2022 23:02:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/25/2022 23:02:07 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7284780578898226 on epoch=674
05/25/2022 23:02:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/25/2022 23:02:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/25/2022 23:02:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/25/2022 23:02:17 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/25/2022 23:02:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/25/2022 23:02:21 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.733655338968432 on epoch=687
05/25/2022 23:02:24 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=689
05/25/2022 23:02:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/25/2022 23:02:29 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/25/2022 23:02:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/25/2022 23:02:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.10 on epoch=699
05/25/2022 23:02:36 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.7013915243516762 on epoch=699
05/25/2022 23:02:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/25/2022 23:02:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/25/2022 23:02:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/25/2022 23:02:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/25/2022 23:02:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/25/2022 23:02:50 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6986453201970444 on epoch=712
05/25/2022 23:02:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/25/2022 23:02:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=717
05/25/2022 23:02:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/25/2022 23:03:01 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/25/2022 23:03:04 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/25/2022 23:03:05 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.683216619981326 on epoch=724
05/25/2022 23:03:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/25/2022 23:03:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/25/2022 23:03:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=732
05/25/2022 23:03:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/25/2022 23:03:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/25/2022 23:03:20 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7018433179723502 on epoch=737
05/25/2022 23:03:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/25/2022 23:03:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/25/2022 23:03:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/25/2022 23:03:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/25/2022 23:03:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/25/2022 23:03:34 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7314863445378151 on epoch=749
05/25/2022 23:03:34 - INFO - __main__ - save last model!
05/25/2022 23:03:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:03:34 - INFO - __main__ - Printing 3 examples
05/25/2022 23:03:34 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 23:03:34 - INFO - __main__ - ['others']
05/25/2022 23:03:34 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 23:03:34 - INFO - __main__ - ['others']
05/25/2022 23:03:34 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 23:03:34 - INFO - __main__ - ['others']
05/25/2022 23:03:34 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:03:34 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:03:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 23:03:34 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 23:03:34 - INFO - __main__ - Printing 3 examples
05/25/2022 23:03:34 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 23:03:34 - INFO - __main__ - ['others']
05/25/2022 23:03:34 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 23:03:34 - INFO - __main__ - ['others']
05/25/2022 23:03:34 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 23:03:34 - INFO - __main__ - ['others']
05/25/2022 23:03:34 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:03:34 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 23:03:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:03:34 - INFO - __main__ - Printing 3 examples
05/25/2022 23:03:34 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/25/2022 23:03:34 - INFO - __main__ - ['others']
05/25/2022 23:03:34 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/25/2022 23:03:34 - INFO - __main__ - ['others']
05/25/2022 23:03:34 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/25/2022 23:03:34 - INFO - __main__ - ['others']
05/25/2022 23:03:34 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:03:34 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:03:34 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 23:03:36 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:03:42 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 23:03:53 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:03:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 23:03:54 - INFO - __main__ - Starting training!
05/25/2022 23:05:32 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_100_0.4_8_predictions.txt
05/25/2022 23:05:32 - INFO - __main__ - Classification-F1 on test data: 0.1810
05/25/2022 23:05:33 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.7462622549019607, test_performance=0.18102206662060677
05/25/2022 23:05:33 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
05/25/2022 23:05:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:05:34 - INFO - __main__ - Printing 3 examples
05/25/2022 23:05:34 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 23:05:34 - INFO - __main__ - ['others']
05/25/2022 23:05:34 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 23:05:34 - INFO - __main__ - ['others']
05/25/2022 23:05:34 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 23:05:34 - INFO - __main__ - ['others']
05/25/2022 23:05:34 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:05:34 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:05:34 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 23:05:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:05:34 - INFO - __main__ - Printing 3 examples
05/25/2022 23:05:34 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/25/2022 23:05:34 - INFO - __main__ - ['others']
05/25/2022 23:05:34 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/25/2022 23:05:34 - INFO - __main__ - ['others']
05/25/2022 23:05:34 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/25/2022 23:05:34 - INFO - __main__ - ['others']
05/25/2022 23:05:34 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:05:34 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:05:34 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 23:05:49 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:05:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 23:05:49 - INFO - __main__ - Starting training!
05/25/2022 23:05:52 - INFO - __main__ - Step 10 Global step 10 Train loss 2.99 on epoch=2
05/25/2022 23:05:55 - INFO - __main__ - Step 20 Global step 20 Train loss 1.41 on epoch=4
05/25/2022 23:05:58 - INFO - __main__ - Step 30 Global step 30 Train loss 1.19 on epoch=7
05/25/2022 23:06:00 - INFO - __main__ - Step 40 Global step 40 Train loss 1.01 on epoch=9
05/25/2022 23:06:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=12
05/25/2022 23:06:04 - INFO - __main__ - Global step 50 Train loss 1.51 Classification-F1 0.1 on epoch=12
05/25/2022 23:06:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/25/2022 23:06:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.92 on epoch=14
05/25/2022 23:06:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.92 on epoch=17
05/25/2022 23:06:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.84 on epoch=19
05/25/2022 23:06:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.87 on epoch=22
05/25/2022 23:06:17 - INFO - __main__ - Step 100 Global step 100 Train loss 1.02 on epoch=24
05/25/2022 23:06:18 - INFO - __main__ - Global step 100 Train loss 0.91 Classification-F1 0.3160951951274532 on epoch=24
05/25/2022 23:06:18 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.3160951951274532 on epoch=24, global_step=100
05/25/2022 23:06:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=27
05/25/2022 23:06:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.73 on epoch=29
05/25/2022 23:06:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=32
05/25/2022 23:06:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=34
05/25/2022 23:06:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=37
05/25/2022 23:06:32 - INFO - __main__ - Global step 150 Train loss 0.83 Classification-F1 0.2766884531590414 on epoch=37
05/25/2022 23:06:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.78 on epoch=39
05/25/2022 23:06:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=42
05/25/2022 23:06:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.83 on epoch=44
05/25/2022 23:06:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.79 on epoch=47
05/25/2022 23:06:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.76 on epoch=49
05/25/2022 23:06:46 - INFO - __main__ - Global step 200 Train loss 0.79 Classification-F1 0.49447489075148654 on epoch=49
05/25/2022 23:06:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3160951951274532 -> 0.49447489075148654 on epoch=49, global_step=200
05/25/2022 23:06:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.73 on epoch=52
05/25/2022 23:06:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.75 on epoch=54
05/25/2022 23:06:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.74 on epoch=57
05/25/2022 23:06:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.69 on epoch=59
05/25/2022 23:06:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.63 on epoch=62
05/25/2022 23:07:00 - INFO - __main__ - Global step 250 Train loss 0.71 Classification-F1 0.4845622119815668 on epoch=62
05/25/2022 23:07:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.72 on epoch=64
05/25/2022 23:07:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.66 on epoch=67
05/25/2022 23:07:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.65 on epoch=69
05/25/2022 23:07:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.65 on epoch=72
05/25/2022 23:07:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.60 on epoch=74
05/25/2022 23:07:14 - INFO - __main__ - Global step 300 Train loss 0.66 Classification-F1 0.4712055539919317 on epoch=74
05/25/2022 23:07:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.65 on epoch=77
05/25/2022 23:07:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.59 on epoch=79
05/25/2022 23:07:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=82
05/25/2022 23:07:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=84
05/25/2022 23:07:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.56 on epoch=87
05/25/2022 23:07:28 - INFO - __main__ - Global step 350 Train loss 0.57 Classification-F1 0.5362030828782747 on epoch=87
05/25/2022 23:07:28 - INFO - __main__ - Saving model with best Classification-F1: 0.49447489075148654 -> 0.5362030828782747 on epoch=87, global_step=350
05/25/2022 23:07:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=89
05/25/2022 23:07:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=92
05/25/2022 23:07:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=94
05/25/2022 23:07:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=97
05/25/2022 23:07:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.54 on epoch=99
05/25/2022 23:07:42 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.5474254742547425 on epoch=99
05/25/2022 23:07:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5362030828782747 -> 0.5474254742547425 on epoch=99, global_step=400
05/25/2022 23:07:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=102
05/25/2022 23:07:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=104
05/25/2022 23:07:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=107
05/25/2022 23:07:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=109
05/25/2022 23:07:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=112
05/25/2022 23:07:56 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.6119594964422551 on epoch=112
05/25/2022 23:07:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5474254742547425 -> 0.6119594964422551 on epoch=112, global_step=450
05/25/2022 23:07:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=114
05/25/2022 23:08:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=117
05/25/2022 23:08:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=119
05/25/2022 23:08:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=122
05/25/2022 23:08:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=124
05/25/2022 23:08:10 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.5968809718809719 on epoch=124
05/25/2022 23:08:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=127
05/25/2022 23:08:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=129
05/25/2022 23:08:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=132
05/25/2022 23:08:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=134
05/25/2022 23:08:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=137
05/25/2022 23:08:24 - INFO - __main__ - Global step 550 Train loss 0.30 Classification-F1 0.6194382169080579 on epoch=137
05/25/2022 23:08:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6119594964422551 -> 0.6194382169080579 on epoch=137, global_step=550
05/25/2022 23:08:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.28 on epoch=139
05/25/2022 23:08:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=142
05/25/2022 23:08:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=144
05/25/2022 23:08:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=147
05/25/2022 23:08:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=149
05/25/2022 23:08:38 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.6276459029530109 on epoch=149
05/25/2022 23:08:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6194382169080579 -> 0.6276459029530109 on epoch=149, global_step=600
05/25/2022 23:08:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=152
05/25/2022 23:08:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=154
05/25/2022 23:08:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.33 on epoch=157
05/25/2022 23:08:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=159
05/25/2022 23:08:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=162
05/25/2022 23:08:52 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.7013915243516762 on epoch=162
05/25/2022 23:08:52 - INFO - __main__ - Saving model with best Classification-F1: 0.6276459029530109 -> 0.7013915243516762 on epoch=162, global_step=650
05/25/2022 23:08:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=164
05/25/2022 23:08:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=167
05/25/2022 23:09:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=169
05/25/2022 23:09:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=172
05/25/2022 23:09:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=174
05/25/2022 23:09:06 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.564270202889068 on epoch=174
05/25/2022 23:09:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.31 on epoch=177
05/25/2022 23:09:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=179
05/25/2022 23:09:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=182
05/25/2022 23:09:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=184
05/25/2022 23:09:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=187
05/25/2022 23:09:20 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.7047812086711518 on epoch=187
05/25/2022 23:09:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7013915243516762 -> 0.7047812086711518 on epoch=187, global_step=750
05/25/2022 23:09:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=189
05/25/2022 23:09:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=192
05/25/2022 23:09:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=194
05/25/2022 23:09:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=197
05/25/2022 23:09:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=199
05/25/2022 23:09:34 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.7197916666666666 on epoch=199
05/25/2022 23:09:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7047812086711518 -> 0.7197916666666666 on epoch=199, global_step=800
05/25/2022 23:09:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=202
05/25/2022 23:09:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=204
05/25/2022 23:09:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=207
05/25/2022 23:09:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=209
05/25/2022 23:09:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=212
05/25/2022 23:09:48 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.6749390456287009 on epoch=212
05/25/2022 23:09:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=214
05/25/2022 23:09:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=217
05/25/2022 23:09:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=219
05/25/2022 23:09:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=222
05/25/2022 23:10:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
05/25/2022 23:10:02 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.696670392196708 on epoch=224
05/25/2022 23:10:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=227
05/25/2022 23:10:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=229
05/25/2022 23:10:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
05/25/2022 23:10:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=234
05/25/2022 23:10:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=237
05/25/2022 23:10:16 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.7171309872922776 on epoch=237
05/25/2022 23:10:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
05/25/2022 23:10:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=242
05/25/2022 23:10:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=244
05/25/2022 23:10:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=247
05/25/2022 23:10:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=249
05/25/2022 23:10:30 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.6839013383131031 on epoch=249
05/25/2022 23:10:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=252
05/25/2022 23:10:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
05/25/2022 23:10:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=257
05/25/2022 23:10:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=259
05/25/2022 23:10:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=262
05/25/2022 23:10:44 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.6846123506874013 on epoch=262
05/25/2022 23:10:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=264
05/25/2022 23:10:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.17 on epoch=267
05/25/2022 23:10:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=269
05/25/2022 23:10:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=272
05/25/2022 23:10:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=274
05/25/2022 23:10:59 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.7025985663082438 on epoch=274
05/25/2022 23:11:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
05/25/2022 23:11:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=279
05/25/2022 23:11:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
05/25/2022 23:11:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
05/25/2022 23:11:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
05/25/2022 23:11:13 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.72093837535014 on epoch=287
05/25/2022 23:11:13 - INFO - __main__ - Saving model with best Classification-F1: 0.7197916666666666 -> 0.72093837535014 on epoch=287, global_step=1150
05/25/2022 23:11:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
05/25/2022 23:11:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=292
05/25/2022 23:11:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=294
05/25/2022 23:11:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=297
05/25/2022 23:11:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=299
05/25/2022 23:11:27 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.6988988557299752 on epoch=299
05/25/2022 23:11:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
05/25/2022 23:11:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=304
05/25/2022 23:11:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
05/25/2022 23:11:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
05/25/2022 23:11:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=312
05/25/2022 23:11:41 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.6963909932659933 on epoch=312
05/25/2022 23:11:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/25/2022 23:11:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
05/25/2022 23:11:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
05/25/2022 23:11:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/25/2022 23:11:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
05/25/2022 23:11:56 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.7000135538086203 on epoch=324
05/25/2022 23:11:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/25/2022 23:12:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
05/25/2022 23:12:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/25/2022 23:12:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/25/2022 23:12:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/25/2022 23:12:10 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.6995924758792406 on epoch=337
05/25/2022 23:12:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=339
05/25/2022 23:12:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/25/2022 23:12:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
05/25/2022 23:12:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/25/2022 23:12:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/25/2022 23:12:24 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6999894062185497 on epoch=349
05/25/2022 23:12:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/25/2022 23:12:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
05/25/2022 23:12:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
05/25/2022 23:12:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
05/25/2022 23:12:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/25/2022 23:12:38 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.7160539215686275 on epoch=362
05/25/2022 23:12:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/25/2022 23:12:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/25/2022 23:12:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
05/25/2022 23:12:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/25/2022 23:12:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=374
05/25/2022 23:12:53 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.6608234126984127 on epoch=374
05/25/2022 23:12:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/25/2022 23:12:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
05/25/2022 23:13:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/25/2022 23:13:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/25/2022 23:13:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/25/2022 23:13:07 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7143804112554113 on epoch=387
05/25/2022 23:13:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/25/2022 23:13:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/25/2022 23:13:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/25/2022 23:13:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=397
05/25/2022 23:13:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/25/2022 23:13:21 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.6827848674622868 on epoch=399
05/25/2022 23:13:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
05/25/2022 23:13:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
05/25/2022 23:13:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=407
05/25/2022 23:13:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/25/2022 23:13:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/25/2022 23:13:36 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.6648253367003367 on epoch=412
05/25/2022 23:13:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=414
05/25/2022 23:13:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/25/2022 23:13:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/25/2022 23:13:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/25/2022 23:13:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/25/2022 23:13:50 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7151935813510357 on epoch=424
05/25/2022 23:13:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
05/25/2022 23:13:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/25/2022 23:13:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/25/2022 23:14:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/25/2022 23:14:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/25/2022 23:14:04 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6988156775502191 on epoch=437
05/25/2022 23:14:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/25/2022 23:14:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=442
05/25/2022 23:14:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/25/2022 23:14:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/25/2022 23:14:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=449
05/25/2022 23:14:18 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.7508538899430741 on epoch=449
05/25/2022 23:14:18 - INFO - __main__ - Saving model with best Classification-F1: 0.72093837535014 -> 0.7508538899430741 on epoch=449, global_step=1800
05/25/2022 23:14:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
05/25/2022 23:14:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
05/25/2022 23:14:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
05/25/2022 23:14:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/25/2022 23:14:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/25/2022 23:14:33 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.63155493931356 on epoch=462
05/25/2022 23:14:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/25/2022 23:14:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
05/25/2022 23:14:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/25/2022 23:14:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/25/2022 23:14:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/25/2022 23:14:47 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6682598039215686 on epoch=474
05/25/2022 23:14:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/25/2022 23:14:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
05/25/2022 23:14:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/25/2022 23:14:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/25/2022 23:15:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=487
05/25/2022 23:15:02 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6827698234719107 on epoch=487
05/25/2022 23:15:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/25/2022 23:15:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/25/2022 23:15:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/25/2022 23:15:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
05/25/2022 23:15:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=499
05/25/2022 23:15:16 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.6983630952380953 on epoch=499
05/25/2022 23:15:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/25/2022 23:15:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/25/2022 23:15:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/25/2022 23:15:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/25/2022 23:15:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/25/2022 23:15:30 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7013409961685824 on epoch=512
05/25/2022 23:15:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/25/2022 23:15:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/25/2022 23:15:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/25/2022 23:15:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/25/2022 23:15:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/25/2022 23:15:45 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7018433179723502 on epoch=524
05/25/2022 23:15:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
05/25/2022 23:15:50 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/25/2022 23:15:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/25/2022 23:15:55 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/25/2022 23:15:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/25/2022 23:15:59 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6983630952380953 on epoch=537
05/25/2022 23:16:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
05/25/2022 23:16:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=542
05/25/2022 23:16:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/25/2022 23:16:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/25/2022 23:16:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/25/2022 23:16:14 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6997567844342039 on epoch=549
05/25/2022 23:16:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/25/2022 23:16:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/25/2022 23:16:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/25/2022 23:16:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=559
05/25/2022 23:16:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/25/2022 23:16:28 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.731289009393153 on epoch=562
05/25/2022 23:16:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=564
05/25/2022 23:16:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/25/2022 23:16:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/25/2022 23:16:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
05/25/2022 23:16:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/25/2022 23:16:42 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7295360938187764 on epoch=574
05/25/2022 23:16:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
05/25/2022 23:16:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/25/2022 23:16:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=582
05/25/2022 23:16:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/25/2022 23:16:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/25/2022 23:16:57 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.6648253367003367 on epoch=587
05/25/2022 23:17:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/25/2022 23:17:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/25/2022 23:17:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/25/2022 23:17:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/25/2022 23:17:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/25/2022 23:17:11 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.7103857572607573 on epoch=599
05/25/2022 23:17:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/25/2022 23:17:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/25/2022 23:17:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/25/2022 23:17:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/25/2022 23:17:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/25/2022 23:17:25 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6999894062185497 on epoch=612
05/25/2022 23:17:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/25/2022 23:17:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/25/2022 23:17:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/25/2022 23:17:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/25/2022 23:17:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/25/2022 23:17:40 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7134208928326575 on epoch=624
05/25/2022 23:17:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/25/2022 23:17:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/25/2022 23:17:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/25/2022 23:17:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/25/2022 23:17:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/25/2022 23:17:54 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.7164510378931632 on epoch=637
05/25/2022 23:17:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
05/25/2022 23:18:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/25/2022 23:18:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/25/2022 23:18:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/25/2022 23:18:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/25/2022 23:18:09 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7294253185145027 on epoch=649
05/25/2022 23:18:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/25/2022 23:18:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/25/2022 23:18:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/25/2022 23:18:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/25/2022 23:18:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/25/2022 23:18:23 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.729202202040691 on epoch=662
05/25/2022 23:18:26 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
05/25/2022 23:18:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/25/2022 23:18:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
05/25/2022 23:18:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/25/2022 23:18:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/25/2022 23:18:37 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7103857572607573 on epoch=674
05/25/2022 23:18:40 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/25/2022 23:18:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/25/2022 23:18:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/25/2022 23:18:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/25/2022 23:18:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/25/2022 23:18:52 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7286661255411256 on epoch=687
05/25/2022 23:18:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/25/2022 23:18:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/25/2022 23:18:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=694
05/25/2022 23:19:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/25/2022 23:19:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/25/2022 23:19:06 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6991477272727273 on epoch=699
05/25/2022 23:19:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/25/2022 23:19:11 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/25/2022 23:19:14 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/25/2022 23:19:16 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
05/25/2022 23:19:19 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/25/2022 23:19:20 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7054112554112554 on epoch=712
05/25/2022 23:19:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/25/2022 23:19:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/25/2022 23:19:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=719
05/25/2022 23:19:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/25/2022 23:19:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/25/2022 23:19:35 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.7420273214390861 on epoch=724
05/25/2022 23:19:37 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/25/2022 23:19:40 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
05/25/2022 23:19:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=732
05/25/2022 23:19:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/25/2022 23:19:48 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
05/25/2022 23:19:49 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.7183755760368663 on epoch=737
05/25/2022 23:19:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
05/25/2022 23:19:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=742
05/25/2022 23:19:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/25/2022 23:20:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/25/2022 23:20:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/25/2022 23:20:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:20:03 - INFO - __main__ - Printing 3 examples
05/25/2022 23:20:03 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 23:20:03 - INFO - __main__ - ['others']
05/25/2022 23:20:03 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 23:20:03 - INFO - __main__ - ['others']
05/25/2022 23:20:03 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 23:20:03 - INFO - __main__ - ['others']
05/25/2022 23:20:03 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:20:03 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:20:03 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.697254940377405 on epoch=749
05/25/2022 23:20:03 - INFO - __main__ - save last model!
05/25/2022 23:20:03 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 23:20:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:20:03 - INFO - __main__ - Printing 3 examples
05/25/2022 23:20:03 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/25/2022 23:20:03 - INFO - __main__ - ['others']
05/25/2022 23:20:03 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/25/2022 23:20:03 - INFO - __main__ - ['others']
05/25/2022 23:20:03 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/25/2022 23:20:03 - INFO - __main__ - ['others']
05/25/2022 23:20:03 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:20:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 23:20:03 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 23:20:03 - INFO - __main__ - Printing 3 examples
05/25/2022 23:20:03 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 23:20:03 - INFO - __main__ - ['others']
05/25/2022 23:20:03 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 23:20:03 - INFO - __main__ - ['others']
05/25/2022 23:20:03 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 23:20:03 - INFO - __main__ - ['others']
05/25/2022 23:20:03 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:20:03 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:20:04 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 23:20:06 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:20:11 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 23:20:22 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:20:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 23:20:23 - INFO - __main__ - Starting training!
05/25/2022 23:22:01 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_100_0.3_8_predictions.txt
05/25/2022 23:22:01 - INFO - __main__ - Classification-F1 on test data: 0.2309
05/25/2022 23:22:02 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.7508538899430741, test_performance=0.23093621252985527
05/25/2022 23:22:02 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
05/25/2022 23:22:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:22:03 - INFO - __main__ - Printing 3 examples
05/25/2022 23:22:03 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 23:22:03 - INFO - __main__ - ['others']
05/25/2022 23:22:03 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 23:22:03 - INFO - __main__ - ['others']
05/25/2022 23:22:03 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 23:22:03 - INFO - __main__ - ['others']
05/25/2022 23:22:03 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:22:03 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:22:03 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 23:22:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:22:03 - INFO - __main__ - Printing 3 examples
05/25/2022 23:22:03 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/25/2022 23:22:03 - INFO - __main__ - ['others']
05/25/2022 23:22:03 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/25/2022 23:22:03 - INFO - __main__ - ['others']
05/25/2022 23:22:03 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/25/2022 23:22:03 - INFO - __main__ - ['others']
05/25/2022 23:22:03 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:22:03 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:22:03 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 23:22:21 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:22:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 23:22:22 - INFO - __main__ - Starting training!
05/25/2022 23:22:26 - INFO - __main__ - Step 10 Global step 10 Train loss 3.23 on epoch=2
05/25/2022 23:22:28 - INFO - __main__ - Step 20 Global step 20 Train loss 1.75 on epoch=4
05/25/2022 23:22:31 - INFO - __main__ - Step 30 Global step 30 Train loss 1.29 on epoch=7
05/25/2022 23:22:33 - INFO - __main__ - Step 40 Global step 40 Train loss 1.10 on epoch=9
05/25/2022 23:22:36 - INFO - __main__ - Step 50 Global step 50 Train loss 1.13 on epoch=12
05/25/2022 23:22:37 - INFO - __main__ - Global step 50 Train loss 1.70 Classification-F1 0.1 on epoch=12
05/25/2022 23:22:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/25/2022 23:22:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.99 on epoch=14
05/25/2022 23:22:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.91 on epoch=17
05/25/2022 23:22:45 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=19
05/25/2022 23:22:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.91 on epoch=22
05/25/2022 23:22:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=24
05/25/2022 23:22:51 - INFO - __main__ - Global step 100 Train loss 0.93 Classification-F1 0.13034188034188032 on epoch=24
05/25/2022 23:22:51 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.13034188034188032 on epoch=24, global_step=100
05/25/2022 23:22:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.89 on epoch=27
05/25/2022 23:22:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.75 on epoch=29
05/25/2022 23:22:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.87 on epoch=32
05/25/2022 23:23:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.83 on epoch=34
05/25/2022 23:23:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=37
05/25/2022 23:23:05 - INFO - __main__ - Global step 150 Train loss 0.84 Classification-F1 0.16138763197586728 on epoch=37
05/25/2022 23:23:05 - INFO - __main__ - Saving model with best Classification-F1: 0.13034188034188032 -> 0.16138763197586728 on epoch=37, global_step=150
05/25/2022 23:23:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.87 on epoch=39
05/25/2022 23:23:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.85 on epoch=42
05/25/2022 23:23:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=44
05/25/2022 23:23:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.80 on epoch=47
05/25/2022 23:23:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.85 on epoch=49
05/25/2022 23:23:19 - INFO - __main__ - Global step 200 Train loss 0.85 Classification-F1 0.2951400239729759 on epoch=49
05/25/2022 23:23:19 - INFO - __main__ - Saving model with best Classification-F1: 0.16138763197586728 -> 0.2951400239729759 on epoch=49, global_step=200
05/25/2022 23:23:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.85 on epoch=52
05/25/2022 23:23:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.80 on epoch=54
05/25/2022 23:23:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.84 on epoch=57
05/25/2022 23:23:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.73 on epoch=59
05/25/2022 23:23:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.88 on epoch=62
05/25/2022 23:23:33 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.47204184704184715 on epoch=62
05/25/2022 23:23:33 - INFO - __main__ - Saving model with best Classification-F1: 0.2951400239729759 -> 0.47204184704184715 on epoch=62, global_step=250
05/25/2022 23:23:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.75 on epoch=64
05/25/2022 23:23:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.69 on epoch=67
05/25/2022 23:23:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.72 on epoch=69
05/25/2022 23:23:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.77 on epoch=72
05/25/2022 23:23:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.69 on epoch=74
05/25/2022 23:23:47 - INFO - __main__ - Global step 300 Train loss 0.73 Classification-F1 0.4942514783691254 on epoch=74
05/25/2022 23:23:47 - INFO - __main__ - Saving model with best Classification-F1: 0.47204184704184715 -> 0.4942514783691254 on epoch=74, global_step=300
05/25/2022 23:23:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.70 on epoch=77
05/25/2022 23:23:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.67 on epoch=79
05/25/2022 23:23:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.66 on epoch=82
05/25/2022 23:23:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.65 on epoch=84
05/25/2022 23:24:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.73 on epoch=87
05/25/2022 23:24:01 - INFO - __main__ - Global step 350 Train loss 0.68 Classification-F1 0.5103610475464022 on epoch=87
05/25/2022 23:24:02 - INFO - __main__ - Saving model with best Classification-F1: 0.4942514783691254 -> 0.5103610475464022 on epoch=87, global_step=350
05/25/2022 23:24:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=89
05/25/2022 23:24:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.63 on epoch=92
05/25/2022 23:24:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.58 on epoch=94
05/25/2022 23:24:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.69 on epoch=97
05/25/2022 23:24:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.65 on epoch=99
05/25/2022 23:24:15 - INFO - __main__ - Global step 400 Train loss 0.66 Classification-F1 0.5658824659505239 on epoch=99
05/25/2022 23:24:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5103610475464022 -> 0.5658824659505239 on epoch=99, global_step=400
05/25/2022 23:24:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.71 on epoch=102
05/25/2022 23:24:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=104
05/25/2022 23:24:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.70 on epoch=107
05/25/2022 23:24:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.59 on epoch=109
05/25/2022 23:24:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.66 on epoch=112
05/25/2022 23:24:29 - INFO - __main__ - Global step 450 Train loss 0.65 Classification-F1 0.5480579791725303 on epoch=112
05/25/2022 23:24:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.60 on epoch=114
05/25/2022 23:24:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.61 on epoch=117
05/25/2022 23:24:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=119
05/25/2022 23:24:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.61 on epoch=122
05/25/2022 23:24:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=124
05/25/2022 23:24:43 - INFO - __main__ - Global step 500 Train loss 0.57 Classification-F1 0.5997756327157416 on epoch=124
05/25/2022 23:24:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5658824659505239 -> 0.5997756327157416 on epoch=124, global_step=500
05/25/2022 23:24:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=127
05/25/2022 23:24:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=129
05/25/2022 23:24:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=132
05/25/2022 23:24:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=134
05/25/2022 23:24:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=137
05/25/2022 23:24:57 - INFO - __main__ - Global step 550 Train loss 0.51 Classification-F1 0.6322850122850123 on epoch=137
05/25/2022 23:24:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5997756327157416 -> 0.6322850122850123 on epoch=137, global_step=550
05/25/2022 23:25:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=139
05/25/2022 23:25:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=142
05/25/2022 23:25:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=144
05/25/2022 23:25:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.54 on epoch=147
05/25/2022 23:25:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=149
05/25/2022 23:25:11 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.6389096924580796 on epoch=149
05/25/2022 23:25:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6322850122850123 -> 0.6389096924580796 on epoch=149, global_step=600
05/25/2022 23:25:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=152
05/25/2022 23:25:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=154
05/25/2022 23:25:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=157
05/25/2022 23:25:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=159
05/25/2022 23:25:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=162
05/25/2022 23:25:26 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.663887093298858 on epoch=162
05/25/2022 23:25:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6389096924580796 -> 0.663887093298858 on epoch=162, global_step=650
05/25/2022 23:25:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=164
05/25/2022 23:25:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=167
05/25/2022 23:25:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=169
05/25/2022 23:25:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=172
05/25/2022 23:25:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.31 on epoch=174
05/25/2022 23:25:40 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.5996031746031746 on epoch=174
05/25/2022 23:25:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=177
05/25/2022 23:25:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=179
05/25/2022 23:25:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=182
05/25/2022 23:25:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=184
05/25/2022 23:25:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=187
05/25/2022 23:25:54 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.719761697181052 on epoch=187
05/25/2022 23:25:54 - INFO - __main__ - Saving model with best Classification-F1: 0.663887093298858 -> 0.719761697181052 on epoch=187, global_step=750
05/25/2022 23:25:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.30 on epoch=189
05/25/2022 23:25:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=192
05/25/2022 23:26:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=194
05/25/2022 23:26:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=197
05/25/2022 23:26:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=199
05/25/2022 23:26:08 - INFO - __main__ - Global step 800 Train loss 0.29 Classification-F1 0.7493387384279224 on epoch=199
05/25/2022 23:26:08 - INFO - __main__ - Saving model with best Classification-F1: 0.719761697181052 -> 0.7493387384279224 on epoch=199, global_step=800
05/25/2022 23:26:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=202
05/25/2022 23:26:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=204
05/25/2022 23:26:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=207
05/25/2022 23:26:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=209
05/25/2022 23:26:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=212
05/25/2022 23:26:22 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.7036326649229875 on epoch=212
05/25/2022 23:26:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=214
05/25/2022 23:26:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=217
05/25/2022 23:26:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=219
05/25/2022 23:26:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=222
05/25/2022 23:26:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=224
05/25/2022 23:26:36 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.5764397735934738 on epoch=224
05/25/2022 23:26:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=227
05/25/2022 23:26:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=229
05/25/2022 23:26:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=232
05/25/2022 23:26:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=234
05/25/2022 23:26:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=237
05/25/2022 23:26:50 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.7167962749615975 on epoch=237
05/25/2022 23:26:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=239
05/25/2022 23:26:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=242
05/25/2022 23:26:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=244
05/25/2022 23:27:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=247
05/25/2022 23:27:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=249
05/25/2022 23:27:04 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.7161031568052441 on epoch=249
05/25/2022 23:27:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=252
05/25/2022 23:27:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=254
05/25/2022 23:27:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=257
05/25/2022 23:27:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=259
05/25/2022 23:27:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=262
05/25/2022 23:27:19 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.7497556207233627 on epoch=262
05/25/2022 23:27:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7493387384279224 -> 0.7497556207233627 on epoch=262, global_step=1050
05/25/2022 23:27:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.23 on epoch=264
05/25/2022 23:27:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.17 on epoch=267
05/25/2022 23:27:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=269
05/25/2022 23:27:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=272
05/25/2022 23:27:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=274
05/25/2022 23:27:33 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.743984453661873 on epoch=274
05/25/2022 23:27:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=277
05/25/2022 23:27:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=279
05/25/2022 23:27:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=282
05/25/2022 23:27:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=284
05/25/2022 23:27:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=287
05/25/2022 23:27:47 - INFO - __main__ - Global step 1150 Train loss 0.17 Classification-F1 0.7505514705882352 on epoch=287
05/25/2022 23:27:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7497556207233627 -> 0.7505514705882352 on epoch=287, global_step=1150
05/25/2022 23:27:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=289
05/25/2022 23:27:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=292
05/25/2022 23:27:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=294
05/25/2022 23:27:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=297
05/25/2022 23:28:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=299
05/25/2022 23:28:01 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.665354763700352 on epoch=299
05/25/2022 23:28:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=302
05/25/2022 23:28:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=304
05/25/2022 23:28:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=307
05/25/2022 23:28:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=309
05/25/2022 23:28:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=312
05/25/2022 23:28:15 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.5192323232323233 on epoch=312
05/25/2022 23:28:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=314
05/25/2022 23:28:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=317
05/25/2022 23:28:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=319
05/25/2022 23:28:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=322
05/25/2022 23:28:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=324
05/25/2022 23:28:29 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.6470520466251016 on epoch=324
05/25/2022 23:28:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
05/25/2022 23:28:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=329
05/25/2022 23:28:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=332
05/25/2022 23:28:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=334
05/25/2022 23:28:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=337
05/25/2022 23:28:43 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.6994364901385774 on epoch=337
05/25/2022 23:28:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=339
05/25/2022 23:28:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=342
05/25/2022 23:28:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=344
05/25/2022 23:28:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=347
05/25/2022 23:28:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=349
05/25/2022 23:28:57 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.6470520466251016 on epoch=349
05/25/2022 23:29:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=352
05/25/2022 23:29:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=354
05/25/2022 23:29:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=357
05/25/2022 23:29:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=359
05/25/2022 23:29:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=362
05/25/2022 23:29:11 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.6654452659818098 on epoch=362
05/25/2022 23:29:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
05/25/2022 23:29:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=367
05/25/2022 23:29:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=369
05/25/2022 23:29:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
05/25/2022 23:29:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
05/25/2022 23:29:25 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.6786316677208517 on epoch=374
05/25/2022 23:29:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=377
05/25/2022 23:29:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=379
05/25/2022 23:29:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=382
05/25/2022 23:29:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/25/2022 23:29:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=387
05/25/2022 23:29:40 - INFO - __main__ - Global step 1550 Train loss 0.08 Classification-F1 0.7144963196877497 on epoch=387
05/25/2022 23:29:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=389
05/25/2022 23:29:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=392
05/25/2022 23:29:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
05/25/2022 23:29:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=397
05/25/2022 23:29:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=399
05/25/2022 23:29:54 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.6598268398268399 on epoch=399
05/25/2022 23:29:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=402
05/25/2022 23:29:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/25/2022 23:30:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=407
05/25/2022 23:30:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=409
05/25/2022 23:30:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=412
05/25/2022 23:30:08 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.6972824613716454 on epoch=412
05/25/2022 23:30:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=414
05/25/2022 23:30:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
05/25/2022 23:30:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=419
05/25/2022 23:30:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=422
05/25/2022 23:30:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=424
05/25/2022 23:30:22 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.6639339826839827 on epoch=424
05/25/2022 23:30:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=427
05/25/2022 23:30:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=429
05/25/2022 23:30:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
05/25/2022 23:30:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=434
05/25/2022 23:30:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=437
05/25/2022 23:30:36 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.661153837117326 on epoch=437
05/25/2022 23:30:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=439
05/25/2022 23:30:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=442
05/25/2022 23:30:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=444
05/25/2022 23:30:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=447
05/25/2022 23:30:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/25/2022 23:30:50 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.6815742982398743 on epoch=449
05/25/2022 23:30:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=452
05/25/2022 23:30:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/25/2022 23:30:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/25/2022 23:31:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=459
05/25/2022 23:31:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=462
05/25/2022 23:31:04 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.6519963196877497 on epoch=462
05/25/2022 23:31:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
05/25/2022 23:31:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=467
05/25/2022 23:31:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.17 on epoch=469
05/25/2022 23:31:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
05/25/2022 23:31:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/25/2022 23:31:18 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.66873460591133 on epoch=474
05/25/2022 23:31:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=477
05/25/2022 23:31:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
05/25/2022 23:31:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=482
05/25/2022 23:31:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
05/25/2022 23:31:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
05/25/2022 23:31:32 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.6820631700956244 on epoch=487
05/25/2022 23:31:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
05/25/2022 23:31:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=492
05/25/2022 23:31:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/25/2022 23:31:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
05/25/2022 23:31:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/25/2022 23:31:47 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.7003472222222222 on epoch=499
05/25/2022 23:31:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/25/2022 23:31:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/25/2022 23:31:54 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/25/2022 23:31:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
05/25/2022 23:32:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
05/25/2022 23:32:01 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.6645891690009338 on epoch=512
05/25/2022 23:32:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=514
05/25/2022 23:32:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/25/2022 23:32:08 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=519
05/25/2022 23:32:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=522
05/25/2022 23:32:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/25/2022 23:32:15 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.7016098484848484 on epoch=524
05/25/2022 23:32:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/25/2022 23:32:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/25/2022 23:32:23 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.26 on epoch=532
05/25/2022 23:32:25 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
05/25/2022 23:32:28 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
05/25/2022 23:32:29 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.630978705978706 on epoch=537
05/25/2022 23:32:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=539
05/25/2022 23:32:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
05/25/2022 23:32:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=544
05/25/2022 23:32:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
05/25/2022 23:32:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/25/2022 23:32:43 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.6694128787878788 on epoch=549
05/25/2022 23:32:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/25/2022 23:32:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=554
05/25/2022 23:32:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
05/25/2022 23:32:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
05/25/2022 23:32:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
05/25/2022 23:32:57 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.6749680715197957 on epoch=562
05/25/2022 23:33:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/25/2022 23:33:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
05/25/2022 23:33:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/25/2022 23:33:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=572
05/25/2022 23:33:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=574
05/25/2022 23:33:12 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.6793283714336347 on epoch=574
05/25/2022 23:33:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/25/2022 23:33:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/25/2022 23:33:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=582
05/25/2022 23:33:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/25/2022 23:33:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/25/2022 23:33:26 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6654562870080112 on epoch=587
05/25/2022 23:33:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
05/25/2022 23:33:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/25/2022 23:33:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/25/2022 23:33:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/25/2022 23:33:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/25/2022 23:33:40 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.661968400625388 on epoch=599
05/25/2022 23:33:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/25/2022 23:33:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
05/25/2022 23:33:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=607
05/25/2022 23:33:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/25/2022 23:33:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/25/2022 23:33:54 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6790160290160291 on epoch=612
05/25/2022 23:33:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/25/2022 23:34:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/25/2022 23:34:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/25/2022 23:34:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/25/2022 23:34:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/25/2022 23:34:09 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6793283714336347 on epoch=624
05/25/2022 23:34:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/25/2022 23:34:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/25/2022 23:34:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/25/2022 23:34:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/25/2022 23:34:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/25/2022 23:34:23 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.661968400625388 on epoch=637
05/25/2022 23:34:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/25/2022 23:34:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/25/2022 23:34:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/25/2022 23:34:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=647
05/25/2022 23:34:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/25/2022 23:34:37 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6967123373373373 on epoch=649
05/25/2022 23:34:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/25/2022 23:34:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/25/2022 23:34:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/25/2022 23:34:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/25/2022 23:34:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
05/25/2022 23:34:52 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6961943398048875 on epoch=662
05/25/2022 23:34:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/25/2022 23:34:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=667
05/25/2022 23:35:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
05/25/2022 23:35:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/25/2022 23:35:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/25/2022 23:35:06 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6833855799373041 on epoch=674
05/25/2022 23:35:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/25/2022 23:35:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=679
05/25/2022 23:35:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/25/2022 23:35:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/25/2022 23:35:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/25/2022 23:35:20 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.6968731718731719 on epoch=687
05/25/2022 23:35:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=689
05/25/2022 23:35:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/25/2022 23:35:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/25/2022 23:35:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/25/2022 23:35:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/25/2022 23:35:35 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6956365957380156 on epoch=699
05/25/2022 23:35:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/25/2022 23:35:40 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/25/2022 23:35:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/25/2022 23:35:45 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/25/2022 23:35:48 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/25/2022 23:35:49 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7149529569892472 on epoch=712
05/25/2022 23:35:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/25/2022 23:35:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/25/2022 23:35:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
05/25/2022 23:35:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
05/25/2022 23:36:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/25/2022 23:36:03 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7123015873015873 on epoch=724
05/25/2022 23:36:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/25/2022 23:36:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/25/2022 23:36:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=732
05/25/2022 23:36:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/25/2022 23:36:17 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/25/2022 23:36:18 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6938657407407407 on epoch=737
05/25/2022 23:36:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/25/2022 23:36:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=742
05/25/2022 23:36:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/25/2022 23:36:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/25/2022 23:36:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=749
05/25/2022 23:36:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:36:33 - INFO - __main__ - Printing 3 examples
05/25/2022 23:36:33 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 23:36:33 - INFO - __main__ - ['others']
05/25/2022 23:36:33 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 23:36:33 - INFO - __main__ - ['others']
05/25/2022 23:36:33 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 23:36:33 - INFO - __main__ - ['others']
05/25/2022 23:36:33 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:36:33 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:36:33 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7126488095238095 on epoch=749
05/25/2022 23:36:33 - INFO - __main__ - save last model!
05/25/2022 23:36:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 23:36:33 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 23:36:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:36:33 - INFO - __main__ - Printing 3 examples
05/25/2022 23:36:33 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/25/2022 23:36:33 - INFO - __main__ - ['others']
05/25/2022 23:36:33 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/25/2022 23:36:33 - INFO - __main__ - ['others']
05/25/2022 23:36:33 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/25/2022 23:36:33 - INFO - __main__ - ['others']
05/25/2022 23:36:33 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:36:33 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 23:36:33 - INFO - __main__ - Printing 3 examples
05/25/2022 23:36:33 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 23:36:33 - INFO - __main__ - ['others']
05/25/2022 23:36:33 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 23:36:33 - INFO - __main__ - ['others']
05/25/2022 23:36:33 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 23:36:33 - INFO - __main__ - ['others']
05/25/2022 23:36:33 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:36:33 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:36:33 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 23:36:35 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:36:40 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 23:36:48 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:36:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 23:36:49 - INFO - __main__ - Starting training!
05/25/2022 23:38:29 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_100_0.2_8_predictions.txt
05/25/2022 23:38:29 - INFO - __main__ - Classification-F1 on test data: 0.1918
05/25/2022 23:38:30 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.7505514705882352, test_performance=0.1918089715263014
05/25/2022 23:38:30 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
05/25/2022 23:38:31 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:38:31 - INFO - __main__ - Printing 3 examples
05/25/2022 23:38:31 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 23:38:31 - INFO - __main__ - ['others']
05/25/2022 23:38:31 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 23:38:31 - INFO - __main__ - ['others']
05/25/2022 23:38:31 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 23:38:31 - INFO - __main__ - ['others']
05/25/2022 23:38:31 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:38:31 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:38:31 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 23:38:31 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:38:31 - INFO - __main__ - Printing 3 examples
05/25/2022 23:38:31 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/25/2022 23:38:31 - INFO - __main__ - ['others']
05/25/2022 23:38:31 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/25/2022 23:38:31 - INFO - __main__ - ['others']
05/25/2022 23:38:31 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/25/2022 23:38:31 - INFO - __main__ - ['others']
05/25/2022 23:38:31 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:38:31 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:38:31 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 23:38:46 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:38:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 23:38:47 - INFO - __main__ - Starting training!
05/25/2022 23:38:50 - INFO - __main__ - Step 10 Global step 10 Train loss 2.44 on epoch=2
05/25/2022 23:38:53 - INFO - __main__ - Step 20 Global step 20 Train loss 1.13 on epoch=4
05/25/2022 23:38:56 - INFO - __main__ - Step 30 Global step 30 Train loss 0.92 on epoch=7
05/25/2022 23:38:59 - INFO - __main__ - Step 40 Global step 40 Train loss 0.91 on epoch=9
05/25/2022 23:39:01 - INFO - __main__ - Step 50 Global step 50 Train loss 0.88 on epoch=12
05/25/2022 23:39:02 - INFO - __main__ - Global step 50 Train loss 1.26 Classification-F1 0.1 on epoch=12
05/25/2022 23:39:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/25/2022 23:39:05 - INFO - __main__ - Step 60 Global step 60 Train loss 0.90 on epoch=14
05/25/2022 23:39:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.90 on epoch=17
05/25/2022 23:39:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.80 on epoch=19
05/25/2022 23:39:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.77 on epoch=22
05/25/2022 23:39:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.79 on epoch=24
05/25/2022 23:39:16 - INFO - __main__ - Global step 100 Train loss 0.83 Classification-F1 0.29413179413179413 on epoch=24
05/25/2022 23:39:16 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.29413179413179413 on epoch=24, global_step=100
05/25/2022 23:39:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.75 on epoch=27
05/25/2022 23:39:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.68 on epoch=29
05/25/2022 23:39:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.70 on epoch=32
05/25/2022 23:39:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.70 on epoch=34
05/25/2022 23:39:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=37
05/25/2022 23:39:30 - INFO - __main__ - Global step 150 Train loss 0.68 Classification-F1 0.5538961038961039 on epoch=37
05/25/2022 23:39:30 - INFO - __main__ - Saving model with best Classification-F1: 0.29413179413179413 -> 0.5538961038961039 on epoch=37, global_step=150
05/25/2022 23:39:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.66 on epoch=39
05/25/2022 23:39:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=42
05/25/2022 23:39:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=44
05/25/2022 23:39:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=47
05/25/2022 23:39:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.53 on epoch=49
05/25/2022 23:39:45 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.5444690810544469 on epoch=49
05/25/2022 23:39:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=52
05/25/2022 23:39:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=54
05/25/2022 23:39:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=57
05/25/2022 23:39:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=59
05/25/2022 23:39:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=62
05/25/2022 23:39:59 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.705229377104377 on epoch=62
05/25/2022 23:39:59 - INFO - __main__ - Saving model with best Classification-F1: 0.5538961038961039 -> 0.705229377104377 on epoch=62, global_step=250
05/25/2022 23:40:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=64
05/25/2022 23:40:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.28 on epoch=67
05/25/2022 23:40:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=69
05/25/2022 23:40:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=72
05/25/2022 23:40:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=74
05/25/2022 23:40:13 - INFO - __main__ - Global step 300 Train loss 0.33 Classification-F1 0.5720779220779222 on epoch=74
05/25/2022 23:40:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=77
05/25/2022 23:40:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=79
05/25/2022 23:40:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.31 on epoch=82
05/25/2022 23:40:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=84
05/25/2022 23:40:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=87
05/25/2022 23:40:27 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.712611567924661 on epoch=87
05/25/2022 23:40:27 - INFO - __main__ - Saving model with best Classification-F1: 0.705229377104377 -> 0.712611567924661 on epoch=87, global_step=350
05/25/2022 23:40:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=89
05/25/2022 23:40:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=92
05/25/2022 23:40:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.18 on epoch=94
05/25/2022 23:40:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=97
05/25/2022 23:40:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=99
05/25/2022 23:40:41 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.7234477124183007 on epoch=99
05/25/2022 23:40:41 - INFO - __main__ - Saving model with best Classification-F1: 0.712611567924661 -> 0.7234477124183007 on epoch=99, global_step=400
05/25/2022 23:40:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=102
05/25/2022 23:40:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=104
05/25/2022 23:40:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=107
05/25/2022 23:40:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.19 on epoch=109
05/25/2022 23:40:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=112
05/25/2022 23:40:55 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.6046957671957672 on epoch=112
05/25/2022 23:40:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=114
05/25/2022 23:41:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.18 on epoch=117
05/25/2022 23:41:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=119
05/25/2022 23:41:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=122
05/25/2022 23:41:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=124
05/25/2022 23:41:09 - INFO - __main__ - Global step 500 Train loss 0.18 Classification-F1 0.7095951417004049 on epoch=124
05/25/2022 23:41:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=127
05/25/2022 23:41:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=129
05/25/2022 23:41:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=132
05/25/2022 23:41:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=134
05/25/2022 23:41:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=137
05/25/2022 23:41:23 - INFO - __main__ - Global step 550 Train loss 0.11 Classification-F1 0.5558626295468401 on epoch=137
05/25/2022 23:41:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=139
05/25/2022 23:41:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.11 on epoch=142
05/25/2022 23:41:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=144
05/25/2022 23:41:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=147
05/25/2022 23:41:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=149
05/25/2022 23:41:38 - INFO - __main__ - Global step 600 Train loss 0.11 Classification-F1 0.6792441792441791 on epoch=149
05/25/2022 23:41:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=152
05/25/2022 23:41:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=154
05/25/2022 23:41:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.06 on epoch=157
05/25/2022 23:41:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=159
05/25/2022 23:41:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=162
05/25/2022 23:41:52 - INFO - __main__ - Global step 650 Train loss 0.07 Classification-F1 0.48931966726084375 on epoch=162
05/25/2022 23:41:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=164
05/25/2022 23:41:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=167
05/25/2022 23:41:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=169
05/25/2022 23:42:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.07 on epoch=172
05/25/2022 23:42:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=174
05/25/2022 23:42:06 - INFO - __main__ - Global step 700 Train loss 0.04 Classification-F1 0.7223529411764705 on epoch=174
05/25/2022 23:42:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=177
05/25/2022 23:42:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=179
05/25/2022 23:42:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=182
05/25/2022 23:42:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=184
05/25/2022 23:42:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=187
05/25/2022 23:42:20 - INFO - __main__ - Global step 750 Train loss 0.04 Classification-F1 0.693688767372978 on epoch=187
05/25/2022 23:42:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=189
05/25/2022 23:42:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=192
05/25/2022 23:42:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=194
05/25/2022 23:42:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
05/25/2022 23:42:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=199
05/25/2022 23:42:34 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.6309523809523809 on epoch=199
05/25/2022 23:42:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=202
05/25/2022 23:42:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=204
05/25/2022 23:42:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=207
05/25/2022 23:42:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=209
05/25/2022 23:42:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=212
05/25/2022 23:42:48 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.7131895881895882 on epoch=212
05/25/2022 23:42:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=214
05/25/2022 23:42:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=217
05/25/2022 23:42:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=219
05/25/2022 23:42:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=222
05/25/2022 23:43:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=224
05/25/2022 23:43:02 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.6429799160062318 on epoch=224
05/25/2022 23:43:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=227
05/25/2022 23:43:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
05/25/2022 23:43:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
05/25/2022 23:43:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=234
05/25/2022 23:43:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=237
05/25/2022 23:43:17 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.7303757194649035 on epoch=237
05/25/2022 23:43:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7234477124183007 -> 0.7303757194649035 on epoch=237, global_step=950
05/25/2022 23:43:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=239
05/25/2022 23:43:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
05/25/2022 23:43:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=244
05/25/2022 23:43:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
05/25/2022 23:43:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=249
05/25/2022 23:43:31 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.7021668087524864 on epoch=249
05/25/2022 23:43:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
05/25/2022 23:43:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=254
05/25/2022 23:43:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
05/25/2022 23:43:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=259
05/25/2022 23:43:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=262
05/25/2022 23:43:46 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.7164956011730206 on epoch=262
05/25/2022 23:43:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
05/25/2022 23:43:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=267
05/25/2022 23:43:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
05/25/2022 23:43:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=272
05/25/2022 23:43:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
05/25/2022 23:44:00 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.6559425524595803 on epoch=274
05/25/2022 23:44:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
05/25/2022 23:44:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/25/2022 23:44:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=282
05/25/2022 23:44:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/25/2022 23:44:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
05/25/2022 23:44:14 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.6945036614154261 on epoch=287
05/25/2022 23:44:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=289
05/25/2022 23:44:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
05/25/2022 23:44:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
05/25/2022 23:44:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
05/25/2022 23:44:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=299
05/25/2022 23:44:29 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.69467228677755 on epoch=299
05/25/2022 23:44:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/25/2022 23:44:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
05/25/2022 23:44:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/25/2022 23:44:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=309
05/25/2022 23:44:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
05/25/2022 23:44:43 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.6300100410007531 on epoch=312
05/25/2022 23:44:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/25/2022 23:44:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
05/25/2022 23:44:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=319
05/25/2022 23:44:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/25/2022 23:44:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/25/2022 23:44:57 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6294066652218129 on epoch=324
05/25/2022 23:45:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
05/25/2022 23:45:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
05/25/2022 23:45:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/25/2022 23:45:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/25/2022 23:45:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
05/25/2022 23:45:12 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.7051435406698565 on epoch=337
05/25/2022 23:45:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=339
05/25/2022 23:45:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/25/2022 23:45:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=344
05/25/2022 23:45:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/25/2022 23:45:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/25/2022 23:45:26 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.723853238265003 on epoch=349
05/25/2022 23:45:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/25/2022 23:45:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/25/2022 23:45:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
05/25/2022 23:45:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/25/2022 23:45:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/25/2022 23:45:40 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.7102547211242864 on epoch=362
05/25/2022 23:45:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
05/25/2022 23:45:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
05/25/2022 23:45:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=369
05/25/2022 23:45:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
05/25/2022 23:45:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/25/2022 23:45:55 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.7437950937950938 on epoch=374
05/25/2022 23:45:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7303757194649035 -> 0.7437950937950938 on epoch=374, global_step=1500
05/25/2022 23:45:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/25/2022 23:46:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
05/25/2022 23:46:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
05/25/2022 23:46:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/25/2022 23:46:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/25/2022 23:46:09 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.7308558558558559 on epoch=387
05/25/2022 23:46:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
05/25/2022 23:46:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/25/2022 23:46:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
05/25/2022 23:46:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/25/2022 23:46:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/25/2022 23:46:23 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.7525772981514981 on epoch=399
05/25/2022 23:46:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7437950937950938 -> 0.7525772981514981 on epoch=399, global_step=1600
05/25/2022 23:46:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
05/25/2022 23:46:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/25/2022 23:46:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/25/2022 23:46:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/25/2022 23:46:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/25/2022 23:46:38 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7083250083250083 on epoch=412
05/25/2022 23:46:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/25/2022 23:46:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/25/2022 23:46:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
05/25/2022 23:46:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
05/25/2022 23:46:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/25/2022 23:46:52 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7110653574068209 on epoch=424
05/25/2022 23:46:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
05/25/2022 23:46:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/25/2022 23:47:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/25/2022 23:47:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/25/2022 23:47:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/25/2022 23:47:07 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7427731092436975 on epoch=437
05/25/2022 23:47:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/25/2022 23:47:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
05/25/2022 23:47:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=444
05/25/2022 23:47:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/25/2022 23:47:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/25/2022 23:47:21 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7528214355660008 on epoch=449
05/25/2022 23:47:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7525772981514981 -> 0.7528214355660008 on epoch=449, global_step=1800
05/25/2022 23:47:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/25/2022 23:47:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/25/2022 23:47:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
05/25/2022 23:47:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
05/25/2022 23:47:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/25/2022 23:47:35 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.7722214192802429 on epoch=462
05/25/2022 23:47:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7528214355660008 -> 0.7722214192802429 on epoch=462, global_step=1850
05/25/2022 23:47:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/25/2022 23:47:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/25/2022 23:47:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/25/2022 23:47:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/25/2022 23:47:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/25/2022 23:47:50 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.7487017231134878 on epoch=474
05/25/2022 23:47:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/25/2022 23:47:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/25/2022 23:47:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/25/2022 23:48:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/25/2022 23:48:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/25/2022 23:48:04 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7459893048128343 on epoch=487
05/25/2022 23:48:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/25/2022 23:48:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/25/2022 23:48:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/25/2022 23:48:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/25/2022 23:48:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/25/2022 23:48:19 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.73832020523197 on epoch=499
05/25/2022 23:48:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/25/2022 23:48:24 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/25/2022 23:48:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/25/2022 23:48:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/25/2022 23:48:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/25/2022 23:48:33 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7317640692640692 on epoch=512
05/25/2022 23:48:36 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/25/2022 23:48:38 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/25/2022 23:48:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/25/2022 23:48:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/25/2022 23:48:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/25/2022 23:48:48 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6948547840188707 on epoch=524
05/25/2022 23:48:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/25/2022 23:48:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/25/2022 23:48:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/25/2022 23:48:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/25/2022 23:49:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/25/2022 23:49:02 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7423529411764705 on epoch=537
05/25/2022 23:49:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/25/2022 23:49:07 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/25/2022 23:49:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/25/2022 23:49:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/25/2022 23:49:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/25/2022 23:49:17 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6795917157759263 on epoch=549
05/25/2022 23:49:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/25/2022 23:49:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/25/2022 23:49:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/25/2022 23:49:27 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/25/2022 23:49:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/25/2022 23:49:31 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.5889077106468411 on epoch=562
05/25/2022 23:49:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/25/2022 23:49:36 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/25/2022 23:49:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/25/2022 23:49:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/25/2022 23:49:44 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/25/2022 23:49:45 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.5906868353748822 on epoch=574
05/25/2022 23:49:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/25/2022 23:49:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/25/2022 23:49:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/25/2022 23:49:56 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=584
05/25/2022 23:49:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/25/2022 23:50:00 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6885552270420546 on epoch=587
05/25/2022 23:50:02 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/25/2022 23:50:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/25/2022 23:50:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/25/2022 23:50:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/25/2022 23:50:13 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/25/2022 23:50:14 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.5271399194225281 on epoch=599
05/25/2022 23:50:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/25/2022 23:50:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/25/2022 23:50:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/25/2022 23:50:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/25/2022 23:50:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/25/2022 23:50:28 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.5510713542750156 on epoch=612
05/25/2022 23:50:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/25/2022 23:50:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
05/25/2022 23:50:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/25/2022 23:50:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/25/2022 23:50:42 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/25/2022 23:50:43 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.5601556776556776 on epoch=624
05/25/2022 23:50:46 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/25/2022 23:50:48 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/25/2022 23:50:51 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/25/2022 23:50:53 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/25/2022 23:50:56 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/25/2022 23:50:57 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.6950358669108669 on epoch=637
05/25/2022 23:51:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/25/2022 23:51:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/25/2022 23:51:05 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/25/2022 23:51:08 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/25/2022 23:51:10 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/25/2022 23:51:12 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.5593650793650793 on epoch=649
05/25/2022 23:51:14 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/25/2022 23:51:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/25/2022 23:51:19 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/25/2022 23:51:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=659
05/25/2022 23:51:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/25/2022 23:51:26 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7090728715728717 on epoch=662
05/25/2022 23:51:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=664
05/25/2022 23:51:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/25/2022 23:51:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/25/2022 23:51:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/25/2022 23:51:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
05/25/2022 23:51:40 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7181651069518716 on epoch=674
05/25/2022 23:51:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/25/2022 23:51:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
05/25/2022 23:51:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/25/2022 23:51:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/25/2022 23:51:53 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/25/2022 23:51:55 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7758743227493228 on epoch=687
05/25/2022 23:51:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7722214192802429 -> 0.7758743227493228 on epoch=687, global_step=2750
05/25/2022 23:51:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/25/2022 23:52:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/25/2022 23:52:02 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/25/2022 23:52:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/25/2022 23:52:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/25/2022 23:52:09 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7612058080808081 on epoch=699
05/25/2022 23:52:11 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/25/2022 23:52:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/25/2022 23:52:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/25/2022 23:52:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/25/2022 23:52:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/25/2022 23:52:23 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7172411975542906 on epoch=712
05/25/2022 23:52:26 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=714
05/25/2022 23:52:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/25/2022 23:52:31 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/25/2022 23:52:34 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=722
05/25/2022 23:52:36 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
05/25/2022 23:52:37 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.7237442781622305 on epoch=724
05/25/2022 23:52:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/25/2022 23:52:43 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/25/2022 23:52:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/25/2022 23:52:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/25/2022 23:52:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/25/2022 23:52:52 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7031263091746963 on epoch=737
05/25/2022 23:52:54 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/25/2022 23:52:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/25/2022 23:53:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/25/2022 23:53:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/25/2022 23:53:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/25/2022 23:53:06 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.5787878787878789 on epoch=749
05/25/2022 23:53:06 - INFO - __main__ - save last model!
05/25/2022 23:53:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 23:53:06 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 23:53:06 - INFO - __main__ - Printing 3 examples
05/25/2022 23:53:06 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 23:53:06 - INFO - __main__ - ['others']
05/25/2022 23:53:06 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 23:53:06 - INFO - __main__ - ['others']
05/25/2022 23:53:06 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 23:53:06 - INFO - __main__ - ['others']
05/25/2022 23:53:06 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:53:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:53:06 - INFO - __main__ - Printing 3 examples
05/25/2022 23:53:06 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 23:53:06 - INFO - __main__ - ['others']
05/25/2022 23:53:06 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 23:53:06 - INFO - __main__ - ['others']
05/25/2022 23:53:06 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 23:53:06 - INFO - __main__ - ['others']
05/25/2022 23:53:06 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:53:06 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:53:06 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 23:53:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:53:06 - INFO - __main__ - Printing 3 examples
05/25/2022 23:53:06 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/25/2022 23:53:06 - INFO - __main__ - ['others']
05/25/2022 23:53:06 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/25/2022 23:53:06 - INFO - __main__ - ['others']
05/25/2022 23:53:06 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/25/2022 23:53:06 - INFO - __main__ - ['others']
05/25/2022 23:53:06 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:53:06 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:53:06 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 23:53:08 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:53:13 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 23:53:22 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:53:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 23:53:22 - INFO - __main__ - Starting training!
05/25/2022 23:54:47 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_13_0.5_8_predictions.txt
05/25/2022 23:54:47 - INFO - __main__ - Classification-F1 on test data: 0.0903
05/25/2022 23:54:47 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.7758743227493228, test_performance=0.09028969077705012
05/25/2022 23:54:47 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
05/25/2022 23:54:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:54:48 - INFO - __main__ - Printing 3 examples
05/25/2022 23:54:48 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 23:54:48 - INFO - __main__ - ['others']
05/25/2022 23:54:48 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 23:54:48 - INFO - __main__ - ['others']
05/25/2022 23:54:48 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 23:54:48 - INFO - __main__ - ['others']
05/25/2022 23:54:48 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:54:48 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:54:48 - INFO - __main__ - Loaded 64 examples from train data
05/25/2022 23:54:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/25/2022 23:54:48 - INFO - __main__ - Printing 3 examples
05/25/2022 23:54:48 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/25/2022 23:54:48 - INFO - __main__ - ['others']
05/25/2022 23:54:48 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/25/2022 23:54:48 - INFO - __main__ - ['others']
05/25/2022 23:54:48 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/25/2022 23:54:48 - INFO - __main__ - ['others']
05/25/2022 23:54:48 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:54:48 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:54:48 - INFO - __main__ - Loaded 64 examples from dev data
05/25/2022 23:55:03 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:55:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/25/2022 23:55:04 - INFO - __main__ - Starting training!
05/25/2022 23:55:07 - INFO - __main__ - Step 10 Global step 10 Train loss 2.66 on epoch=2
05/25/2022 23:55:10 - INFO - __main__ - Step 20 Global step 20 Train loss 1.28 on epoch=4
05/25/2022 23:55:13 - INFO - __main__ - Step 30 Global step 30 Train loss 0.99 on epoch=7
05/25/2022 23:55:15 - INFO - __main__ - Step 40 Global step 40 Train loss 1.03 on epoch=9
05/25/2022 23:55:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=12
05/25/2022 23:55:19 - INFO - __main__ - Global step 50 Train loss 1.38 Classification-F1 0.1 on epoch=12
05/25/2022 23:55:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/25/2022 23:55:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.88 on epoch=14
05/25/2022 23:55:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.84 on epoch=17
05/25/2022 23:55:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.77 on epoch=19
05/25/2022 23:55:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.79 on epoch=22
05/25/2022 23:55:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.69 on epoch=24
05/25/2022 23:55:33 - INFO - __main__ - Global step 100 Train loss 0.80 Classification-F1 0.20666666666666667 on epoch=24
05/25/2022 23:55:33 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.20666666666666667 on epoch=24, global_step=100
05/25/2022 23:55:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=27
05/25/2022 23:55:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.75 on epoch=29
05/25/2022 23:55:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.69 on epoch=32
05/25/2022 23:55:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.70 on epoch=34
05/25/2022 23:55:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.60 on epoch=37
05/25/2022 23:55:47 - INFO - __main__ - Global step 150 Train loss 0.69 Classification-F1 0.445 on epoch=37
05/25/2022 23:55:47 - INFO - __main__ - Saving model with best Classification-F1: 0.20666666666666667 -> 0.445 on epoch=37, global_step=150
05/25/2022 23:55:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.64 on epoch=39
05/25/2022 23:55:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.62 on epoch=42
05/25/2022 23:55:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.60 on epoch=44
05/25/2022 23:55:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=47
05/25/2022 23:56:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=49
05/25/2022 23:56:02 - INFO - __main__ - Global step 200 Train loss 0.59 Classification-F1 0.4422657952069717 on epoch=49
05/25/2022 23:56:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=52
05/25/2022 23:56:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=54
05/25/2022 23:56:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=57
05/25/2022 23:56:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=59
05/25/2022 23:56:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=62
05/25/2022 23:56:16 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.7284780578898226 on epoch=62
05/25/2022 23:56:16 - INFO - __main__ - Saving model with best Classification-F1: 0.445 -> 0.7284780578898226 on epoch=62, global_step=250
05/25/2022 23:56:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=64
05/25/2022 23:56:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=67
05/25/2022 23:56:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=69
05/25/2022 23:56:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=72
05/25/2022 23:56:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=74
05/25/2022 23:56:30 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.6926653544300604 on epoch=74
05/25/2022 23:56:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=77
05/25/2022 23:56:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=79
05/25/2022 23:56:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=82
05/25/2022 23:56:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=84
05/25/2022 23:56:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=87
05/25/2022 23:56:44 - INFO - __main__ - Global step 350 Train loss 0.31 Classification-F1 0.6319754970711988 on epoch=87
05/25/2022 23:56:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=89
05/25/2022 23:56:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=92
05/25/2022 23:56:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.22 on epoch=94
05/25/2022 23:56:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=97
05/25/2022 23:56:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=99
05/25/2022 23:56:58 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.5165166236273797 on epoch=99
05/25/2022 23:57:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=102
05/25/2022 23:57:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=104
05/25/2022 23:57:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=107
05/25/2022 23:57:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.19 on epoch=109
05/25/2022 23:57:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.17 on epoch=112
05/25/2022 23:57:12 - INFO - __main__ - Global step 450 Train loss 0.21 Classification-F1 0.6043371877975536 on epoch=112
05/25/2022 23:57:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=114
05/25/2022 23:57:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=117
05/25/2022 23:57:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=119
05/25/2022 23:57:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=122
05/25/2022 23:57:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=124
05/25/2022 23:57:26 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.5532020872865275 on epoch=124
05/25/2022 23:57:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=127
05/25/2022 23:57:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=129
05/25/2022 23:57:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=132
05/25/2022 23:57:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=134
05/25/2022 23:57:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=137
05/25/2022 23:57:40 - INFO - __main__ - Global step 550 Train loss 0.17 Classification-F1 0.5590925511978143 on epoch=137
05/25/2022 23:57:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=139
05/25/2022 23:57:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=142
05/25/2022 23:57:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=144
05/25/2022 23:57:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=147
05/25/2022 23:57:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=149
05/25/2022 23:57:55 - INFO - __main__ - Global step 600 Train loss 0.10 Classification-F1 0.5967049977885891 on epoch=149
05/25/2022 23:57:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=152
05/25/2022 23:58:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=154
05/25/2022 23:58:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=157
05/25/2022 23:58:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=159
05/25/2022 23:58:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=162
05/25/2022 23:58:09 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.6336336336336336 on epoch=162
05/25/2022 23:58:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.07 on epoch=164
05/25/2022 23:58:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=167
05/25/2022 23:58:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=169
05/25/2022 23:58:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=172
05/25/2022 23:58:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=174
05/25/2022 23:58:23 - INFO - __main__ - Global step 700 Train loss 0.07 Classification-F1 0.5499245852187028 on epoch=174
05/25/2022 23:58:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=177
05/25/2022 23:58:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=179
05/25/2022 23:58:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=182
05/25/2022 23:58:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=184
05/25/2022 23:58:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=187
05/25/2022 23:58:37 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.6323497867615514 on epoch=187
05/25/2022 23:58:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=189
05/25/2022 23:58:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=192
05/25/2022 23:58:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=194
05/25/2022 23:58:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=197
05/25/2022 23:58:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=199
05/25/2022 23:58:52 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.6593997035573123 on epoch=199
05/25/2022 23:58:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=202
05/25/2022 23:58:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=204
05/25/2022 23:59:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=207
05/25/2022 23:59:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=209
05/25/2022 23:59:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=212
05/25/2022 23:59:06 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.4538875103391233 on epoch=212
05/25/2022 23:59:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=214
05/25/2022 23:59:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=217
05/25/2022 23:59:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=219
05/25/2022 23:59:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=222
05/25/2022 23:59:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=224
05/25/2022 23:59:20 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.702572964669739 on epoch=224
05/25/2022 23:59:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=227
05/25/2022 23:59:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=229
05/25/2022 23:59:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=232
05/25/2022 23:59:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=234
05/25/2022 23:59:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
05/25/2022 23:59:34 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.6170223352498894 on epoch=237
05/25/2022 23:59:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
05/25/2022 23:59:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=242
05/25/2022 23:59:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
05/25/2022 23:59:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=247
05/25/2022 23:59:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
05/25/2022 23:59:49 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.5976313000306809 on epoch=249
05/25/2022 23:59:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
05/25/2022 23:59:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
05/25/2022 23:59:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
05/25/2022 23:59:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=259
05/26/2022 00:00:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
05/26/2022 00:00:03 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.5212698412698413 on epoch=262
05/26/2022 00:00:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=264
05/26/2022 00:00:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
05/26/2022 00:00:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/26/2022 00:00:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
05/26/2022 00:00:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=274
05/26/2022 00:00:17 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.5522039897039897 on epoch=274
05/26/2022 00:00:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
05/26/2022 00:00:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/26/2022 00:00:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/26/2022 00:00:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/26/2022 00:00:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
05/26/2022 00:00:32 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.6118483978957436 on epoch=287
05/26/2022 00:00:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
05/26/2022 00:00:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/26/2022 00:00:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
05/26/2022 00:00:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
05/26/2022 00:00:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/26/2022 00:00:46 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.6470556691144926 on epoch=299
05/26/2022 00:00:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
05/26/2022 00:00:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=304
05/26/2022 00:00:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/26/2022 00:00:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/26/2022 00:01:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
05/26/2022 00:01:01 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.39064992978036456 on epoch=312
05/26/2022 00:01:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/26/2022 00:01:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=317
05/26/2022 00:01:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
05/26/2022 00:01:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/26/2022 00:01:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/26/2022 00:01:15 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.519118232924616 on epoch=324
05/26/2022 00:01:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/26/2022 00:01:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/26/2022 00:01:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/26/2022 00:01:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/26/2022 00:01:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/26/2022 00:01:30 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.5598976982097186 on epoch=337
05/26/2022 00:01:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/26/2022 00:01:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/26/2022 00:01:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/26/2022 00:01:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=347
05/26/2022 00:01:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/26/2022 00:01:44 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.529706083390294 on epoch=349
05/26/2022 00:01:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/26/2022 00:01:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/26/2022 00:01:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
05/26/2022 00:01:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/26/2022 00:01:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/26/2022 00:01:58 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.3858238947753014 on epoch=362
05/26/2022 00:02:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/26/2022 00:02:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/26/2022 00:02:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/26/2022 00:02:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/26/2022 00:02:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/26/2022 00:02:13 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5676647154066509 on epoch=374
05/26/2022 00:02:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/26/2022 00:02:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/26/2022 00:02:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/26/2022 00:02:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
05/26/2022 00:02:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
05/26/2022 00:02:27 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6728063517427344 on epoch=387
05/26/2022 00:02:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
05/26/2022 00:02:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/26/2022 00:02:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
05/26/2022 00:02:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/26/2022 00:02:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/26/2022 00:02:41 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5261734997029114 on epoch=399
05/26/2022 00:02:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/26/2022 00:02:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/26/2022 00:02:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/26/2022 00:02:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=409
05/26/2022 00:02:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
05/26/2022 00:02:56 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6307923707117254 on epoch=412
05/26/2022 00:02:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/26/2022 00:03:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
05/26/2022 00:03:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/26/2022 00:03:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/26/2022 00:03:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/26/2022 00:03:10 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.619570707070707 on epoch=424
05/26/2022 00:03:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/26/2022 00:03:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/26/2022 00:03:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
05/26/2022 00:03:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
05/26/2022 00:03:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/26/2022 00:03:24 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.701890756302521 on epoch=437
05/26/2022 00:03:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/26/2022 00:03:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/26/2022 00:03:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/26/2022 00:03:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/26/2022 00:03:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/26/2022 00:03:39 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.6319232238349884 on epoch=449
05/26/2022 00:03:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/26/2022 00:03:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/26/2022 00:03:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/26/2022 00:03:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/26/2022 00:03:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/26/2022 00:03:53 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6578035999088632 on epoch=462
05/26/2022 00:03:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/26/2022 00:03:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/26/2022 00:04:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/26/2022 00:04:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/26/2022 00:04:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
05/26/2022 00:04:08 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7310682893847195 on epoch=474
05/26/2022 00:04:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7284780578898226 -> 0.7310682893847195 on epoch=474, global_step=1900
05/26/2022 00:04:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/26/2022 00:04:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/26/2022 00:04:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/26/2022 00:04:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/26/2022 00:04:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/26/2022 00:04:22 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6748060183544053 on epoch=487
05/26/2022 00:04:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/26/2022 00:04:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/26/2022 00:04:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/26/2022 00:04:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
05/26/2022 00:04:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/26/2022 00:04:36 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6270205898155082 on epoch=499
05/26/2022 00:04:39 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/26/2022 00:04:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/26/2022 00:04:44 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/26/2022 00:04:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/26/2022 00:04:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/26/2022 00:04:51 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.6048229000531632 on epoch=512
05/26/2022 00:04:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/26/2022 00:04:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/26/2022 00:04:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/26/2022 00:05:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/26/2022 00:05:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=524
05/26/2022 00:05:05 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.5398856209150327 on epoch=524
05/26/2022 00:05:08 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/26/2022 00:05:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/26/2022 00:05:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/26/2022 00:05:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/26/2022 00:05:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/26/2022 00:05:20 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.5340376305893547 on epoch=537
05/26/2022 00:05:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/26/2022 00:05:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/26/2022 00:05:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/26/2022 00:05:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/26/2022 00:05:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/26/2022 00:05:34 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.6530950305143852 on epoch=549
05/26/2022 00:05:37 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/26/2022 00:05:39 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/26/2022 00:05:42 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/26/2022 00:05:44 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/26/2022 00:05:47 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/26/2022 00:05:48 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7032837301587301 on epoch=562
05/26/2022 00:05:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/26/2022 00:05:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/26/2022 00:05:56 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/26/2022 00:05:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=572
05/26/2022 00:06:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/26/2022 00:06:03 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6887017231134879 on epoch=574
05/26/2022 00:06:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/26/2022 00:06:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/26/2022 00:06:11 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/26/2022 00:06:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/26/2022 00:06:16 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/26/2022 00:06:17 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.7081447963800904 on epoch=587
05/26/2022 00:06:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/26/2022 00:06:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/26/2022 00:06:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/26/2022 00:06:28 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/26/2022 00:06:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/26/2022 00:06:32 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.6751091269841271 on epoch=599
05/26/2022 00:06:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/26/2022 00:06:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/26/2022 00:06:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=607
05/26/2022 00:06:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/26/2022 00:06:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/26/2022 00:06:46 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.5840056717476073 on epoch=612
05/26/2022 00:06:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/26/2022 00:06:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/26/2022 00:06:54 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/26/2022 00:06:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/26/2022 00:06:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/26/2022 00:07:00 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.680568824047085 on epoch=624
05/26/2022 00:07:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/26/2022 00:07:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/26/2022 00:07:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/26/2022 00:07:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/26/2022 00:07:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/26/2022 00:07:15 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.575136165577342 on epoch=637
05/26/2022 00:07:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/26/2022 00:07:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/26/2022 00:07:23 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/26/2022 00:07:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
05/26/2022 00:07:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/26/2022 00:07:29 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6902416415284063 on epoch=649
05/26/2022 00:07:32 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=652
05/26/2022 00:07:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/26/2022 00:07:37 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/26/2022 00:07:40 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 00:07:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
05/26/2022 00:07:44 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6598455598455598 on epoch=662
05/26/2022 00:07:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/26/2022 00:07:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/26/2022 00:07:51 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
05/26/2022 00:07:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/26/2022 00:07:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/26/2022 00:07:58 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.707051282051282 on epoch=674
05/26/2022 00:08:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
05/26/2022 00:08:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/26/2022 00:08:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/26/2022 00:08:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/26/2022 00:08:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/26/2022 00:08:12 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5088810260946485 on epoch=687
05/26/2022 00:08:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/26/2022 00:08:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/26/2022 00:08:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/26/2022 00:08:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/26/2022 00:08:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/26/2022 00:08:27 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5841863799283155 on epoch=699
05/26/2022 00:08:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/26/2022 00:08:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 00:08:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 00:08:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/26/2022 00:08:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/26/2022 00:08:41 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.5186843222137341 on epoch=712
05/26/2022 00:08:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/26/2022 00:08:47 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/26/2022 00:08:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/26/2022 00:08:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/26/2022 00:08:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/26/2022 00:08:56 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6561593105710752 on epoch=724
05/26/2022 00:08:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/26/2022 00:09:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 00:09:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/26/2022 00:09:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/26/2022 00:09:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 00:09:10 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.5514663256606991 on epoch=737
05/26/2022 00:09:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/26/2022 00:09:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/26/2022 00:09:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/26/2022 00:09:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/26/2022 00:09:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 00:09:25 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6595854595854596 on epoch=749
05/26/2022 00:09:25 - INFO - __main__ - save last model!
05/26/2022 00:09:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 00:09:25 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 00:09:25 - INFO - __main__ - Printing 3 examples
05/26/2022 00:09:25 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 00:09:25 - INFO - __main__ - ['others']
05/26/2022 00:09:25 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 00:09:25 - INFO - __main__ - ['others']
05/26/2022 00:09:25 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 00:09:25 - INFO - __main__ - ['others']
05/26/2022 00:09:25 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:09:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:09:25 - INFO - __main__ - Printing 3 examples
05/26/2022 00:09:25 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/26/2022 00:09:25 - INFO - __main__ - ['others']
05/26/2022 00:09:25 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/26/2022 00:09:25 - INFO - __main__ - ['others']
05/26/2022 00:09:25 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/26/2022 00:09:25 - INFO - __main__ - ['others']
05/26/2022 00:09:25 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:09:25 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:09:25 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 00:09:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:09:25 - INFO - __main__ - Printing 3 examples
05/26/2022 00:09:25 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/26/2022 00:09:25 - INFO - __main__ - ['others']
05/26/2022 00:09:25 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/26/2022 00:09:25 - INFO - __main__ - ['others']
05/26/2022 00:09:25 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/26/2022 00:09:25 - INFO - __main__ - ['others']
05/26/2022 00:09:25 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:09:25 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:09:25 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 00:09:27 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:09:32 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 00:09:40 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 00:09:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 00:09:41 - INFO - __main__ - Starting training!
05/26/2022 00:11:21 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_13_0.4_8_predictions.txt
05/26/2022 00:11:21 - INFO - __main__ - Classification-F1 on test data: 0.0645
05/26/2022 00:11:21 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.7310682893847195, test_performance=0.06445594023868034
05/26/2022 00:11:21 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
05/26/2022 00:11:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:11:22 - INFO - __main__ - Printing 3 examples
05/26/2022 00:11:22 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/26/2022 00:11:22 - INFO - __main__ - ['others']
05/26/2022 00:11:22 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/26/2022 00:11:22 - INFO - __main__ - ['others']
05/26/2022 00:11:22 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/26/2022 00:11:22 - INFO - __main__ - ['others']
05/26/2022 00:11:22 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:11:22 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:11:22 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 00:11:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:11:22 - INFO - __main__ - Printing 3 examples
05/26/2022 00:11:22 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/26/2022 00:11:22 - INFO - __main__ - ['others']
05/26/2022 00:11:22 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/26/2022 00:11:22 - INFO - __main__ - ['others']
05/26/2022 00:11:22 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/26/2022 00:11:22 - INFO - __main__ - ['others']
05/26/2022 00:11:22 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:11:22 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:11:22 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 00:11:41 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 00:11:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 00:11:42 - INFO - __main__ - Starting training!
05/26/2022 00:11:45 - INFO - __main__ - Step 10 Global step 10 Train loss 2.96 on epoch=2
05/26/2022 00:11:48 - INFO - __main__ - Step 20 Global step 20 Train loss 1.31 on epoch=4
05/26/2022 00:11:51 - INFO - __main__ - Step 30 Global step 30 Train loss 1.11 on epoch=7
05/26/2022 00:11:53 - INFO - __main__ - Step 40 Global step 40 Train loss 0.92 on epoch=9
05/26/2022 00:11:56 - INFO - __main__ - Step 50 Global step 50 Train loss 1.02 on epoch=12
05/26/2022 00:11:57 - INFO - __main__ - Global step 50 Train loss 1.46 Classification-F1 0.1 on epoch=12
05/26/2022 00:11:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/26/2022 00:12:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=14
05/26/2022 00:12:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.86 on epoch=17
05/26/2022 00:12:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.79 on epoch=19
05/26/2022 00:12:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.79 on epoch=22
05/26/2022 00:12:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=24
05/26/2022 00:12:11 - INFO - __main__ - Global step 100 Train loss 0.85 Classification-F1 0.32741901108269394 on epoch=24
05/26/2022 00:12:11 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.32741901108269394 on epoch=24, global_step=100
05/26/2022 00:12:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=27
05/26/2022 00:12:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.77 on epoch=29
05/26/2022 00:12:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.71 on epoch=32
05/26/2022 00:12:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.70 on epoch=34
05/26/2022 00:12:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.71 on epoch=37
05/26/2022 00:12:26 - INFO - __main__ - Global step 150 Train loss 0.75 Classification-F1 0.5180742017416545 on epoch=37
05/26/2022 00:12:26 - INFO - __main__ - Saving model with best Classification-F1: 0.32741901108269394 -> 0.5180742017416545 on epoch=37, global_step=150
05/26/2022 00:12:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.68 on epoch=39
05/26/2022 00:12:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.72 on epoch=42
05/26/2022 00:12:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.64 on epoch=44
05/26/2022 00:12:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.68 on epoch=47
05/26/2022 00:12:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.60 on epoch=49
05/26/2022 00:12:40 - INFO - __main__ - Global step 200 Train loss 0.67 Classification-F1 0.5250982800982801 on epoch=49
05/26/2022 00:12:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5180742017416545 -> 0.5250982800982801 on epoch=49, global_step=200
05/26/2022 00:12:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.62 on epoch=52
05/26/2022 00:12:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=54
05/26/2022 00:12:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=57
05/26/2022 00:12:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.58 on epoch=59
05/26/2022 00:12:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.57 on epoch=62
05/26/2022 00:12:54 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.6300371155885471 on epoch=62
05/26/2022 00:12:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5250982800982801 -> 0.6300371155885471 on epoch=62, global_step=250
05/26/2022 00:12:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=64
05/26/2022 00:13:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=67
05/26/2022 00:13:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=69
05/26/2022 00:13:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=72
05/26/2022 00:13:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=74
05/26/2022 00:13:09 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.4874072602013778 on epoch=74
05/26/2022 00:13:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=77
05/26/2022 00:13:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=79
05/26/2022 00:13:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=82
05/26/2022 00:13:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=84
05/26/2022 00:13:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=87
05/26/2022 00:13:23 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.6442857591754652 on epoch=87
05/26/2022 00:13:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6300371155885471 -> 0.6442857591754652 on epoch=87, global_step=350
05/26/2022 00:13:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=89
05/26/2022 00:13:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=92
05/26/2022 00:13:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=94
05/26/2022 00:13:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=97
05/26/2022 00:13:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=99
05/26/2022 00:13:37 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.6496120107962213 on epoch=99
05/26/2022 00:13:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6442857591754652 -> 0.6496120107962213 on epoch=99, global_step=400
05/26/2022 00:13:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.33 on epoch=102
05/26/2022 00:13:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=104
05/26/2022 00:13:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=107
05/26/2022 00:13:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.30 on epoch=109
05/26/2022 00:13:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=112
05/26/2022 00:13:51 - INFO - __main__ - Global step 450 Train loss 0.28 Classification-F1 0.6463320463320463 on epoch=112
05/26/2022 00:13:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=114
05/26/2022 00:13:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=117
05/26/2022 00:13:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=119
05/26/2022 00:14:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=122
05/26/2022 00:14:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=124
05/26/2022 00:14:05 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.6487240829346093 on epoch=124
05/26/2022 00:14:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=127
05/26/2022 00:14:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.17 on epoch=129
05/26/2022 00:14:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=132
05/26/2022 00:14:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=134
05/26/2022 00:14:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=137
05/26/2022 00:14:20 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.6167831624728177 on epoch=137
05/26/2022 00:14:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.28 on epoch=139
05/26/2022 00:14:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=142
05/26/2022 00:14:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=144
05/26/2022 00:14:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=147
05/26/2022 00:14:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=149
05/26/2022 00:14:34 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.6406926406926408 on epoch=149
05/26/2022 00:14:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=152
05/26/2022 00:14:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=154
05/26/2022 00:14:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=157
05/26/2022 00:14:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=159
05/26/2022 00:14:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=162
05/26/2022 00:14:48 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.7266082326566199 on epoch=162
05/26/2022 00:14:48 - INFO - __main__ - Saving model with best Classification-F1: 0.6496120107962213 -> 0.7266082326566199 on epoch=162, global_step=650
05/26/2022 00:14:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
05/26/2022 00:14:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
05/26/2022 00:14:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=169
05/26/2022 00:14:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=172
05/26/2022 00:15:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=174
05/26/2022 00:15:03 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.5783052906083663 on epoch=174
05/26/2022 00:15:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=177
05/26/2022 00:15:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=179
05/26/2022 00:15:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=182
05/26/2022 00:15:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=184
05/26/2022 00:15:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=187
05/26/2022 00:15:17 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.6746267018006148 on epoch=187
05/26/2022 00:15:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=189
05/26/2022 00:15:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=192
05/26/2022 00:15:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=194
05/26/2022 00:15:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
05/26/2022 00:15:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=199
05/26/2022 00:15:31 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.6662581699346406 on epoch=199
05/26/2022 00:15:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=202
05/26/2022 00:15:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=204
05/26/2022 00:15:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
05/26/2022 00:15:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=209
05/26/2022 00:15:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=212
05/26/2022 00:15:46 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.6060591078883761 on epoch=212
05/26/2022 00:15:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=214
05/26/2022 00:15:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=217
05/26/2022 00:15:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=219
05/26/2022 00:15:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=222
05/26/2022 00:15:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
05/26/2022 00:16:00 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.6311783804430864 on epoch=224
05/26/2022 00:16:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=227
05/26/2022 00:16:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=229
05/26/2022 00:16:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
05/26/2022 00:16:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=234
05/26/2022 00:16:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
05/26/2022 00:16:14 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.6804885099002745 on epoch=237
05/26/2022 00:16:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
05/26/2022 00:16:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
05/26/2022 00:16:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
05/26/2022 00:16:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=247
05/26/2022 00:16:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=249
05/26/2022 00:16:29 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.6046660769570986 on epoch=249
05/26/2022 00:16:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
05/26/2022 00:16:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=254
05/26/2022 00:16:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
05/26/2022 00:16:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
05/26/2022 00:16:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=262
05/26/2022 00:16:43 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.6425533530796688 on epoch=262
05/26/2022 00:16:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
05/26/2022 00:16:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
05/26/2022 00:16:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/26/2022 00:16:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
05/26/2022 00:16:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=274
05/26/2022 00:16:58 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.6298796791443851 on epoch=274
05/26/2022 00:17:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
05/26/2022 00:17:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
05/26/2022 00:17:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=282
05/26/2022 00:17:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
05/26/2022 00:17:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
05/26/2022 00:17:12 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.6434755228872876 on epoch=287
05/26/2022 00:17:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
05/26/2022 00:17:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/26/2022 00:17:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/26/2022 00:17:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
05/26/2022 00:17:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=299
05/26/2022 00:17:27 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.6155506679700229 on epoch=299
05/26/2022 00:17:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/26/2022 00:17:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/26/2022 00:17:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
05/26/2022 00:17:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/26/2022 00:17:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
05/26/2022 00:17:41 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.6301646649472736 on epoch=312
05/26/2022 00:17:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
05/26/2022 00:17:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/26/2022 00:17:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
05/26/2022 00:17:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/26/2022 00:17:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/26/2022 00:17:56 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6060423678070737 on epoch=324
05/26/2022 00:17:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/26/2022 00:18:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
05/26/2022 00:18:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
05/26/2022 00:18:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
05/26/2022 00:18:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/26/2022 00:18:11 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.6596462639109698 on epoch=337
05/26/2022 00:18:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
05/26/2022 00:18:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/26/2022 00:18:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/26/2022 00:18:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/26/2022 00:18:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/26/2022 00:18:26 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6569890364008011 on epoch=349
05/26/2022 00:18:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/26/2022 00:18:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
05/26/2022 00:18:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/26/2022 00:18:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/26/2022 00:18:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=362
05/26/2022 00:18:41 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.6654173088955697 on epoch=362
05/26/2022 00:18:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
05/26/2022 00:18:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/26/2022 00:18:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/26/2022 00:18:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/26/2022 00:18:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/26/2022 00:18:55 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.6732467532467533 on epoch=374
05/26/2022 00:18:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
05/26/2022 00:19:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/26/2022 00:19:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/26/2022 00:19:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/26/2022 00:19:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/26/2022 00:19:10 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.6800308529208785 on epoch=387
05/26/2022 00:19:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
05/26/2022 00:19:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/26/2022 00:19:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
05/26/2022 00:19:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/26/2022 00:19:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/26/2022 00:19:25 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.6382707688338494 on epoch=399
05/26/2022 00:19:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/26/2022 00:19:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/26/2022 00:19:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/26/2022 00:19:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/26/2022 00:19:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
05/26/2022 00:19:39 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.5412240047534165 on epoch=412
05/26/2022 00:19:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/26/2022 00:19:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/26/2022 00:19:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/26/2022 00:19:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/26/2022 00:19:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/26/2022 00:19:54 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6179855028692238 on epoch=424
05/26/2022 00:19:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/26/2022 00:19:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
05/26/2022 00:20:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/26/2022 00:20:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/26/2022 00:20:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/26/2022 00:20:08 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.5434747474747474 on epoch=437
05/26/2022 00:20:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/26/2022 00:20:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/26/2022 00:20:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/26/2022 00:20:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/26/2022 00:20:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/26/2022 00:20:23 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6046660769570986 on epoch=449
05/26/2022 00:20:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/26/2022 00:20:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
05/26/2022 00:20:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
05/26/2022 00:20:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/26/2022 00:20:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/26/2022 00:20:38 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6294372294372295 on epoch=462
05/26/2022 00:20:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/26/2022 00:20:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/26/2022 00:20:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
05/26/2022 00:20:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/26/2022 00:20:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/26/2022 00:20:52 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.679609242109242 on epoch=474
05/26/2022 00:20:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
05/26/2022 00:20:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/26/2022 00:21:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=482
05/26/2022 00:21:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/26/2022 00:21:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/26/2022 00:21:07 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.625 on epoch=487
05/26/2022 00:21:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/26/2022 00:21:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/26/2022 00:21:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/26/2022 00:21:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/26/2022 00:21:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/26/2022 00:21:22 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.630017278043594 on epoch=499
05/26/2022 00:21:24 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
05/26/2022 00:21:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/26/2022 00:21:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/26/2022 00:21:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/26/2022 00:21:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/26/2022 00:21:36 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.38603174603174606 on epoch=512
05/26/2022 00:21:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/26/2022 00:21:42 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/26/2022 00:21:44 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/26/2022 00:21:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/26/2022 00:21:50 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/26/2022 00:21:51 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6952214452214452 on epoch=524
05/26/2022 00:21:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/26/2022 00:21:56 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/26/2022 00:21:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/26/2022 00:22:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/26/2022 00:22:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/26/2022 00:22:06 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.5478213507625272 on epoch=537
05/26/2022 00:22:08 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/26/2022 00:22:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/26/2022 00:22:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/26/2022 00:22:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/26/2022 00:22:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/26/2022 00:22:20 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.5087282765543636 on epoch=549
05/26/2022 00:22:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/26/2022 00:22:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
05/26/2022 00:22:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/26/2022 00:22:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/26/2022 00:22:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/26/2022 00:22:35 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.5425956937799044 on epoch=562
05/26/2022 00:22:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/26/2022 00:22:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
05/26/2022 00:22:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/26/2022 00:22:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/26/2022 00:22:48 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
05/26/2022 00:22:50 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.5269041769041769 on epoch=574
05/26/2022 00:22:52 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=577
05/26/2022 00:22:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/26/2022 00:22:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/26/2022 00:23:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/26/2022 00:23:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/26/2022 00:23:04 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6443479193479194 on epoch=587
05/26/2022 00:23:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/26/2022 00:23:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/26/2022 00:23:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/26/2022 00:23:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/26/2022 00:23:18 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/26/2022 00:23:19 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.6896150696150697 on epoch=599
05/26/2022 00:23:22 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/26/2022 00:23:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/26/2022 00:23:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/26/2022 00:23:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/26/2022 00:23:32 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/26/2022 00:23:34 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6715472774475644 on epoch=612
05/26/2022 00:23:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/26/2022 00:23:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/26/2022 00:23:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/26/2022 00:23:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/26/2022 00:23:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/26/2022 00:23:48 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6855901451489688 on epoch=624
05/26/2022 00:23:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/26/2022 00:23:54 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/26/2022 00:23:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/26/2022 00:23:59 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/26/2022 00:24:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/26/2022 00:24:03 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6800308529208785 on epoch=637
05/26/2022 00:24:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/26/2022 00:24:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/26/2022 00:24:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/26/2022 00:24:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 00:24:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/26/2022 00:24:18 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.5402427637721755 on epoch=649
05/26/2022 00:24:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/26/2022 00:24:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/26/2022 00:24:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/26/2022 00:24:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 00:24:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/26/2022 00:24:32 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.5710883626177744 on epoch=662
05/26/2022 00:24:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/26/2022 00:24:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/26/2022 00:24:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/26/2022 00:24:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/26/2022 00:24:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/26/2022 00:24:47 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.5370405553525759 on epoch=674
05/26/2022 00:24:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/26/2022 00:24:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/26/2022 00:24:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/26/2022 00:24:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/26/2022 00:25:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/26/2022 00:25:02 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6840206577048682 on epoch=687
05/26/2022 00:25:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/26/2022 00:25:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/26/2022 00:25:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/26/2022 00:25:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
05/26/2022 00:25:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/26/2022 00:25:16 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.4466625716625716 on epoch=699
05/26/2022 00:25:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/26/2022 00:25:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 00:25:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 00:25:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/26/2022 00:25:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/26/2022 00:25:31 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.5660505677896982 on epoch=712
05/26/2022 00:25:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/26/2022 00:25:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/26/2022 00:25:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 00:25:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
05/26/2022 00:25:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/26/2022 00:25:46 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.4696681096681097 on epoch=724
05/26/2022 00:25:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/26/2022 00:25:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 00:25:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/26/2022 00:25:57 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/26/2022 00:25:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=737
05/26/2022 00:26:01 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.49790896159317216 on epoch=737
05/26/2022 00:26:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/26/2022 00:26:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/26/2022 00:26:09 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/26/2022 00:26:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/26/2022 00:26:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/26/2022 00:26:15 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.55998667998668 on epoch=749
05/26/2022 00:26:15 - INFO - __main__ - save last model!
05/26/2022 00:26:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 00:26:15 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 00:26:15 - INFO - __main__ - Printing 3 examples
05/26/2022 00:26:15 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 00:26:15 - INFO - __main__ - ['others']
05/26/2022 00:26:15 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 00:26:15 - INFO - __main__ - ['others']
05/26/2022 00:26:15 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 00:26:15 - INFO - __main__ - ['others']
05/26/2022 00:26:15 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:26:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:26:15 - INFO - __main__ - Printing 3 examples
05/26/2022 00:26:15 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/26/2022 00:26:15 - INFO - __main__ - ['others']
05/26/2022 00:26:15 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/26/2022 00:26:15 - INFO - __main__ - ['others']
05/26/2022 00:26:15 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/26/2022 00:26:15 - INFO - __main__ - ['others']
05/26/2022 00:26:15 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:26:15 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:26:15 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 00:26:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:26:15 - INFO - __main__ - Printing 3 examples
05/26/2022 00:26:15 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/26/2022 00:26:15 - INFO - __main__ - ['others']
05/26/2022 00:26:15 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/26/2022 00:26:15 - INFO - __main__ - ['others']
05/26/2022 00:26:15 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/26/2022 00:26:15 - INFO - __main__ - ['others']
05/26/2022 00:26:15 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:26:15 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:26:16 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 00:26:17 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:26:23 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 00:26:34 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 00:26:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 00:26:35 - INFO - __main__ - Starting training!
05/26/2022 00:28:13 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_13_0.3_8_predictions.txt
05/26/2022 00:28:13 - INFO - __main__ - Classification-F1 on test data: 0.0930
05/26/2022 00:28:14 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.7266082326566199, test_performance=0.0930301140086244
05/26/2022 00:28:14 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
05/26/2022 00:28:14 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:28:14 - INFO - __main__ - Printing 3 examples
05/26/2022 00:28:14 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/26/2022 00:28:14 - INFO - __main__ - ['others']
05/26/2022 00:28:14 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/26/2022 00:28:14 - INFO - __main__ - ['others']
05/26/2022 00:28:14 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/26/2022 00:28:14 - INFO - __main__ - ['others']
05/26/2022 00:28:14 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:28:15 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:28:15 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 00:28:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:28:15 - INFO - __main__ - Printing 3 examples
05/26/2022 00:28:15 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/26/2022 00:28:15 - INFO - __main__ - ['others']
05/26/2022 00:28:15 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/26/2022 00:28:15 - INFO - __main__ - ['others']
05/26/2022 00:28:15 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/26/2022 00:28:15 - INFO - __main__ - ['others']
05/26/2022 00:28:15 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:28:15 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:28:15 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 00:28:33 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 00:28:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 00:28:34 - INFO - __main__ - Starting training!
05/26/2022 00:28:38 - INFO - __main__ - Step 10 Global step 10 Train loss 3.25 on epoch=2
05/26/2022 00:28:40 - INFO - __main__ - Step 20 Global step 20 Train loss 1.89 on epoch=4
05/26/2022 00:28:43 - INFO - __main__ - Step 30 Global step 30 Train loss 1.40 on epoch=7
05/26/2022 00:28:46 - INFO - __main__ - Step 40 Global step 40 Train loss 1.01 on epoch=9
05/26/2022 00:28:48 - INFO - __main__ - Step 50 Global step 50 Train loss 1.04 on epoch=12
05/26/2022 00:28:49 - INFO - __main__ - Global step 50 Train loss 1.72 Classification-F1 0.1 on epoch=12
05/26/2022 00:28:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/26/2022 00:28:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.92 on epoch=14
05/26/2022 00:28:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.88 on epoch=17
05/26/2022 00:28:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=19
05/26/2022 00:29:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=22
05/26/2022 00:29:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.89 on epoch=24
05/26/2022 00:29:04 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.20190476190476192 on epoch=24
05/26/2022 00:29:04 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.20190476190476192 on epoch=24, global_step=100
05/26/2022 00:29:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.79 on epoch=27
05/26/2022 00:29:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.85 on epoch=29
05/26/2022 00:29:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.82 on epoch=32
05/26/2022 00:29:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=34
05/26/2022 00:29:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.70 on epoch=37
05/26/2022 00:29:18 - INFO - __main__ - Global step 150 Train loss 0.80 Classification-F1 0.28529657477025894 on epoch=37
05/26/2022 00:29:18 - INFO - __main__ - Saving model with best Classification-F1: 0.20190476190476192 -> 0.28529657477025894 on epoch=37, global_step=150
05/26/2022 00:29:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.78 on epoch=39
05/26/2022 00:29:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.61 on epoch=42
05/26/2022 00:29:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.69 on epoch=44
05/26/2022 00:29:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.72 on epoch=47
05/26/2022 00:29:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.66 on epoch=49
05/26/2022 00:29:32 - INFO - __main__ - Global step 200 Train loss 0.69 Classification-F1 0.5064291383719904 on epoch=49
05/26/2022 00:29:32 - INFO - __main__ - Saving model with best Classification-F1: 0.28529657477025894 -> 0.5064291383719904 on epoch=49, global_step=200
05/26/2022 00:29:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.73 on epoch=52
05/26/2022 00:29:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.68 on epoch=54
05/26/2022 00:29:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.67 on epoch=57
05/26/2022 00:29:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.61 on epoch=59
05/26/2022 00:29:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.68 on epoch=62
05/26/2022 00:29:46 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.5836209052263066 on epoch=62
05/26/2022 00:29:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5064291383719904 -> 0.5836209052263066 on epoch=62, global_step=250
05/26/2022 00:29:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.72 on epoch=64
05/26/2022 00:29:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=67
05/26/2022 00:29:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=69
05/26/2022 00:29:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=72
05/26/2022 00:30:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.62 on epoch=74
05/26/2022 00:30:01 - INFO - __main__ - Global step 300 Train loss 0.61 Classification-F1 0.563030738611234 on epoch=74
05/26/2022 00:30:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.54 on epoch=77
05/26/2022 00:30:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.58 on epoch=79
05/26/2022 00:30:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=82
05/26/2022 00:30:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=84
05/26/2022 00:30:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=87
05/26/2022 00:30:15 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.6568627450980391 on epoch=87
05/26/2022 00:30:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5836209052263066 -> 0.6568627450980391 on epoch=87, global_step=350
05/26/2022 00:30:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=89
05/26/2022 00:30:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=92
05/26/2022 00:30:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=94
05/26/2022 00:30:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=97
05/26/2022 00:30:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=99
05/26/2022 00:30:29 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.6563393778564057 on epoch=99
05/26/2022 00:30:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=102
05/26/2022 00:30:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=104
05/26/2022 00:30:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=107
05/26/2022 00:30:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=109
05/26/2022 00:30:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=112
05/26/2022 00:30:44 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.7111884180195375 on epoch=112
05/26/2022 00:30:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6568627450980391 -> 0.7111884180195375 on epoch=112, global_step=450
05/26/2022 00:30:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=114
05/26/2022 00:30:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=117
05/26/2022 00:30:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=119
05/26/2022 00:30:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=122
05/26/2022 00:30:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=124
05/26/2022 00:30:58 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.6127939547912188 on epoch=124
05/26/2022 00:31:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=127
05/26/2022 00:31:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=129
05/26/2022 00:31:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=132
05/26/2022 00:31:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=134
05/26/2022 00:31:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=137
05/26/2022 00:31:12 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.6190705128205128 on epoch=137
05/26/2022 00:31:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=139
05/26/2022 00:31:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=142
05/26/2022 00:31:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=144
05/26/2022 00:31:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=147
05/26/2022 00:31:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=149
05/26/2022 00:31:26 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.6248983739837399 on epoch=149
05/26/2022 00:31:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=152
05/26/2022 00:31:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=154
05/26/2022 00:31:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=157
05/26/2022 00:31:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=159
05/26/2022 00:31:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=162
05/26/2022 00:31:41 - INFO - __main__ - Global step 650 Train loss 0.24 Classification-F1 0.5670447567098941 on epoch=162
05/26/2022 00:31:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=164
05/26/2022 00:31:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=167
05/26/2022 00:31:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=169
05/26/2022 00:31:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=172
05/26/2022 00:31:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=174
05/26/2022 00:31:55 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.6294372294372295 on epoch=174
05/26/2022 00:31:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=177
05/26/2022 00:32:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=179
05/26/2022 00:32:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
05/26/2022 00:32:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=184
05/26/2022 00:32:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=187
05/26/2022 00:32:09 - INFO - __main__ - Global step 750 Train loss 0.18 Classification-F1 0.6143426748689909 on epoch=187
05/26/2022 00:32:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=189
05/26/2022 00:32:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=192
05/26/2022 00:32:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=194
05/26/2022 00:32:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=197
05/26/2022 00:32:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=199
05/26/2022 00:32:23 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.6007226681568787 on epoch=199
05/26/2022 00:32:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=202
05/26/2022 00:32:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=204
05/26/2022 00:32:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.13 on epoch=207
05/26/2022 00:32:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=209
05/26/2022 00:32:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=212
05/26/2022 00:32:38 - INFO - __main__ - Global step 850 Train loss 0.14 Classification-F1 0.6654305948423596 on epoch=212
05/26/2022 00:32:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
05/26/2022 00:32:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=217
05/26/2022 00:32:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=219
05/26/2022 00:32:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=222
05/26/2022 00:32:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=224
05/26/2022 00:32:52 - INFO - __main__ - Global step 900 Train loss 0.13 Classification-F1 0.6284756007666226 on epoch=224
05/26/2022 00:32:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=227
05/26/2022 00:32:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=229
05/26/2022 00:33:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=232
05/26/2022 00:33:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=234
05/26/2022 00:33:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=237
05/26/2022 00:33:06 - INFO - __main__ - Global step 950 Train loss 0.13 Classification-F1 0.6720323264440912 on epoch=237
05/26/2022 00:33:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=239
05/26/2022 00:33:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=242
05/26/2022 00:33:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=244
05/26/2022 00:33:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.13 on epoch=247
05/26/2022 00:33:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=249
05/26/2022 00:33:20 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.6625956838860064 on epoch=249
05/26/2022 00:33:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=252
05/26/2022 00:33:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
05/26/2022 00:33:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
05/26/2022 00:33:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=259
05/26/2022 00:33:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=262
05/26/2022 00:33:35 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.688293131841519 on epoch=262
05/26/2022 00:33:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=264
05/26/2022 00:33:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=267
05/26/2022 00:33:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=269
05/26/2022 00:33:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=272
05/26/2022 00:33:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=274
05/26/2022 00:33:49 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.5893353174603174 on epoch=274
05/26/2022 00:33:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
05/26/2022 00:33:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=279
05/26/2022 00:33:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=282
05/26/2022 00:34:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=284
05/26/2022 00:34:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
05/26/2022 00:34:03 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.7074390968508615 on epoch=287
05/26/2022 00:34:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=289
05/26/2022 00:34:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=292
05/26/2022 00:34:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.12 on epoch=294
05/26/2022 00:34:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
05/26/2022 00:34:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
05/26/2022 00:34:18 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.7075324675324675 on epoch=299
05/26/2022 00:34:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
05/26/2022 00:34:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/26/2022 00:34:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=307
05/26/2022 00:34:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
05/26/2022 00:34:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
05/26/2022 00:34:32 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.6884633994608418 on epoch=312
05/26/2022 00:34:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
05/26/2022 00:34:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
05/26/2022 00:34:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/26/2022 00:34:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
05/26/2022 00:34:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
05/26/2022 00:34:46 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7024044795783926 on epoch=324
05/26/2022 00:34:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=327
05/26/2022 00:34:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=329
05/26/2022 00:34:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
05/26/2022 00:34:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/26/2022 00:35:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
05/26/2022 00:35:01 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6598973285486445 on epoch=337
05/26/2022 00:35:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/26/2022 00:35:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
05/26/2022 00:35:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
05/26/2022 00:35:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
05/26/2022 00:35:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
05/26/2022 00:35:15 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6934389140271493 on epoch=349
05/26/2022 00:35:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/26/2022 00:35:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/26/2022 00:35:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/26/2022 00:35:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
05/26/2022 00:35:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=362
05/26/2022 00:35:29 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.7127649039413746 on epoch=362
05/26/2022 00:35:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7111884180195375 -> 0.7127649039413746 on epoch=362, global_step=1450
05/26/2022 00:35:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
05/26/2022 00:35:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/26/2022 00:35:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/26/2022 00:35:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=372
05/26/2022 00:35:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
05/26/2022 00:35:44 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.5596738745125842 on epoch=374
05/26/2022 00:35:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/26/2022 00:35:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
05/26/2022 00:35:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
05/26/2022 00:35:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
05/26/2022 00:35:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=387
05/26/2022 00:35:58 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.7281344437416545 on epoch=387
05/26/2022 00:35:58 - INFO - __main__ - Saving model with best Classification-F1: 0.7127649039413746 -> 0.7281344437416545 on epoch=387, global_step=1550
05/26/2022 00:36:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/26/2022 00:36:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/26/2022 00:36:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/26/2022 00:36:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/26/2022 00:36:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/26/2022 00:36:13 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.6988238238238238 on epoch=399
05/26/2022 00:36:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/26/2022 00:36:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
05/26/2022 00:36:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=407
05/26/2022 00:36:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=409
05/26/2022 00:36:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/26/2022 00:36:27 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7114242388435936 on epoch=412
05/26/2022 00:36:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/26/2022 00:36:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/26/2022 00:36:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/26/2022 00:36:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/26/2022 00:36:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/26/2022 00:36:42 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6559427022841657 on epoch=424
05/26/2022 00:36:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/26/2022 00:36:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
05/26/2022 00:36:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
05/26/2022 00:36:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
05/26/2022 00:36:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/26/2022 00:36:56 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6440457516976646 on epoch=437
05/26/2022 00:36:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/26/2022 00:37:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/26/2022 00:37:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=444
05/26/2022 00:37:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/26/2022 00:37:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
05/26/2022 00:37:11 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.6500509294626942 on epoch=449
05/26/2022 00:37:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/26/2022 00:37:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/26/2022 00:37:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
05/26/2022 00:37:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=459
05/26/2022 00:37:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=462
05/26/2022 00:37:25 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.7080302050890286 on epoch=462
05/26/2022 00:37:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/26/2022 00:37:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/26/2022 00:37:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/26/2022 00:37:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/26/2022 00:37:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
05/26/2022 00:37:39 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6697470832113188 on epoch=474
05/26/2022 00:37:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/26/2022 00:37:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
05/26/2022 00:37:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/26/2022 00:37:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/26/2022 00:37:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
05/26/2022 00:37:54 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6803708133971291 on epoch=487
05/26/2022 00:37:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
05/26/2022 00:37:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/26/2022 00:38:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/26/2022 00:38:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/26/2022 00:38:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
05/26/2022 00:38:08 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.5750067769043101 on epoch=499
05/26/2022 00:38:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/26/2022 00:38:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/26/2022 00:38:16 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
05/26/2022 00:38:19 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/26/2022 00:38:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/26/2022 00:38:23 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6455862977602108 on epoch=512
05/26/2022 00:38:25 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/26/2022 00:38:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/26/2022 00:38:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/26/2022 00:38:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/26/2022 00:38:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/26/2022 00:38:37 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6736429258168388 on epoch=524
05/26/2022 00:38:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/26/2022 00:38:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=529
05/26/2022 00:38:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/26/2022 00:38:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/26/2022 00:38:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/26/2022 00:38:51 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6582649613899614 on epoch=537
05/26/2022 00:38:54 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/26/2022 00:38:57 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/26/2022 00:38:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/26/2022 00:39:02 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/26/2022 00:39:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/26/2022 00:39:06 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6417281612527792 on epoch=549
05/26/2022 00:39:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
05/26/2022 00:39:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/26/2022 00:39:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/26/2022 00:39:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
05/26/2022 00:39:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/26/2022 00:39:20 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6263250595257509 on epoch=562
05/26/2022 00:39:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/26/2022 00:39:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/26/2022 00:39:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/26/2022 00:39:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=572
05/26/2022 00:39:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/26/2022 00:39:34 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.759469696969697 on epoch=574
05/26/2022 00:39:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7281344437416545 -> 0.759469696969697 on epoch=574, global_step=2300
05/26/2022 00:39:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.10 on epoch=577
05/26/2022 00:39:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/26/2022 00:39:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/26/2022 00:39:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/26/2022 00:39:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/26/2022 00:39:49 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.7272609743197979 on epoch=587
05/26/2022 00:39:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/26/2022 00:39:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/26/2022 00:39:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/26/2022 00:39:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/26/2022 00:40:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/26/2022 00:40:03 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7174383484867356 on epoch=599
05/26/2022 00:40:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/26/2022 00:40:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/26/2022 00:40:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/26/2022 00:40:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/26/2022 00:40:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/26/2022 00:40:17 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7253261784511785 on epoch=612
05/26/2022 00:40:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/26/2022 00:40:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/26/2022 00:40:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/26/2022 00:40:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/26/2022 00:40:30 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/26/2022 00:40:32 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6428382575121705 on epoch=624
05/26/2022 00:40:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/26/2022 00:40:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/26/2022 00:40:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/26/2022 00:40:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
05/26/2022 00:40:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/26/2022 00:40:46 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.679484126984127 on epoch=637
05/26/2022 00:40:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/26/2022 00:40:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/26/2022 00:40:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/26/2022 00:40:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 00:40:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/26/2022 00:41:01 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.679609242109242 on epoch=649
05/26/2022 00:41:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/26/2022 00:41:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/26/2022 00:41:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=657
05/26/2022 00:41:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 00:41:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/26/2022 00:41:15 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.5861279849183075 on epoch=662
05/26/2022 00:41:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=664
05/26/2022 00:41:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/26/2022 00:41:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/26/2022 00:41:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/26/2022 00:41:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/26/2022 00:41:30 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6247232764073372 on epoch=674
05/26/2022 00:41:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/26/2022 00:41:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/26/2022 00:41:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
05/26/2022 00:41:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/26/2022 00:41:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/26/2022 00:41:45 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.663984242109242 on epoch=687
05/26/2022 00:41:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/26/2022 00:41:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
05/26/2022 00:41:53 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/26/2022 00:41:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/26/2022 00:41:58 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/26/2022 00:41:59 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6406931669300484 on epoch=699
05/26/2022 00:42:02 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/26/2022 00:42:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/26/2022 00:42:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/26/2022 00:42:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/26/2022 00:42:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/26/2022 00:42:14 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6975484006734007 on epoch=712
05/26/2022 00:42:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/26/2022 00:42:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/26/2022 00:42:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 00:42:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/26/2022 00:42:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=724
05/26/2022 00:42:29 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6391898864809082 on epoch=724
05/26/2022 00:42:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/26/2022 00:42:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/26/2022 00:42:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/26/2022 00:42:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/26/2022 00:42:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 00:42:44 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.5931372549019608 on epoch=737
05/26/2022 00:42:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/26/2022 00:42:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/26/2022 00:42:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/26/2022 00:42:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/26/2022 00:42:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 00:42:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:42:59 - INFO - __main__ - Printing 3 examples
05/26/2022 00:42:59 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/26/2022 00:42:59 - INFO - __main__ - ['sad']
05/26/2022 00:42:59 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/26/2022 00:42:59 - INFO - __main__ - ['sad']
05/26/2022 00:42:59 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/26/2022 00:42:59 - INFO - __main__ - ['sad']
05/26/2022 00:42:59 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:42:59 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6735502079619726 on epoch=749
05/26/2022 00:42:59 - INFO - __main__ - save last model!
05/26/2022 00:42:59 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:42:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 00:42:59 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 00:42:59 - INFO - __main__ - Printing 3 examples
05/26/2022 00:42:59 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 00:42:59 - INFO - __main__ - ['others']
05/26/2022 00:42:59 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 00:42:59 - INFO - __main__ - ['others']
05/26/2022 00:42:59 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 00:42:59 - INFO - __main__ - ['others']
05/26/2022 00:42:59 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:42:59 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 00:42:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:42:59 - INFO - __main__ - Printing 3 examples
05/26/2022 00:42:59 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/26/2022 00:42:59 - INFO - __main__ - ['sad']
05/26/2022 00:42:59 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/26/2022 00:42:59 - INFO - __main__ - ['sad']
05/26/2022 00:42:59 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/26/2022 00:42:59 - INFO - __main__ - ['sad']
05/26/2022 00:42:59 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:42:59 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:42:59 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 00:43:01 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:43:06 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 00:43:17 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 00:43:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 00:43:18 - INFO - __main__ - Starting training!
05/26/2022 00:45:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_13_0.2_8_predictions.txt
05/26/2022 00:45:10 - INFO - __main__ - Classification-F1 on test data: 0.0929
05/26/2022 00:45:10 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.759469696969697, test_performance=0.09285954507134321
05/26/2022 00:45:10 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
05/26/2022 00:45:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:45:11 - INFO - __main__ - Printing 3 examples
05/26/2022 00:45:11 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/26/2022 00:45:11 - INFO - __main__ - ['sad']
05/26/2022 00:45:11 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/26/2022 00:45:11 - INFO - __main__ - ['sad']
05/26/2022 00:45:11 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/26/2022 00:45:11 - INFO - __main__ - ['sad']
05/26/2022 00:45:11 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:45:11 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:45:11 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 00:45:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 00:45:11 - INFO - __main__ - Printing 3 examples
05/26/2022 00:45:11 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/26/2022 00:45:11 - INFO - __main__ - ['sad']
05/26/2022 00:45:11 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/26/2022 00:45:11 - INFO - __main__ - ['sad']
05/26/2022 00:45:11 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/26/2022 00:45:11 - INFO - __main__ - ['sad']
05/26/2022 00:45:11 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:45:11 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:45:11 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 00:45:30 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 00:45:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 00:45:31 - INFO - __main__ - Starting training!
05/26/2022 00:45:34 - INFO - __main__ - Step 10 Global step 10 Train loss 2.70 on epoch=2
05/26/2022 00:45:37 - INFO - __main__ - Step 20 Global step 20 Train loss 1.20 on epoch=4
05/26/2022 00:45:39 - INFO - __main__ - Step 30 Global step 30 Train loss 1.01 on epoch=7
05/26/2022 00:45:42 - INFO - __main__ - Step 40 Global step 40 Train loss 1.01 on epoch=9
05/26/2022 00:45:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.93 on epoch=12
05/26/2022 00:45:45 - INFO - __main__ - Global step 50 Train loss 1.37 Classification-F1 0.1 on epoch=12
05/26/2022 00:45:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/26/2022 00:45:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.87 on epoch=14
05/26/2022 00:45:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.78 on epoch=17
05/26/2022 00:45:53 - INFO - __main__ - Step 80 Global step 80 Train loss 0.81 on epoch=19
05/26/2022 00:45:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.78 on epoch=22
05/26/2022 00:45:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.70 on epoch=24
05/26/2022 00:46:00 - INFO - __main__ - Global step 100 Train loss 0.79 Classification-F1 0.518270902953011 on epoch=24
05/26/2022 00:46:00 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.518270902953011 on epoch=24, global_step=100
05/26/2022 00:46:02 - INFO - __main__ - Step 110 Global step 110 Train loss 0.76 on epoch=27
05/26/2022 00:46:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.64 on epoch=29
05/26/2022 00:46:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.65 on epoch=32
05/26/2022 00:46:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=34
05/26/2022 00:46:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=37
05/26/2022 00:46:14 - INFO - __main__ - Global step 150 Train loss 0.62 Classification-F1 0.626177118813865 on epoch=37
05/26/2022 00:46:14 - INFO - __main__ - Saving model with best Classification-F1: 0.518270902953011 -> 0.626177118813865 on epoch=37, global_step=150
05/26/2022 00:46:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.58 on epoch=39
05/26/2022 00:46:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.62 on epoch=42
05/26/2022 00:46:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=44
05/26/2022 00:46:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=47
05/26/2022 00:46:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=49
05/26/2022 00:46:28 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.662905757279623 on epoch=49
05/26/2022 00:46:28 - INFO - __main__ - Saving model with best Classification-F1: 0.626177118813865 -> 0.662905757279623 on epoch=49, global_step=200
05/26/2022 00:46:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=52
05/26/2022 00:46:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=54
05/26/2022 00:46:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=57
05/26/2022 00:46:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.29 on epoch=59
05/26/2022 00:46:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=62
05/26/2022 00:46:43 - INFO - __main__ - Global step 250 Train loss 0.35 Classification-F1 0.6969864426760979 on epoch=62
05/26/2022 00:46:43 - INFO - __main__ - Saving model with best Classification-F1: 0.662905757279623 -> 0.6969864426760979 on epoch=62, global_step=250
05/26/2022 00:46:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=64
05/26/2022 00:46:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=67
05/26/2022 00:46:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=69
05/26/2022 00:46:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.12 on epoch=72
05/26/2022 00:46:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=74
05/26/2022 00:46:57 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.7083504730563555 on epoch=74
05/26/2022 00:46:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6969864426760979 -> 0.7083504730563555 on epoch=74, global_step=300
05/26/2022 00:47:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=77
05/26/2022 00:47:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.17 on epoch=79
05/26/2022 00:47:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.15 on epoch=82
05/26/2022 00:47:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.20 on epoch=84
05/26/2022 00:47:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.17 on epoch=87
05/26/2022 00:47:11 - INFO - __main__ - Global step 350 Train loss 0.19 Classification-F1 0.7153032036613272 on epoch=87
05/26/2022 00:47:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7083504730563555 -> 0.7153032036613272 on epoch=87, global_step=350
05/26/2022 00:47:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=89
05/26/2022 00:47:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.11 on epoch=92
05/26/2022 00:47:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=94
05/26/2022 00:47:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.12 on epoch=97
05/26/2022 00:47:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.13 on epoch=99
05/26/2022 00:47:26 - INFO - __main__ - Global step 400 Train loss 0.15 Classification-F1 0.7612058080808081 on epoch=99
05/26/2022 00:47:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7153032036613272 -> 0.7612058080808081 on epoch=99, global_step=400
05/26/2022 00:47:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.07 on epoch=102
05/26/2022 00:47:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.11 on epoch=104
05/26/2022 00:47:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.13 on epoch=107
05/26/2022 00:47:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.10 on epoch=109
05/26/2022 00:47:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.06 on epoch=112
05/26/2022 00:47:40 - INFO - __main__ - Global step 450 Train loss 0.09 Classification-F1 0.7517936117936119 on epoch=112
05/26/2022 00:47:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=114
05/26/2022 00:47:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=117
05/26/2022 00:47:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.08 on epoch=119
05/26/2022 00:47:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.07 on epoch=122
05/26/2022 00:47:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=124
05/26/2022 00:47:54 - INFO - __main__ - Global step 500 Train loss 0.08 Classification-F1 0.6736703280820927 on epoch=124
05/26/2022 00:47:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.06 on epoch=127
05/26/2022 00:48:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=129
05/26/2022 00:48:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.10 on epoch=132
05/26/2022 00:48:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=134
05/26/2022 00:48:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.05 on epoch=137
05/26/2022 00:48:09 - INFO - __main__ - Global step 550 Train loss 0.06 Classification-F1 0.7417326589652566 on epoch=137
05/26/2022 00:48:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=139
05/26/2022 00:48:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=142
05/26/2022 00:48:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=144
05/26/2022 00:48:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=147
05/26/2022 00:48:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=149
05/26/2022 00:48:23 - INFO - __main__ - Global step 600 Train loss 0.08 Classification-F1 0.7342887242080791 on epoch=149
05/26/2022 00:48:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=152
05/26/2022 00:48:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=154
05/26/2022 00:48:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=157
05/26/2022 00:48:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.09 on epoch=159
05/26/2022 00:48:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=162
05/26/2022 00:48:38 - INFO - __main__ - Global step 650 Train loss 0.04 Classification-F1 0.7317879253363125 on epoch=162
05/26/2022 00:48:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=164
05/26/2022 00:48:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=167
05/26/2022 00:48:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=169
05/26/2022 00:48:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=172
05/26/2022 00:48:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=174
05/26/2022 00:48:52 - INFO - __main__ - Global step 700 Train loss 0.04 Classification-F1 0.6976133887898593 on epoch=174
05/26/2022 00:48:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=177
05/26/2022 00:48:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=179
05/26/2022 00:49:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=182
05/26/2022 00:49:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=184
05/26/2022 00:49:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=187
05/26/2022 00:49:07 - INFO - __main__ - Global step 750 Train loss 0.05 Classification-F1 0.6734848484848484 on epoch=187
05/26/2022 00:49:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=189
05/26/2022 00:49:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=192
05/26/2022 00:49:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=194
05/26/2022 00:49:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=197
05/26/2022 00:49:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=199
05/26/2022 00:49:22 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.7227240896358543 on epoch=199
05/26/2022 00:49:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=202
05/26/2022 00:49:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=204
05/26/2022 00:49:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=207
05/26/2022 00:49:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=209
05/26/2022 00:49:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=212
05/26/2022 00:49:36 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.7248483595257789 on epoch=212
05/26/2022 00:49:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=214
05/26/2022 00:49:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=217
05/26/2022 00:49:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=219
05/26/2022 00:49:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=222
05/26/2022 00:49:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=224
05/26/2022 00:49:51 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.746668014966124 on epoch=224
05/26/2022 00:49:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=227
05/26/2022 00:49:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=229
05/26/2022 00:49:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=232
05/26/2022 00:50:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=234
05/26/2022 00:50:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
05/26/2022 00:50:06 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.7147194361615615 on epoch=237
05/26/2022 00:50:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=239
05/26/2022 00:50:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=242
05/26/2022 00:50:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=244
05/26/2022 00:50:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
05/26/2022 00:50:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=249
05/26/2022 00:50:20 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.7109097435520911 on epoch=249
05/26/2022 00:50:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=252
05/26/2022 00:50:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=254
05/26/2022 00:50:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
05/26/2022 00:50:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=259
05/26/2022 00:50:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=262
05/26/2022 00:50:35 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.7036764705882352 on epoch=262
05/26/2022 00:50:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=264
05/26/2022 00:50:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=267
05/26/2022 00:50:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
05/26/2022 00:50:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=272
05/26/2022 00:50:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=274
05/26/2022 00:50:49 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.7435873373373373 on epoch=274
05/26/2022 00:50:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=277
05/26/2022 00:50:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=279
05/26/2022 00:50:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=282
05/26/2022 00:51:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=284
05/26/2022 00:51:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
05/26/2022 00:51:04 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.7355126728110599 on epoch=287
05/26/2022 00:51:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
05/26/2022 00:51:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/26/2022 00:51:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
05/26/2022 00:51:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
05/26/2022 00:51:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/26/2022 00:51:19 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.750563330170778 on epoch=299
05/26/2022 00:51:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/26/2022 00:51:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=304
05/26/2022 00:51:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=307
05/26/2022 00:51:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/26/2022 00:51:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
05/26/2022 00:51:33 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.7248483595257789 on epoch=312
05/26/2022 00:51:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
05/26/2022 00:51:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=317
05/26/2022 00:51:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
05/26/2022 00:51:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=322
05/26/2022 00:51:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=324
05/26/2022 00:51:48 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.6830459664623079 on epoch=324
05/26/2022 00:51:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
05/26/2022 00:51:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=329
05/26/2022 00:51:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=332
05/26/2022 00:51:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/26/2022 00:52:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
05/26/2022 00:52:02 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.7023353567471214 on epoch=337
05/26/2022 00:52:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
05/26/2022 00:52:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=342
05/26/2022 00:52:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=344
05/26/2022 00:52:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/26/2022 00:52:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=349
05/26/2022 00:52:17 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.7094438188188188 on epoch=349
05/26/2022 00:52:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/26/2022 00:52:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/26/2022 00:52:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/26/2022 00:52:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=359
05/26/2022 00:52:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/26/2022 00:52:32 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.689236111111111 on epoch=362
05/26/2022 00:52:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
05/26/2022 00:52:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/26/2022 00:52:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/26/2022 00:52:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
05/26/2022 00:52:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/26/2022 00:52:46 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.7174023892773893 on epoch=374
05/26/2022 00:52:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/26/2022 00:52:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
05/26/2022 00:52:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/26/2022 00:52:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
05/26/2022 00:53:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/26/2022 00:53:01 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7266619560737207 on epoch=387
05/26/2022 00:53:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/26/2022 00:53:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=392
05/26/2022 00:53:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/26/2022 00:53:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
05/26/2022 00:53:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/26/2022 00:53:15 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.7475874094339056 on epoch=399
05/26/2022 00:53:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
05/26/2022 00:53:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
05/26/2022 00:53:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/26/2022 00:53:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
05/26/2022 00:53:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
05/26/2022 00:53:30 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.6872529644268774 on epoch=412
05/26/2022 00:53:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
05/26/2022 00:53:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/26/2022 00:53:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
05/26/2022 00:53:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
05/26/2022 00:53:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/26/2022 00:53:45 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.6879084967320261 on epoch=424
05/26/2022 00:53:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/26/2022 00:53:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
05/26/2022 00:53:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/26/2022 00:53:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/26/2022 00:53:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/26/2022 00:53:59 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.7234546703296704 on epoch=437
05/26/2022 00:54:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/26/2022 00:54:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
05/26/2022 00:54:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/26/2022 00:54:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/26/2022 00:54:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/26/2022 00:54:14 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.7234546703296704 on epoch=449
05/26/2022 00:54:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/26/2022 00:54:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/26/2022 00:54:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/26/2022 00:54:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
05/26/2022 00:54:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/26/2022 00:54:29 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.7465232683982684 on epoch=462
05/26/2022 00:54:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/26/2022 00:54:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/26/2022 00:54:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/26/2022 00:54:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/26/2022 00:54:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/26/2022 00:54:43 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.7084758378876026 on epoch=474
05/26/2022 00:54:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/26/2022 00:54:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/26/2022 00:54:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/26/2022 00:54:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/26/2022 00:54:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/26/2022 00:54:58 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7170955882352942 on epoch=487
05/26/2022 00:55:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/26/2022 00:55:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/26/2022 00:55:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/26/2022 00:55:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/26/2022 00:55:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/26/2022 00:55:12 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.6813717532467533 on epoch=499
05/26/2022 00:55:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/26/2022 00:55:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/26/2022 00:55:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/26/2022 00:55:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/26/2022 00:55:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/26/2022 00:55:27 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.6941317941317942 on epoch=512
05/26/2022 00:55:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/26/2022 00:55:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/26/2022 00:55:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/26/2022 00:55:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/26/2022 00:55:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/26/2022 00:55:42 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.7078477078477078 on epoch=524
05/26/2022 00:55:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/26/2022 00:55:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/26/2022 00:55:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/26/2022 00:55:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/26/2022 00:55:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/26/2022 00:55:56 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7094724107591754 on epoch=537
05/26/2022 00:55:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/26/2022 00:56:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/26/2022 00:56:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/26/2022 00:56:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/26/2022 00:56:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/26/2022 00:56:11 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.7093052232854864 on epoch=549
05/26/2022 00:56:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/26/2022 00:56:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/26/2022 00:56:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/26/2022 00:56:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/26/2022 00:56:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/26/2022 00:56:25 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.7300598147372342 on epoch=562
05/26/2022 00:56:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/26/2022 00:56:31 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/26/2022 00:56:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=569
05/26/2022 00:56:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/26/2022 00:56:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/26/2022 00:56:40 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7300598147372342 on epoch=574
05/26/2022 00:56:43 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/26/2022 00:56:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/26/2022 00:56:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/26/2022 00:56:51 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/26/2022 00:56:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/26/2022 00:56:55 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.7025252525252526 on epoch=587
05/26/2022 00:56:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/26/2022 00:57:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/26/2022 00:57:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/26/2022 00:57:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/26/2022 00:57:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/26/2022 00:57:09 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.7456826956826957 on epoch=599
05/26/2022 00:57:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/26/2022 00:57:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/26/2022 00:57:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/26/2022 00:57:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/26/2022 00:57:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/26/2022 00:57:24 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.7035119969040248 on epoch=612
05/26/2022 00:57:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/26/2022 00:57:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/26/2022 00:57:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/26/2022 00:57:35 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/26/2022 00:57:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/26/2022 00:57:39 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.7110390751282593 on epoch=624
05/26/2022 00:57:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/26/2022 00:57:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/26/2022 00:57:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/26/2022 00:57:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/26/2022 00:57:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/26/2022 00:57:53 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6810661764705883 on epoch=637
05/26/2022 00:57:56 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/26/2022 00:57:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/26/2022 00:58:01 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/26/2022 00:58:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 00:58:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/26/2022 00:58:08 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6883762104350339 on epoch=649
05/26/2022 00:58:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/26/2022 00:58:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/26/2022 00:58:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/26/2022 00:58:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 00:58:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/26/2022 00:58:23 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6760034728784727 on epoch=662
05/26/2022 00:58:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/26/2022 00:58:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/26/2022 00:58:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/26/2022 00:58:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/26/2022 00:58:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/26/2022 00:58:37 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6813717532467533 on epoch=674
05/26/2022 00:58:40 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/26/2022 00:58:43 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/26/2022 00:58:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/26/2022 00:58:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/26/2022 00:58:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/26/2022 00:58:52 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6627209595959596 on epoch=687
05/26/2022 00:58:55 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/26/2022 00:58:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/26/2022 00:59:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/26/2022 00:59:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/26/2022 00:59:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/26/2022 00:59:07 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6806537892934952 on epoch=699
05/26/2022 00:59:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/26/2022 00:59:12 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 00:59:15 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 00:59:17 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/26/2022 00:59:20 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/26/2022 00:59:21 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6652732683982684 on epoch=712
05/26/2022 00:59:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/26/2022 00:59:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/26/2022 00:59:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 00:59:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/26/2022 00:59:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/26/2022 00:59:36 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6652732683982684 on epoch=724
05/26/2022 00:59:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/26/2022 00:59:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 00:59:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/26/2022 00:59:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/26/2022 00:59:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 00:59:51 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6927332144979204 on epoch=737
05/26/2022 00:59:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/26/2022 00:59:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=742
05/26/2022 00:59:59 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/26/2022 01:00:01 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/26/2022 01:00:04 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 01:00:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:00:05 - INFO - __main__ - Printing 3 examples
05/26/2022 01:00:05 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/26/2022 01:00:05 - INFO - __main__ - ['sad']
05/26/2022 01:00:05 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/26/2022 01:00:05 - INFO - __main__ - ['sad']
05/26/2022 01:00:05 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/26/2022 01:00:05 - INFO - __main__ - ['sad']
05/26/2022 01:00:05 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:00:05 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.6652732683982684 on epoch=749
05/26/2022 01:00:05 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:00:05 - INFO - __main__ - save last model!
05/26/2022 01:00:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 01:00:05 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 01:00:05 - INFO - __main__ - Printing 3 examples
05/26/2022 01:00:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 01:00:05 - INFO - __main__ - ['others']
05/26/2022 01:00:05 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 01:00:05 - INFO - __main__ - ['others']
05/26/2022 01:00:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 01:00:05 - INFO - __main__ - ['others']
05/26/2022 01:00:05 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:00:05 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 01:00:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:00:05 - INFO - __main__ - Printing 3 examples
05/26/2022 01:00:05 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/26/2022 01:00:05 - INFO - __main__ - ['sad']
05/26/2022 01:00:05 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/26/2022 01:00:05 - INFO - __main__ - ['sad']
05/26/2022 01:00:05 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/26/2022 01:00:05 - INFO - __main__ - ['sad']
05/26/2022 01:00:05 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:00:05 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:00:06 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 01:00:08 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:00:13 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 01:00:24 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 01:00:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 01:00:25 - INFO - __main__ - Starting training!
05/26/2022 01:02:04 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_21_0.5_8_predictions.txt
05/26/2022 01:02:04 - INFO - __main__ - Classification-F1 on test data: 0.1270
05/26/2022 01:02:04 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.7612058080808081, test_performance=0.12701664712016822
05/26/2022 01:02:04 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
05/26/2022 01:02:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:02:05 - INFO - __main__ - Printing 3 examples
05/26/2022 01:02:05 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/26/2022 01:02:05 - INFO - __main__ - ['sad']
05/26/2022 01:02:05 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/26/2022 01:02:05 - INFO - __main__ - ['sad']
05/26/2022 01:02:05 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/26/2022 01:02:05 - INFO - __main__ - ['sad']
05/26/2022 01:02:05 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:02:05 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:02:05 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 01:02:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:02:05 - INFO - __main__ - Printing 3 examples
05/26/2022 01:02:05 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/26/2022 01:02:05 - INFO - __main__ - ['sad']
05/26/2022 01:02:05 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/26/2022 01:02:05 - INFO - __main__ - ['sad']
05/26/2022 01:02:05 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/26/2022 01:02:05 - INFO - __main__ - ['sad']
05/26/2022 01:02:05 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:02:05 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:02:05 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 01:02:24 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 01:02:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 01:02:25 - INFO - __main__ - Starting training!
05/26/2022 01:02:28 - INFO - __main__ - Step 10 Global step 10 Train loss 2.89 on epoch=2
05/26/2022 01:02:31 - INFO - __main__ - Step 20 Global step 20 Train loss 1.41 on epoch=4
05/26/2022 01:02:34 - INFO - __main__ - Step 30 Global step 30 Train loss 1.01 on epoch=7
05/26/2022 01:02:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.96 on epoch=9
05/26/2022 01:02:39 - INFO - __main__ - Step 50 Global step 50 Train loss 0.86 on epoch=12
05/26/2022 01:02:40 - INFO - __main__ - Global step 50 Train loss 1.42 Classification-F1 0.2257205513784461 on epoch=12
05/26/2022 01:02:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2257205513784461 on epoch=12, global_step=50
05/26/2022 01:02:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.89 on epoch=14
05/26/2022 01:02:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.81 on epoch=17
05/26/2022 01:02:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.77 on epoch=19
05/26/2022 01:02:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.77 on epoch=22
05/26/2022 01:02:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.75 on epoch=24
05/26/2022 01:02:55 - INFO - __main__ - Global step 100 Train loss 0.80 Classification-F1 0.6307748538011696 on epoch=24
05/26/2022 01:02:55 - INFO - __main__ - Saving model with best Classification-F1: 0.2257205513784461 -> 0.6307748538011696 on epoch=24, global_step=100
05/26/2022 01:02:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=27
05/26/2022 01:03:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=29
05/26/2022 01:03:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.66 on epoch=32
05/26/2022 01:03:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.63 on epoch=34
05/26/2022 01:03:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=37
05/26/2022 01:03:09 - INFO - __main__ - Global step 150 Train loss 0.64 Classification-F1 0.4872391304347826 on epoch=37
05/26/2022 01:03:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=39
05/26/2022 01:03:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=42
05/26/2022 01:03:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=44
05/26/2022 01:03:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=47
05/26/2022 01:03:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=49
05/26/2022 01:03:24 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.7760180995475113 on epoch=49
05/26/2022 01:03:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6307748538011696 -> 0.7760180995475113 on epoch=49, global_step=200
05/26/2022 01:03:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=52
05/26/2022 01:03:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=54
05/26/2022 01:03:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=57
05/26/2022 01:03:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=59
05/26/2022 01:03:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=62
05/26/2022 01:03:38 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.7346260505499939 on epoch=62
05/26/2022 01:03:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=64
05/26/2022 01:03:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.34 on epoch=67
05/26/2022 01:03:46 - INFO - __main__ - Step 280 Global step 280 Train loss 1.16 on epoch=69
05/26/2022 01:03:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=72
05/26/2022 01:03:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=74
05/26/2022 01:03:53 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.773571256329877 on epoch=74
05/26/2022 01:03:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=77
05/26/2022 01:03:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=79
05/26/2022 01:04:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.20 on epoch=82
05/26/2022 01:04:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=84
05/26/2022 01:04:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=87
05/26/2022 01:04:07 - INFO - __main__ - Global step 350 Train loss 0.22 Classification-F1 0.7358438940092166 on epoch=87
05/26/2022 01:04:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=89
05/26/2022 01:04:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=92
05/26/2022 01:04:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.19 on epoch=94
05/26/2022 01:04:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.13 on epoch=97
05/26/2022 01:04:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=99
05/26/2022 01:04:22 - INFO - __main__ - Global step 400 Train loss 0.18 Classification-F1 0.6978606017305709 on epoch=99
05/26/2022 01:04:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.15 on epoch=102
05/26/2022 01:04:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.14 on epoch=104
05/26/2022 01:04:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=107
05/26/2022 01:04:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.14 on epoch=109
05/26/2022 01:04:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.15 on epoch=112
05/26/2022 01:04:37 - INFO - __main__ - Global step 450 Train loss 0.15 Classification-F1 0.7471521942110178 on epoch=112
05/26/2022 01:04:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.14 on epoch=114
05/26/2022 01:04:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=117
05/26/2022 01:04:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=119
05/26/2022 01:04:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.18 on epoch=122
05/26/2022 01:04:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.11 on epoch=124
05/26/2022 01:04:51 - INFO - __main__ - Global step 500 Train loss 0.13 Classification-F1 0.7003681077694236 on epoch=124
05/26/2022 01:04:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.13 on epoch=127
05/26/2022 01:04:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.17 on epoch=129
05/26/2022 01:04:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.17 on epoch=132
05/26/2022 01:05:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=134
05/26/2022 01:05:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.13 on epoch=137
05/26/2022 01:05:06 - INFO - __main__ - Global step 550 Train loss 0.14 Classification-F1 0.7667533430603508 on epoch=137
05/26/2022 01:05:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=139
05/26/2022 01:05:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=142
05/26/2022 01:05:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.10 on epoch=144
05/26/2022 01:05:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=147
05/26/2022 01:05:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=149
05/26/2022 01:05:21 - INFO - __main__ - Global step 600 Train loss 0.12 Classification-F1 0.7334945549047109 on epoch=149
05/26/2022 01:05:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.10 on epoch=152
05/26/2022 01:05:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=154
05/26/2022 01:05:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.06 on epoch=157
05/26/2022 01:05:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.09 on epoch=159
05/26/2022 01:05:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=162
05/26/2022 01:05:36 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.7084628077275136 on epoch=162
05/26/2022 01:05:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=164
05/26/2022 01:05:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=167
05/26/2022 01:05:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=169
05/26/2022 01:05:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.07 on epoch=172
05/26/2022 01:05:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=174
05/26/2022 01:05:50 - INFO - __main__ - Global step 700 Train loss 0.08 Classification-F1 0.746705608556941 on epoch=174
05/26/2022 01:05:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=177
05/26/2022 01:05:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=179
05/26/2022 01:05:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=182
05/26/2022 01:06:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=184
05/26/2022 01:06:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=187
05/26/2022 01:06:05 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.7338215421303657 on epoch=187
05/26/2022 01:06:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=189
05/26/2022 01:06:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=192
05/26/2022 01:06:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=194
05/26/2022 01:06:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
05/26/2022 01:06:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=199
05/26/2022 01:06:21 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.7448487514011708 on epoch=199
05/26/2022 01:06:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
05/26/2022 01:06:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=204
05/26/2022 01:06:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=207
05/26/2022 01:06:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=209
05/26/2022 01:06:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=212
05/26/2022 01:06:36 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.7177703089244851 on epoch=212
05/26/2022 01:06:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=214
05/26/2022 01:06:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=217
05/26/2022 01:06:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=219
05/26/2022 01:06:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=222
05/26/2022 01:06:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=224
05/26/2022 01:06:51 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.75875504000504 on epoch=224
05/26/2022 01:06:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=227
05/26/2022 01:06:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
05/26/2022 01:06:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=232
05/26/2022 01:07:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=234
05/26/2022 01:07:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=237
05/26/2022 01:07:06 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.7608389179470415 on epoch=237
05/26/2022 01:07:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
05/26/2022 01:07:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
05/26/2022 01:07:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=244
05/26/2022 01:07:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=247
05/26/2022 01:07:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=249
05/26/2022 01:07:21 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.795838133640553 on epoch=249
05/26/2022 01:07:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7760180995475113 -> 0.795838133640553 on epoch=249, global_step=1000
05/26/2022 01:07:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=252
05/26/2022 01:07:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
05/26/2022 01:07:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=257
05/26/2022 01:07:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=259
05/26/2022 01:07:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=262
05/26/2022 01:07:36 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.7575640046228282 on epoch=262
05/26/2022 01:07:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
05/26/2022 01:07:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=267
05/26/2022 01:07:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=269
05/26/2022 01:07:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/26/2022 01:07:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
05/26/2022 01:07:51 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.7620689655172413 on epoch=274
05/26/2022 01:07:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
05/26/2022 01:07:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=279
05/26/2022 01:07:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=282
05/26/2022 01:08:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
05/26/2022 01:08:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
05/26/2022 01:08:06 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.7757606490872211 on epoch=287
05/26/2022 01:08:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
05/26/2022 01:08:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/26/2022 01:08:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
05/26/2022 01:08:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
05/26/2022 01:08:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/26/2022 01:08:21 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.769023569023569 on epoch=299
05/26/2022 01:08:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=302
05/26/2022 01:08:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
05/26/2022 01:08:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
05/26/2022 01:08:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/26/2022 01:08:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
05/26/2022 01:08:36 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.7569937652480347 on epoch=312
05/26/2022 01:08:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
05/26/2022 01:08:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/26/2022 01:08:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=319
05/26/2022 01:08:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/26/2022 01:08:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/26/2022 01:08:51 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.7881334351922588 on epoch=324
05/26/2022 01:08:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
05/26/2022 01:08:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/26/2022 01:09:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/26/2022 01:09:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/26/2022 01:09:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
05/26/2022 01:09:07 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.7204162560699063 on epoch=337
05/26/2022 01:09:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/26/2022 01:09:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
05/26/2022 01:09:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=344
05/26/2022 01:09:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/26/2022 01:09:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
05/26/2022 01:09:22 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.7535155476331947 on epoch=349
05/26/2022 01:09:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
05/26/2022 01:09:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/26/2022 01:09:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
05/26/2022 01:09:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=359
05/26/2022 01:09:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/26/2022 01:09:37 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.7076200245236748 on epoch=362
05/26/2022 01:09:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/26/2022 01:09:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/26/2022 01:09:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/26/2022 01:09:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/26/2022 01:09:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/26/2022 01:09:52 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.7492229992229993 on epoch=374
05/26/2022 01:09:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/26/2022 01:09:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/26/2022 01:10:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/26/2022 01:10:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/26/2022 01:10:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/26/2022 01:10:07 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.7594426406926408 on epoch=387
05/26/2022 01:10:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/26/2022 01:10:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/26/2022 01:10:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
05/26/2022 01:10:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/26/2022 01:10:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/26/2022 01:10:22 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.7284716478264865 on epoch=399
05/26/2022 01:10:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/26/2022 01:10:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/26/2022 01:10:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/26/2022 01:10:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=409
05/26/2022 01:10:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
05/26/2022 01:10:37 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.7141645988420182 on epoch=412
05/26/2022 01:10:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
05/26/2022 01:10:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/26/2022 01:10:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/26/2022 01:10:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/26/2022 01:10:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/26/2022 01:10:52 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7553504962779156 on epoch=424
05/26/2022 01:10:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/26/2022 01:10:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/26/2022 01:11:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
05/26/2022 01:11:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/26/2022 01:11:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/26/2022 01:11:07 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7637992831541219 on epoch=437
05/26/2022 01:11:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/26/2022 01:11:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/26/2022 01:11:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/26/2022 01:11:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/26/2022 01:11:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/26/2022 01:11:22 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7285653650254669 on epoch=449
05/26/2022 01:11:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
05/26/2022 01:11:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=454
05/26/2022 01:11:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/26/2022 01:11:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/26/2022 01:11:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/26/2022 01:11:37 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7278668091168091 on epoch=462
05/26/2022 01:11:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/26/2022 01:11:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/26/2022 01:11:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/26/2022 01:11:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/26/2022 01:11:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/26/2022 01:11:53 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7349715318465319 on epoch=474
05/26/2022 01:11:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/26/2022 01:11:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=479
05/26/2022 01:12:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/26/2022 01:12:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/26/2022 01:12:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/26/2022 01:12:08 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.7557689654463848 on epoch=487
05/26/2022 01:12:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/26/2022 01:12:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/26/2022 01:12:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=494
05/26/2022 01:12:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/26/2022 01:12:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/26/2022 01:12:23 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7557689654463848 on epoch=499
05/26/2022 01:12:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/26/2022 01:12:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/26/2022 01:12:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/26/2022 01:12:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=509
05/26/2022 01:12:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/26/2022 01:12:38 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7285653650254669 on epoch=512
05/26/2022 01:12:40 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/26/2022 01:12:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/26/2022 01:12:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/26/2022 01:12:49 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/26/2022 01:12:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/26/2022 01:12:53 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7609994172494172 on epoch=524
05/26/2022 01:12:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
05/26/2022 01:12:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/26/2022 01:13:01 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/26/2022 01:13:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/26/2022 01:13:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/26/2022 01:13:08 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7167508057385699 on epoch=537
05/26/2022 01:13:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/26/2022 01:13:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/26/2022 01:13:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/26/2022 01:13:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/26/2022 01:13:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/26/2022 01:13:23 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7179772915256787 on epoch=549
05/26/2022 01:13:26 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/26/2022 01:13:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/26/2022 01:13:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/26/2022 01:13:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/26/2022 01:13:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
05/26/2022 01:13:38 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7430409663865547 on epoch=562
05/26/2022 01:13:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/26/2022 01:13:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/26/2022 01:13:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/26/2022 01:13:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/26/2022 01:13:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/26/2022 01:13:53 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7015868365193701 on epoch=574
05/26/2022 01:13:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/26/2022 01:13:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/26/2022 01:14:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/26/2022 01:14:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/26/2022 01:14:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/26/2022 01:14:08 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6789141414141414 on epoch=587
05/26/2022 01:14:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/26/2022 01:14:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/26/2022 01:14:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/26/2022 01:14:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/26/2022 01:14:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/26/2022 01:14:23 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7194567384370016 on epoch=599
05/26/2022 01:14:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/26/2022 01:14:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/26/2022 01:14:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/26/2022 01:14:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/26/2022 01:14:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
05/26/2022 01:14:38 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7199180999181 on epoch=612
05/26/2022 01:14:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/26/2022 01:14:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/26/2022 01:14:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/26/2022 01:14:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/26/2022 01:14:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/26/2022 01:14:53 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7604737514011708 on epoch=624
05/26/2022 01:14:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/26/2022 01:14:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/26/2022 01:15:01 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/26/2022 01:15:03 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/26/2022 01:15:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/26/2022 01:15:08 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.7194567384370016 on epoch=637
05/26/2022 01:15:10 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/26/2022 01:15:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/26/2022 01:15:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/26/2022 01:15:18 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 01:15:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/26/2022 01:15:23 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7145250582750583 on epoch=649
05/26/2022 01:15:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/26/2022 01:15:28 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/26/2022 01:15:31 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/26/2022 01:15:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 01:15:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/26/2022 01:15:37 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7408653846153845 on epoch=662
05/26/2022 01:15:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/26/2022 01:15:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/26/2022 01:15:45 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/26/2022 01:15:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
05/26/2022 01:15:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/26/2022 01:15:52 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7306540306540307 on epoch=674
05/26/2022 01:15:55 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/26/2022 01:15:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/26/2022 01:16:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/26/2022 01:16:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/26/2022 01:16:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/26/2022 01:16:07 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7052377536380406 on epoch=687
05/26/2022 01:16:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/26/2022 01:16:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/26/2022 01:16:14 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/26/2022 01:16:17 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/26/2022 01:16:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
05/26/2022 01:16:21 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.729631507775524 on epoch=699
05/26/2022 01:16:24 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/26/2022 01:16:26 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 01:16:29 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/26/2022 01:16:32 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/26/2022 01:16:34 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/26/2022 01:16:36 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.730894105894106 on epoch=712
05/26/2022 01:16:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/26/2022 01:16:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/26/2022 01:16:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 01:16:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/26/2022 01:16:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
05/26/2022 01:16:50 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7259875541125541 on epoch=724
05/26/2022 01:16:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/26/2022 01:16:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
05/26/2022 01:16:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/26/2022 01:17:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/26/2022 01:17:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/26/2022 01:17:05 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7434918091168091 on epoch=737
05/26/2022 01:17:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/26/2022 01:17:10 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/26/2022 01:17:13 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/26/2022 01:17:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/26/2022 01:17:18 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 01:17:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:17:19 - INFO - __main__ - Printing 3 examples
05/26/2022 01:17:19 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/26/2022 01:17:19 - INFO - __main__ - ['sad']
05/26/2022 01:17:19 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/26/2022 01:17:19 - INFO - __main__ - ['sad']
05/26/2022 01:17:19 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/26/2022 01:17:19 - INFO - __main__ - ['sad']
05/26/2022 01:17:19 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:17:19 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7530503978779841 on epoch=749
05/26/2022 01:17:19 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:17:19 - INFO - __main__ - save last model!
05/26/2022 01:17:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 01:17:19 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 01:17:19 - INFO - __main__ - Printing 3 examples
05/26/2022 01:17:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 01:17:19 - INFO - __main__ - ['others']
05/26/2022 01:17:19 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 01:17:19 - INFO - __main__ - ['others']
05/26/2022 01:17:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 01:17:19 - INFO - __main__ - ['others']
05/26/2022 01:17:19 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:17:19 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 01:17:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:17:19 - INFO - __main__ - Printing 3 examples
05/26/2022 01:17:19 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/26/2022 01:17:19 - INFO - __main__ - ['sad']
05/26/2022 01:17:19 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/26/2022 01:17:19 - INFO - __main__ - ['sad']
05/26/2022 01:17:19 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/26/2022 01:17:19 - INFO - __main__ - ['sad']
05/26/2022 01:17:19 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:17:19 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:17:19 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 01:17:21 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:17:27 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 01:17:35 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 01:17:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 01:17:36 - INFO - __main__ - Starting training!
05/26/2022 01:19:17 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_21_0.4_8_predictions.txt
05/26/2022 01:19:17 - INFO - __main__ - Classification-F1 on test data: 0.1583
05/26/2022 01:19:18 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.795838133640553, test_performance=0.15828918165339115
05/26/2022 01:19:18 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
05/26/2022 01:19:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:19:18 - INFO - __main__ - Printing 3 examples
05/26/2022 01:19:18 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/26/2022 01:19:18 - INFO - __main__ - ['sad']
05/26/2022 01:19:18 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/26/2022 01:19:18 - INFO - __main__ - ['sad']
05/26/2022 01:19:18 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/26/2022 01:19:18 - INFO - __main__ - ['sad']
05/26/2022 01:19:18 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:19:19 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:19:19 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 01:19:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:19:19 - INFO - __main__ - Printing 3 examples
05/26/2022 01:19:19 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/26/2022 01:19:19 - INFO - __main__ - ['sad']
05/26/2022 01:19:19 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/26/2022 01:19:19 - INFO - __main__ - ['sad']
05/26/2022 01:19:19 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/26/2022 01:19:19 - INFO - __main__ - ['sad']
05/26/2022 01:19:19 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:19:19 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:19:19 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 01:19:37 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 01:19:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 01:19:38 - INFO - __main__ - Starting training!
05/26/2022 01:19:41 - INFO - __main__ - Step 10 Global step 10 Train loss 3.13 on epoch=2
05/26/2022 01:19:44 - INFO - __main__ - Step 20 Global step 20 Train loss 1.58 on epoch=4
05/26/2022 01:19:46 - INFO - __main__ - Step 30 Global step 30 Train loss 1.27 on epoch=7
05/26/2022 01:19:49 - INFO - __main__ - Step 40 Global step 40 Train loss 0.93 on epoch=9
05/26/2022 01:19:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.97 on epoch=12
05/26/2022 01:19:53 - INFO - __main__ - Global step 50 Train loss 1.58 Classification-F1 0.1 on epoch=12
05/26/2022 01:19:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/26/2022 01:19:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.94 on epoch=14
05/26/2022 01:19:58 - INFO - __main__ - Step 70 Global step 70 Train loss 0.90 on epoch=17
05/26/2022 01:20:01 - INFO - __main__ - Step 80 Global step 80 Train loss 0.87 on epoch=19
05/26/2022 01:20:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.91 on epoch=22
05/26/2022 01:20:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.84 on epoch=24
05/26/2022 01:20:07 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.5085273409394594 on epoch=24
05/26/2022 01:20:07 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.5085273409394594 on epoch=24, global_step=100
05/26/2022 01:20:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.92 on epoch=27
05/26/2022 01:20:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=29
05/26/2022 01:20:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.78 on epoch=32
05/26/2022 01:20:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.79 on epoch=34
05/26/2022 01:20:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.72 on epoch=37
05/26/2022 01:20:22 - INFO - __main__ - Global step 150 Train loss 0.80 Classification-F1 0.6193487097914459 on epoch=37
05/26/2022 01:20:22 - INFO - __main__ - Saving model with best Classification-F1: 0.5085273409394594 -> 0.6193487097914459 on epoch=37, global_step=150
05/26/2022 01:20:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.68 on epoch=39
05/26/2022 01:20:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.78 on epoch=42
05/26/2022 01:20:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.61 on epoch=44
05/26/2022 01:20:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.58 on epoch=47
05/26/2022 01:20:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.62 on epoch=49
05/26/2022 01:20:36 - INFO - __main__ - Global step 200 Train loss 0.65 Classification-F1 0.5573401534526854 on epoch=49
05/26/2022 01:20:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=52
05/26/2022 01:20:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=54
05/26/2022 01:20:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=57
05/26/2022 01:20:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=59
05/26/2022 01:20:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=62
05/26/2022 01:20:50 - INFO - __main__ - Global step 250 Train loss 0.52 Classification-F1 0.6743743743743743 on epoch=62
05/26/2022 01:20:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6193487097914459 -> 0.6743743743743743 on epoch=62, global_step=250
05/26/2022 01:20:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=64
05/26/2022 01:20:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=67
05/26/2022 01:20:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=69
05/26/2022 01:21:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=72
05/26/2022 01:21:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=74
05/26/2022 01:21:04 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.6845588235294118 on epoch=74
05/26/2022 01:21:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6743743743743743 -> 0.6845588235294118 on epoch=74, global_step=300
05/26/2022 01:21:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=77
05/26/2022 01:21:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=79
05/26/2022 01:21:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=82
05/26/2022 01:21:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=84
05/26/2022 01:21:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=87
05/26/2022 01:21:19 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.7157425907425908 on epoch=87
05/26/2022 01:21:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6845588235294118 -> 0.7157425907425908 on epoch=87, global_step=350
05/26/2022 01:21:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=89
05/26/2022 01:21:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=92
05/26/2022 01:21:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.33 on epoch=94
05/26/2022 01:21:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=97
05/26/2022 01:21:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=99
05/26/2022 01:21:34 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.6940902235019882 on epoch=99
05/26/2022 01:21:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.17 on epoch=102
05/26/2022 01:21:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=104
05/26/2022 01:21:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
05/26/2022 01:21:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=109
05/26/2022 01:21:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=112
05/26/2022 01:21:48 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.7277602007203525 on epoch=112
05/26/2022 01:21:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7157425907425908 -> 0.7277602007203525 on epoch=112, global_step=450
05/26/2022 01:21:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=114
05/26/2022 01:21:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=117
05/26/2022 01:21:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=119
05/26/2022 01:21:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.17 on epoch=122
05/26/2022 01:22:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.11 on epoch=124
05/26/2022 01:22:03 - INFO - __main__ - Global step 500 Train loss 0.16 Classification-F1 0.7455436720142602 on epoch=124
05/26/2022 01:22:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7277602007203525 -> 0.7455436720142602 on epoch=124, global_step=500
05/26/2022 01:22:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=127
05/26/2022 01:22:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.16 on epoch=129
05/26/2022 01:22:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.10 on epoch=132
05/26/2022 01:22:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=134
05/26/2022 01:22:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=137
05/26/2022 01:22:17 - INFO - __main__ - Global step 550 Train loss 0.16 Classification-F1 0.6881455105780443 on epoch=137
05/26/2022 01:22:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.18 on epoch=139
05/26/2022 01:22:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=142
05/26/2022 01:22:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.12 on epoch=144
05/26/2022 01:22:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=147
05/26/2022 01:22:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=149
05/26/2022 01:22:32 - INFO - __main__ - Global step 600 Train loss 0.14 Classification-F1 0.763531624644667 on epoch=149
05/26/2022 01:22:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7455436720142602 -> 0.763531624644667 on epoch=149, global_step=600
05/26/2022 01:22:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.11 on epoch=152
05/26/2022 01:22:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=154
05/26/2022 01:22:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=157
05/26/2022 01:22:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=159
05/26/2022 01:22:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=162
05/26/2022 01:22:46 - INFO - __main__ - Global step 650 Train loss 0.10 Classification-F1 0.7408559577677225 on epoch=162
05/26/2022 01:22:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=164
05/26/2022 01:22:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
05/26/2022 01:22:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.06 on epoch=169
05/26/2022 01:22:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.07 on epoch=172
05/26/2022 01:23:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=174
05/26/2022 01:23:01 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.7225906120023767 on epoch=174
05/26/2022 01:23:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=177
05/26/2022 01:23:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=179
05/26/2022 01:23:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=182
05/26/2022 01:23:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=184
05/26/2022 01:23:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=187
05/26/2022 01:23:16 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.7811430840664713 on epoch=187
05/26/2022 01:23:16 - INFO - __main__ - Saving model with best Classification-F1: 0.763531624644667 -> 0.7811430840664713 on epoch=187, global_step=750
05/26/2022 01:23:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=189
05/26/2022 01:23:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=192
05/26/2022 01:23:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=194
05/26/2022 01:23:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=197
05/26/2022 01:23:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=199
05/26/2022 01:23:31 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.7199582027168234 on epoch=199
05/26/2022 01:23:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=202
05/26/2022 01:23:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=204
05/26/2022 01:23:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
05/26/2022 01:23:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=209
05/26/2022 01:23:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=212
05/26/2022 01:23:46 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.7328674646037076 on epoch=212
05/26/2022 01:23:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=214
05/26/2022 01:23:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=217
05/26/2022 01:23:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=219
05/26/2022 01:23:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=222
05/26/2022 01:23:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=224
05/26/2022 01:24:01 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.6949033589408303 on epoch=224
05/26/2022 01:24:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=227
05/26/2022 01:24:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=229
05/26/2022 01:24:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
05/26/2022 01:24:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=234
05/26/2022 01:24:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
05/26/2022 01:24:15 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.7036764705882352 on epoch=237
05/26/2022 01:24:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=239
05/26/2022 01:24:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
05/26/2022 01:24:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
05/26/2022 01:24:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=247
05/26/2022 01:24:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=249
05/26/2022 01:24:30 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.7234841628959277 on epoch=249
05/26/2022 01:24:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
05/26/2022 01:24:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=254
05/26/2022 01:24:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=257
05/26/2022 01:24:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/26/2022 01:24:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
05/26/2022 01:24:45 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.6946717989474475 on epoch=262
05/26/2022 01:24:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
05/26/2022 01:24:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
05/26/2022 01:24:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
05/26/2022 01:24:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=272
05/26/2022 01:24:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
05/26/2022 01:25:00 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.7493585043988269 on epoch=274
05/26/2022 01:25:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=277
05/26/2022 01:25:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/26/2022 01:25:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=282
05/26/2022 01:25:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/26/2022 01:25:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
05/26/2022 01:25:15 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.7474866029822926 on epoch=287
05/26/2022 01:25:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
05/26/2022 01:25:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=292
05/26/2022 01:25:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
05/26/2022 01:25:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
05/26/2022 01:25:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/26/2022 01:25:30 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.7578255675029869 on epoch=299
05/26/2022 01:25:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/26/2022 01:25:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
05/26/2022 01:25:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
05/26/2022 01:25:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=309
05/26/2022 01:25:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=312
05/26/2022 01:25:45 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.7657748248202102 on epoch=312
05/26/2022 01:25:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
05/26/2022 01:25:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/26/2022 01:25:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
05/26/2022 01:25:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/26/2022 01:25:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/26/2022 01:26:00 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.7500946969696969 on epoch=324
05/26/2022 01:26:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/26/2022 01:26:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
05/26/2022 01:26:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/26/2022 01:26:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/26/2022 01:26:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=337
05/26/2022 01:26:15 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.7803030303030303 on epoch=337
05/26/2022 01:26:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
05/26/2022 01:26:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/26/2022 01:26:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
05/26/2022 01:26:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/26/2022 01:26:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/26/2022 01:26:30 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.7650278879880399 on epoch=349
05/26/2022 01:26:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/26/2022 01:26:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/26/2022 01:26:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/26/2022 01:26:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
05/26/2022 01:26:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/26/2022 01:26:45 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.7456768142252014 on epoch=362
05/26/2022 01:26:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
05/26/2022 01:26:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/26/2022 01:26:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/26/2022 01:26:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/26/2022 01:26:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/26/2022 01:27:00 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.7813982447817837 on epoch=374
05/26/2022 01:27:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7811430840664713 -> 0.7813982447817837 on epoch=374, global_step=1500
05/26/2022 01:27:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/26/2022 01:27:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/26/2022 01:27:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/26/2022 01:27:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/26/2022 01:27:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/26/2022 01:27:15 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.763531624644667 on epoch=387
05/26/2022 01:27:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/26/2022 01:27:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/26/2022 01:27:23 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/26/2022 01:27:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/26/2022 01:27:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/26/2022 01:27:30 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.808837083220385 on epoch=399
05/26/2022 01:27:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7813982447817837 -> 0.808837083220385 on epoch=399, global_step=1600
05/26/2022 01:27:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/26/2022 01:27:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/26/2022 01:27:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
05/26/2022 01:27:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/26/2022 01:27:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=412
05/26/2022 01:27:46 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.7768375070741369 on epoch=412
05/26/2022 01:27:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/26/2022 01:27:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/26/2022 01:27:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/26/2022 01:27:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/26/2022 01:27:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/26/2022 01:28:00 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7800067204301075 on epoch=424
05/26/2022 01:28:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
05/26/2022 01:28:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/26/2022 01:28:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/26/2022 01:28:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/26/2022 01:28:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/26/2022 01:28:15 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7783908430143218 on epoch=437
05/26/2022 01:28:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/26/2022 01:28:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/26/2022 01:28:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/26/2022 01:28:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/26/2022 01:28:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/26/2022 01:28:30 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.8107837374761817 on epoch=449
05/26/2022 01:28:30 - INFO - __main__ - Saving model with best Classification-F1: 0.808837083220385 -> 0.8107837374761817 on epoch=449, global_step=1800
05/26/2022 01:28:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/26/2022 01:28:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/26/2022 01:28:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/26/2022 01:28:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=459
05/26/2022 01:28:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/26/2022 01:28:45 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.7637310606060607 on epoch=462
05/26/2022 01:28:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/26/2022 01:28:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/26/2022 01:28:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/26/2022 01:28:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/26/2022 01:28:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/26/2022 01:29:00 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.7658846529814272 on epoch=474
05/26/2022 01:29:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/26/2022 01:29:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/26/2022 01:29:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/26/2022 01:29:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/26/2022 01:29:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/26/2022 01:29:15 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7757575757575758 on epoch=487
05/26/2022 01:29:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/26/2022 01:29:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/26/2022 01:29:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=494
05/26/2022 01:29:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/26/2022 01:29:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=499
05/26/2022 01:29:30 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7637310606060607 on epoch=499
05/26/2022 01:29:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/26/2022 01:29:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/26/2022 01:29:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/26/2022 01:29:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/26/2022 01:29:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/26/2022 01:29:45 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.778241495806151 on epoch=512
05/26/2022 01:29:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/26/2022 01:29:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
05/26/2022 01:29:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/26/2022 01:29:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/26/2022 01:29:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/26/2022 01:30:00 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7805443548387097 on epoch=524
05/26/2022 01:30:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/26/2022 01:30:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/26/2022 01:30:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/26/2022 01:30:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/26/2022 01:30:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/26/2022 01:30:14 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7666847041847042 on epoch=537
05/26/2022 01:30:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
05/26/2022 01:30:20 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/26/2022 01:30:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/26/2022 01:30:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/26/2022 01:30:28 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/26/2022 01:30:29 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7586939881057528 on epoch=549
05/26/2022 01:30:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/26/2022 01:30:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/26/2022 01:30:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/26/2022 01:30:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/26/2022 01:30:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/26/2022 01:30:44 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7787990196078431 on epoch=562
05/26/2022 01:30:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/26/2022 01:30:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/26/2022 01:30:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/26/2022 01:30:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
05/26/2022 01:30:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
05/26/2022 01:30:59 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7500946969696971 on epoch=574
05/26/2022 01:31:02 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/26/2022 01:31:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/26/2022 01:31:07 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/26/2022 01:31:10 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/26/2022 01:31:13 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/26/2022 01:31:14 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7073010809852915 on epoch=587
05/26/2022 01:31:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/26/2022 01:31:19 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/26/2022 01:31:22 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/26/2022 01:31:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/26/2022 01:31:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/26/2022 01:31:29 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7034076054664291 on epoch=599
05/26/2022 01:31:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/26/2022 01:31:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/26/2022 01:31:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/26/2022 01:31:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/26/2022 01:31:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
05/26/2022 01:31:44 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7377403846153846 on epoch=612
05/26/2022 01:31:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/26/2022 01:31:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/26/2022 01:31:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/26/2022 01:31:55 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/26/2022 01:31:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/26/2022 01:31:59 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.8128360215053763 on epoch=624
05/26/2022 01:31:59 - INFO - __main__ - Saving model with best Classification-F1: 0.8107837374761817 -> 0.8128360215053763 on epoch=624, global_step=2500
05/26/2022 01:32:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=627
05/26/2022 01:32:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/26/2022 01:32:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/26/2022 01:32:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/26/2022 01:32:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/26/2022 01:32:14 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7625953829807786 on epoch=637
05/26/2022 01:32:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/26/2022 01:32:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/26/2022 01:32:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/26/2022 01:32:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 01:32:27 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/26/2022 01:32:28 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7766942260775278 on epoch=649
05/26/2022 01:32:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/26/2022 01:32:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/26/2022 01:32:37 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/26/2022 01:32:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 01:32:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/26/2022 01:32:43 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7578255675029869 on epoch=662
05/26/2022 01:32:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=664
05/26/2022 01:32:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/26/2022 01:32:51 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/26/2022 01:32:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
05/26/2022 01:32:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
05/26/2022 01:32:58 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.8110143177505607 on epoch=674
05/26/2022 01:33:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/26/2022 01:33:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/26/2022 01:33:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/26/2022 01:33:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/26/2022 01:33:12 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/26/2022 01:33:13 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7535912698412699 on epoch=687
05/26/2022 01:33:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/26/2022 01:33:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/26/2022 01:33:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/26/2022 01:33:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/26/2022 01:33:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/26/2022 01:33:28 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7794372294372294 on epoch=699
05/26/2022 01:33:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/26/2022 01:33:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 01:33:36 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 01:33:39 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/26/2022 01:33:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=712
05/26/2022 01:33:43 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7668385536032596 on epoch=712
05/26/2022 01:33:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
05/26/2022 01:33:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/26/2022 01:33:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 01:33:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/26/2022 01:33:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/26/2022 01:33:58 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7336265884652982 on epoch=724
05/26/2022 01:34:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/26/2022 01:34:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 01:34:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/26/2022 01:34:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/26/2022 01:34:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 01:34:13 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7687869320933838 on epoch=737
05/26/2022 01:34:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/26/2022 01:34:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/26/2022 01:34:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/26/2022 01:34:24 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/26/2022 01:34:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 01:34:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:34:28 - INFO - __main__ - Printing 3 examples
05/26/2022 01:34:28 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/26/2022 01:34:28 - INFO - __main__ - ['sad']
05/26/2022 01:34:28 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/26/2022 01:34:28 - INFO - __main__ - ['sad']
05/26/2022 01:34:28 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/26/2022 01:34:28 - INFO - __main__ - ['sad']
05/26/2022 01:34:28 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:34:28 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:34:28 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7616747835497836 on epoch=749
05/26/2022 01:34:28 - INFO - __main__ - save last model!
05/26/2022 01:34:28 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 01:34:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:34:28 - INFO - __main__ - Printing 3 examples
05/26/2022 01:34:28 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/26/2022 01:34:28 - INFO - __main__ - ['sad']
05/26/2022 01:34:28 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/26/2022 01:34:28 - INFO - __main__ - ['sad']
05/26/2022 01:34:28 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/26/2022 01:34:28 - INFO - __main__ - ['sad']
05/26/2022 01:34:28 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:34:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 01:34:28 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:34:28 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 01:34:28 - INFO - __main__ - Printing 3 examples
05/26/2022 01:34:28 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 01:34:28 - INFO - __main__ - ['others']
05/26/2022 01:34:28 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 01:34:28 - INFO - __main__ - ['others']
05/26/2022 01:34:28 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 01:34:28 - INFO - __main__ - ['others']
05/26/2022 01:34:28 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:34:28 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 01:34:30 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:34:35 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 01:34:46 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 01:34:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 01:34:47 - INFO - __main__ - Starting training!
05/26/2022 01:36:27 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_21_0.3_8_predictions.txt
05/26/2022 01:36:27 - INFO - __main__ - Classification-F1 on test data: 0.1227
05/26/2022 01:36:27 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.8128360215053763, test_performance=0.12267542248533131
05/26/2022 01:36:27 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
05/26/2022 01:36:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:36:28 - INFO - __main__ - Printing 3 examples
05/26/2022 01:36:28 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/26/2022 01:36:28 - INFO - __main__ - ['sad']
05/26/2022 01:36:28 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/26/2022 01:36:28 - INFO - __main__ - ['sad']
05/26/2022 01:36:28 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/26/2022 01:36:28 - INFO - __main__ - ['sad']
05/26/2022 01:36:28 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:36:28 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:36:28 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 01:36:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:36:28 - INFO - __main__ - Printing 3 examples
05/26/2022 01:36:28 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/26/2022 01:36:28 - INFO - __main__ - ['sad']
05/26/2022 01:36:28 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/26/2022 01:36:28 - INFO - __main__ - ['sad']
05/26/2022 01:36:28 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/26/2022 01:36:28 - INFO - __main__ - ['sad']
05/26/2022 01:36:28 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:36:28 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:36:28 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 01:36:43 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 01:36:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 01:36:44 - INFO - __main__ - Starting training!
05/26/2022 01:36:47 - INFO - __main__ - Step 10 Global step 10 Train loss 3.44 on epoch=2
05/26/2022 01:36:50 - INFO - __main__ - Step 20 Global step 20 Train loss 2.16 on epoch=4
05/26/2022 01:36:52 - INFO - __main__ - Step 30 Global step 30 Train loss 1.59 on epoch=7
05/26/2022 01:36:55 - INFO - __main__ - Step 40 Global step 40 Train loss 1.18 on epoch=9
05/26/2022 01:36:58 - INFO - __main__ - Step 50 Global step 50 Train loss 1.10 on epoch=12
05/26/2022 01:36:59 - INFO - __main__ - Global step 50 Train loss 1.90 Classification-F1 0.13034188034188032 on epoch=12
05/26/2022 01:36:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13034188034188032 on epoch=12, global_step=50
05/26/2022 01:37:01 - INFO - __main__ - Step 60 Global step 60 Train loss 0.97 on epoch=14
05/26/2022 01:37:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.97 on epoch=17
05/26/2022 01:37:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=19
05/26/2022 01:37:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.91 on epoch=22
05/26/2022 01:37:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.86 on epoch=24
05/26/2022 01:37:13 - INFO - __main__ - Global step 100 Train loss 0.93 Classification-F1 0.28853640951694304 on epoch=24
05/26/2022 01:37:13 - INFO - __main__ - Saving model with best Classification-F1: 0.13034188034188032 -> 0.28853640951694304 on epoch=24, global_step=100
05/26/2022 01:37:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.84 on epoch=27
05/26/2022 01:37:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=29
05/26/2022 01:37:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=32
05/26/2022 01:37:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=34
05/26/2022 01:37:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.80 on epoch=37
05/26/2022 01:37:27 - INFO - __main__ - Global step 150 Train loss 0.85 Classification-F1 0.5393983680428693 on epoch=37
05/26/2022 01:37:27 - INFO - __main__ - Saving model with best Classification-F1: 0.28853640951694304 -> 0.5393983680428693 on epoch=37, global_step=150
05/26/2022 01:37:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.70 on epoch=39
05/26/2022 01:37:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=42
05/26/2022 01:37:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.70 on epoch=44
05/26/2022 01:37:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.68 on epoch=47
05/26/2022 01:37:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.69 on epoch=49
05/26/2022 01:37:41 - INFO - __main__ - Global step 200 Train loss 0.73 Classification-F1 0.7199269737220402 on epoch=49
05/26/2022 01:37:41 - INFO - __main__ - Saving model with best Classification-F1: 0.5393983680428693 -> 0.7199269737220402 on epoch=49, global_step=200
05/26/2022 01:37:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.71 on epoch=52
05/26/2022 01:37:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.62 on epoch=54
05/26/2022 01:37:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.69 on epoch=57
05/26/2022 01:37:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.67 on epoch=59
05/26/2022 01:37:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.61 on epoch=62
05/26/2022 01:37:56 - INFO - __main__ - Global step 250 Train loss 0.66 Classification-F1 0.7332706148629069 on epoch=62
05/26/2022 01:37:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7199269737220402 -> 0.7332706148629069 on epoch=62, global_step=250
05/26/2022 01:37:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=64
05/26/2022 01:38:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.58 on epoch=67
05/26/2022 01:38:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.57 on epoch=69
05/26/2022 01:38:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=72
05/26/2022 01:38:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=74
05/26/2022 01:38:10 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.7105614973262032 on epoch=74
05/26/2022 01:38:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=77
05/26/2022 01:38:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=79
05/26/2022 01:38:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=82
05/26/2022 01:38:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=84
05/26/2022 01:38:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=87
05/26/2022 01:38:24 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.7159498207885304 on epoch=87
05/26/2022 01:38:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=89
05/26/2022 01:38:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=92
05/26/2022 01:38:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=94
05/26/2022 01:38:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=97
05/26/2022 01:38:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=99
05/26/2022 01:38:38 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.670628078817734 on epoch=99
05/26/2022 01:38:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.33 on epoch=102
05/26/2022 01:38:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=104
05/26/2022 01:38:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.29 on epoch=107
05/26/2022 01:38:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=109
05/26/2022 01:38:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=112
05/26/2022 01:38:52 - INFO - __main__ - Global step 450 Train loss 0.33 Classification-F1 0.7666125541125541 on epoch=112
05/26/2022 01:38:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7332706148629069 -> 0.7666125541125541 on epoch=112, global_step=450
05/26/2022 01:38:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=114
05/26/2022 01:38:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=117
05/26/2022 01:39:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=119
05/26/2022 01:39:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=122
05/26/2022 01:39:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=124
05/26/2022 01:39:07 - INFO - __main__ - Global step 500 Train loss 0.29 Classification-F1 0.7148228297421845 on epoch=124
05/26/2022 01:39:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=127
05/26/2022 01:39:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=129
05/26/2022 01:39:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=132
05/26/2022 01:39:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=134
05/26/2022 01:39:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=137
05/26/2022 01:39:21 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.6787247161440709 on epoch=137
05/26/2022 01:39:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=139
05/26/2022 01:39:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=142
05/26/2022 01:39:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=144
05/26/2022 01:39:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=147
05/26/2022 01:39:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=149
05/26/2022 01:39:35 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.6869824016563146 on epoch=149
05/26/2022 01:39:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=152
05/26/2022 01:39:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=154
05/26/2022 01:39:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=157
05/26/2022 01:39:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=159
05/26/2022 01:39:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=162
05/26/2022 01:39:49 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.747259118701244 on epoch=162
05/26/2022 01:39:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=164
05/26/2022 01:39:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=167
05/26/2022 01:39:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=169
05/26/2022 01:40:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=172
05/26/2022 01:40:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=174
05/26/2022 01:40:03 - INFO - __main__ - Global step 700 Train loss 0.17 Classification-F1 0.7011938511938511 on epoch=174
05/26/2022 01:40:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=177
05/26/2022 01:40:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=179
05/26/2022 01:40:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=182
05/26/2022 01:40:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=184
05/26/2022 01:40:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=187
05/26/2022 01:40:18 - INFO - __main__ - Global step 750 Train loss 0.17 Classification-F1 0.6662148730459926 on epoch=187
05/26/2022 01:40:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=189
05/26/2022 01:40:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=192
05/26/2022 01:40:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=194
05/26/2022 01:40:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
05/26/2022 01:40:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=199
05/26/2022 01:40:32 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.7816940985977489 on epoch=199
05/26/2022 01:40:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7666125541125541 -> 0.7816940985977489 on epoch=199, global_step=800
05/26/2022 01:40:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=202
05/26/2022 01:40:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.13 on epoch=204
05/26/2022 01:40:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=207
05/26/2022 01:40:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=209
05/26/2022 01:40:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=212
05/26/2022 01:40:47 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.7222222222222222 on epoch=212
05/26/2022 01:40:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
05/26/2022 01:40:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=217
05/26/2022 01:40:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=219
05/26/2022 01:40:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=222
05/26/2022 01:41:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=224
05/26/2022 01:41:01 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.745384517443341 on epoch=224
05/26/2022 01:41:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=227
05/26/2022 01:41:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=229
05/26/2022 01:41:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=232
05/26/2022 01:41:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=234
05/26/2022 01:41:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=237
05/26/2022 01:41:16 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.712566484083512 on epoch=237
05/26/2022 01:41:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
05/26/2022 01:41:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
05/26/2022 01:41:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
05/26/2022 01:41:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
05/26/2022 01:41:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
05/26/2022 01:41:30 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.7115452289645837 on epoch=249
05/26/2022 01:41:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=252
05/26/2022 01:41:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
05/26/2022 01:41:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=257
05/26/2022 01:41:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=259
05/26/2022 01:41:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=262
05/26/2022 01:41:45 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.7312920661995205 on epoch=262
05/26/2022 01:41:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=264
05/26/2022 01:41:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=267
05/26/2022 01:41:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
05/26/2022 01:41:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=272
05/26/2022 01:41:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=274
05/26/2022 01:41:59 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.7017876129718235 on epoch=274
05/26/2022 01:42:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
05/26/2022 01:42:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=279
05/26/2022 01:42:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/26/2022 01:42:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
05/26/2022 01:42:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
05/26/2022 01:42:14 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.7493045494059692 on epoch=287
05/26/2022 01:42:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
05/26/2022 01:42:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
05/26/2022 01:42:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
05/26/2022 01:42:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=297
05/26/2022 01:42:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=299
05/26/2022 01:42:28 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.7375605984301637 on epoch=299
05/26/2022 01:42:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
05/26/2022 01:42:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/26/2022 01:42:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/26/2022 01:42:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/26/2022 01:42:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/26/2022 01:42:43 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.7311688311688311 on epoch=312
05/26/2022 01:42:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.09 on epoch=314
05/26/2022 01:42:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
05/26/2022 01:42:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=319
05/26/2022 01:42:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/26/2022 01:42:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/26/2022 01:42:58 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7311688311688311 on epoch=324
05/26/2022 01:43:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
05/26/2022 01:43:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
05/26/2022 01:43:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
05/26/2022 01:43:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=334
05/26/2022 01:43:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
05/26/2022 01:43:12 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.7176183083203955 on epoch=337
05/26/2022 01:43:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
05/26/2022 01:43:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
05/26/2022 01:43:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/26/2022 01:43:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/26/2022 01:43:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
05/26/2022 01:43:27 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.7459893048128342 on epoch=349
05/26/2022 01:43:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=352
05/26/2022 01:43:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/26/2022 01:43:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/26/2022 01:43:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/26/2022 01:43:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/26/2022 01:43:42 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.6981004693897933 on epoch=362
05/26/2022 01:43:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
05/26/2022 01:43:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/26/2022 01:43:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/26/2022 01:43:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/26/2022 01:43:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/26/2022 01:43:57 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.7090728715728717 on epoch=374
05/26/2022 01:43:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=377
05/26/2022 01:44:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/26/2022 01:44:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/26/2022 01:44:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/26/2022 01:44:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/26/2022 01:44:12 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.709418657788223 on epoch=387
05/26/2022 01:44:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/26/2022 01:44:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/26/2022 01:44:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/26/2022 01:44:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/26/2022 01:44:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/26/2022 01:44:26 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.732308949220714 on epoch=399
05/26/2022 01:44:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/26/2022 01:44:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
05/26/2022 01:44:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/26/2022 01:44:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/26/2022 01:44:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/26/2022 01:44:41 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7461978381096028 on epoch=412
05/26/2022 01:44:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
05/26/2022 01:44:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/26/2022 01:44:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=419
05/26/2022 01:44:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
05/26/2022 01:44:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/26/2022 01:44:56 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.7459893048128342 on epoch=424
05/26/2022 01:44:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/26/2022 01:45:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/26/2022 01:45:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
05/26/2022 01:45:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
05/26/2022 01:45:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/26/2022 01:45:11 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.7241436100131753 on epoch=437
05/26/2022 01:45:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=439
05/26/2022 01:45:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=442
05/26/2022 01:45:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/26/2022 01:45:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/26/2022 01:45:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/26/2022 01:45:26 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.7625906120023767 on epoch=449
05/26/2022 01:45:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
05/26/2022 01:45:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/26/2022 01:45:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/26/2022 01:45:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/26/2022 01:45:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/26/2022 01:45:41 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.7625906120023767 on epoch=462
05/26/2022 01:45:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/26/2022 01:45:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/26/2022 01:45:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/26/2022 01:45:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=472
05/26/2022 01:45:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
05/26/2022 01:45:55 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7165915915915917 on epoch=474
05/26/2022 01:45:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/26/2022 01:46:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/26/2022 01:46:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/26/2022 01:46:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/26/2022 01:46:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/26/2022 01:46:10 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7625906120023767 on epoch=487
05/26/2022 01:46:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/26/2022 01:46:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/26/2022 01:46:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/26/2022 01:46:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/26/2022 01:46:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/26/2022 01:46:25 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7478656597774244 on epoch=499
05/26/2022 01:46:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/26/2022 01:46:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/26/2022 01:46:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/26/2022 01:46:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/26/2022 01:46:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/26/2022 01:46:40 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.707615245904166 on epoch=512
05/26/2022 01:46:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/26/2022 01:46:45 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/26/2022 01:46:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/26/2022 01:46:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/26/2022 01:46:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/26/2022 01:46:55 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7304292929292928 on epoch=524
05/26/2022 01:46:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/26/2022 01:47:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/26/2022 01:47:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/26/2022 01:47:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/26/2022 01:47:08 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/26/2022 01:47:09 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7443693693693694 on epoch=537
05/26/2022 01:47:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/26/2022 01:47:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/26/2022 01:47:17 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=544
05/26/2022 01:47:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/26/2022 01:47:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/26/2022 01:47:24 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.73349464773269 on epoch=549
05/26/2022 01:47:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/26/2022 01:47:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/26/2022 01:47:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/26/2022 01:47:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/26/2022 01:47:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/26/2022 01:47:39 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6887285012285012 on epoch=562
05/26/2022 01:47:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
05/26/2022 01:47:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/26/2022 01:47:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/26/2022 01:47:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/26/2022 01:47:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/26/2022 01:47:54 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6887285012285012 on epoch=574
05/26/2022 01:47:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/26/2022 01:47:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/26/2022 01:48:02 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/26/2022 01:48:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/26/2022 01:48:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/26/2022 01:48:08 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7290701415701415 on epoch=587
05/26/2022 01:48:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/26/2022 01:48:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/26/2022 01:48:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/26/2022 01:48:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/26/2022 01:48:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/26/2022 01:48:23 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7157467532467532 on epoch=599
05/26/2022 01:48:26 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/26/2022 01:48:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/26/2022 01:48:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/26/2022 01:48:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/26/2022 01:48:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/26/2022 01:48:38 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6983722976370036 on epoch=612
05/26/2022 01:48:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/26/2022 01:48:44 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/26/2022 01:48:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/26/2022 01:48:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/26/2022 01:48:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/26/2022 01:48:53 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6937698412698413 on epoch=624
05/26/2022 01:48:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/26/2022 01:48:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/26/2022 01:49:01 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/26/2022 01:49:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/26/2022 01:49:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/26/2022 01:49:08 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7454453441295548 on epoch=637
05/26/2022 01:49:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/26/2022 01:49:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/26/2022 01:49:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/26/2022 01:49:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 01:49:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/26/2022 01:49:23 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7227566198103956 on epoch=649
05/26/2022 01:49:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=652
05/26/2022 01:49:28 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/26/2022 01:49:31 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/26/2022 01:49:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 01:49:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/26/2022 01:49:37 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.7443693693693694 on epoch=662
05/26/2022 01:49:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/26/2022 01:49:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/26/2022 01:49:45 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/26/2022 01:49:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/26/2022 01:49:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/26/2022 01:49:52 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.741379173290938 on epoch=674
05/26/2022 01:49:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/26/2022 01:49:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/26/2022 01:50:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/26/2022 01:50:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/26/2022 01:50:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/26/2022 01:50:06 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.745524463703977 on epoch=687
05/26/2022 01:50:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/26/2022 01:50:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/26/2022 01:50:14 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/26/2022 01:50:17 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/26/2022 01:50:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/26/2022 01:50:21 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7309299895506792 on epoch=699
05/26/2022 01:50:24 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/26/2022 01:50:26 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 01:50:29 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 01:50:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=709
05/26/2022 01:50:34 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=712
05/26/2022 01:50:35 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6967592592592593 on epoch=712
05/26/2022 01:50:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/26/2022 01:50:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/26/2022 01:50:43 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 01:50:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/26/2022 01:50:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/26/2022 01:50:50 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7140108335354516 on epoch=724
05/26/2022 01:50:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/26/2022 01:50:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 01:50:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=732
05/26/2022 01:51:00 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/26/2022 01:51:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 01:51:04 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7625906120023767 on epoch=737
05/26/2022 01:51:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/26/2022 01:51:10 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/26/2022 01:51:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/26/2022 01:51:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/26/2022 01:51:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 01:51:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:51:19 - INFO - __main__ - Printing 3 examples
05/26/2022 01:51:19 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/26/2022 01:51:19 - INFO - __main__ - ['happy']
05/26/2022 01:51:19 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/26/2022 01:51:19 - INFO - __main__ - ['happy']
05/26/2022 01:51:19 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/26/2022 01:51:19 - INFO - __main__ - ['happy']
05/26/2022 01:51:19 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:51:19 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:51:19 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7339969834087481 on epoch=749
05/26/2022 01:51:19 - INFO - __main__ - save last model!
05/26/2022 01:51:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 01:51:19 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 01:51:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:51:19 - INFO - __main__ - Printing 3 examples
05/26/2022 01:51:19 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/26/2022 01:51:19 - INFO - __main__ - ['happy']
05/26/2022 01:51:19 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/26/2022 01:51:19 - INFO - __main__ - ['happy']
05/26/2022 01:51:19 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/26/2022 01:51:19 - INFO - __main__ - ['happy']
05/26/2022 01:51:19 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:51:19 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 01:51:19 - INFO - __main__ - Printing 3 examples
05/26/2022 01:51:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 01:51:19 - INFO - __main__ - ['others']
05/26/2022 01:51:19 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 01:51:19 - INFO - __main__ - ['others']
05/26/2022 01:51:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 01:51:19 - INFO - __main__ - ['others']
05/26/2022 01:51:19 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:51:19 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:51:19 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 01:51:21 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:51:26 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 01:51:37 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 01:51:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 01:51:38 - INFO - __main__ - Starting training!
05/26/2022 01:53:19 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_21_0.2_8_predictions.txt
05/26/2022 01:53:19 - INFO - __main__ - Classification-F1 on test data: 0.1072
05/26/2022 01:53:19 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.7816940985977489, test_performance=0.1072208163071027
05/26/2022 01:53:19 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
05/26/2022 01:53:20 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:53:20 - INFO - __main__ - Printing 3 examples
05/26/2022 01:53:20 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/26/2022 01:53:20 - INFO - __main__ - ['happy']
05/26/2022 01:53:20 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/26/2022 01:53:20 - INFO - __main__ - ['happy']
05/26/2022 01:53:20 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/26/2022 01:53:20 - INFO - __main__ - ['happy']
05/26/2022 01:53:20 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:53:20 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:53:20 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 01:53:20 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 01:53:20 - INFO - __main__ - Printing 3 examples
05/26/2022 01:53:20 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/26/2022 01:53:20 - INFO - __main__ - ['happy']
05/26/2022 01:53:20 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/26/2022 01:53:20 - INFO - __main__ - ['happy']
05/26/2022 01:53:20 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/26/2022 01:53:20 - INFO - __main__ - ['happy']
05/26/2022 01:53:20 - INFO - __main__ - Tokenizing Input ...
05/26/2022 01:53:20 - INFO - __main__ - Tokenizing Output ...
05/26/2022 01:53:20 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 01:53:39 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 01:53:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 01:53:40 - INFO - __main__ - Starting training!
05/26/2022 01:53:43 - INFO - __main__ - Step 10 Global step 10 Train loss 2.53 on epoch=2
05/26/2022 01:53:46 - INFO - __main__ - Step 20 Global step 20 Train loss 1.30 on epoch=4
05/26/2022 01:53:48 - INFO - __main__ - Step 30 Global step 30 Train loss 1.02 on epoch=7
05/26/2022 01:53:51 - INFO - __main__ - Step 40 Global step 40 Train loss 1.05 on epoch=9
05/26/2022 01:53:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.93 on epoch=12
05/26/2022 01:53:55 - INFO - __main__ - Global step 50 Train loss 1.37 Classification-F1 0.38970033296337403 on epoch=12
05/26/2022 01:53:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.38970033296337403 on epoch=12, global_step=50
05/26/2022 01:53:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.90 on epoch=14
05/26/2022 01:54:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.91 on epoch=17
05/26/2022 01:54:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.87 on epoch=19
05/26/2022 01:54:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.74 on epoch=22
05/26/2022 01:54:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=24
05/26/2022 01:54:09 - INFO - __main__ - Global step 100 Train loss 0.85 Classification-F1 0.4533791866028708 on epoch=24
05/26/2022 01:54:09 - INFO - __main__ - Saving model with best Classification-F1: 0.38970033296337403 -> 0.4533791866028708 on epoch=24, global_step=100
05/26/2022 01:54:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.77 on epoch=27
05/26/2022 01:54:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.79 on epoch=29
05/26/2022 01:54:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=32
05/26/2022 01:54:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.70 on epoch=34
05/26/2022 01:54:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.65 on epoch=37
05/26/2022 01:54:23 - INFO - __main__ - Global step 150 Train loss 0.70 Classification-F1 0.47769472856018885 on epoch=37
05/26/2022 01:54:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4533791866028708 -> 0.47769472856018885 on epoch=37, global_step=150
05/26/2022 01:54:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.68 on epoch=39
05/26/2022 01:54:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.61 on epoch=42
05/26/2022 01:54:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.71 on epoch=44
05/26/2022 01:54:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.63 on epoch=47
05/26/2022 01:54:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=49
05/26/2022 01:54:37 - INFO - __main__ - Global step 200 Train loss 0.64 Classification-F1 0.5677083333333334 on epoch=49
05/26/2022 01:54:37 - INFO - __main__ - Saving model with best Classification-F1: 0.47769472856018885 -> 0.5677083333333334 on epoch=49, global_step=200
05/26/2022 01:54:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.59 on epoch=52
05/26/2022 01:54:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.63 on epoch=54
05/26/2022 01:54:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=57
05/26/2022 01:54:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=59
05/26/2022 01:54:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=62
05/26/2022 01:54:51 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.6351159951159951 on epoch=62
05/26/2022 01:54:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5677083333333334 -> 0.6351159951159951 on epoch=62, global_step=250
05/26/2022 01:54:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.56 on epoch=64
05/26/2022 01:54:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=67
05/26/2022 01:54:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=69
05/26/2022 01:55:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.37 on epoch=72
05/26/2022 01:55:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=74
05/26/2022 01:55:05 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.7228448075222269 on epoch=74
05/26/2022 01:55:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6351159951159951 -> 0.7228448075222269 on epoch=74, global_step=300
05/26/2022 01:55:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.30 on epoch=77
05/26/2022 01:55:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=79
05/26/2022 01:55:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=82
05/26/2022 01:55:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=84
05/26/2022 01:55:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=87
05/26/2022 01:55:19 - INFO - __main__ - Global step 350 Train loss 0.31 Classification-F1 0.6887681159420289 on epoch=87
05/26/2022 01:55:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=89
05/26/2022 01:55:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=92
05/26/2022 01:55:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=94
05/26/2022 01:55:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=97
05/26/2022 01:55:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=99
05/26/2022 01:55:33 - INFO - __main__ - Global step 400 Train loss 0.30 Classification-F1 0.6977486559139785 on epoch=99
05/26/2022 01:55:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=102
05/26/2022 01:55:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=104
05/26/2022 01:55:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.14 on epoch=107
05/26/2022 01:55:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.13 on epoch=109
05/26/2022 01:55:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.14 on epoch=112
05/26/2022 01:55:47 - INFO - __main__ - Global step 450 Train loss 0.17 Classification-F1 0.6871564393303523 on epoch=112
05/26/2022 01:55:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=114
05/26/2022 01:55:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=117
05/26/2022 01:55:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.15 on epoch=119
05/26/2022 01:55:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.14 on epoch=122
05/26/2022 01:56:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=124
05/26/2022 01:56:01 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.733165322580645 on epoch=124
05/26/2022 01:56:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7228448075222269 -> 0.733165322580645 on epoch=124, global_step=500
05/26/2022 01:56:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=127
05/26/2022 01:56:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=129
05/26/2022 01:56:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=132
05/26/2022 01:56:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=134
05/26/2022 01:56:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=137
05/26/2022 01:56:15 - INFO - __main__ - Global step 550 Train loss 0.12 Classification-F1 0.7153044871794872 on epoch=137
05/26/2022 01:56:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.13 on epoch=139
05/26/2022 01:56:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.04 on epoch=142
05/26/2022 01:56:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.10 on epoch=144
05/26/2022 01:56:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=147
05/26/2022 01:56:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=149
05/26/2022 01:56:29 - INFO - __main__ - Global step 600 Train loss 0.07 Classification-F1 0.637973137973138 on epoch=149
05/26/2022 01:56:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.07 on epoch=152
05/26/2022 01:56:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=154
05/26/2022 01:56:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=157
05/26/2022 01:56:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=159
05/26/2022 01:56:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=162
05/26/2022 01:56:44 - INFO - __main__ - Global step 650 Train loss 0.07 Classification-F1 0.7035119969040248 on epoch=162
05/26/2022 01:56:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=164
05/26/2022 01:56:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=167
05/26/2022 01:56:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=169
05/26/2022 01:56:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=172
05/26/2022 01:56:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=174
05/26/2022 01:56:58 - INFO - __main__ - Global step 700 Train loss 0.06 Classification-F1 0.7030284749034749 on epoch=174
05/26/2022 01:57:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=177
05/26/2022 01:57:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=179
05/26/2022 01:57:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=182
05/26/2022 01:57:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=184
05/26/2022 01:57:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=187
05/26/2022 01:57:12 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.7313886776802377 on epoch=187
05/26/2022 01:57:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=189
05/26/2022 01:57:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=192
05/26/2022 01:57:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=194
05/26/2022 01:57:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=197
05/26/2022 01:57:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=199
05/26/2022 01:57:26 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.685039689857134 on epoch=199
05/26/2022 01:57:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=202
05/26/2022 01:57:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=204
05/26/2022 01:57:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
05/26/2022 01:57:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=209
05/26/2022 01:57:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=212
05/26/2022 01:57:41 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.7301121794871795 on epoch=212
05/26/2022 01:57:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=214
05/26/2022 01:57:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=217
05/26/2022 01:57:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=219
05/26/2022 01:57:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=222
05/26/2022 01:57:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=224
05/26/2022 01:57:55 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.7314863445378151 on epoch=224
05/26/2022 01:57:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=227
05/26/2022 01:58:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=229
05/26/2022 01:58:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=232
05/26/2022 01:58:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
05/26/2022 01:58:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
05/26/2022 01:58:09 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.7390512125902994 on epoch=237
05/26/2022 01:58:09 - INFO - __main__ - Saving model with best Classification-F1: 0.733165322580645 -> 0.7390512125902994 on epoch=237, global_step=950
05/26/2022 01:58:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=239
05/26/2022 01:58:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=242
05/26/2022 01:58:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=244
05/26/2022 01:58:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=247
05/26/2022 01:58:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=249
05/26/2022 01:58:23 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.7443066801619433 on epoch=249
05/26/2022 01:58:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7390512125902994 -> 0.7443066801619433 on epoch=249, global_step=1000
05/26/2022 01:58:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=252
05/26/2022 01:58:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
05/26/2022 01:58:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
05/26/2022 01:58:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/26/2022 01:58:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
05/26/2022 01:58:38 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.6597930201565884 on epoch=262
05/26/2022 01:58:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=264
05/26/2022 01:58:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
05/26/2022 01:58:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
05/26/2022 01:58:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=272
05/26/2022 01:58:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
05/26/2022 01:58:52 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.704602231384914 on epoch=274
05/26/2022 01:58:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=277
05/26/2022 01:58:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/26/2022 01:59:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=282
05/26/2022 01:59:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=284
05/26/2022 01:59:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
05/26/2022 01:59:06 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.6818376068376067 on epoch=287
05/26/2022 01:59:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/26/2022 01:59:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/26/2022 01:59:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
05/26/2022 01:59:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
05/26/2022 01:59:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
05/26/2022 01:59:21 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.7070488428669454 on epoch=299
05/26/2022 01:59:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/26/2022 01:59:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
05/26/2022 01:59:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
05/26/2022 01:59:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
05/26/2022 01:59:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
05/26/2022 01:59:35 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.717770308924485 on epoch=312
05/26/2022 01:59:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/26/2022 01:59:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/26/2022 01:59:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
05/26/2022 01:59:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=322
05/26/2022 01:59:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/26/2022 01:59:49 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.7031499202551835 on epoch=324
05/26/2022 01:59:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
05/26/2022 01:59:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
05/26/2022 01:59:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/26/2022 02:00:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/26/2022 02:00:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=337
05/26/2022 02:00:03 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.737296494355318 on epoch=337
05/26/2022 02:00:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/26/2022 02:00:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/26/2022 02:00:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/26/2022 02:00:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
05/26/2022 02:00:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/26/2022 02:00:18 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.695133245133245 on epoch=349
05/26/2022 02:00:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=352
05/26/2022 02:00:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
05/26/2022 02:00:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
05/26/2022 02:00:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/26/2022 02:00:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/26/2022 02:00:32 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.7360850556438793 on epoch=362
05/26/2022 02:00:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/26/2022 02:00:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/26/2022 02:00:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/26/2022 02:00:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/26/2022 02:00:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/26/2022 02:00:46 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.7026596402937051 on epoch=374
05/26/2022 02:00:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/26/2022 02:00:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
05/26/2022 02:00:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/26/2022 02:00:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
05/26/2022 02:00:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
05/26/2022 02:01:01 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7517936117936118 on epoch=387
05/26/2022 02:01:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7443066801619433 -> 0.7517936117936118 on epoch=387, global_step=1550
05/26/2022 02:01:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
05/26/2022 02:01:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/26/2022 02:01:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
05/26/2022 02:01:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
05/26/2022 02:01:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/26/2022 02:01:15 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.6806628097628938 on epoch=399
05/26/2022 02:01:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/26/2022 02:01:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/26/2022 02:01:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/26/2022 02:01:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
05/26/2022 02:01:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
05/26/2022 02:01:29 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.660318515704154 on epoch=412
05/26/2022 02:01:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/26/2022 02:01:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/26/2022 02:01:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/26/2022 02:01:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/26/2022 02:01:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/26/2022 02:01:44 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7022508741258741 on epoch=424
05/26/2022 02:01:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/26/2022 02:01:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
05/26/2022 02:01:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/26/2022 02:01:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
05/26/2022 02:01:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/26/2022 02:01:58 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6741823629416418 on epoch=437
05/26/2022 02:02:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/26/2022 02:02:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/26/2022 02:02:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/26/2022 02:02:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=447
05/26/2022 02:02:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/26/2022 02:02:12 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7647751951422697 on epoch=449
05/26/2022 02:02:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7517936117936118 -> 0.7647751951422697 on epoch=449, global_step=1800
05/26/2022 02:02:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/26/2022 02:02:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/26/2022 02:02:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/26/2022 02:02:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/26/2022 02:02:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/26/2022 02:02:27 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6651984126984127 on epoch=462
05/26/2022 02:02:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/26/2022 02:02:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/26/2022 02:02:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/26/2022 02:02:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/26/2022 02:02:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/26/2022 02:02:41 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.7084873949579833 on epoch=474
05/26/2022 02:02:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/26/2022 02:02:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/26/2022 02:02:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/26/2022 02:02:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
05/26/2022 02:02:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/26/2022 02:02:55 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7324110734037205 on epoch=487
05/26/2022 02:02:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/26/2022 02:03:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/26/2022 02:03:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/26/2022 02:03:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/26/2022 02:03:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/26/2022 02:03:09 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.709469696969697 on epoch=499
05/26/2022 02:03:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/26/2022 02:03:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/26/2022 02:03:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
05/26/2022 02:03:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/26/2022 02:03:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/26/2022 02:03:24 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6958017077798861 on epoch=512
05/26/2022 02:03:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/26/2022 02:03:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/26/2022 02:03:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/26/2022 02:03:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
05/26/2022 02:03:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/26/2022 02:03:38 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6645098039215687 on epoch=524
05/26/2022 02:03:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/26/2022 02:03:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/26/2022 02:03:46 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/26/2022 02:03:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/26/2022 02:03:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
05/26/2022 02:03:52 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6831002331002332 on epoch=537
05/26/2022 02:03:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/26/2022 02:03:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
05/26/2022 02:04:00 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/26/2022 02:04:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/26/2022 02:04:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/26/2022 02:04:07 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6725964748718077 on epoch=549
05/26/2022 02:04:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
05/26/2022 02:04:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/26/2022 02:04:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.10 on epoch=557
05/26/2022 02:04:17 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/26/2022 02:04:20 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/26/2022 02:04:21 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.7027914764824174 on epoch=562
05/26/2022 02:04:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/26/2022 02:04:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
05/26/2022 02:04:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=569
05/26/2022 02:04:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/26/2022 02:04:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/26/2022 02:04:35 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6928009575923393 on epoch=574
05/26/2022 02:04:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/26/2022 02:04:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/26/2022 02:04:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/26/2022 02:04:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=584
05/26/2022 02:04:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/26/2022 02:04:49 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6747016999019286 on epoch=587
05/26/2022 02:04:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/26/2022 02:04:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/26/2022 02:04:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/26/2022 02:05:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/26/2022 02:05:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/26/2022 02:05:04 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.7270315551565552 on epoch=599
05/26/2022 02:05:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=602
05/26/2022 02:05:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/26/2022 02:05:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/26/2022 02:05:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/26/2022 02:05:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/26/2022 02:05:18 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7096551765669413 on epoch=612
05/26/2022 02:05:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/26/2022 02:05:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
05/26/2022 02:05:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/26/2022 02:05:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/26/2022 02:05:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/26/2022 02:05:33 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7231442577030813 on epoch=624
05/26/2022 02:05:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/26/2022 02:05:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/26/2022 02:05:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/26/2022 02:05:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/26/2022 02:05:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/26/2022 02:05:47 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.7179347826086956 on epoch=637
05/26/2022 02:05:50 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/26/2022 02:05:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/26/2022 02:05:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/26/2022 02:05:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 02:06:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/26/2022 02:06:01 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7166827953272966 on epoch=649
05/26/2022 02:06:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/26/2022 02:06:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/26/2022 02:06:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/26/2022 02:06:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=659
05/26/2022 02:06:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/26/2022 02:06:16 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7311008863707746 on epoch=662
05/26/2022 02:06:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/26/2022 02:06:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/26/2022 02:06:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/26/2022 02:06:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/26/2022 02:06:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/26/2022 02:06:30 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6893092414831545 on epoch=674
05/26/2022 02:06:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/26/2022 02:06:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=679
05/26/2022 02:06:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/26/2022 02:06:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=684
05/26/2022 02:06:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/26/2022 02:06:44 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.7612230906348554 on epoch=687
05/26/2022 02:06:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/26/2022 02:06:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/26/2022 02:06:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/26/2022 02:06:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/26/2022 02:06:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/26/2022 02:06:59 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6872529644268774 on epoch=699
05/26/2022 02:07:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/26/2022 02:07:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 02:07:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 02:07:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/26/2022 02:07:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/26/2022 02:07:13 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6887663236988575 on epoch=712
05/26/2022 02:07:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/26/2022 02:07:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/26/2022 02:07:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 02:07:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/26/2022 02:07:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/26/2022 02:07:27 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7311008863707746 on epoch=724
05/26/2022 02:07:30 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/26/2022 02:07:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 02:07:35 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/26/2022 02:07:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/26/2022 02:07:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 02:07:42 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7166818688557819 on epoch=737
05/26/2022 02:07:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/26/2022 02:07:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/26/2022 02:07:50 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/26/2022 02:07:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/26/2022 02:07:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 02:07:56 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7324198942712266 on epoch=749
05/26/2022 02:07:56 - INFO - __main__ - save last model!
05/26/2022 02:07:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 02:07:56 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 02:07:56 - INFO - __main__ - Printing 3 examples
05/26/2022 02:07:56 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 02:07:56 - INFO - __main__ - ['others']
05/26/2022 02:07:56 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 02:07:56 - INFO - __main__ - ['others']
05/26/2022 02:07:56 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 02:07:56 - INFO - __main__ - ['others']
05/26/2022 02:07:56 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:07:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:07:56 - INFO - __main__ - Printing 3 examples
05/26/2022 02:07:56 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/26/2022 02:07:56 - INFO - __main__ - ['happy']
05/26/2022 02:07:56 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/26/2022 02:07:56 - INFO - __main__ - ['happy']
05/26/2022 02:07:56 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/26/2022 02:07:56 - INFO - __main__ - ['happy']
05/26/2022 02:07:56 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:07:56 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:07:56 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 02:07:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:07:56 - INFO - __main__ - Printing 3 examples
05/26/2022 02:07:56 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/26/2022 02:07:56 - INFO - __main__ - ['happy']
05/26/2022 02:07:56 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/26/2022 02:07:56 - INFO - __main__ - ['happy']
05/26/2022 02:07:56 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/26/2022 02:07:56 - INFO - __main__ - ['happy']
05/26/2022 02:07:56 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:07:56 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:07:56 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 02:07:58 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:08:04 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 02:08:15 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 02:08:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 02:08:16 - INFO - __main__ - Starting training!
05/26/2022 02:09:53 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_42_0.5_8_predictions.txt
05/26/2022 02:09:54 - INFO - __main__ - Classification-F1 on test data: 0.1425
05/26/2022 02:09:54 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.7647751951422697, test_performance=0.14249583493489146
05/26/2022 02:09:54 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
05/26/2022 02:09:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:09:55 - INFO - __main__ - Printing 3 examples
05/26/2022 02:09:55 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/26/2022 02:09:55 - INFO - __main__ - ['happy']
05/26/2022 02:09:55 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/26/2022 02:09:55 - INFO - __main__ - ['happy']
05/26/2022 02:09:55 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/26/2022 02:09:55 - INFO - __main__ - ['happy']
05/26/2022 02:09:55 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:09:55 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:09:55 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 02:09:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:09:55 - INFO - __main__ - Printing 3 examples
05/26/2022 02:09:55 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/26/2022 02:09:55 - INFO - __main__ - ['happy']
05/26/2022 02:09:55 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/26/2022 02:09:55 - INFO - __main__ - ['happy']
05/26/2022 02:09:55 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/26/2022 02:09:55 - INFO - __main__ - ['happy']
05/26/2022 02:09:55 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:09:55 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:09:55 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 02:10:14 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 02:10:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 02:10:15 - INFO - __main__ - Starting training!
05/26/2022 02:10:18 - INFO - __main__ - Step 10 Global step 10 Train loss 2.73 on epoch=2
05/26/2022 02:10:21 - INFO - __main__ - Step 20 Global step 20 Train loss 1.39 on epoch=4
05/26/2022 02:10:23 - INFO - __main__ - Step 30 Global step 30 Train loss 1.12 on epoch=7
05/26/2022 02:10:26 - INFO - __main__ - Step 40 Global step 40 Train loss 1.11 on epoch=9
05/26/2022 02:10:28 - INFO - __main__ - Step 50 Global step 50 Train loss 0.95 on epoch=12
05/26/2022 02:10:29 - INFO - __main__ - Global step 50 Train loss 1.46 Classification-F1 0.1 on epoch=12
05/26/2022 02:10:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/26/2022 02:10:32 - INFO - __main__ - Step 60 Global step 60 Train loss 0.89 on epoch=14
05/26/2022 02:10:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.87 on epoch=17
05/26/2022 02:10:37 - INFO - __main__ - Step 80 Global step 80 Train loss 0.87 on epoch=19
05/26/2022 02:10:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.81 on epoch=22
05/26/2022 02:10:42 - INFO - __main__ - Step 100 Global step 100 Train loss 0.89 on epoch=24
05/26/2022 02:10:43 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.49985307561708237 on epoch=24
05/26/2022 02:10:43 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.49985307561708237 on epoch=24, global_step=100
05/26/2022 02:10:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.75 on epoch=27
05/26/2022 02:10:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.73 on epoch=29
05/26/2022 02:10:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.66 on epoch=32
05/26/2022 02:10:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.73 on epoch=34
05/26/2022 02:10:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.65 on epoch=37
05/26/2022 02:10:58 - INFO - __main__ - Global step 150 Train loss 0.70 Classification-F1 0.5481270744428639 on epoch=37
05/26/2022 02:10:58 - INFO - __main__ - Saving model with best Classification-F1: 0.49985307561708237 -> 0.5481270744428639 on epoch=37, global_step=150
05/26/2022 02:11:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.67 on epoch=39
05/26/2022 02:11:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.69 on epoch=42
05/26/2022 02:11:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.59 on epoch=44
05/26/2022 02:11:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=47
05/26/2022 02:11:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=49
05/26/2022 02:11:12 - INFO - __main__ - Global step 200 Train loss 0.62 Classification-F1 0.6239393463230672 on epoch=49
05/26/2022 02:11:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5481270744428639 -> 0.6239393463230672 on epoch=49, global_step=200
05/26/2022 02:11:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.56 on epoch=52
05/26/2022 02:11:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=54
05/26/2022 02:11:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=57
05/26/2022 02:11:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=59
05/26/2022 02:11:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=62
05/26/2022 02:11:26 - INFO - __main__ - Global step 250 Train loss 0.52 Classification-F1 0.6455829054666264 on epoch=62
05/26/2022 02:11:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6239393463230672 -> 0.6455829054666264 on epoch=62, global_step=250
05/26/2022 02:11:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=64
05/26/2022 02:11:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=67
05/26/2022 02:11:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=69
05/26/2022 02:11:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=72
05/26/2022 02:11:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=74
05/26/2022 02:11:40 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.7215652337356192 on epoch=74
05/26/2022 02:11:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6455829054666264 -> 0.7215652337356192 on epoch=74, global_step=300
05/26/2022 02:11:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=77
05/26/2022 02:11:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=79
05/26/2022 02:11:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=82
05/26/2022 02:11:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=84
05/26/2022 02:11:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=87
05/26/2022 02:11:54 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.5864035087719298 on epoch=87
05/26/2022 02:11:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=89
05/26/2022 02:11:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=92
05/26/2022 02:12:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=94
05/26/2022 02:12:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=97
05/26/2022 02:12:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=99
05/26/2022 02:12:08 - INFO - __main__ - Global step 400 Train loss 0.28 Classification-F1 0.5986453201970444 on epoch=99
05/26/2022 02:12:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=102
05/26/2022 02:12:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.17 on epoch=104
05/26/2022 02:12:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
05/26/2022 02:12:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=109
05/26/2022 02:12:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=112
05/26/2022 02:12:22 - INFO - __main__ - Global step 450 Train loss 0.20 Classification-F1 0.6115216201423097 on epoch=112
05/26/2022 02:12:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=114
05/26/2022 02:12:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.18 on epoch=117
05/26/2022 02:12:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=119
05/26/2022 02:12:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.15 on epoch=122
05/26/2022 02:12:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=124
05/26/2022 02:12:36 - INFO - __main__ - Global step 500 Train loss 0.16 Classification-F1 0.655952380952381 on epoch=124
05/26/2022 02:12:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=127
05/26/2022 02:12:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=129
05/26/2022 02:12:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=132
05/26/2022 02:12:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=134
05/26/2022 02:12:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=137
05/26/2022 02:12:50 - INFO - __main__ - Global step 550 Train loss 0.11 Classification-F1 0.7087017231134879 on epoch=137
05/26/2022 02:12:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=139
05/26/2022 02:12:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=142
05/26/2022 02:12:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.08 on epoch=144
05/26/2022 02:13:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=147
05/26/2022 02:13:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=149
05/26/2022 02:13:04 - INFO - __main__ - Global step 600 Train loss 0.09 Classification-F1 0.5809178743961353 on epoch=149
05/26/2022 02:13:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=152
05/26/2022 02:13:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=154
05/26/2022 02:13:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.06 on epoch=157
05/26/2022 02:13:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=159
05/26/2022 02:13:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=162
05/26/2022 02:13:18 - INFO - __main__ - Global step 650 Train loss 0.10 Classification-F1 0.6689776646673199 on epoch=162
05/26/2022 02:13:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=164
05/26/2022 02:13:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=167
05/26/2022 02:13:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=169
05/26/2022 02:13:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=172
05/26/2022 02:13:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=174
05/26/2022 02:13:33 - INFO - __main__ - Global step 700 Train loss 0.07 Classification-F1 0.6941125541125541 on epoch=174
05/26/2022 02:13:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=177
05/26/2022 02:13:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=179
05/26/2022 02:13:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=182
05/26/2022 02:13:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=184
05/26/2022 02:13:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=187
05/26/2022 02:13:47 - INFO - __main__ - Global step 750 Train loss 0.04 Classification-F1 0.5534575569358178 on epoch=187
05/26/2022 02:13:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=189
05/26/2022 02:13:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=192
05/26/2022 02:13:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=194
05/26/2022 02:13:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
05/26/2022 02:14:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=199
05/26/2022 02:14:01 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.7287031799899447 on epoch=199
05/26/2022 02:14:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7215652337356192 -> 0.7287031799899447 on epoch=199, global_step=800
05/26/2022 02:14:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.13 on epoch=202
05/26/2022 02:14:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.13 on epoch=204
05/26/2022 02:14:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
05/26/2022 02:14:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=209
05/26/2022 02:14:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=212
05/26/2022 02:14:16 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.6890804597701149 on epoch=212
05/26/2022 02:14:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=214
05/26/2022 02:14:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=217
05/26/2022 02:14:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=219
05/26/2022 02:14:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=222
05/26/2022 02:14:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=224
05/26/2022 02:14:30 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.7018897239485474 on epoch=224
05/26/2022 02:14:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=227
05/26/2022 02:14:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=229
05/26/2022 02:14:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=232
05/26/2022 02:14:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
05/26/2022 02:14:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=237
05/26/2022 02:14:44 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.6456766917293233 on epoch=237
05/26/2022 02:14:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=239
05/26/2022 02:14:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=242
05/26/2022 02:14:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=244
05/26/2022 02:14:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=247
05/26/2022 02:14:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
05/26/2022 02:14:58 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.76588754219455 on epoch=249
05/26/2022 02:14:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7287031799899447 -> 0.76588754219455 on epoch=249, global_step=1000
05/26/2022 02:15:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
05/26/2022 02:15:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
05/26/2022 02:15:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
05/26/2022 02:15:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
05/26/2022 02:15:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=262
05/26/2022 02:15:13 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.7311688311688311 on epoch=262
05/26/2022 02:15:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
05/26/2022 02:15:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
05/26/2022 02:15:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=269
05/26/2022 02:15:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=272
05/26/2022 02:15:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=274
05/26/2022 02:15:27 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.7925743611227483 on epoch=274
05/26/2022 02:15:27 - INFO - __main__ - Saving model with best Classification-F1: 0.76588754219455 -> 0.7925743611227483 on epoch=274, global_step=1100
05/26/2022 02:15:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
05/26/2022 02:15:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
05/26/2022 02:15:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=282
05/26/2022 02:15:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
05/26/2022 02:15:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
05/26/2022 02:15:42 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.7319893278628066 on epoch=287
05/26/2022 02:15:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
05/26/2022 02:15:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
05/26/2022 02:15:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
05/26/2022 02:15:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
05/26/2022 02:15:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/26/2022 02:15:56 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.6937815126050421 on epoch=299
05/26/2022 02:15:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/26/2022 02:16:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
05/26/2022 02:16:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/26/2022 02:16:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=309
05/26/2022 02:16:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/26/2022 02:16:10 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.7424660830732938 on epoch=312
05/26/2022 02:16:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/26/2022 02:16:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=317
05/26/2022 02:16:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
05/26/2022 02:16:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/26/2022 02:16:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/26/2022 02:16:24 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.6741660138399268 on epoch=324
05/26/2022 02:16:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/26/2022 02:16:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/26/2022 02:16:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/26/2022 02:16:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/26/2022 02:16:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
05/26/2022 02:16:39 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.7272609743197979 on epoch=337
05/26/2022 02:16:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/26/2022 02:16:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=342
05/26/2022 02:16:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
05/26/2022 02:16:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/26/2022 02:16:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
05/26/2022 02:16:53 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.7582170688788336 on epoch=349
05/26/2022 02:16:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/26/2022 02:16:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/26/2022 02:17:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
05/26/2022 02:17:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/26/2022 02:17:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/26/2022 02:17:07 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.6997863247863249 on epoch=362
05/26/2022 02:17:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/26/2022 02:17:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=367
05/26/2022 02:17:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/26/2022 02:17:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=372
05/26/2022 02:17:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/26/2022 02:17:22 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.6984848484848485 on epoch=374
05/26/2022 02:17:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
05/26/2022 02:17:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/26/2022 02:17:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/26/2022 02:17:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/26/2022 02:17:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/26/2022 02:17:36 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6705128205128206 on epoch=387
05/26/2022 02:17:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
05/26/2022 02:17:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/26/2022 02:17:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/26/2022 02:17:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/26/2022 02:17:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/26/2022 02:17:51 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.6980042016806723 on epoch=399
05/26/2022 02:17:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/26/2022 02:17:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
05/26/2022 02:17:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/26/2022 02:18:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/26/2022 02:18:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
05/26/2022 02:18:05 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7011363636363637 on epoch=412
05/26/2022 02:18:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
05/26/2022 02:18:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/26/2022 02:18:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/26/2022 02:18:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/26/2022 02:18:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=424
05/26/2022 02:18:19 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6422064777327936 on epoch=424
05/26/2022 02:18:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/26/2022 02:18:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/26/2022 02:18:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
05/26/2022 02:18:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
05/26/2022 02:18:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/26/2022 02:18:33 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7416729512317748 on epoch=437
05/26/2022 02:18:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/26/2022 02:18:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
05/26/2022 02:18:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/26/2022 02:18:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/26/2022 02:18:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/26/2022 02:18:48 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6292471042471043 on epoch=449
05/26/2022 02:18:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=452
05/26/2022 02:18:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/26/2022 02:18:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/26/2022 02:18:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/26/2022 02:19:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/26/2022 02:19:02 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.7131895881895882 on epoch=462
05/26/2022 02:19:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/26/2022 02:19:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/26/2022 02:19:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/26/2022 02:19:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/26/2022 02:19:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/26/2022 02:19:16 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7272609743197979 on epoch=474
05/26/2022 02:19:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/26/2022 02:19:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/26/2022 02:19:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/26/2022 02:19:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/26/2022 02:19:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/26/2022 02:19:30 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.742858122269887 on epoch=487
05/26/2022 02:19:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/26/2022 02:19:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/26/2022 02:19:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/26/2022 02:19:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/26/2022 02:19:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=499
05/26/2022 02:19:45 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7075324675324675 on epoch=499
05/26/2022 02:19:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/26/2022 02:19:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
05/26/2022 02:19:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
05/26/2022 02:19:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/26/2022 02:19:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/26/2022 02:19:59 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6592105263157895 on epoch=512
05/26/2022 02:20:02 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/26/2022 02:20:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/26/2022 02:20:07 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/26/2022 02:20:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/26/2022 02:20:12 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/26/2022 02:20:13 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7091537081339713 on epoch=524
05/26/2022 02:20:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/26/2022 02:20:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/26/2022 02:20:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/26/2022 02:20:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/26/2022 02:20:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/26/2022 02:20:28 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7279785248535248 on epoch=537
05/26/2022 02:20:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/26/2022 02:20:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/26/2022 02:20:35 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/26/2022 02:20:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/26/2022 02:20:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/26/2022 02:20:42 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7244883040935672 on epoch=549
05/26/2022 02:20:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/26/2022 02:20:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/26/2022 02:20:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/26/2022 02:20:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/26/2022 02:20:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/26/2022 02:20:56 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.7377421271538919 on epoch=562
05/26/2022 02:20:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/26/2022 02:21:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/26/2022 02:21:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/26/2022 02:21:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/26/2022 02:21:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/26/2022 02:21:11 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.743889232978417 on epoch=574
05/26/2022 02:21:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/26/2022 02:21:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/26/2022 02:21:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/26/2022 02:21:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/26/2022 02:21:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/26/2022 02:21:25 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.7292331507501786 on epoch=587
05/26/2022 02:21:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/26/2022 02:21:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/26/2022 02:21:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/26/2022 02:21:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/26/2022 02:21:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/26/2022 02:21:40 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.7091473559120618 on epoch=599
05/26/2022 02:21:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/26/2022 02:21:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/26/2022 02:21:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/26/2022 02:21:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/26/2022 02:21:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/26/2022 02:21:54 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6638843813387425 on epoch=612
05/26/2022 02:21:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/26/2022 02:21:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/26/2022 02:22:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=619
05/26/2022 02:22:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/26/2022 02:22:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/26/2022 02:22:08 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7422299922299922 on epoch=624
05/26/2022 02:22:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/26/2022 02:22:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/26/2022 02:22:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/26/2022 02:22:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/26/2022 02:22:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/26/2022 02:22:23 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.7422299922299922 on epoch=637
05/26/2022 02:22:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/26/2022 02:22:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/26/2022 02:22:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/26/2022 02:22:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 02:22:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/26/2022 02:22:37 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7422299922299922 on epoch=649
05/26/2022 02:22:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/26/2022 02:22:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/26/2022 02:22:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/26/2022 02:22:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 02:22:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/26/2022 02:22:51 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7415466886055121 on epoch=662
05/26/2022 02:22:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/26/2022 02:22:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/26/2022 02:22:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=669
05/26/2022 02:23:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/26/2022 02:23:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/26/2022 02:23:06 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7119269619269618 on epoch=674
05/26/2022 02:23:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/26/2022 02:23:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/26/2022 02:23:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/26/2022 02:23:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/26/2022 02:23:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/26/2022 02:23:20 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7075324675324675 on epoch=687
05/26/2022 02:23:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/26/2022 02:23:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/26/2022 02:23:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/26/2022 02:23:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/26/2022 02:23:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/26/2022 02:23:35 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7227731092436975 on epoch=699
05/26/2022 02:23:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
05/26/2022 02:23:40 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 02:23:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 02:23:45 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/26/2022 02:23:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/26/2022 02:23:49 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7305672268907563 on epoch=712
05/26/2022 02:23:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/26/2022 02:23:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/26/2022 02:23:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 02:23:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/26/2022 02:24:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/26/2022 02:24:03 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7391156597774244 on epoch=724
05/26/2022 02:24:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/26/2022 02:24:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 02:24:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/26/2022 02:24:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/26/2022 02:24:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 02:24:17 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7093948412698413 on epoch=737
05/26/2022 02:24:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/26/2022 02:24:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/26/2022 02:24:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/26/2022 02:24:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
05/26/2022 02:24:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 02:24:32 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7453431372549019 on epoch=749
05/26/2022 02:24:32 - INFO - __main__ - save last model!
05/26/2022 02:24:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 02:24:32 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 02:24:32 - INFO - __main__ - Printing 3 examples
05/26/2022 02:24:32 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 02:24:32 - INFO - __main__ - ['others']
05/26/2022 02:24:32 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 02:24:32 - INFO - __main__ - ['others']
05/26/2022 02:24:32 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 02:24:32 - INFO - __main__ - ['others']
05/26/2022 02:24:32 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:24:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:24:32 - INFO - __main__ - Printing 3 examples
05/26/2022 02:24:32 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/26/2022 02:24:32 - INFO - __main__ - ['happy']
05/26/2022 02:24:32 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/26/2022 02:24:32 - INFO - __main__ - ['happy']
05/26/2022 02:24:32 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/26/2022 02:24:32 - INFO - __main__ - ['happy']
05/26/2022 02:24:32 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:24:32 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:24:32 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 02:24:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:24:32 - INFO - __main__ - Printing 3 examples
05/26/2022 02:24:32 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/26/2022 02:24:32 - INFO - __main__ - ['happy']
05/26/2022 02:24:32 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/26/2022 02:24:32 - INFO - __main__ - ['happy']
05/26/2022 02:24:32 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/26/2022 02:24:32 - INFO - __main__ - ['happy']
05/26/2022 02:24:32 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:24:32 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:24:32 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 02:24:34 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:24:39 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 02:24:51 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 02:24:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 02:24:51 - INFO - __main__ - Starting training!
05/26/2022 02:26:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_42_0.4_8_predictions.txt
05/26/2022 02:26:30 - INFO - __main__ - Classification-F1 on test data: 0.0691
05/26/2022 02:26:30 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.7925743611227483, test_performance=0.06907711941133479
05/26/2022 02:26:30 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
05/26/2022 02:26:31 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:26:31 - INFO - __main__ - Printing 3 examples
05/26/2022 02:26:31 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/26/2022 02:26:31 - INFO - __main__ - ['happy']
05/26/2022 02:26:31 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/26/2022 02:26:31 - INFO - __main__ - ['happy']
05/26/2022 02:26:31 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/26/2022 02:26:31 - INFO - __main__ - ['happy']
05/26/2022 02:26:31 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:26:31 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:26:31 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 02:26:31 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:26:31 - INFO - __main__ - Printing 3 examples
05/26/2022 02:26:31 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/26/2022 02:26:31 - INFO - __main__ - ['happy']
05/26/2022 02:26:31 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/26/2022 02:26:31 - INFO - __main__ - ['happy']
05/26/2022 02:26:31 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/26/2022 02:26:31 - INFO - __main__ - ['happy']
05/26/2022 02:26:31 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:26:31 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:26:31 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 02:26:47 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 02:26:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 02:26:47 - INFO - __main__ - Starting training!
05/26/2022 02:26:51 - INFO - __main__ - Step 10 Global step 10 Train loss 2.91 on epoch=2
05/26/2022 02:26:53 - INFO - __main__ - Step 20 Global step 20 Train loss 1.55 on epoch=4
05/26/2022 02:26:56 - INFO - __main__ - Step 30 Global step 30 Train loss 1.06 on epoch=7
05/26/2022 02:26:59 - INFO - __main__ - Step 40 Global step 40 Train loss 1.07 on epoch=9
05/26/2022 02:27:01 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=12
05/26/2022 02:27:02 - INFO - __main__ - Global step 50 Train loss 1.51 Classification-F1 0.20606060606060606 on epoch=12
05/26/2022 02:27:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.20606060606060606 on epoch=12, global_step=50
05/26/2022 02:27:05 - INFO - __main__ - Step 60 Global step 60 Train loss 0.90 on epoch=14
05/26/2022 02:27:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.98 on epoch=17
05/26/2022 02:27:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.83 on epoch=19
05/26/2022 02:27:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.80 on epoch=22
05/26/2022 02:27:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=24
05/26/2022 02:27:17 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.4254869578638679 on epoch=24
05/26/2022 02:27:17 - INFO - __main__ - Saving model with best Classification-F1: 0.20606060606060606 -> 0.4254869578638679 on epoch=24, global_step=100
05/26/2022 02:27:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=27
05/26/2022 02:27:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.79 on epoch=29
05/26/2022 02:27:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.82 on epoch=32
05/26/2022 02:27:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.81 on epoch=34
05/26/2022 02:27:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.78 on epoch=37
05/26/2022 02:27:31 - INFO - __main__ - Global step 150 Train loss 0.82 Classification-F1 0.4833642547928262 on epoch=37
05/26/2022 02:27:31 - INFO - __main__ - Saving model with best Classification-F1: 0.4254869578638679 -> 0.4833642547928262 on epoch=37, global_step=150
05/26/2022 02:27:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.80 on epoch=39
05/26/2022 02:27:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.72 on epoch=42
05/26/2022 02:27:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.69 on epoch=44
05/26/2022 02:27:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.73 on epoch=47
05/26/2022 02:27:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.72 on epoch=49
05/26/2022 02:27:45 - INFO - __main__ - Global step 200 Train loss 0.73 Classification-F1 0.6249881982690795 on epoch=49
05/26/2022 02:27:45 - INFO - __main__ - Saving model with best Classification-F1: 0.4833642547928262 -> 0.6249881982690795 on epoch=49, global_step=200
05/26/2022 02:27:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.69 on epoch=52
05/26/2022 02:27:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.71 on epoch=54
05/26/2022 02:27:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.59 on epoch=57
05/26/2022 02:27:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.62 on epoch=59
05/26/2022 02:27:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=62
05/26/2022 02:27:59 - INFO - __main__ - Global step 250 Train loss 0.62 Classification-F1 0.6334303053053053 on epoch=62
05/26/2022 02:27:59 - INFO - __main__ - Saving model with best Classification-F1: 0.6249881982690795 -> 0.6334303053053053 on epoch=62, global_step=250
05/26/2022 02:28:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=64
05/26/2022 02:28:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=67
05/26/2022 02:28:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=69
05/26/2022 02:28:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=72
05/26/2022 02:28:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=74
05/26/2022 02:28:14 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.669557766888134 on epoch=74
05/26/2022 02:28:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6334303053053053 -> 0.669557766888134 on epoch=74, global_step=300
05/26/2022 02:28:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=77
05/26/2022 02:28:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=79
05/26/2022 02:28:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=82
05/26/2022 02:28:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=84
05/26/2022 02:28:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=87
05/26/2022 02:28:28 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.6711805555555556 on epoch=87
05/26/2022 02:28:28 - INFO - __main__ - Saving model with best Classification-F1: 0.669557766888134 -> 0.6711805555555556 on epoch=87, global_step=350
05/26/2022 02:28:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=89
05/26/2022 02:28:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=92
05/26/2022 02:28:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=94
05/26/2022 02:28:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=97
05/26/2022 02:28:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=99
05/26/2022 02:28:42 - INFO - __main__ - Global step 400 Train loss 0.33 Classification-F1 0.6664270613107822 on epoch=99
05/26/2022 02:28:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.31 on epoch=102
05/26/2022 02:28:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.30 on epoch=104
05/26/2022 02:28:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=107
05/26/2022 02:28:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=109
05/26/2022 02:28:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=112
05/26/2022 02:28:56 - INFO - __main__ - Global step 450 Train loss 0.30 Classification-F1 0.6748405103668262 on epoch=112
05/26/2022 02:28:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6711805555555556 -> 0.6748405103668262 on epoch=112, global_step=450
05/26/2022 02:28:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=114
05/26/2022 02:29:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=117
05/26/2022 02:29:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=119
05/26/2022 02:29:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=122
05/26/2022 02:29:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=124
05/26/2022 02:29:11 - INFO - __main__ - Global step 500 Train loss 0.27 Classification-F1 0.7008658008658009 on epoch=124
05/26/2022 02:29:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6748405103668262 -> 0.7008658008658009 on epoch=124, global_step=500
05/26/2022 02:29:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=127
05/26/2022 02:29:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.16 on epoch=129
05/26/2022 02:29:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=132
05/26/2022 02:29:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=134
05/26/2022 02:29:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=137
05/26/2022 02:29:25 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.508288770053476 on epoch=137
05/26/2022 02:29:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=139
05/26/2022 02:29:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=142
05/26/2022 02:29:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=144
05/26/2022 02:29:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=147
05/26/2022 02:29:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=149
05/26/2022 02:29:39 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.6100568136984139 on epoch=149
05/26/2022 02:29:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=152
05/26/2022 02:29:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=154
05/26/2022 02:29:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=157
05/26/2022 02:29:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=159
05/26/2022 02:29:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=162
05/26/2022 02:29:54 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.5047979797979798 on epoch=162
05/26/2022 02:29:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
05/26/2022 02:29:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=167
05/26/2022 02:30:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=169
05/26/2022 02:30:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=172
05/26/2022 02:30:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=174
05/26/2022 02:30:08 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.6745098039215687 on epoch=174
05/26/2022 02:30:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=177
05/26/2022 02:30:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=179
05/26/2022 02:30:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=182
05/26/2022 02:30:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=184
05/26/2022 02:30:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=187
05/26/2022 02:30:22 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.6299651567944251 on epoch=187
05/26/2022 02:30:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=189
05/26/2022 02:30:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=192
05/26/2022 02:30:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=194
05/26/2022 02:30:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
05/26/2022 02:30:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=199
05/26/2022 02:30:37 - INFO - __main__ - Global step 800 Train loss 0.12 Classification-F1 0.6306759906759907 on epoch=199
05/26/2022 02:30:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=202
05/26/2022 02:30:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=204
05/26/2022 02:30:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=207
05/26/2022 02:30:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=209
05/26/2022 02:30:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=212
05/26/2022 02:30:51 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.6343999138116785 on epoch=212
05/26/2022 02:30:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=214
05/26/2022 02:30:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=217
05/26/2022 02:30:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=219
05/26/2022 02:31:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=222
05/26/2022 02:31:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=224
05/26/2022 02:31:06 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.626978947663499 on epoch=224
05/26/2022 02:31:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=227
05/26/2022 02:31:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=229
05/26/2022 02:31:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=232
05/26/2022 02:31:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=234
05/26/2022 02:31:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=237
05/26/2022 02:31:20 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.585140606500566 on epoch=237
05/26/2022 02:31:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=239
05/26/2022 02:31:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
05/26/2022 02:31:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=244
05/26/2022 02:31:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
05/26/2022 02:31:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=249
05/26/2022 02:31:35 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.7180581910183428 on epoch=249
05/26/2022 02:31:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7008658008658009 -> 0.7180581910183428 on epoch=249, global_step=1000
05/26/2022 02:31:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
05/26/2022 02:31:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=254
05/26/2022 02:31:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
05/26/2022 02:31:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=259
05/26/2022 02:31:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=262
05/26/2022 02:31:49 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.7606951871657754 on epoch=262
05/26/2022 02:31:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7180581910183428 -> 0.7606951871657754 on epoch=262, global_step=1050
05/26/2022 02:31:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
05/26/2022 02:31:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
05/26/2022 02:31:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=269
05/26/2022 02:32:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/26/2022 02:32:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=274
05/26/2022 02:32:04 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.7087017231134879 on epoch=274
05/26/2022 02:32:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
05/26/2022 02:32:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=279
05/26/2022 02:32:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/26/2022 02:32:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=284
05/26/2022 02:32:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
05/26/2022 02:32:18 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.714114010989011 on epoch=287
05/26/2022 02:32:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/26/2022 02:32:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/26/2022 02:32:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
05/26/2022 02:32:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
05/26/2022 02:32:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
05/26/2022 02:32:33 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.6466269841269842 on epoch=299
05/26/2022 02:32:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
05/26/2022 02:32:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/26/2022 02:32:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
05/26/2022 02:32:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/26/2022 02:32:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
05/26/2022 02:32:47 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.7180223285486445 on epoch=312
05/26/2022 02:32:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/26/2022 02:32:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
05/26/2022 02:32:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/26/2022 02:32:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
05/26/2022 02:33:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/26/2022 02:33:02 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6997754933238804 on epoch=324
05/26/2022 02:33:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=327
05/26/2022 02:33:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
05/26/2022 02:33:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/26/2022 02:33:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/26/2022 02:33:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/26/2022 02:33:17 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.7322375541125541 on epoch=337
05/26/2022 02:33:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/26/2022 02:33:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/26/2022 02:33:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/26/2022 02:33:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/26/2022 02:33:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
05/26/2022 02:33:31 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.7227906976744186 on epoch=349
05/26/2022 02:33:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/26/2022 02:33:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
05/26/2022 02:33:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/26/2022 02:33:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
05/26/2022 02:33:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/26/2022 02:33:46 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.6973966836524349 on epoch=362
05/26/2022 02:33:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/26/2022 02:33:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=367
05/26/2022 02:33:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/26/2022 02:33:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/26/2022 02:33:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=374
05/26/2022 02:34:00 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.6899920255183414 on epoch=374
05/26/2022 02:34:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/26/2022 02:34:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/26/2022 02:34:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/26/2022 02:34:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
05/26/2022 02:34:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/26/2022 02:34:15 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.714114010989011 on epoch=387
05/26/2022 02:34:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/26/2022 02:34:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
05/26/2022 02:34:23 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=394
05/26/2022 02:34:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/26/2022 02:34:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=399
05/26/2022 02:34:29 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.6843417452113104 on epoch=399
05/26/2022 02:34:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/26/2022 02:34:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/26/2022 02:34:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/26/2022 02:34:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/26/2022 02:34:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/26/2022 02:34:44 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.6814248349732221 on epoch=412
05/26/2022 02:34:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/26/2022 02:34:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/26/2022 02:34:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=419
05/26/2022 02:34:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/26/2022 02:34:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/26/2022 02:34:58 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.7357561793045664 on epoch=424
05/26/2022 02:35:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/26/2022 02:35:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/26/2022 02:35:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/26/2022 02:35:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=434
05/26/2022 02:35:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
05/26/2022 02:35:13 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6807706093189965 on epoch=437
05/26/2022 02:35:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/26/2022 02:35:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/26/2022 02:35:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
05/26/2022 02:35:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/26/2022 02:35:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
05/26/2022 02:35:28 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7357538468668892 on epoch=449
05/26/2022 02:35:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/26/2022 02:35:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/26/2022 02:35:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/26/2022 02:35:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/26/2022 02:35:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/26/2022 02:35:42 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7485795454545454 on epoch=462
05/26/2022 02:35:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=464
05/26/2022 02:35:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/26/2022 02:35:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=469
05/26/2022 02:35:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/26/2022 02:35:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/26/2022 02:35:57 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7158741668238697 on epoch=474
05/26/2022 02:35:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/26/2022 02:36:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/26/2022 02:36:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/26/2022 02:36:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/26/2022 02:36:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/26/2022 02:36:11 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7232822043986495 on epoch=487
05/26/2022 02:36:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
05/26/2022 02:36:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/26/2022 02:36:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/26/2022 02:36:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=497
05/26/2022 02:36:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/26/2022 02:36:26 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6658251658251658 on epoch=499
05/26/2022 02:36:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/26/2022 02:36:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/26/2022 02:36:34 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/26/2022 02:36:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/26/2022 02:36:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/26/2022 02:36:41 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7185124675565443 on epoch=512
05/26/2022 02:36:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/26/2022 02:36:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/26/2022 02:36:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/26/2022 02:36:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/26/2022 02:36:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/26/2022 02:36:55 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.703792909288599 on epoch=524
05/26/2022 02:36:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/26/2022 02:37:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/26/2022 02:37:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/26/2022 02:37:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/26/2022 02:37:08 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=537
05/26/2022 02:37:10 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7000324227291239 on epoch=537
05/26/2022 02:37:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/26/2022 02:37:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/26/2022 02:37:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/26/2022 02:37:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=547
05/26/2022 02:37:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
05/26/2022 02:37:24 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.747916957594377 on epoch=549
05/26/2022 02:37:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/26/2022 02:37:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/26/2022 02:37:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/26/2022 02:37:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/26/2022 02:37:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
05/26/2022 02:37:39 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7608981092436975 on epoch=562
05/26/2022 02:37:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7606951871657754 -> 0.7608981092436975 on epoch=562, global_step=2250
05/26/2022 02:37:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/26/2022 02:37:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/26/2022 02:37:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/26/2022 02:37:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/26/2022 02:37:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/26/2022 02:37:54 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6948566376152584 on epoch=574
05/26/2022 02:37:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/26/2022 02:37:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/26/2022 02:38:02 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/26/2022 02:38:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/26/2022 02:38:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/26/2022 02:38:08 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6746979074565281 on epoch=587
05/26/2022 02:38:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/26/2022 02:38:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/26/2022 02:38:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
05/26/2022 02:38:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/26/2022 02:38:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/26/2022 02:38:23 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7142601970188177 on epoch=599
05/26/2022 02:38:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/26/2022 02:38:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/26/2022 02:38:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/26/2022 02:38:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/26/2022 02:38:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/26/2022 02:38:37 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6981272643462509 on epoch=612
05/26/2022 02:38:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/26/2022 02:38:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/26/2022 02:38:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=619
05/26/2022 02:38:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/26/2022 02:38:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/26/2022 02:38:52 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6821136942840798 on epoch=624
05/26/2022 02:38:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/26/2022 02:38:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/26/2022 02:39:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/26/2022 02:39:03 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/26/2022 02:39:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/26/2022 02:39:07 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.7170347744360903 on epoch=637
05/26/2022 02:39:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/26/2022 02:39:12 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/26/2022 02:39:15 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/26/2022 02:39:17 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
05/26/2022 02:39:20 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/26/2022 02:39:21 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6765633503191016 on epoch=649
05/26/2022 02:39:24 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/26/2022 02:39:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/26/2022 02:39:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/26/2022 02:39:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 02:39:35 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/26/2022 02:39:36 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6973565767005633 on epoch=662
05/26/2022 02:39:39 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/26/2022 02:39:41 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/26/2022 02:39:44 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/26/2022 02:39:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/26/2022 02:39:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/26/2022 02:39:50 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7154168694915723 on epoch=674
05/26/2022 02:39:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/26/2022 02:39:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/26/2022 02:39:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/26/2022 02:40:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/26/2022 02:40:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/26/2022 02:40:05 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6753694581280789 on epoch=687
05/26/2022 02:40:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/26/2022 02:40:10 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/26/2022 02:40:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/26/2022 02:40:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/26/2022 02:40:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/26/2022 02:40:20 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7000324227291239 on epoch=699
05/26/2022 02:40:22 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/26/2022 02:40:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/26/2022 02:40:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 02:40:30 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/26/2022 02:40:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/26/2022 02:40:34 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7228506787330317 on epoch=712
05/26/2022 02:40:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/26/2022 02:40:40 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/26/2022 02:40:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 02:40:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/26/2022 02:40:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/26/2022 02:40:49 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6937699555346615 on epoch=724
05/26/2022 02:40:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/26/2022 02:40:54 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 02:40:57 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/26/2022 02:40:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/26/2022 02:41:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 02:41:03 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7067528735632184 on epoch=737
05/26/2022 02:41:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/26/2022 02:41:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/26/2022 02:41:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/26/2022 02:41:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
05/26/2022 02:41:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 02:41:18 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6937699555346615 on epoch=749
05/26/2022 02:41:18 - INFO - __main__ - save last model!
05/26/2022 02:41:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 02:41:18 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 02:41:18 - INFO - __main__ - Printing 3 examples
05/26/2022 02:41:18 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 02:41:18 - INFO - __main__ - ['others']
05/26/2022 02:41:18 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 02:41:18 - INFO - __main__ - ['others']
05/26/2022 02:41:18 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 02:41:18 - INFO - __main__ - ['others']
05/26/2022 02:41:18 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:41:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:41:18 - INFO - __main__ - Printing 3 examples
05/26/2022 02:41:18 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/26/2022 02:41:18 - INFO - __main__ - ['happy']
05/26/2022 02:41:18 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/26/2022 02:41:18 - INFO - __main__ - ['happy']
05/26/2022 02:41:18 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/26/2022 02:41:18 - INFO - __main__ - ['happy']
05/26/2022 02:41:18 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:41:18 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:41:18 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 02:41:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:41:18 - INFO - __main__ - Printing 3 examples
05/26/2022 02:41:18 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/26/2022 02:41:18 - INFO - __main__ - ['happy']
05/26/2022 02:41:18 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/26/2022 02:41:18 - INFO - __main__ - ['happy']
05/26/2022 02:41:18 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/26/2022 02:41:18 - INFO - __main__ - ['happy']
05/26/2022 02:41:18 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:41:18 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:41:18 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 02:41:20 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:41:26 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 02:41:37 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 02:41:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 02:41:38 - INFO - __main__ - Starting training!
05/26/2022 02:43:15 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_42_0.3_8_predictions.txt
05/26/2022 02:43:15 - INFO - __main__ - Classification-F1 on test data: 0.1468
05/26/2022 02:43:15 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.7608981092436975, test_performance=0.14675165062933637
05/26/2022 02:43:16 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
05/26/2022 02:43:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:43:16 - INFO - __main__ - Printing 3 examples
05/26/2022 02:43:16 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/26/2022 02:43:16 - INFO - __main__ - ['happy']
05/26/2022 02:43:16 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/26/2022 02:43:16 - INFO - __main__ - ['happy']
05/26/2022 02:43:16 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/26/2022 02:43:16 - INFO - __main__ - ['happy']
05/26/2022 02:43:16 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:43:16 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:43:17 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 02:43:17 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:43:17 - INFO - __main__ - Printing 3 examples
05/26/2022 02:43:17 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/26/2022 02:43:17 - INFO - __main__ - ['happy']
05/26/2022 02:43:17 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/26/2022 02:43:17 - INFO - __main__ - ['happy']
05/26/2022 02:43:17 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/26/2022 02:43:17 - INFO - __main__ - ['happy']
05/26/2022 02:43:17 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:43:17 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:43:17 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 02:43:32 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 02:43:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 02:43:32 - INFO - __main__ - Starting training!
05/26/2022 02:43:36 - INFO - __main__ - Step 10 Global step 10 Train loss 3.19 on epoch=2
05/26/2022 02:43:38 - INFO - __main__ - Step 20 Global step 20 Train loss 1.95 on epoch=4
05/26/2022 02:43:41 - INFO - __main__ - Step 30 Global step 30 Train loss 1.42 on epoch=7
05/26/2022 02:43:43 - INFO - __main__ - Step 40 Global step 40 Train loss 1.24 on epoch=9
05/26/2022 02:43:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.93 on epoch=12
05/26/2022 02:43:47 - INFO - __main__ - Global step 50 Train loss 1.75 Classification-F1 0.1 on epoch=12
05/26/2022 02:43:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/26/2022 02:43:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.88 on epoch=14
05/26/2022 02:43:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.92 on epoch=17
05/26/2022 02:43:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.87 on epoch=19
05/26/2022 02:43:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=22
05/26/2022 02:44:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=24
05/26/2022 02:44:01 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.36588315217391304 on epoch=24
05/26/2022 02:44:01 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.36588315217391304 on epoch=24, global_step=100
05/26/2022 02:44:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.88 on epoch=27
05/26/2022 02:44:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.81 on epoch=29
05/26/2022 02:44:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.84 on epoch=32
05/26/2022 02:44:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.95 on epoch=34
05/26/2022 02:44:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=37
05/26/2022 02:44:15 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.4743148180648181 on epoch=37
05/26/2022 02:44:15 - INFO - __main__ - Saving model with best Classification-F1: 0.36588315217391304 -> 0.4743148180648181 on epoch=37, global_step=150
05/26/2022 02:44:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=39
05/26/2022 02:44:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.84 on epoch=42
05/26/2022 02:44:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.81 on epoch=44
05/26/2022 02:44:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.73 on epoch=47
05/26/2022 02:44:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.79 on epoch=49
05/26/2022 02:44:29 - INFO - __main__ - Global step 200 Train loss 0.80 Classification-F1 0.5335511982570806 on epoch=49
05/26/2022 02:44:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4743148180648181 -> 0.5335511982570806 on epoch=49, global_step=200
05/26/2022 02:44:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.80 on epoch=52
05/26/2022 02:44:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=54
05/26/2022 02:44:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.71 on epoch=57
05/26/2022 02:44:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.76 on epoch=59
05/26/2022 02:44:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.65 on epoch=62
05/26/2022 02:44:43 - INFO - __main__ - Global step 250 Train loss 0.75 Classification-F1 0.5619399881164587 on epoch=62
05/26/2022 02:44:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5335511982570806 -> 0.5619399881164587 on epoch=62, global_step=250
05/26/2022 02:44:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.71 on epoch=64
05/26/2022 02:44:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.65 on epoch=67
05/26/2022 02:44:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.77 on epoch=69
05/26/2022 02:44:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.67 on epoch=72
05/26/2022 02:44:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.59 on epoch=74
05/26/2022 02:44:57 - INFO - __main__ - Global step 300 Train loss 0.68 Classification-F1 0.6109190988223246 on epoch=74
05/26/2022 02:44:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5619399881164587 -> 0.6109190988223246 on epoch=74, global_step=300
05/26/2022 02:45:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.60 on epoch=77
05/26/2022 02:45:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.68 on epoch=79
05/26/2022 02:45:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.60 on epoch=82
05/26/2022 02:45:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=84
05/26/2022 02:45:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.60 on epoch=87
05/26/2022 02:45:11 - INFO - __main__ - Global step 350 Train loss 0.60 Classification-F1 0.5924387229408093 on epoch=87
05/26/2022 02:45:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=89
05/26/2022 02:45:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=92
05/26/2022 02:45:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.56 on epoch=94
05/26/2022 02:45:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=97
05/26/2022 02:45:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=99
05/26/2022 02:45:25 - INFO - __main__ - Global step 400 Train loss 0.52 Classification-F1 0.643764400921659 on epoch=99
05/26/2022 02:45:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6109190988223246 -> 0.643764400921659 on epoch=99, global_step=400
05/26/2022 02:45:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=102
05/26/2022 02:45:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=104
05/26/2022 02:45:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=107
05/26/2022 02:45:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=109
05/26/2022 02:45:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=112
05/26/2022 02:45:39 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.6389652014652015 on epoch=112
05/26/2022 02:45:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=114
05/26/2022 02:45:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=117
05/26/2022 02:45:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=119
05/26/2022 02:45:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=122
05/26/2022 02:45:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=124
05/26/2022 02:45:53 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.6892361111111112 on epoch=124
05/26/2022 02:45:53 - INFO - __main__ - Saving model with best Classification-F1: 0.643764400921659 -> 0.6892361111111112 on epoch=124, global_step=500
05/26/2022 02:45:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=127
05/26/2022 02:45:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=129
05/26/2022 02:46:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=132
05/26/2022 02:46:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=134
05/26/2022 02:46:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=137
05/26/2022 02:46:07 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.579159870336341 on epoch=137
05/26/2022 02:46:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=139
05/26/2022 02:46:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=142
05/26/2022 02:46:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=144
05/26/2022 02:46:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.31 on epoch=147
05/26/2022 02:46:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=149
05/26/2022 02:46:21 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.5801242236024844 on epoch=149
05/26/2022 02:46:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.34 on epoch=152
05/26/2022 02:46:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.29 on epoch=154
05/26/2022 02:46:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=157
05/26/2022 02:46:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=159
05/26/2022 02:46:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=162
05/26/2022 02:46:35 - INFO - __main__ - Global step 650 Train loss 0.28 Classification-F1 0.6356532569435795 on epoch=162
05/26/2022 02:46:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=164
05/26/2022 02:46:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=167
05/26/2022 02:46:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=169
05/26/2022 02:46:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=172
05/26/2022 02:46:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=174
05/26/2022 02:46:49 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.594573738051999 on epoch=174
05/26/2022 02:46:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
05/26/2022 02:46:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=179
05/26/2022 02:46:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=182
05/26/2022 02:47:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=184
05/26/2022 02:47:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=187
05/26/2022 02:47:03 - INFO - __main__ - Global step 750 Train loss 0.24 Classification-F1 0.6122402890695574 on epoch=187
05/26/2022 02:47:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=189
05/26/2022 02:47:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=192
05/26/2022 02:47:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=194
05/26/2022 02:47:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=197
05/26/2022 02:47:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=199
05/26/2022 02:47:18 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.7052338919398811 on epoch=199
05/26/2022 02:47:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6892361111111112 -> 0.7052338919398811 on epoch=199, global_step=800
05/26/2022 02:47:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=202
05/26/2022 02:47:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=204
05/26/2022 02:47:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.13 on epoch=207
05/26/2022 02:47:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=209
05/26/2022 02:47:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=212
05/26/2022 02:47:32 - INFO - __main__ - Global step 850 Train loss 0.17 Classification-F1 0.6122402890695574 on epoch=212
05/26/2022 02:47:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=214
05/26/2022 02:47:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=217
05/26/2022 02:47:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=219
05/26/2022 02:47:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=222
05/26/2022 02:47:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=224
05/26/2022 02:47:46 - INFO - __main__ - Global step 900 Train loss 0.14 Classification-F1 0.5853153153153153 on epoch=224
05/26/2022 02:47:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=227
05/26/2022 02:47:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=229
05/26/2022 02:47:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=232
05/26/2022 02:47:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=234
05/26/2022 02:47:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=237
05/26/2022 02:48:01 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.6688461538461539 on epoch=237
05/26/2022 02:48:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=239
05/26/2022 02:48:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=242
05/26/2022 02:48:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=244
05/26/2022 02:48:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=247
05/26/2022 02:48:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=249
05/26/2022 02:48:15 - INFO - __main__ - Global step 1000 Train loss 0.15 Classification-F1 0.6435574229691876 on epoch=249
05/26/2022 02:48:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=252
05/26/2022 02:48:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
05/26/2022 02:48:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=257
05/26/2022 02:48:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=259
05/26/2022 02:48:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=262
05/26/2022 02:48:29 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.6676405308223607 on epoch=262
05/26/2022 02:48:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=264
05/26/2022 02:48:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
05/26/2022 02:48:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=269
05/26/2022 02:48:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=272
05/26/2022 02:48:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=274
05/26/2022 02:48:43 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.678763440860215 on epoch=274
05/26/2022 02:48:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=277
05/26/2022 02:48:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=279
05/26/2022 02:48:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=282
05/26/2022 02:48:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
05/26/2022 02:48:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=287
05/26/2022 02:48:57 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.6811764705882353 on epoch=287
05/26/2022 02:49:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=289
05/26/2022 02:49:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
05/26/2022 02:49:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=294
05/26/2022 02:49:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
05/26/2022 02:49:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
05/26/2022 02:49:12 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.7323967086834733 on epoch=299
05/26/2022 02:49:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7052338919398811 -> 0.7323967086834733 on epoch=299, global_step=1200
05/26/2022 02:49:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=302
05/26/2022 02:49:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=304
05/26/2022 02:49:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=307
05/26/2022 02:49:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
05/26/2022 02:49:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
05/26/2022 02:49:26 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.6491453948350501 on epoch=312
05/26/2022 02:49:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/26/2022 02:49:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=317
05/26/2022 02:49:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/26/2022 02:49:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
05/26/2022 02:49:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=324
05/26/2022 02:49:40 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.6807140047206924 on epoch=324
05/26/2022 02:49:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
05/26/2022 02:49:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=329
05/26/2022 02:49:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
05/26/2022 02:49:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
05/26/2022 02:49:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/26/2022 02:49:54 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6319648093841643 on epoch=337
05/26/2022 02:49:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=339
05/26/2022 02:50:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
05/26/2022 02:50:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
05/26/2022 02:50:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=347
05/26/2022 02:50:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=349
05/26/2022 02:50:09 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.6561388339920948 on epoch=349
05/26/2022 02:50:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
05/26/2022 02:50:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=354
05/26/2022 02:50:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=357
05/26/2022 02:50:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
05/26/2022 02:50:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/26/2022 02:50:23 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.6328210329224527 on epoch=362
05/26/2022 02:50:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/26/2022 02:50:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/26/2022 02:50:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=369
05/26/2022 02:50:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=372
05/26/2022 02:50:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
05/26/2022 02:50:37 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.6680687213104446 on epoch=374
05/26/2022 02:50:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/26/2022 02:50:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
05/26/2022 02:50:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/26/2022 02:50:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
05/26/2022 02:50:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/26/2022 02:50:51 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.657944683181665 on epoch=387
05/26/2022 02:50:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
05/26/2022 02:50:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/26/2022 02:50:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/26/2022 02:51:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/26/2022 02:51:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/26/2022 02:51:06 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.6678273967747652 on epoch=399
05/26/2022 02:51:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
05/26/2022 02:51:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
05/26/2022 02:51:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/26/2022 02:51:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
05/26/2022 02:51:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=412
05/26/2022 02:51:20 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.6436378205128206 on epoch=412
05/26/2022 02:51:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
05/26/2022 02:51:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
05/26/2022 02:51:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=419
05/26/2022 02:51:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=422
05/26/2022 02:51:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=424
05/26/2022 02:51:34 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.7103223862974704 on epoch=424
05/26/2022 02:51:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
05/26/2022 02:51:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
05/26/2022 02:51:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/26/2022 02:51:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
05/26/2022 02:51:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/26/2022 02:51:48 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6010506975356329 on epoch=437
05/26/2022 02:51:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/26/2022 02:51:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/26/2022 02:51:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=444
05/26/2022 02:51:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
05/26/2022 02:52:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/26/2022 02:52:03 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.649496336996337 on epoch=449
05/26/2022 02:52:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
05/26/2022 02:52:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/26/2022 02:52:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
05/26/2022 02:52:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
05/26/2022 02:52:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/26/2022 02:52:17 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.68006993006993 on epoch=462
05/26/2022 02:52:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/26/2022 02:52:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
05/26/2022 02:52:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/26/2022 02:52:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=472
05/26/2022 02:52:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
05/26/2022 02:52:32 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.6188146790392179 on epoch=474
05/26/2022 02:52:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
05/26/2022 02:52:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/26/2022 02:52:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/26/2022 02:52:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/26/2022 02:52:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/26/2022 02:52:46 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.63489010989011 on epoch=487
05/26/2022 02:52:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/26/2022 02:52:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/26/2022 02:52:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/26/2022 02:52:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
05/26/2022 02:52:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/26/2022 02:53:01 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.6416666666666667 on epoch=499
05/26/2022 02:53:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/26/2022 02:53:06 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
05/26/2022 02:53:09 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/26/2022 02:53:11 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
05/26/2022 02:53:14 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/26/2022 02:53:15 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6444070512820512 on epoch=512
05/26/2022 02:53:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/26/2022 02:53:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
05/26/2022 02:53:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/26/2022 02:53:26 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=522
05/26/2022 02:53:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/26/2022 02:53:30 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.6598498400823982 on epoch=524
05/26/2022 02:53:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=527
05/26/2022 02:53:35 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/26/2022 02:53:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/26/2022 02:53:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/26/2022 02:53:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=537
05/26/2022 02:53:44 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.6858653864918136 on epoch=537
05/26/2022 02:53:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/26/2022 02:53:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
05/26/2022 02:53:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/26/2022 02:53:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/26/2022 02:53:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/26/2022 02:53:58 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6979037877936383 on epoch=549
05/26/2022 02:54:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/26/2022 02:54:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/26/2022 02:54:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/26/2022 02:54:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/26/2022 02:54:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
05/26/2022 02:54:13 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6839285714285716 on epoch=562
05/26/2022 02:54:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/26/2022 02:54:18 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/26/2022 02:54:21 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/26/2022 02:54:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/26/2022 02:54:26 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/26/2022 02:54:27 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6445098039215686 on epoch=574
05/26/2022 02:54:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/26/2022 02:54:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
05/26/2022 02:54:35 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/26/2022 02:54:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/26/2022 02:54:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=587
05/26/2022 02:54:42 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.651707357948391 on epoch=587
05/26/2022 02:54:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/26/2022 02:54:47 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/26/2022 02:54:50 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/26/2022 02:54:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=597
05/26/2022 02:54:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/26/2022 02:54:56 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.6944055944055945 on epoch=599
05/26/2022 02:54:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=602
05/26/2022 02:55:01 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/26/2022 02:55:04 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
05/26/2022 02:55:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/26/2022 02:55:09 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/26/2022 02:55:11 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6683321308321308 on epoch=612
05/26/2022 02:55:13 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/26/2022 02:55:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/26/2022 02:55:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/26/2022 02:55:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/26/2022 02:55:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/26/2022 02:55:25 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.70057628403665 on epoch=624
05/26/2022 02:55:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/26/2022 02:55:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.11 on epoch=629
05/26/2022 02:55:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=632
05/26/2022 02:55:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
05/26/2022 02:55:38 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/26/2022 02:55:40 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.7107759511993383 on epoch=637
05/26/2022 02:55:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/26/2022 02:55:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
05/26/2022 02:55:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/26/2022 02:55:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=647
05/26/2022 02:55:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/26/2022 02:55:54 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.6725508501509614 on epoch=649
05/26/2022 02:55:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/26/2022 02:55:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/26/2022 02:56:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/26/2022 02:56:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 02:56:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/26/2022 02:56:08 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6012509850275807 on epoch=662
05/26/2022 02:56:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/26/2022 02:56:14 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/26/2022 02:56:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=669
05/26/2022 02:56:19 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/26/2022 02:56:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/26/2022 02:56:23 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.6343999138116785 on epoch=674
05/26/2022 02:56:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
05/26/2022 02:56:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/26/2022 02:56:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/26/2022 02:56:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/26/2022 02:56:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/26/2022 02:56:38 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7153194249968443 on epoch=687
05/26/2022 02:56:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/26/2022 02:56:43 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/26/2022 02:56:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/26/2022 02:56:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/26/2022 02:56:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
05/26/2022 02:56:52 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6861388384754992 on epoch=699
05/26/2022 02:56:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/26/2022 02:56:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 02:57:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/26/2022 02:57:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/26/2022 02:57:05 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/26/2022 02:57:07 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6343999138116785 on epoch=712
05/26/2022 02:57:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/26/2022 02:57:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/26/2022 02:57:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
05/26/2022 02:57:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
05/26/2022 02:57:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/26/2022 02:57:21 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.598903743315508 on epoch=724
05/26/2022 02:57:24 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/26/2022 02:57:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/26/2022 02:57:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/26/2022 02:57:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/26/2022 02:57:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 02:57:36 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6169546459012942 on epoch=737
05/26/2022 02:57:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/26/2022 02:57:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/26/2022 02:57:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/26/2022 02:57:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/26/2022 02:57:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/26/2022 02:57:50 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7144488238238238 on epoch=749
05/26/2022 02:57:50 - INFO - __main__ - save last model!
05/26/2022 02:57:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 02:57:50 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 02:57:50 - INFO - __main__ - Printing 3 examples
05/26/2022 02:57:50 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 02:57:50 - INFO - __main__ - ['others']
05/26/2022 02:57:50 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 02:57:50 - INFO - __main__ - ['others']
05/26/2022 02:57:50 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 02:57:50 - INFO - __main__ - ['others']
05/26/2022 02:57:50 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:57:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:57:50 - INFO - __main__ - Printing 3 examples
05/26/2022 02:57:50 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/26/2022 02:57:50 - INFO - __main__ - ['others']
05/26/2022 02:57:50 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/26/2022 02:57:50 - INFO - __main__ - ['others']
05/26/2022 02:57:50 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/26/2022 02:57:50 - INFO - __main__ - ['others']
05/26/2022 02:57:50 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:57:50 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:57:50 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 02:57:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:57:50 - INFO - __main__ - Printing 3 examples
05/26/2022 02:57:50 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/26/2022 02:57:50 - INFO - __main__ - ['others']
05/26/2022 02:57:50 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/26/2022 02:57:50 - INFO - __main__ - ['others']
05/26/2022 02:57:50 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/26/2022 02:57:50 - INFO - __main__ - ['others']
05/26/2022 02:57:50 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:57:50 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:57:50 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 02:57:52 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:57:57 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 02:58:06 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 02:58:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 02:58:07 - INFO - __main__ - Starting training!
05/26/2022 02:59:48 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_42_0.2_8_predictions.txt
05/26/2022 02:59:48 - INFO - __main__ - Classification-F1 on test data: 0.1688
05/26/2022 02:59:49 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.7323967086834733, test_performance=0.16879527997373803
05/26/2022 02:59:49 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
05/26/2022 02:59:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:59:50 - INFO - __main__ - Printing 3 examples
05/26/2022 02:59:50 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/26/2022 02:59:50 - INFO - __main__ - ['others']
05/26/2022 02:59:50 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/26/2022 02:59:50 - INFO - __main__ - ['others']
05/26/2022 02:59:50 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/26/2022 02:59:50 - INFO - __main__ - ['others']
05/26/2022 02:59:50 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:59:50 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:59:50 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 02:59:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 02:59:50 - INFO - __main__ - Printing 3 examples
05/26/2022 02:59:50 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/26/2022 02:59:50 - INFO - __main__ - ['others']
05/26/2022 02:59:50 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/26/2022 02:59:50 - INFO - __main__ - ['others']
05/26/2022 02:59:50 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/26/2022 02:59:50 - INFO - __main__ - ['others']
05/26/2022 02:59:50 - INFO - __main__ - Tokenizing Input ...
05/26/2022 02:59:50 - INFO - __main__ - Tokenizing Output ...
05/26/2022 02:59:50 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 03:00:08 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 03:00:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 03:00:09 - INFO - __main__ - Starting training!
05/26/2022 03:00:13 - INFO - __main__ - Step 10 Global step 10 Train loss 2.49 on epoch=2
05/26/2022 03:00:15 - INFO - __main__ - Step 20 Global step 20 Train loss 1.19 on epoch=4
05/26/2022 03:00:18 - INFO - __main__ - Step 30 Global step 30 Train loss 1.03 on epoch=7
05/26/2022 03:00:21 - INFO - __main__ - Step 40 Global step 40 Train loss 1.01 on epoch=9
05/26/2022 03:00:24 - INFO - __main__ - Step 50 Global step 50 Train loss 0.99 on epoch=12
05/26/2022 03:00:25 - INFO - __main__ - Global step 50 Train loss 1.34 Classification-F1 0.1 on epoch=12
05/26/2022 03:00:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/26/2022 03:00:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.89 on epoch=14
05/26/2022 03:00:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.96 on epoch=17
05/26/2022 03:00:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.89 on epoch=19
05/26/2022 03:00:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.88 on epoch=22
05/26/2022 03:00:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.79 on epoch=24
05/26/2022 03:00:39 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.3013415892672859 on epoch=24
05/26/2022 03:00:39 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.3013415892672859 on epoch=24, global_step=100
05/26/2022 03:00:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.78 on epoch=27
05/26/2022 03:00:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.76 on epoch=29
05/26/2022 03:00:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.74 on epoch=32
05/26/2022 03:00:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.63 on epoch=34
05/26/2022 03:00:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.67 on epoch=37
05/26/2022 03:00:54 - INFO - __main__ - Global step 150 Train loss 0.72 Classification-F1 0.5887989203778677 on epoch=37
05/26/2022 03:00:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3013415892672859 -> 0.5887989203778677 on epoch=37, global_step=150
05/26/2022 03:00:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.61 on epoch=39
05/26/2022 03:00:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=42
05/26/2022 03:01:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=44
05/26/2022 03:01:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=47
05/26/2022 03:01:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=49
05/26/2022 03:01:08 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.507046332046332 on epoch=49
05/26/2022 03:01:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=52
05/26/2022 03:01:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.55 on epoch=54
05/26/2022 03:01:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.62 on epoch=57
05/26/2022 03:01:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=59
05/26/2022 03:01:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=62
05/26/2022 03:01:23 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.7443693693693694 on epoch=62
05/26/2022 03:01:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5887989203778677 -> 0.7443693693693694 on epoch=62, global_step=250
05/26/2022 03:01:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=64
05/26/2022 03:01:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=67
05/26/2022 03:01:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.35 on epoch=69
05/26/2022 03:01:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=72
05/26/2022 03:01:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=74
05/26/2022 03:01:38 - INFO - __main__ - Global step 300 Train loss 0.34 Classification-F1 0.5898268398268398 on epoch=74
05/26/2022 03:01:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.33 on epoch=77
05/26/2022 03:01:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=79
05/26/2022 03:01:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=82
05/26/2022 03:01:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=84
05/26/2022 03:01:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=87
05/26/2022 03:01:52 - INFO - __main__ - Global step 350 Train loss 0.31 Classification-F1 0.844410491469315 on epoch=87
05/26/2022 03:01:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7443693693693694 -> 0.844410491469315 on epoch=87, global_step=350
05/26/2022 03:01:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=89
05/26/2022 03:01:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=92
05/26/2022 03:02:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=94
05/26/2022 03:02:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=97
05/26/2022 03:02:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=99
05/26/2022 03:02:07 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.6730130249867092 on epoch=99
05/26/2022 03:02:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=102
05/26/2022 03:02:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=104
05/26/2022 03:02:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=107
05/26/2022 03:02:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=109
05/26/2022 03:02:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.11 on epoch=112
05/26/2022 03:02:21 - INFO - __main__ - Global step 450 Train loss 0.21 Classification-F1 0.7540808597780112 on epoch=112
05/26/2022 03:02:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=114
05/26/2022 03:02:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=117
05/26/2022 03:02:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=119
05/26/2022 03:02:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.12 on epoch=122
05/26/2022 03:02:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.16 on epoch=124
05/26/2022 03:02:36 - INFO - __main__ - Global step 500 Train loss 0.17 Classification-F1 0.7446969696969696 on epoch=124
05/26/2022 03:02:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=127
05/26/2022 03:02:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=129
05/26/2022 03:02:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=132
05/26/2022 03:02:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.07 on epoch=134
05/26/2022 03:02:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.11 on epoch=137
05/26/2022 03:02:51 - INFO - __main__ - Global step 550 Train loss 0.13 Classification-F1 0.7813472039814752 on epoch=137
05/26/2022 03:02:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=139
05/26/2022 03:02:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=142
05/26/2022 03:02:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=144
05/26/2022 03:03:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=147
05/26/2022 03:03:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=149
05/26/2022 03:03:05 - INFO - __main__ - Global step 600 Train loss 0.11 Classification-F1 0.7927625479807643 on epoch=149
05/26/2022 03:03:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=152
05/26/2022 03:03:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=154
05/26/2022 03:03:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=157
05/26/2022 03:03:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.09 on epoch=159
05/26/2022 03:03:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=162
05/26/2022 03:03:20 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.8115681756573597 on epoch=162
05/26/2022 03:03:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.09 on epoch=164
05/26/2022 03:03:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=167
05/26/2022 03:03:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=169
05/26/2022 03:03:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=172
05/26/2022 03:03:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=174
05/26/2022 03:03:34 - INFO - __main__ - Global step 700 Train loss 0.06 Classification-F1 0.7950310559006212 on epoch=174
05/26/2022 03:03:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=177
05/26/2022 03:03:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=179
05/26/2022 03:03:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=182
05/26/2022 03:03:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=184
05/26/2022 03:03:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=187
05/26/2022 03:03:49 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.8165584415584416 on epoch=187
05/26/2022 03:03:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=189
05/26/2022 03:03:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=192
05/26/2022 03:03:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=194
05/26/2022 03:04:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=197
05/26/2022 03:04:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=199
05/26/2022 03:04:04 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.696104242979243 on epoch=199
05/26/2022 03:04:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=202
05/26/2022 03:04:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=204
05/26/2022 03:04:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=207
05/26/2022 03:04:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=209
05/26/2022 03:04:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=212
05/26/2022 03:04:19 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.7222983930414271 on epoch=212
05/26/2022 03:04:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=214
05/26/2022 03:04:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=217
05/26/2022 03:04:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=219
05/26/2022 03:04:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=222
05/26/2022 03:04:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=224
05/26/2022 03:04:34 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.7723905723905724 on epoch=224
05/26/2022 03:04:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=227
05/26/2022 03:04:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
05/26/2022 03:04:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
05/26/2022 03:04:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=234
05/26/2022 03:04:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=237
05/26/2022 03:04:49 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.8006926406926408 on epoch=237
05/26/2022 03:04:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=239
05/26/2022 03:04:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
05/26/2022 03:04:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=244
05/26/2022 03:05:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
05/26/2022 03:05:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=249
05/26/2022 03:05:03 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.8245969716557953 on epoch=249
05/26/2022 03:05:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=252
05/26/2022 03:05:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
05/26/2022 03:05:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
05/26/2022 03:05:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=259
05/26/2022 03:05:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
05/26/2022 03:05:18 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.8245969716557953 on epoch=262
05/26/2022 03:05:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=264
05/26/2022 03:05:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
05/26/2022 03:05:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=269
05/26/2022 03:05:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
05/26/2022 03:05:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
05/26/2022 03:05:33 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.7450237670825905 on epoch=274
05/26/2022 03:05:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
05/26/2022 03:05:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/26/2022 03:05:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
05/26/2022 03:05:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/26/2022 03:05:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
05/26/2022 03:05:48 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.8369940476190477 on epoch=287
05/26/2022 03:05:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
05/26/2022 03:05:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/26/2022 03:05:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
05/26/2022 03:05:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=297
05/26/2022 03:06:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/26/2022 03:06:03 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.6975225225225224 on epoch=299
05/26/2022 03:06:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/26/2022 03:06:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
05/26/2022 03:06:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
05/26/2022 03:06:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/26/2022 03:06:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/26/2022 03:06:18 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.7262899896800826 on epoch=312
05/26/2022 03:06:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/26/2022 03:06:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=317
05/26/2022 03:06:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
05/26/2022 03:06:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
05/26/2022 03:06:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=324
05/26/2022 03:06:32 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.758982683982684 on epoch=324
05/26/2022 03:06:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=327
05/26/2022 03:06:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
05/26/2022 03:06:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/26/2022 03:06:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/26/2022 03:06:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
05/26/2022 03:06:47 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.8014069264069265 on epoch=337
05/26/2022 03:06:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/26/2022 03:06:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/26/2022 03:06:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=344
05/26/2022 03:06:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
05/26/2022 03:07:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/26/2022 03:07:02 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.8014705882352942 on epoch=349
05/26/2022 03:07:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=352
05/26/2022 03:07:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
05/26/2022 03:07:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/26/2022 03:07:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=359
05/26/2022 03:07:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/26/2022 03:07:17 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.7806753136037025 on epoch=362
05/26/2022 03:07:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
05/26/2022 03:07:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
05/26/2022 03:07:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/26/2022 03:07:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
05/26/2022 03:07:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/26/2022 03:07:32 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.7953811959566436 on epoch=374
05/26/2022 03:07:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
05/26/2022 03:07:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/26/2022 03:07:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/26/2022 03:07:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/26/2022 03:07:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
05/26/2022 03:07:47 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.7721059577677225 on epoch=387
05/26/2022 03:07:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/26/2022 03:07:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=392
05/26/2022 03:07:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
05/26/2022 03:07:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
05/26/2022 03:08:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/26/2022 03:08:01 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.7659313725490197 on epoch=399
05/26/2022 03:08:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
05/26/2022 03:08:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
05/26/2022 03:08:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/26/2022 03:08:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/26/2022 03:08:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/26/2022 03:08:16 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.786336898395722 on epoch=412
05/26/2022 03:08:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/26/2022 03:08:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/26/2022 03:08:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
05/26/2022 03:08:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
05/26/2022 03:08:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/26/2022 03:08:31 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7589126559714795 on epoch=424
05/26/2022 03:08:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
05/26/2022 03:08:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
05/26/2022 03:08:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/26/2022 03:08:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
05/26/2022 03:08:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/26/2022 03:08:46 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.7595444117183249 on epoch=437
05/26/2022 03:08:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/26/2022 03:08:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
05/26/2022 03:08:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/26/2022 03:08:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/26/2022 03:09:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/26/2022 03:09:01 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7360360360360361 on epoch=449
05/26/2022 03:09:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/26/2022 03:09:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/26/2022 03:09:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/26/2022 03:09:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/26/2022 03:09:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/26/2022 03:09:16 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7219696969696969 on epoch=462
05/26/2022 03:09:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/26/2022 03:09:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/26/2022 03:09:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/26/2022 03:09:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/26/2022 03:09:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/26/2022 03:09:31 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7360360360360361 on epoch=474
05/26/2022 03:09:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/26/2022 03:09:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/26/2022 03:09:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/26/2022 03:09:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/26/2022 03:09:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/26/2022 03:09:46 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.7223665223665223 on epoch=487
05/26/2022 03:09:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/26/2022 03:09:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/26/2022 03:09:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/26/2022 03:09:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/26/2022 03:09:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/26/2022 03:10:01 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.711036036036036 on epoch=499
05/26/2022 03:10:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
05/26/2022 03:10:06 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/26/2022 03:10:09 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/26/2022 03:10:12 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/26/2022 03:10:14 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/26/2022 03:10:16 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7228136446886447 on epoch=512
05/26/2022 03:10:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/26/2022 03:10:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/26/2022 03:10:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/26/2022 03:10:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/26/2022 03:10:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/26/2022 03:10:31 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.8354621848739495 on epoch=524
05/26/2022 03:10:33 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/26/2022 03:10:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/26/2022 03:10:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/26/2022 03:10:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/26/2022 03:10:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/26/2022 03:10:46 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.7452586974326104 on epoch=537
05/26/2022 03:10:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=539
05/26/2022 03:10:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/26/2022 03:10:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/26/2022 03:10:56 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/26/2022 03:10:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/26/2022 03:11:01 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6779638553832101 on epoch=549
05/26/2022 03:11:03 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/26/2022 03:11:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/26/2022 03:11:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/26/2022 03:11:12 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/26/2022 03:11:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/26/2022 03:11:16 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.7087079057667293 on epoch=562
05/26/2022 03:11:18 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/26/2022 03:11:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/26/2022 03:11:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/26/2022 03:11:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=572
05/26/2022 03:11:29 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/26/2022 03:11:30 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7235195669978278 on epoch=574
05/26/2022 03:11:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/26/2022 03:11:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/26/2022 03:11:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/26/2022 03:11:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/26/2022 03:11:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/26/2022 03:11:45 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7220771191359427 on epoch=587
05/26/2022 03:11:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/26/2022 03:11:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/26/2022 03:11:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/26/2022 03:11:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/26/2022 03:11:59 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/26/2022 03:12:00 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.766336898395722 on epoch=599
05/26/2022 03:12:03 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/26/2022 03:12:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/26/2022 03:12:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/26/2022 03:12:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/26/2022 03:12:14 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/26/2022 03:12:15 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.7227694905713482 on epoch=612
05/26/2022 03:12:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/26/2022 03:12:20 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/26/2022 03:12:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/26/2022 03:12:26 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/26/2022 03:12:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/26/2022 03:12:30 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.7079744816586923 on epoch=624
05/26/2022 03:12:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/26/2022 03:12:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
05/26/2022 03:12:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=632
05/26/2022 03:12:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/26/2022 03:12:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/26/2022 03:12:45 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.708407605466429 on epoch=637
05/26/2022 03:12:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
05/26/2022 03:12:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/26/2022 03:12:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/26/2022 03:12:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 03:12:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/26/2022 03:13:00 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6942396942396942 on epoch=649
05/26/2022 03:13:02 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/26/2022 03:13:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/26/2022 03:13:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/26/2022 03:13:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 03:13:13 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/26/2022 03:13:15 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.745093795093795 on epoch=662
05/26/2022 03:13:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/26/2022 03:13:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/26/2022 03:13:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/26/2022 03:13:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/26/2022 03:13:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/26/2022 03:13:29 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.7446969696969696 on epoch=674
05/26/2022 03:13:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/26/2022 03:13:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/26/2022 03:13:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/26/2022 03:13:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/26/2022 03:13:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/26/2022 03:13:44 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7736185383244207 on epoch=687
05/26/2022 03:13:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/26/2022 03:13:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/26/2022 03:13:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/26/2022 03:13:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/26/2022 03:13:58 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/26/2022 03:13:59 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7449486920075156 on epoch=699
05/26/2022 03:14:02 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/26/2022 03:14:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 03:14:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 03:14:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/26/2022 03:14:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/26/2022 03:14:14 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7313015018607123 on epoch=712
05/26/2022 03:14:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/26/2022 03:14:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/26/2022 03:14:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 03:14:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/26/2022 03:14:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/26/2022 03:14:29 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.766641321628534 on epoch=724
05/26/2022 03:14:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/26/2022 03:14:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 03:14:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/26/2022 03:14:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/26/2022 03:14:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 03:14:44 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7523943840069833 on epoch=737
05/26/2022 03:14:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/26/2022 03:14:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/26/2022 03:14:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/26/2022 03:14:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/26/2022 03:14:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 03:14:59 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7217503217503217 on epoch=749
05/26/2022 03:14:59 - INFO - __main__ - save last model!
05/26/2022 03:14:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 03:14:59 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 03:14:59 - INFO - __main__ - Printing 3 examples
05/26/2022 03:14:59 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 03:14:59 - INFO - __main__ - ['others']
05/26/2022 03:14:59 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 03:14:59 - INFO - __main__ - ['others']
05/26/2022 03:14:59 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 03:14:59 - INFO - __main__ - ['others']
05/26/2022 03:14:59 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:14:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:14:59 - INFO - __main__ - Printing 3 examples
05/26/2022 03:14:59 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/26/2022 03:14:59 - INFO - __main__ - ['others']
05/26/2022 03:14:59 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/26/2022 03:14:59 - INFO - __main__ - ['others']
05/26/2022 03:14:59 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/26/2022 03:14:59 - INFO - __main__ - ['others']
05/26/2022 03:14:59 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:14:59 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:14:59 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 03:14:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:14:59 - INFO - __main__ - Printing 3 examples
05/26/2022 03:14:59 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/26/2022 03:14:59 - INFO - __main__ - ['others']
05/26/2022 03:14:59 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/26/2022 03:14:59 - INFO - __main__ - ['others']
05/26/2022 03:14:59 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/26/2022 03:14:59 - INFO - __main__ - ['others']
05/26/2022 03:14:59 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:14:59 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:14:59 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 03:15:01 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:15:07 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 03:15:15 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 03:15:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 03:15:15 - INFO - __main__ - Starting training!
05/26/2022 03:16:57 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_87_0.5_8_predictions.txt
05/26/2022 03:16:57 - INFO - __main__ - Classification-F1 on test data: 0.1212
05/26/2022 03:16:57 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.844410491469315, test_performance=0.1212166483751579
05/26/2022 03:16:57 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
05/26/2022 03:16:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:16:58 - INFO - __main__ - Printing 3 examples
05/26/2022 03:16:58 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/26/2022 03:16:58 - INFO - __main__ - ['others']
05/26/2022 03:16:58 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/26/2022 03:16:58 - INFO - __main__ - ['others']
05/26/2022 03:16:58 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/26/2022 03:16:58 - INFO - __main__ - ['others']
05/26/2022 03:16:58 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:16:58 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:16:58 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 03:16:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:16:58 - INFO - __main__ - Printing 3 examples
05/26/2022 03:16:58 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/26/2022 03:16:58 - INFO - __main__ - ['others']
05/26/2022 03:16:58 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/26/2022 03:16:58 - INFO - __main__ - ['others']
05/26/2022 03:16:58 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/26/2022 03:16:58 - INFO - __main__ - ['others']
05/26/2022 03:16:58 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:16:58 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:16:58 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 03:17:17 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 03:17:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 03:17:18 - INFO - __main__ - Starting training!
05/26/2022 03:17:21 - INFO - __main__ - Step 10 Global step 10 Train loss 2.66 on epoch=2
05/26/2022 03:17:24 - INFO - __main__ - Step 20 Global step 20 Train loss 1.29 on epoch=4
05/26/2022 03:17:27 - INFO - __main__ - Step 30 Global step 30 Train loss 1.06 on epoch=7
05/26/2022 03:17:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=9
05/26/2022 03:17:32 - INFO - __main__ - Step 50 Global step 50 Train loss 0.89 on epoch=12
05/26/2022 03:17:33 - INFO - __main__ - Global step 50 Train loss 1.38 Classification-F1 0.1 on epoch=12
05/26/2022 03:17:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/26/2022 03:17:36 - INFO - __main__ - Step 60 Global step 60 Train loss 0.91 on epoch=14
05/26/2022 03:17:39 - INFO - __main__ - Step 70 Global step 70 Train loss 1.02 on epoch=17
05/26/2022 03:17:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=19
05/26/2022 03:17:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=22
05/26/2022 03:17:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=24
05/26/2022 03:17:48 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.3647837125280734 on epoch=24
05/26/2022 03:17:48 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.3647837125280734 on epoch=24, global_step=100
05/26/2022 03:17:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.80 on epoch=27
05/26/2022 03:17:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.70 on epoch=29
05/26/2022 03:17:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.75 on epoch=32
05/26/2022 03:17:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.70 on epoch=34
05/26/2022 03:18:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.65 on epoch=37
05/26/2022 03:18:03 - INFO - __main__ - Global step 150 Train loss 0.72 Classification-F1 0.49514652014652016 on epoch=37
05/26/2022 03:18:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3647837125280734 -> 0.49514652014652016 on epoch=37, global_step=150
05/26/2022 03:18:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=39
05/26/2022 03:18:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.60 on epoch=42
05/26/2022 03:18:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=44
05/26/2022 03:18:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=47
05/26/2022 03:18:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=49
05/26/2022 03:18:17 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.5910207718338254 on epoch=49
05/26/2022 03:18:17 - INFO - __main__ - Saving model with best Classification-F1: 0.49514652014652016 -> 0.5910207718338254 on epoch=49, global_step=200
05/26/2022 03:18:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=52
05/26/2022 03:18:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=54
05/26/2022 03:18:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=57
05/26/2022 03:18:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=59
05/26/2022 03:18:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=62
05/26/2022 03:18:32 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.687529178338002 on epoch=62
05/26/2022 03:18:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5910207718338254 -> 0.687529178338002 on epoch=62, global_step=250
05/26/2022 03:18:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=64
05/26/2022 03:18:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.34 on epoch=67
05/26/2022 03:18:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=69
05/26/2022 03:18:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.34 on epoch=72
05/26/2022 03:18:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=74
05/26/2022 03:18:47 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.6835273072115178 on epoch=74
05/26/2022 03:18:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=77
05/26/2022 03:18:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.32 on epoch=79
05/26/2022 03:18:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=82
05/26/2022 03:18:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=84
05/26/2022 03:19:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=87
05/26/2022 03:19:01 - INFO - __main__ - Global step 350 Train loss 0.31 Classification-F1 0.6820887445887447 on epoch=87
05/26/2022 03:19:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=89
05/26/2022 03:19:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=92
05/26/2022 03:19:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=94
05/26/2022 03:19:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=97
05/26/2022 03:19:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=99
05/26/2022 03:19:15 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.7026334776334776 on epoch=99
05/26/2022 03:19:15 - INFO - __main__ - Saving model with best Classification-F1: 0.687529178338002 -> 0.7026334776334776 on epoch=99, global_step=400
05/26/2022 03:19:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=102
05/26/2022 03:19:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=104
05/26/2022 03:19:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=107
05/26/2022 03:19:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.19 on epoch=109
05/26/2022 03:19:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=112
05/26/2022 03:19:30 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.7183080808080807 on epoch=112
05/26/2022 03:19:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7026334776334776 -> 0.7183080808080807 on epoch=112, global_step=450
05/26/2022 03:19:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=114
05/26/2022 03:19:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.14 on epoch=117
05/26/2022 03:19:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=119
05/26/2022 03:19:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=122
05/26/2022 03:19:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=124
05/26/2022 03:19:44 - INFO - __main__ - Global step 500 Train loss 0.17 Classification-F1 0.6976952184382526 on epoch=124
05/26/2022 03:19:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=127
05/26/2022 03:19:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.13 on epoch=129
05/26/2022 03:19:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=132
05/26/2022 03:19:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=134
05/26/2022 03:19:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=137
05/26/2022 03:19:59 - INFO - __main__ - Global step 550 Train loss 0.13 Classification-F1 0.751846312715878 on epoch=137
05/26/2022 03:19:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7183080808080807 -> 0.751846312715878 on epoch=137, global_step=550
05/26/2022 03:20:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=139
05/26/2022 03:20:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=142
05/26/2022 03:20:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.10 on epoch=144
05/26/2022 03:20:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=147
05/26/2022 03:20:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=149
05/26/2022 03:20:13 - INFO - __main__ - Global step 600 Train loss 0.11 Classification-F1 0.758080808080808 on epoch=149
05/26/2022 03:20:13 - INFO - __main__ - Saving model with best Classification-F1: 0.751846312715878 -> 0.758080808080808 on epoch=149, global_step=600
05/26/2022 03:20:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=152
05/26/2022 03:20:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=154
05/26/2022 03:20:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.06 on epoch=157
05/26/2022 03:20:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.09 on epoch=159
05/26/2022 03:20:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=162
05/26/2022 03:20:28 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.7304804804804805 on epoch=162
05/26/2022 03:20:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=164
05/26/2022 03:20:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=167
05/26/2022 03:20:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=169
05/26/2022 03:20:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=172
05/26/2022 03:20:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.17 on epoch=174
05/26/2022 03:20:43 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.7728758169934641 on epoch=174
05/26/2022 03:20:43 - INFO - __main__ - Saving model with best Classification-F1: 0.758080808080808 -> 0.7728758169934641 on epoch=174, global_step=700
05/26/2022 03:20:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=177
05/26/2022 03:20:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=179
05/26/2022 03:20:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=182
05/26/2022 03:20:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=184
05/26/2022 03:20:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=187
05/26/2022 03:20:57 - INFO - __main__ - Global step 750 Train loss 0.05 Classification-F1 0.7953811959566436 on epoch=187
05/26/2022 03:20:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7728758169934641 -> 0.7953811959566436 on epoch=187, global_step=750
05/26/2022 03:21:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=189
05/26/2022 03:21:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=192
05/26/2022 03:21:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=194
05/26/2022 03:21:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=197
05/26/2022 03:21:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=199
05/26/2022 03:21:12 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.7522091073271414 on epoch=199
05/26/2022 03:21:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=202
05/26/2022 03:21:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=204
05/26/2022 03:21:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=207
05/26/2022 03:21:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=209
05/26/2022 03:21:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=212
05/26/2022 03:21:27 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.7914583333333334 on epoch=212
05/26/2022 03:21:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=214
05/26/2022 03:21:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=217
05/26/2022 03:21:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=219
05/26/2022 03:21:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=222
05/26/2022 03:21:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=224
05/26/2022 03:21:42 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.7604855067722307 on epoch=224
05/26/2022 03:21:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=227
05/26/2022 03:21:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
05/26/2022 03:21:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
05/26/2022 03:21:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=234
05/26/2022 03:21:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
05/26/2022 03:21:56 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.7520805423618723 on epoch=237
05/26/2022 03:21:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=239
05/26/2022 03:22:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
05/26/2022 03:22:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
05/26/2022 03:22:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=247
05/26/2022 03:22:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
05/26/2022 03:22:11 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.74585326953748 on epoch=249
05/26/2022 03:22:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=252
05/26/2022 03:22:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=254
05/26/2022 03:22:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=257
05/26/2022 03:22:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/26/2022 03:22:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
05/26/2022 03:22:26 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.6936507936507936 on epoch=262
05/26/2022 03:22:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=264
05/26/2022 03:22:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
05/26/2022 03:22:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/26/2022 03:22:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=272
05/26/2022 03:22:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
05/26/2022 03:22:41 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.7608153907496014 on epoch=274
05/26/2022 03:22:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
05/26/2022 03:22:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/26/2022 03:22:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
05/26/2022 03:22:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/26/2022 03:22:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
05/26/2022 03:22:56 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.7321184821184821 on epoch=287
05/26/2022 03:22:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
05/26/2022 03:23:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
05/26/2022 03:23:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
05/26/2022 03:23:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
05/26/2022 03:23:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
05/26/2022 03:23:11 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.7269669669669669 on epoch=299
05/26/2022 03:23:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/26/2022 03:23:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/26/2022 03:23:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/26/2022 03:23:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
05/26/2022 03:23:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/26/2022 03:23:26 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.7378881987577639 on epoch=312
05/26/2022 03:23:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/26/2022 03:23:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/26/2022 03:23:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
05/26/2022 03:23:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
05/26/2022 03:23:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/26/2022 03:23:41 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.7781926406926407 on epoch=324
05/26/2022 03:23:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
05/26/2022 03:23:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=329
05/26/2022 03:23:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/26/2022 03:23:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/26/2022 03:23:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
05/26/2022 03:23:56 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.7724943693693693 on epoch=337
05/26/2022 03:23:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/26/2022 03:24:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=342
05/26/2022 03:24:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=344
05/26/2022 03:24:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/26/2022 03:24:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/26/2022 03:24:11 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.7586805555555555 on epoch=349
05/26/2022 03:24:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=352
05/26/2022 03:24:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/26/2022 03:24:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
05/26/2022 03:24:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=359
05/26/2022 03:24:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
05/26/2022 03:24:26 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.7389325466771118 on epoch=362
05/26/2022 03:24:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=364
05/26/2022 03:24:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/26/2022 03:24:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/26/2022 03:24:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
05/26/2022 03:24:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
05/26/2022 03:24:41 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7767991610905822 on epoch=374
05/26/2022 03:24:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/26/2022 03:24:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/26/2022 03:24:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/26/2022 03:24:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
05/26/2022 03:24:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/26/2022 03:24:55 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.7175438596491228 on epoch=387
05/26/2022 03:24:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
05/26/2022 03:25:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/26/2022 03:25:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/26/2022 03:25:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
05/26/2022 03:25:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/26/2022 03:25:10 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.7722214192802429 on epoch=399
05/26/2022 03:25:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/26/2022 03:25:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/26/2022 03:25:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/26/2022 03:25:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/26/2022 03:25:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
05/26/2022 03:25:25 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7523943840069833 on epoch=412
05/26/2022 03:25:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/26/2022 03:25:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/26/2022 03:25:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
05/26/2022 03:25:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/26/2022 03:25:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=424
05/26/2022 03:25:40 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.756969696969697 on epoch=424
05/26/2022 03:25:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
05/26/2022 03:25:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
05/26/2022 03:25:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
05/26/2022 03:25:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/26/2022 03:25:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/26/2022 03:25:55 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7800439735923607 on epoch=437
05/26/2022 03:25:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/26/2022 03:26:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
05/26/2022 03:26:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/26/2022 03:26:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/26/2022 03:26:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/26/2022 03:26:10 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7714583333333334 on epoch=449
05/26/2022 03:26:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/26/2022 03:26:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/26/2022 03:26:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/26/2022 03:26:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
05/26/2022 03:26:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/26/2022 03:26:25 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7376885016540421 on epoch=462
05/26/2022 03:26:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/26/2022 03:26:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/26/2022 03:26:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
05/26/2022 03:26:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/26/2022 03:26:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/26/2022 03:26:40 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.787372934431758 on epoch=474
05/26/2022 03:26:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
05/26/2022 03:26:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/26/2022 03:26:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/26/2022 03:26:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/26/2022 03:26:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/26/2022 03:26:55 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7525007104290992 on epoch=487
05/26/2022 03:26:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/26/2022 03:27:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=492
05/26/2022 03:27:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=494
05/26/2022 03:27:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/26/2022 03:27:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/26/2022 03:27:10 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.7763573232323232 on epoch=499
05/26/2022 03:27:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=502
05/26/2022 03:27:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/26/2022 03:27:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/26/2022 03:27:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/26/2022 03:27:24 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/26/2022 03:27:25 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7724480095068331 on epoch=512
05/26/2022 03:27:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
05/26/2022 03:27:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/26/2022 03:27:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/26/2022 03:27:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/26/2022 03:27:39 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/26/2022 03:27:40 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7905011655011656 on epoch=524
05/26/2022 03:27:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/26/2022 03:27:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=529
05/26/2022 03:27:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/26/2022 03:27:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/26/2022 03:27:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/26/2022 03:27:55 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.786764705882353 on epoch=537
05/26/2022 03:27:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/26/2022 03:28:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/26/2022 03:28:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/26/2022 03:28:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/26/2022 03:28:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/26/2022 03:28:10 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.771764705882353 on epoch=549
05/26/2022 03:28:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/26/2022 03:28:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/26/2022 03:28:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/26/2022 03:28:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/26/2022 03:28:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/26/2022 03:28:25 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.7763573232323232 on epoch=562
05/26/2022 03:28:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/26/2022 03:28:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/26/2022 03:28:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/26/2022 03:28:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/26/2022 03:28:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/26/2022 03:28:40 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.6223169191919192 on epoch=574
05/26/2022 03:28:43 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
05/26/2022 03:28:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=579
05/26/2022 03:28:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/26/2022 03:28:51 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/26/2022 03:28:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/26/2022 03:28:55 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7566982037570273 on epoch=587
05/26/2022 03:28:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/26/2022 03:29:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
05/26/2022 03:29:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/26/2022 03:29:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/26/2022 03:29:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/26/2022 03:29:10 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7373366013071896 on epoch=599
05/26/2022 03:29:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/26/2022 03:29:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/26/2022 03:29:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/26/2022 03:29:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/26/2022 03:29:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/26/2022 03:29:25 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.7221667690417691 on epoch=612
05/26/2022 03:29:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/26/2022 03:29:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/26/2022 03:29:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/26/2022 03:29:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/26/2022 03:29:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/26/2022 03:29:40 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.7421449792038027 on epoch=624
05/26/2022 03:29:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/26/2022 03:29:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/26/2022 03:29:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/26/2022 03:29:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/26/2022 03:29:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/26/2022 03:29:55 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6932997557997558 on epoch=637
05/26/2022 03:29:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/26/2022 03:30:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/26/2022 03:30:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/26/2022 03:30:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 03:30:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/26/2022 03:30:10 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7228809837505489 on epoch=649
05/26/2022 03:30:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/26/2022 03:30:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
05/26/2022 03:30:18 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/26/2022 03:30:21 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 03:30:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/26/2022 03:30:25 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7614718614718615 on epoch=662
05/26/2022 03:30:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/26/2022 03:30:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/26/2022 03:30:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/26/2022 03:30:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=672
05/26/2022 03:30:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/26/2022 03:30:40 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7581699346405228 on epoch=674
05/26/2022 03:30:42 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/26/2022 03:30:45 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/26/2022 03:30:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/26/2022 03:30:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/26/2022 03:30:53 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/26/2022 03:30:54 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7377948280761579 on epoch=687
05/26/2022 03:30:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/26/2022 03:31:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/26/2022 03:31:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=694
05/26/2022 03:31:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/26/2022 03:31:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/26/2022 03:31:09 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7763573232323232 on epoch=699
05/26/2022 03:31:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/26/2022 03:31:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/26/2022 03:31:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 03:31:20 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/26/2022 03:31:23 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/26/2022 03:31:24 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7763573232323232 on epoch=712
05/26/2022 03:31:27 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/26/2022 03:31:30 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/26/2022 03:31:32 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 03:31:35 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=722
05/26/2022 03:31:38 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/26/2022 03:31:39 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.7458301502419149 on epoch=724
05/26/2022 03:31:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/26/2022 03:31:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 03:31:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/26/2022 03:31:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/26/2022 03:31:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
05/26/2022 03:31:54 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7601450257522364 on epoch=737
05/26/2022 03:31:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/26/2022 03:32:00 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/26/2022 03:32:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/26/2022 03:32:05 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/26/2022 03:32:08 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 03:32:09 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7582633053221288 on epoch=749
05/26/2022 03:32:09 - INFO - __main__ - save last model!
05/26/2022 03:32:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 03:32:09 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 03:32:09 - INFO - __main__ - Printing 3 examples
05/26/2022 03:32:09 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 03:32:09 - INFO - __main__ - ['others']
05/26/2022 03:32:09 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 03:32:09 - INFO - __main__ - ['others']
05/26/2022 03:32:09 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 03:32:09 - INFO - __main__ - ['others']
05/26/2022 03:32:09 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:32:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:32:09 - INFO - __main__ - Printing 3 examples
05/26/2022 03:32:09 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/26/2022 03:32:09 - INFO - __main__ - ['others']
05/26/2022 03:32:09 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/26/2022 03:32:09 - INFO - __main__ - ['others']
05/26/2022 03:32:09 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/26/2022 03:32:09 - INFO - __main__ - ['others']
05/26/2022 03:32:09 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:32:09 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:32:09 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 03:32:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:32:09 - INFO - __main__ - Printing 3 examples
05/26/2022 03:32:09 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/26/2022 03:32:09 - INFO - __main__ - ['others']
05/26/2022 03:32:09 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/26/2022 03:32:09 - INFO - __main__ - ['others']
05/26/2022 03:32:09 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/26/2022 03:32:09 - INFO - __main__ - ['others']
05/26/2022 03:32:09 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:32:09 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:32:09 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 03:32:11 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:32:17 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 03:32:28 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 03:32:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 03:32:29 - INFO - __main__ - Starting training!
05/26/2022 03:34:06 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_87_0.4_8_predictions.txt
05/26/2022 03:34:06 - INFO - __main__ - Classification-F1 on test data: 0.0834
05/26/2022 03:34:06 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.7953811959566436, test_performance=0.08336442896622882
05/26/2022 03:34:06 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
05/26/2022 03:34:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:34:07 - INFO - __main__ - Printing 3 examples
05/26/2022 03:34:07 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/26/2022 03:34:07 - INFO - __main__ - ['others']
05/26/2022 03:34:07 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/26/2022 03:34:07 - INFO - __main__ - ['others']
05/26/2022 03:34:07 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/26/2022 03:34:07 - INFO - __main__ - ['others']
05/26/2022 03:34:07 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:34:07 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:34:07 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 03:34:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:34:07 - INFO - __main__ - Printing 3 examples
05/26/2022 03:34:07 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/26/2022 03:34:07 - INFO - __main__ - ['others']
05/26/2022 03:34:07 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/26/2022 03:34:07 - INFO - __main__ - ['others']
05/26/2022 03:34:07 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/26/2022 03:34:07 - INFO - __main__ - ['others']
05/26/2022 03:34:07 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:34:07 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:34:07 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 03:34:23 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 03:34:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 03:34:23 - INFO - __main__ - Starting training!
05/26/2022 03:34:27 - INFO - __main__ - Step 10 Global step 10 Train loss 2.90 on epoch=2
05/26/2022 03:34:29 - INFO - __main__ - Step 20 Global step 20 Train loss 1.35 on epoch=4
05/26/2022 03:34:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.13 on epoch=7
05/26/2022 03:34:35 - INFO - __main__ - Step 40 Global step 40 Train loss 1.01 on epoch=9
05/26/2022 03:34:37 - INFO - __main__ - Step 50 Global step 50 Train loss 0.93 on epoch=12
05/26/2022 03:34:38 - INFO - __main__ - Global step 50 Train loss 1.46 Classification-F1 0.1 on epoch=12
05/26/2022 03:34:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/26/2022 03:34:41 - INFO - __main__ - Step 60 Global step 60 Train loss 0.87 on epoch=14
05/26/2022 03:34:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.83 on epoch=17
05/26/2022 03:34:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.82 on epoch=19
05/26/2022 03:34:49 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=22
05/26/2022 03:34:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.85 on epoch=24
05/26/2022 03:34:53 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.2928104575163399 on epoch=24
05/26/2022 03:34:53 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.2928104575163399 on epoch=24, global_step=100
05/26/2022 03:34:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.81 on epoch=27
05/26/2022 03:34:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.81 on epoch=29
05/26/2022 03:35:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.76 on epoch=32
05/26/2022 03:35:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.80 on epoch=34
05/26/2022 03:35:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.77 on epoch=37
05/26/2022 03:35:07 - INFO - __main__ - Global step 150 Train loss 0.79 Classification-F1 0.37681061628430046 on epoch=37
05/26/2022 03:35:07 - INFO - __main__ - Saving model with best Classification-F1: 0.2928104575163399 -> 0.37681061628430046 on epoch=37, global_step=150
05/26/2022 03:35:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.78 on epoch=39
05/26/2022 03:35:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.73 on epoch=42
05/26/2022 03:35:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.63 on epoch=44
05/26/2022 03:35:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.58 on epoch=47
05/26/2022 03:35:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.59 on epoch=49
05/26/2022 03:35:21 - INFO - __main__ - Global step 200 Train loss 0.66 Classification-F1 0.4852040816326531 on epoch=49
05/26/2022 03:35:21 - INFO - __main__ - Saving model with best Classification-F1: 0.37681061628430046 -> 0.4852040816326531 on epoch=49, global_step=200
05/26/2022 03:35:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=52
05/26/2022 03:35:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.61 on epoch=54
05/26/2022 03:35:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=57
05/26/2022 03:35:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.54 on epoch=59
05/26/2022 03:35:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=62
05/26/2022 03:35:36 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.6938945712037765 on epoch=62
05/26/2022 03:35:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4852040816326531 -> 0.6938945712037765 on epoch=62, global_step=250
05/26/2022 03:35:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.56 on epoch=64
05/26/2022 03:35:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=67
05/26/2022 03:35:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=69
05/26/2022 03:35:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=72
05/26/2022 03:35:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=74
05/26/2022 03:35:50 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.5347222222222222 on epoch=74
05/26/2022 03:35:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=77
05/26/2022 03:35:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=79
05/26/2022 03:35:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=82
05/26/2022 03:36:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=84
05/26/2022 03:36:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=87
05/26/2022 03:36:04 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.6919934640522876 on epoch=87
05/26/2022 03:36:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=89
05/26/2022 03:36:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.29 on epoch=92
05/26/2022 03:36:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=94
05/26/2022 03:36:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=97
05/26/2022 03:36:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=99
05/26/2022 03:36:19 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.680568824047085 on epoch=99
05/26/2022 03:36:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.31 on epoch=102
05/26/2022 03:36:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=104
05/26/2022 03:36:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=107
05/26/2022 03:36:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=109
05/26/2022 03:36:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=112
05/26/2022 03:36:33 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.7904876373626374 on epoch=112
05/26/2022 03:36:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6938945712037765 -> 0.7904876373626374 on epoch=112, global_step=450
05/26/2022 03:36:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=114
05/26/2022 03:36:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=117
05/26/2022 03:36:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=119
05/26/2022 03:36:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=122
05/26/2022 03:36:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=124
05/26/2022 03:36:48 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.6993535327151627 on epoch=124
05/26/2022 03:36:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=127
05/26/2022 03:36:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.31 on epoch=129
05/26/2022 03:36:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=132
05/26/2022 03:36:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=134
05/26/2022 03:37:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=137
05/26/2022 03:37:02 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.6712405682993918 on epoch=137
05/26/2022 03:37:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=139
05/26/2022 03:37:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=142
05/26/2022 03:37:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=144
05/26/2022 03:37:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=147
05/26/2022 03:37:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=149
05/26/2022 03:37:16 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.6568759757614246 on epoch=149
05/26/2022 03:37:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.16 on epoch=152
05/26/2022 03:37:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=154
05/26/2022 03:37:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=157
05/26/2022 03:37:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=159
05/26/2022 03:37:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=162
05/26/2022 03:37:31 - INFO - __main__ - Global step 650 Train loss 0.13 Classification-F1 0.6835273072115178 on epoch=162
05/26/2022 03:37:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=164
05/26/2022 03:37:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
05/26/2022 03:37:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=169
05/26/2022 03:37:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.07 on epoch=172
05/26/2022 03:37:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.10 on epoch=174
05/26/2022 03:37:45 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.7073328482496565 on epoch=174
05/26/2022 03:37:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=177
05/26/2022 03:37:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=179
05/26/2022 03:37:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=182
05/26/2022 03:37:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.12 on epoch=184
05/26/2022 03:37:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=187
05/26/2022 03:38:00 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.7087302292894399 on epoch=187
05/26/2022 03:38:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=189
05/26/2022 03:38:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=192
05/26/2022 03:38:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=194
05/26/2022 03:38:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=197
05/26/2022 03:38:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=199
05/26/2022 03:38:14 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.6837218337218337 on epoch=199
05/26/2022 03:38:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=202
05/26/2022 03:38:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=204
05/26/2022 03:38:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=207
05/26/2022 03:38:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=209
05/26/2022 03:38:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=212
05/26/2022 03:38:29 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.6970408207250313 on epoch=212
05/26/2022 03:38:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=214
05/26/2022 03:38:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=217
05/26/2022 03:38:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=219
05/26/2022 03:38:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=222
05/26/2022 03:38:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
05/26/2022 03:38:43 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.7161248278575645 on epoch=224
05/26/2022 03:38:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=227
05/26/2022 03:38:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
05/26/2022 03:38:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
05/26/2022 03:38:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=234
05/26/2022 03:38:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
05/26/2022 03:38:58 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.7073712367830015 on epoch=237
05/26/2022 03:39:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=239
05/26/2022 03:39:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=242
05/26/2022 03:39:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
05/26/2022 03:39:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=247
05/26/2022 03:39:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=249
05/26/2022 03:39:12 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.6706273640276509 on epoch=249
05/26/2022 03:39:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
05/26/2022 03:39:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
05/26/2022 03:39:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=257
05/26/2022 03:39:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
05/26/2022 03:39:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=262
05/26/2022 03:39:27 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.683751089799477 on epoch=262
05/26/2022 03:39:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
05/26/2022 03:39:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=267
05/26/2022 03:39:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=269
05/26/2022 03:39:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
05/26/2022 03:39:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
05/26/2022 03:39:42 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.6694444444444444 on epoch=274
05/26/2022 03:39:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=277
05/26/2022 03:39:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
05/26/2022 03:39:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
05/26/2022 03:39:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
05/26/2022 03:39:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
05/26/2022 03:39:56 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.6840996840996841 on epoch=287
05/26/2022 03:39:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
05/26/2022 03:40:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
05/26/2022 03:40:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
05/26/2022 03:40:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
05/26/2022 03:40:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
05/26/2022 03:40:11 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.6970408207250313 on epoch=299
05/26/2022 03:40:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
05/26/2022 03:40:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
05/26/2022 03:40:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/26/2022 03:40:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/26/2022 03:40:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
05/26/2022 03:40:25 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.7567991610905823 on epoch=312
05/26/2022 03:40:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/26/2022 03:40:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/26/2022 03:40:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
05/26/2022 03:40:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
05/26/2022 03:40:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/26/2022 03:40:40 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.694047619047619 on epoch=324
05/26/2022 03:40:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
05/26/2022 03:40:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
05/26/2022 03:40:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
05/26/2022 03:40:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
05/26/2022 03:40:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/26/2022 03:40:54 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.7448940417690417 on epoch=337
05/26/2022 03:40:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/26/2022 03:41:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/26/2022 03:41:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/26/2022 03:41:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/26/2022 03:41:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
05/26/2022 03:41:09 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.7368705855547961 on epoch=349
05/26/2022 03:41:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/26/2022 03:41:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
05/26/2022 03:41:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
05/26/2022 03:41:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/26/2022 03:41:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/26/2022 03:41:24 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.7217503217503217 on epoch=362
05/26/2022 03:41:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
05/26/2022 03:41:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/26/2022 03:41:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/26/2022 03:41:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/26/2022 03:41:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
05/26/2022 03:41:39 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.7381410256410256 on epoch=374
05/26/2022 03:41:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/26/2022 03:41:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
05/26/2022 03:41:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/26/2022 03:41:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/26/2022 03:41:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/26/2022 03:41:53 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7171869193608322 on epoch=387
05/26/2022 03:41:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
05/26/2022 03:41:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/26/2022 03:42:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
05/26/2022 03:42:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/26/2022 03:42:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/26/2022 03:42:08 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.7086532555282555 on epoch=399
05/26/2022 03:42:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=402
05/26/2022 03:42:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/26/2022 03:42:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/26/2022 03:42:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/26/2022 03:42:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/26/2022 03:42:23 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.698135395010395 on epoch=412
05/26/2022 03:42:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/26/2022 03:42:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/26/2022 03:42:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
05/26/2022 03:42:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
05/26/2022 03:42:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/26/2022 03:42:37 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.694047619047619 on epoch=424
05/26/2022 03:42:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/26/2022 03:42:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/26/2022 03:42:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
05/26/2022 03:42:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/26/2022 03:42:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/26/2022 03:42:52 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7446504688832055 on epoch=437
05/26/2022 03:42:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/26/2022 03:42:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
05/26/2022 03:43:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/26/2022 03:43:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/26/2022 03:43:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/26/2022 03:43:06 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6971471471471471 on epoch=449
05/26/2022 03:43:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/26/2022 03:43:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/26/2022 03:43:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/26/2022 03:43:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/26/2022 03:43:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=462
05/26/2022 03:43:21 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.684469696969697 on epoch=462
05/26/2022 03:43:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/26/2022 03:43:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/26/2022 03:43:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
05/26/2022 03:43:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/26/2022 03:43:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/26/2022 03:43:36 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7368705855547961 on epoch=474
05/26/2022 03:43:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/26/2022 03:43:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=479
05/26/2022 03:43:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/26/2022 03:43:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/26/2022 03:43:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/26/2022 03:43:50 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7228136446886447 on epoch=487
05/26/2022 03:43:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/26/2022 03:43:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/26/2022 03:43:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/26/2022 03:44:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/26/2022 03:44:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
05/26/2022 03:44:05 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7449486920075155 on epoch=499
05/26/2022 03:44:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/26/2022 03:44:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/26/2022 03:44:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/26/2022 03:44:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/26/2022 03:44:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/26/2022 03:44:20 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.7087302292894399 on epoch=512
05/26/2022 03:44:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/26/2022 03:44:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/26/2022 03:44:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/26/2022 03:44:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/26/2022 03:44:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
05/26/2022 03:44:34 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.721719070403281 on epoch=524
05/26/2022 03:44:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/26/2022 03:44:40 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/26/2022 03:44:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/26/2022 03:44:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/26/2022 03:44:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/26/2022 03:44:49 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7228136446886447 on epoch=537
05/26/2022 03:44:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/26/2022 03:44:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/26/2022 03:44:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/26/2022 03:45:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/26/2022 03:45:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/26/2022 03:45:04 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6865330198946498 on epoch=549
05/26/2022 03:45:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/26/2022 03:45:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
05/26/2022 03:45:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/26/2022 03:45:15 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/26/2022 03:45:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/26/2022 03:45:19 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6830827067669173 on epoch=562
05/26/2022 03:45:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/26/2022 03:45:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/26/2022 03:45:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/26/2022 03:45:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/26/2022 03:45:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/26/2022 03:45:33 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.684469696969697 on epoch=574
05/26/2022 03:45:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/26/2022 03:45:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/26/2022 03:45:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/26/2022 03:45:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/26/2022 03:45:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/26/2022 03:45:48 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7310823051312181 on epoch=587
05/26/2022 03:45:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=589
05/26/2022 03:45:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/26/2022 03:45:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/26/2022 03:45:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/26/2022 03:46:01 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/26/2022 03:46:03 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.7090858480661112 on epoch=599
05/26/2022 03:46:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/26/2022 03:46:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/26/2022 03:46:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/26/2022 03:46:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=609
05/26/2022 03:46:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/26/2022 03:46:17 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7313805282555282 on epoch=612
05/26/2022 03:46:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/26/2022 03:46:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/26/2022 03:46:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/26/2022 03:46:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/26/2022 03:46:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/26/2022 03:46:32 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7090858480661112 on epoch=624
05/26/2022 03:46:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/26/2022 03:46:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/26/2022 03:46:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/26/2022 03:46:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/26/2022 03:46:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/26/2022 03:46:47 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.7090858480661112 on epoch=637
05/26/2022 03:46:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/26/2022 03:46:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/26/2022 03:46:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/26/2022 03:46:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 03:47:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/26/2022 03:47:01 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6853148821898822 on epoch=649
05/26/2022 03:47:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/26/2022 03:47:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/26/2022 03:47:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/26/2022 03:47:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 03:47:15 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/26/2022 03:47:16 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7090858480661112 on epoch=662
05/26/2022 03:47:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
05/26/2022 03:47:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/26/2022 03:47:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/26/2022 03:47:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/26/2022 03:47:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=674
05/26/2022 03:47:31 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7359660080248316 on epoch=674
05/26/2022 03:47:34 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/26/2022 03:47:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/26/2022 03:47:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/26/2022 03:47:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/26/2022 03:47:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/26/2022 03:47:46 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7228136446886447 on epoch=687
05/26/2022 03:47:48 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/26/2022 03:47:51 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/26/2022 03:47:54 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=694
05/26/2022 03:47:57 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/26/2022 03:47:59 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/26/2022 03:48:01 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7220771191359427 on epoch=699
05/26/2022 03:48:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/26/2022 03:48:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 03:48:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 03:48:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/26/2022 03:48:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/26/2022 03:48:15 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.787372934431758 on epoch=712
05/26/2022 03:48:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/26/2022 03:48:21 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/26/2022 03:48:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 03:48:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/26/2022 03:48:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/26/2022 03:48:30 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7456319956319957 on epoch=724
05/26/2022 03:48:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/26/2022 03:48:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 03:48:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/26/2022 03:48:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/26/2022 03:48:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 03:48:45 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7220771191359427 on epoch=737
05/26/2022 03:48:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=739
05/26/2022 03:48:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/26/2022 03:48:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/26/2022 03:48:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/26/2022 03:48:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 03:49:00 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7230607230607231 on epoch=749
05/26/2022 03:49:00 - INFO - __main__ - save last model!
05/26/2022 03:49:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 03:49:00 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 03:49:00 - INFO - __main__ - Printing 3 examples
05/26/2022 03:49:00 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 03:49:00 - INFO - __main__ - ['others']
05/26/2022 03:49:00 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 03:49:00 - INFO - __main__ - ['others']
05/26/2022 03:49:00 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 03:49:00 - INFO - __main__ - ['others']
05/26/2022 03:49:00 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:49:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:49:00 - INFO - __main__ - Printing 3 examples
05/26/2022 03:49:00 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/26/2022 03:49:00 - INFO - __main__ - ['others']
05/26/2022 03:49:00 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/26/2022 03:49:00 - INFO - __main__ - ['others']
05/26/2022 03:49:00 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/26/2022 03:49:00 - INFO - __main__ - ['others']
05/26/2022 03:49:00 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:49:00 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:49:00 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 03:49:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:49:00 - INFO - __main__ - Printing 3 examples
05/26/2022 03:49:00 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/26/2022 03:49:00 - INFO - __main__ - ['others']
05/26/2022 03:49:00 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/26/2022 03:49:00 - INFO - __main__ - ['others']
05/26/2022 03:49:00 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/26/2022 03:49:00 - INFO - __main__ - ['others']
05/26/2022 03:49:00 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:49:00 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:49:00 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 03:49:02 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:49:07 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 03:49:15 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 03:49:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 03:49:16 - INFO - __main__ - Starting training!
05/26/2022 03:50:56 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_87_0.3_8_predictions.txt
05/26/2022 03:50:56 - INFO - __main__ - Classification-F1 on test data: 0.0651
05/26/2022 03:50:56 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.7904876373626374, test_performance=0.06509901904597903
05/26/2022 03:50:56 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
05/26/2022 03:50:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:50:57 - INFO - __main__ - Printing 3 examples
05/26/2022 03:50:57 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/26/2022 03:50:57 - INFO - __main__ - ['others']
05/26/2022 03:50:57 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/26/2022 03:50:57 - INFO - __main__ - ['others']
05/26/2022 03:50:57 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/26/2022 03:50:57 - INFO - __main__ - ['others']
05/26/2022 03:50:57 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:50:57 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:50:58 - INFO - __main__ - Loaded 64 examples from train data
05/26/2022 03:50:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/26/2022 03:50:58 - INFO - __main__ - Printing 3 examples
05/26/2022 03:50:58 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/26/2022 03:50:58 - INFO - __main__ - ['others']
05/26/2022 03:50:58 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/26/2022 03:50:58 - INFO - __main__ - ['others']
05/26/2022 03:50:58 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/26/2022 03:50:58 - INFO - __main__ - ['others']
05/26/2022 03:50:58 - INFO - __main__ - Tokenizing Input ...
05/26/2022 03:50:58 - INFO - __main__ - Tokenizing Output ...
05/26/2022 03:50:58 - INFO - __main__ - Loaded 64 examples from dev data
05/26/2022 03:51:13 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 03:51:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 747.07M parameters
05/26/2022 03:51:13 - INFO - __main__ - Starting training!
05/26/2022 03:51:17 - INFO - __main__ - Step 10 Global step 10 Train loss 3.18 on epoch=2
05/26/2022 03:51:19 - INFO - __main__ - Step 20 Global step 20 Train loss 1.80 on epoch=4
05/26/2022 03:51:22 - INFO - __main__ - Step 30 Global step 30 Train loss 1.28 on epoch=7
05/26/2022 03:51:25 - INFO - __main__ - Step 40 Global step 40 Train loss 1.13 on epoch=9
05/26/2022 03:51:27 - INFO - __main__ - Step 50 Global step 50 Train loss 1.03 on epoch=12
05/26/2022 03:51:28 - INFO - __main__ - Global step 50 Train loss 1.68 Classification-F1 0.1 on epoch=12
05/26/2022 03:51:28 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/26/2022 03:51:31 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=14
05/26/2022 03:51:34 - INFO - __main__ - Step 70 Global step 70 Train loss 0.91 on epoch=17
05/26/2022 03:51:36 - INFO - __main__ - Step 80 Global step 80 Train loss 0.94 on epoch=19
05/26/2022 03:51:39 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=22
05/26/2022 03:51:42 - INFO - __main__ - Step 100 Global step 100 Train loss 0.86 on epoch=24
05/26/2022 03:51:43 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.1888634241575418 on epoch=24
05/26/2022 03:51:43 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1888634241575418 on epoch=24, global_step=100
05/26/2022 03:51:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.88 on epoch=27
05/26/2022 03:51:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.86 on epoch=29
05/26/2022 03:51:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=32
05/26/2022 03:51:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=34
05/26/2022 03:51:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.81 on epoch=37
05/26/2022 03:51:57 - INFO - __main__ - Global step 150 Train loss 0.86 Classification-F1 0.21362229102167182 on epoch=37
05/26/2022 03:51:57 - INFO - __main__ - Saving model with best Classification-F1: 0.1888634241575418 -> 0.21362229102167182 on epoch=37, global_step=150
05/26/2022 03:52:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.75 on epoch=39
05/26/2022 03:52:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.82 on epoch=42
05/26/2022 03:52:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.78 on epoch=44
05/26/2022 03:52:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.85 on epoch=47
05/26/2022 03:52:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.77 on epoch=49
05/26/2022 03:52:11 - INFO - __main__ - Global step 200 Train loss 0.79 Classification-F1 0.4081196581196581 on epoch=49
05/26/2022 03:52:11 - INFO - __main__ - Saving model with best Classification-F1: 0.21362229102167182 -> 0.4081196581196581 on epoch=49, global_step=200
05/26/2022 03:52:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.62 on epoch=52
05/26/2022 03:52:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.71 on epoch=54
05/26/2022 03:52:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.63 on epoch=57
05/26/2022 03:52:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.63 on epoch=59
05/26/2022 03:52:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.65 on epoch=62
05/26/2022 03:52:26 - INFO - __main__ - Global step 250 Train loss 0.65 Classification-F1 0.5581532674567871 on epoch=62
05/26/2022 03:52:26 - INFO - __main__ - Saving model with best Classification-F1: 0.4081196581196581 -> 0.5581532674567871 on epoch=62, global_step=250
05/26/2022 03:52:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.60 on epoch=64
05/26/2022 03:52:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.54 on epoch=67
05/26/2022 03:52:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.60 on epoch=69
05/26/2022 03:52:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=72
05/26/2022 03:52:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=74
05/26/2022 03:52:40 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.4307606416917328 on epoch=74
05/26/2022 03:52:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=77
05/26/2022 03:52:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=79
05/26/2022 03:52:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=82
05/26/2022 03:52:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=84
05/26/2022 03:52:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=87
05/26/2022 03:52:54 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.6226425748164879 on epoch=87
05/26/2022 03:52:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5581532674567871 -> 0.6226425748164879 on epoch=87, global_step=350
05/26/2022 03:52:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=89
05/26/2022 03:53:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=92
05/26/2022 03:53:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=94
05/26/2022 03:53:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=97
05/26/2022 03:53:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=99
05/26/2022 03:53:09 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.6586805555555556 on epoch=99
05/26/2022 03:53:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6226425748164879 -> 0.6586805555555556 on epoch=99, global_step=400
05/26/2022 03:53:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=102
05/26/2022 03:53:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=104
05/26/2022 03:53:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=107
05/26/2022 03:53:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=109
05/26/2022 03:53:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.32 on epoch=112
05/26/2022 03:53:23 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.6288215421303657 on epoch=112
05/26/2022 03:53:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=114
05/26/2022 03:53:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=117
05/26/2022 03:53:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=119
05/26/2022 03:53:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=122
05/26/2022 03:53:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=124
05/26/2022 03:53:37 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.521499060150376 on epoch=124
05/26/2022 03:53:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=127
05/26/2022 03:53:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=129
05/26/2022 03:53:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=132
05/26/2022 03:53:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.30 on epoch=134
05/26/2022 03:53:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=137
05/26/2022 03:53:52 - INFO - __main__ - Global step 550 Train loss 0.29 Classification-F1 0.6499515190691662 on epoch=137
05/26/2022 03:53:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=139
05/26/2022 03:53:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=142
05/26/2022 03:54:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=144
05/26/2022 03:54:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=147
05/26/2022 03:54:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=149
05/26/2022 03:54:06 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.6598002724663067 on epoch=149
05/26/2022 03:54:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6586805555555556 -> 0.6598002724663067 on epoch=149, global_step=600
05/26/2022 03:54:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=152
05/26/2022 03:54:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=154
05/26/2022 03:54:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=157
05/26/2022 03:54:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=159
05/26/2022 03:54:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=162
05/26/2022 03:54:20 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.7528791737408036 on epoch=162
05/26/2022 03:54:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6598002724663067 -> 0.7528791737408036 on epoch=162, global_step=650
05/26/2022 03:54:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=164
05/26/2022 03:54:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=167
05/26/2022 03:54:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=169
05/26/2022 03:54:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=172
05/26/2022 03:54:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=174
05/26/2022 03:54:35 - INFO - __main__ - Global step 700 Train loss 0.25 Classification-F1 0.7308080808080808 on epoch=174
05/26/2022 03:54:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=177
05/26/2022 03:54:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=179
05/26/2022 03:54:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=182
05/26/2022 03:54:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=184
05/26/2022 03:54:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=187
05/26/2022 03:54:49 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.6853309744950612 on epoch=187
05/26/2022 03:54:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=189
05/26/2022 03:54:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=192
05/26/2022 03:54:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=194
05/26/2022 03:55:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=197
05/26/2022 03:55:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=199
05/26/2022 03:55:03 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.7075925106884859 on epoch=199
05/26/2022 03:55:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=202
05/26/2022 03:55:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=204
05/26/2022 03:55:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=207
05/26/2022 03:55:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=209
05/26/2022 03:55:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=212
05/26/2022 03:55:18 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.7085636056224291 on epoch=212
05/26/2022 03:55:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=214
05/26/2022 03:55:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=217
05/26/2022 03:55:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=219
05/26/2022 03:55:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=222
05/26/2022 03:55:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=224
05/26/2022 03:55:32 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.6548018292682927 on epoch=224
05/26/2022 03:55:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=227
05/26/2022 03:55:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=229
05/26/2022 03:55:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=232
05/26/2022 03:55:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=234
05/26/2022 03:55:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
05/26/2022 03:55:47 - INFO - __main__ - Global step 950 Train loss 0.14 Classification-F1 0.6860789755526597 on epoch=237
05/26/2022 03:55:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=239
05/26/2022 03:55:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=242
05/26/2022 03:55:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=244
05/26/2022 03:55:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
05/26/2022 03:56:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
05/26/2022 03:56:01 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.6708392603129445 on epoch=249
05/26/2022 03:56:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=252
05/26/2022 03:56:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=254
05/26/2022 03:56:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=257
05/26/2022 03:56:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=259
05/26/2022 03:56:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=262
05/26/2022 03:56:15 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.711287758346582 on epoch=262
05/26/2022 03:56:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=264
05/26/2022 03:56:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=267
05/26/2022 03:56:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=269
05/26/2022 03:56:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=272
05/26/2022 03:56:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=274
05/26/2022 03:56:30 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.6742155870445344 on epoch=274
05/26/2022 03:56:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
05/26/2022 03:56:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=279
05/26/2022 03:56:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
05/26/2022 03:56:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=284
05/26/2022 03:56:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
05/26/2022 03:56:44 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.683751089799477 on epoch=287
05/26/2022 03:56:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
05/26/2022 03:56:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=292
05/26/2022 03:56:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=294
05/26/2022 03:56:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=297
05/26/2022 03:56:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=299
05/26/2022 03:56:59 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.7127622377622378 on epoch=299
05/26/2022 03:57:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.09 on epoch=302
05/26/2022 03:57:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
05/26/2022 03:57:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
05/26/2022 03:57:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=309
05/26/2022 03:57:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
05/26/2022 03:57:13 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.7066187428029533 on epoch=312
05/26/2022 03:57:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/26/2022 03:57:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=317
05/26/2022 03:57:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
05/26/2022 03:57:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/26/2022 03:57:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/26/2022 03:57:28 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7368705855547961 on epoch=324
05/26/2022 03:57:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
05/26/2022 03:57:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
05/26/2022 03:57:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
05/26/2022 03:57:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/26/2022 03:57:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
05/26/2022 03:57:42 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.6990344411397044 on epoch=337
05/26/2022 03:57:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
05/26/2022 03:57:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/26/2022 03:57:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
05/26/2022 03:57:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/26/2022 03:57:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/26/2022 03:57:57 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.7525007104290992 on epoch=349
05/26/2022 03:58:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.10 on epoch=352
05/26/2022 03:58:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
05/26/2022 03:58:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
05/26/2022 03:58:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
05/26/2022 03:58:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/26/2022 03:58:12 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.6836336336336335 on epoch=362
05/26/2022 03:58:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
05/26/2022 03:58:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/26/2022 03:58:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=369
05/26/2022 03:58:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/26/2022 03:58:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=374
05/26/2022 03:58:26 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.6851653088495193 on epoch=374
05/26/2022 03:58:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/26/2022 03:58:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/26/2022 03:58:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=382
05/26/2022 03:58:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/26/2022 03:58:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/26/2022 03:58:41 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6845588235294118 on epoch=387
05/26/2022 03:58:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
05/26/2022 03:58:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/26/2022 03:58:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/26/2022 03:58:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/26/2022 03:58:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/26/2022 03:58:56 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.7469810890863523 on epoch=399
05/26/2022 03:58:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/26/2022 03:59:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/26/2022 03:59:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/26/2022 03:59:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
05/26/2022 03:59:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/26/2022 03:59:10 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7230153540637412 on epoch=412
05/26/2022 03:59:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/26/2022 03:59:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
05/26/2022 03:59:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/26/2022 03:59:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/26/2022 03:59:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/26/2022 03:59:25 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.7315233785822022 on epoch=424
05/26/2022 03:59:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/26/2022 03:59:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/26/2022 03:59:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
05/26/2022 03:59:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=434
05/26/2022 03:59:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/26/2022 03:59:40 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6845373237014104 on epoch=437
05/26/2022 03:59:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/26/2022 03:59:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/26/2022 03:59:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
05/26/2022 03:59:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/26/2022 03:59:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/26/2022 03:59:54 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7450237670825907 on epoch=449
05/26/2022 03:59:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/26/2022 04:00:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/26/2022 04:00:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/26/2022 04:00:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/26/2022 04:00:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/26/2022 04:00:09 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7082368082368081 on epoch=462
05/26/2022 04:00:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/26/2022 04:00:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=467
05/26/2022 04:00:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
05/26/2022 04:00:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/26/2022 04:00:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/26/2022 04:00:24 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7308080808080807 on epoch=474
05/26/2022 04:00:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
05/26/2022 04:00:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
05/26/2022 04:00:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/26/2022 04:00:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/26/2022 04:00:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/26/2022 04:00:38 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7525007104290992 on epoch=487
05/26/2022 04:00:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=489
05/26/2022 04:00:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
05/26/2022 04:00:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/26/2022 04:00:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=497
05/26/2022 04:00:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/26/2022 04:00:53 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.7319690113807761 on epoch=499
05/26/2022 04:00:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
05/26/2022 04:00:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/26/2022 04:01:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/26/2022 04:01:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/26/2022 04:01:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/26/2022 04:01:08 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7313015018607123 on epoch=512
05/26/2022 04:01:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
05/26/2022 04:01:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/26/2022 04:01:16 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/26/2022 04:01:19 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/26/2022 04:01:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/26/2022 04:01:23 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7446969696969696 on epoch=524
05/26/2022 04:01:25 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/26/2022 04:01:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/26/2022 04:01:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/26/2022 04:01:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/26/2022 04:01:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=537
05/26/2022 04:01:37 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7525007104290992 on epoch=537
05/26/2022 04:01:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/26/2022 04:01:43 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/26/2022 04:01:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/26/2022 04:01:48 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/26/2022 04:01:51 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=549
05/26/2022 04:01:52 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7450237670825905 on epoch=549
05/26/2022 04:01:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/26/2022 04:01:57 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/26/2022 04:02:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/26/2022 04:02:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/26/2022 04:02:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/26/2022 04:02:07 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7450237670825907 on epoch=562
05/26/2022 04:02:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=564
05/26/2022 04:02:12 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/26/2022 04:02:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/26/2022 04:02:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/26/2022 04:02:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/26/2022 04:02:21 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7456319956319957 on epoch=574
05/26/2022 04:02:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/26/2022 04:02:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/26/2022 04:02:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/26/2022 04:02:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/26/2022 04:02:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/26/2022 04:02:36 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7446969696969696 on epoch=587
05/26/2022 04:02:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/26/2022 04:02:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/26/2022 04:02:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/26/2022 04:02:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
05/26/2022 04:02:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/26/2022 04:02:51 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7446969696969696 on epoch=599
05/26/2022 04:02:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/26/2022 04:02:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=604
05/26/2022 04:02:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/26/2022 04:03:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/26/2022 04:03:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/26/2022 04:03:05 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7446969696969696 on epoch=612
05/26/2022 04:03:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/26/2022 04:03:11 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/26/2022 04:03:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/26/2022 04:03:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/26/2022 04:03:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/26/2022 04:03:20 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.7446969696969696 on epoch=624
05/26/2022 04:03:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/26/2022 04:03:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/26/2022 04:03:28 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/26/2022 04:03:31 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/26/2022 04:03:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/26/2022 04:03:35 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7446969696969696 on epoch=637
05/26/2022 04:03:37 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/26/2022 04:03:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/26/2022 04:03:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/26/2022 04:03:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/26/2022 04:03:48 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/26/2022 04:03:49 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7085611756664388 on epoch=649
05/26/2022 04:03:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/26/2022 04:03:55 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/26/2022 04:03:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/26/2022 04:04:00 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/26/2022 04:04:03 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/26/2022 04:04:04 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7320324960127591 on epoch=662
05/26/2022 04:04:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=664
05/26/2022 04:04:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=667
05/26/2022 04:04:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/26/2022 04:04:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
05/26/2022 04:04:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/26/2022 04:04:19 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.7663895993179881 on epoch=674
05/26/2022 04:04:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7528791737408036 -> 0.7663895993179881 on epoch=674, global_step=2700
05/26/2022 04:04:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/26/2022 04:04:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/26/2022 04:04:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=682
05/26/2022 04:04:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/26/2022 04:04:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/26/2022 04:04:34 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7308080808080808 on epoch=687
05/26/2022 04:04:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/26/2022 04:04:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/26/2022 04:04:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/26/2022 04:04:45 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/26/2022 04:04:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
05/26/2022 04:04:49 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7082368082368081 on epoch=699
05/26/2022 04:04:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/26/2022 04:04:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/26/2022 04:04:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/26/2022 04:04:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/26/2022 04:05:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
05/26/2022 04:05:03 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.745093795093795 on epoch=712
05/26/2022 04:05:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/26/2022 04:05:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/26/2022 04:05:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/26/2022 04:05:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/26/2022 04:05:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/26/2022 04:05:18 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.745093795093795 on epoch=724
05/26/2022 04:05:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/26/2022 04:05:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/26/2022 04:05:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=732
05/26/2022 04:05:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/26/2022 04:05:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/26/2022 04:05:33 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7450237670825907 on epoch=737
05/26/2022 04:05:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/26/2022 04:05:38 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/26/2022 04:05:41 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=744
05/26/2022 04:05:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/26/2022 04:05:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/26/2022 04:05:47 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7386280386280386 on epoch=749
05/26/2022 04:05:47 - INFO - __main__ - save last model!
05/26/2022 04:05:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 04:05:48 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 04:05:48 - INFO - __main__ - Printing 3 examples
05/26/2022 04:05:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 04:05:48 - INFO - __main__ - ['others']
05/26/2022 04:05:48 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 04:05:48 - INFO - __main__ - ['others']
05/26/2022 04:05:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 04:05:48 - INFO - __main__ - ['others']
05/26/2022 04:05:48 - INFO - __main__ - Tokenizing Input ...
05/26/2022 04:05:50 - INFO - __main__ - Tokenizing Output ...
05/26/2022 04:05:55 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 04:07:44 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-200prompt/singletask-emo/emo_16_87_0.2_8_predictions.txt
05/26/2022 04:07:44 - INFO - __main__ - Classification-F1 on test data: 0.0520
05/26/2022 04:07:45 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.7663895993179881, test_performance=0.0520395148336873
