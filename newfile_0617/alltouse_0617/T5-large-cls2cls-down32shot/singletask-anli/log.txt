05/21/2022 21:31:06 - INFO - __main__ - Namespace(task_dir='data_32/anli/', task_name='anli', identifier='T5-large-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down32shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/21/2022 21:31:06 - INFO - __main__ - models/T5-large-cls2cls-down32shot/singletask-anli
05/21/2022 21:31:06 - INFO - __main__ - Namespace(task_dir='data_32/anli/', task_name='anli', identifier='T5-large-cls2cls-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down32shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/21/2022 21:31:06 - INFO - __main__ - models/T5-large-cls2cls-down32shot/singletask-anli
05/21/2022 21:31:06 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:31:06 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:31:06 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:31:06 - INFO - __main__ - Using 2 gpus
05/21/2022 21:31:06 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:31:06 - INFO - __main__ - Using 2 gpus
05/21/2022 21:31:06 - INFO - __main__ - Fine-tuning the following samples: ['anli_32_100', 'anli_32_13', 'anli_32_21', 'anli_32_42', 'anli_32_87']
05/21/2022 21:31:06 - INFO - __main__ - Fine-tuning the following samples: ['anli_32_100', 'anli_32_13', 'anli_32_21', 'anli_32_42', 'anli_32_87']
05/21/2022 21:31:12 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.5, bsz=8 ...
05/21/2022 21:31:13 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 21:31:13 - INFO - __main__ - Printing 3 examples
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:31:13 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 21:31:13 - INFO - __main__ - Printing 3 examples
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:31:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:31:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:31:13 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 21:31:13 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 21:31:13 - INFO - __main__ - Printing 3 examples
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:31:13 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 21:31:13 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 21:31:13 - INFO - __main__ - Printing 3 examples
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/21/2022 21:31:13 - INFO - __main__ - ['neutral']
05/21/2022 21:31:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:31:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:31:14 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:31:14 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 21:31:14 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 21:31:32 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 21:31:32 - INFO - __main__ - task name: anli
05/21/2022 21:31:32 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 21:31:32 - INFO - __main__ - task name: anli
05/21/2022 21:31:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 21:31:33 - INFO - __main__ - Starting training!
05/21/2022 21:31:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 21:31:33 - INFO - __main__ - Starting training!
05/21/2022 21:31:36 - INFO - __main__ - Step 10 Global step 10 Train loss 6.25 on epoch=1
05/21/2022 21:31:39 - INFO - __main__ - Step 20 Global step 20 Train loss 2.47 on epoch=3
05/21/2022 21:31:42 - INFO - __main__ - Step 30 Global step 30 Train loss 1.06 on epoch=4
05/21/2022 21:31:44 - INFO - __main__ - Step 40 Global step 40 Train loss 0.75 on epoch=6
05/21/2022 21:31:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.76 on epoch=8
05/21/2022 21:31:49 - INFO - __main__ - Global step 50 Train loss 2.26 Classification-F1 0.16666666666666666 on epoch=8
05/21/2022 21:31:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/21/2022 21:31:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.62 on epoch=9
05/21/2022 21:31:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.60 on epoch=11
05/21/2022 21:31:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=13
05/21/2022 21:32:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=14
05/21/2022 21:32:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.62 on epoch=16
05/21/2022 21:32:05 - INFO - __main__ - Global step 100 Train loss 0.61 Classification-F1 0.16666666666666666 on epoch=16
05/21/2022 21:32:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=18
05/21/2022 21:32:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=19
05/21/2022 21:32:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=21
05/21/2022 21:32:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=23
05/21/2022 21:32:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=24
05/21/2022 21:32:22 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=24
05/21/2022 21:32:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=26
05/21/2022 21:32:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=28
05/21/2022 21:32:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=29
05/21/2022 21:32:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=31
05/21/2022 21:32:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=33
05/21/2022 21:32:38 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/21/2022 21:32:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=34
05/21/2022 21:32:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=36
05/21/2022 21:32:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=38
05/21/2022 21:32:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=39
05/21/2022 21:32:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=41
05/21/2022 21:32:53 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
05/21/2022 21:32:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=43
05/21/2022 21:32:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=44
05/21/2022 21:33:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=46
05/21/2022 21:33:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=48
05/21/2022 21:33:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=49
05/21/2022 21:33:09 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.2124098124098124 on epoch=49
05/21/2022 21:33:09 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2124098124098124 on epoch=49, global_step=300
05/21/2022 21:33:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=51
05/21/2022 21:33:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=53
05/21/2022 21:33:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=54
05/21/2022 21:33:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=56
05/21/2022 21:33:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=58
05/21/2022 21:33:25 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=58
05/21/2022 21:33:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=59
05/21/2022 21:33:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=61
05/21/2022 21:33:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=63
05/21/2022 21:33:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=64
05/21/2022 21:33:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=66
05/21/2022 21:33:40 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=66
05/21/2022 21:33:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=68
05/21/2022 21:33:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=69
05/21/2022 21:33:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=71
05/21/2022 21:33:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=73
05/21/2022 21:33:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=74
05/21/2022 21:33:56 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.24487456764922685 on epoch=74
05/21/2022 21:33:56 - INFO - __main__ - Saving model with best Classification-F1: 0.2124098124098124 -> 0.24487456764922685 on epoch=74, global_step=450
05/21/2022 21:33:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=76
05/21/2022 21:34:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=78
05/21/2022 21:34:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=79
05/21/2022 21:34:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=81
05/21/2022 21:34:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=83
05/21/2022 21:34:13 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=83
05/21/2022 21:34:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=84
05/21/2022 21:34:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=86
05/21/2022 21:34:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=88
05/21/2022 21:34:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=89
05/21/2022 21:34:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=91
05/21/2022 21:34:29 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=91
05/21/2022 21:34:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=93
05/21/2022 21:34:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=94
05/21/2022 21:34:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=96
05/21/2022 21:34:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=98
05/21/2022 21:34:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=99
05/21/2022 21:34:45 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.3116146775165763 on epoch=99
05/21/2022 21:34:45 - INFO - __main__ - Saving model with best Classification-F1: 0.24487456764922685 -> 0.3116146775165763 on epoch=99, global_step=600
05/21/2022 21:34:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=101
05/21/2022 21:34:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=103
05/21/2022 21:34:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=104
05/21/2022 21:34:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=106
05/21/2022 21:34:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=108
05/21/2022 21:35:02 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.16666666666666666 on epoch=108
05/21/2022 21:35:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=109
05/21/2022 21:35:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=111
05/21/2022 21:35:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=113
05/21/2022 21:35:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=114
05/21/2022 21:35:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=116
05/21/2022 21:35:18 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=116
05/21/2022 21:35:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=118
05/21/2022 21:35:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=119
05/21/2022 21:35:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=121
05/21/2022 21:35:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=123
05/21/2022 21:35:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=124
05/21/2022 21:35:34 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.2087619047619048 on epoch=124
05/21/2022 21:35:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=126
05/21/2022 21:35:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=128
05/21/2022 21:35:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=129
05/21/2022 21:35:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=131
05/21/2022 21:35:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=133
05/21/2022 21:35:50 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.17054932412999713 on epoch=133
05/21/2022 21:35:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=134
05/21/2022 21:35:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=136
05/21/2022 21:35:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=138
05/21/2022 21:36:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=139
05/21/2022 21:36:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=141
05/21/2022 21:36:07 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=141
05/21/2022 21:36:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=143
05/21/2022 21:36:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=144
05/21/2022 21:36:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=146
05/21/2022 21:36:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=148
05/21/2022 21:36:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=149
05/21/2022 21:36:23 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.1843809523809524 on epoch=149
05/21/2022 21:36:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=151
05/21/2022 21:36:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.42 on epoch=153
05/21/2022 21:36:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=154
05/21/2022 21:36:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=156
05/21/2022 21:36:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=158
05/21/2022 21:36:39 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.2697032436162871 on epoch=158
05/21/2022 21:36:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=159
05/21/2022 21:36:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=161
05/21/2022 21:36:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=163
05/21/2022 21:36:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=164
05/21/2022 21:36:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=166
05/21/2022 21:36:56 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.16666666666666666 on epoch=166
05/21/2022 21:36:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=168
05/21/2022 21:37:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=169
05/21/2022 21:37:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=171
05/21/2022 21:37:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=173
05/21/2022 21:37:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=174
05/21/2022 21:37:12 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.22691778465772275 on epoch=174
05/21/2022 21:37:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=176
05/21/2022 21:37:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=178
05/21/2022 21:37:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=179
05/21/2022 21:37:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=181
05/21/2022 21:37:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=183
05/21/2022 21:37:28 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.17980884109916365 on epoch=183
05/21/2022 21:37:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=184
05/21/2022 21:37:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=186
05/21/2022 21:37:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=188
05/21/2022 21:37:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=189
05/21/2022 21:37:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=191
05/21/2022 21:37:44 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.17904761904761904 on epoch=191
05/21/2022 21:37:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=193
05/21/2022 21:37:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=194
05/21/2022 21:37:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=196
05/21/2022 21:37:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=198
05/21/2022 21:37:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=199
05/21/2022 21:38:00 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.1851851851851852 on epoch=199
05/21/2022 21:38:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=201
05/21/2022 21:38:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=203
05/21/2022 21:38:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=204
05/21/2022 21:38:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=206
05/21/2022 21:38:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.34 on epoch=208
05/21/2022 21:38:16 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.4357622861351113 on epoch=208
05/21/2022 21:38:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3116146775165763 -> 0.4357622861351113 on epoch=208, global_step=1250
05/21/2022 21:38:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=209
05/21/2022 21:38:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=211
05/21/2022 21:38:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=213
05/21/2022 21:38:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=214
05/21/2022 21:38:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=216
05/21/2022 21:38:32 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.1836290071584189 on epoch=216
05/21/2022 21:38:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=218
05/21/2022 21:38:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=219
05/21/2022 21:38:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=221
05/21/2022 21:38:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=223
05/21/2022 21:38:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=224
05/21/2022 21:38:48 - INFO - __main__ - Global step 1350 Train loss 0.40 Classification-F1 0.31319637048450605 on epoch=224
05/21/2022 21:38:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=226
05/21/2022 21:38:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=228
05/21/2022 21:38:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=229
05/21/2022 21:38:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=231
05/21/2022 21:39:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=233
05/21/2022 21:39:05 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.2897196000514188 on epoch=233
05/21/2022 21:39:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=234
05/21/2022 21:39:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=236
05/21/2022 21:39:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=238
05/21/2022 21:39:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=239
05/21/2022 21:39:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.30 on epoch=241
05/21/2022 21:39:21 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.20086580086580086 on epoch=241
05/21/2022 21:39:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=243
05/21/2022 21:39:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=244
05/21/2022 21:39:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.38 on epoch=246
05/21/2022 21:39:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=248
05/21/2022 21:39:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=249
05/21/2022 21:39:37 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.3464960337841694 on epoch=249
05/21/2022 21:39:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=251
05/21/2022 21:39:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.35 on epoch=253
05/21/2022 21:39:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=254
05/21/2022 21:39:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=256
05/21/2022 21:39:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=258
05/21/2022 21:39:54 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.35242932004916866 on epoch=258
05/21/2022 21:39:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=259
05/21/2022 21:39:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=261
05/21/2022 21:40:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=263
05/21/2022 21:40:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=264
05/21/2022 21:40:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=266
05/21/2022 21:40:10 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.19526211315263384 on epoch=266
05/21/2022 21:40:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=268
05/21/2022 21:40:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.33 on epoch=269
05/21/2022 21:40:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=271
05/21/2022 21:40:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=273
05/21/2022 21:40:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=274
05/21/2022 21:40:27 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.29810172921439054 on epoch=274
05/21/2022 21:40:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=276
05/21/2022 21:40:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.48 on epoch=278
05/21/2022 21:40:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.79 on epoch=279
05/21/2022 21:40:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.31 on epoch=281
05/21/2022 21:40:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 2.27 on epoch=283
05/21/2022 21:40:43 - INFO - __main__ - Global step 1700 Train loss 1.07 Classification-F1 0.2984270631329455 on epoch=283
05/21/2022 21:40:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 2.05 on epoch=284
05/21/2022 21:40:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.24 on epoch=286
05/21/2022 21:40:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.17 on epoch=288
05/21/2022 21:40:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=289
05/21/2022 21:40:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.49 on epoch=291
05/21/2022 21:40:58 - INFO - __main__ - Global step 1750 Train loss 1.08 Classification-F1 0.25580140313005933 on epoch=291
05/21/2022 21:41:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=293
05/21/2022 21:41:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.51 on epoch=294
05/21/2022 21:41:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=296
05/21/2022 21:41:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.47 on epoch=298
05/21/2022 21:41:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.49 on epoch=299
05/21/2022 21:41:14 - INFO - __main__ - Global step 1800 Train loss 0.46 Classification-F1 0.21684771469563435 on epoch=299
05/21/2022 21:41:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=301
05/21/2022 21:41:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=303
05/21/2022 21:41:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=304
05/21/2022 21:41:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=306
05/21/2022 21:41:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=308
05/21/2022 21:41:30 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.22756165776760742 on epoch=308
05/21/2022 21:41:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.44 on epoch=309
05/21/2022 21:41:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=311
05/21/2022 21:41:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=313
05/21/2022 21:41:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=314
05/21/2022 21:41:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=316
05/21/2022 21:41:46 - INFO - __main__ - Global step 1900 Train loss 0.38 Classification-F1 0.24969696969696967 on epoch=316
05/21/2022 21:41:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=318
05/21/2022 21:41:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=319
05/21/2022 21:41:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=321
05/21/2022 21:41:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=323
05/21/2022 21:41:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=324
05/21/2022 21:42:02 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.26674390378461876 on epoch=324
05/21/2022 21:42:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=326
05/21/2022 21:42:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.36 on epoch=328
05/21/2022 21:42:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=329
05/21/2022 21:42:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=331
05/21/2022 21:42:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.32 on epoch=333
05/21/2022 21:42:18 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3367071094082588 on epoch=333
05/21/2022 21:42:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.42 on epoch=334
05/21/2022 21:42:23 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=336
05/21/2022 21:42:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.33 on epoch=338
05/21/2022 21:42:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.36 on epoch=339
05/21/2022 21:42:31 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.38 on epoch=341
05/21/2022 21:42:34 - INFO - __main__ - Global step 2050 Train loss 0.38 Classification-F1 0.36080031080031083 on epoch=341
05/21/2022 21:42:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.34 on epoch=343
05/21/2022 21:42:40 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.38 on epoch=344
05/21/2022 21:42:42 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.41 on epoch=346
05/21/2022 21:42:45 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.37 on epoch=348
05/21/2022 21:42:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.40 on epoch=349
05/21/2022 21:42:50 - INFO - __main__ - Global step 2100 Train loss 0.38 Classification-F1 0.3298941798941799 on epoch=349
05/21/2022 21:42:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=351
05/21/2022 21:42:56 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.35 on epoch=353
05/21/2022 21:42:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.35 on epoch=354
05/21/2022 21:43:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.37 on epoch=356
05/21/2022 21:43:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=358
05/21/2022 21:43:06 - INFO - __main__ - Global step 2150 Train loss 0.36 Classification-F1 0.3307555189908131 on epoch=358
05/21/2022 21:43:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.37 on epoch=359
05/21/2022 21:43:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.36 on epoch=361
05/21/2022 21:43:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.34 on epoch=363
05/21/2022 21:43:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.37 on epoch=364
05/21/2022 21:43:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.32 on epoch=366
05/21/2022 21:43:23 - INFO - __main__ - Global step 2200 Train loss 0.35 Classification-F1 0.34586241276171487 on epoch=366
05/21/2022 21:43:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.37 on epoch=368
05/21/2022 21:43:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.37 on epoch=369
05/21/2022 21:43:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.33 on epoch=371
05/21/2022 21:43:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.35 on epoch=373
05/21/2022 21:43:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.39 on epoch=374
05/21/2022 21:43:39 - INFO - __main__ - Global step 2250 Train loss 0.36 Classification-F1 0.36352427172099305 on epoch=374
05/21/2022 21:43:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.39 on epoch=376
05/21/2022 21:43:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.35 on epoch=378
05/21/2022 21:43:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.33 on epoch=379
05/21/2022 21:43:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.34 on epoch=381
05/21/2022 21:43:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=383
05/21/2022 21:43:55 - INFO - __main__ - Global step 2300 Train loss 0.35 Classification-F1 0.34909929116501665 on epoch=383
05/21/2022 21:43:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.38 on epoch=384
05/21/2022 21:44:00 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.36 on epoch=386
05/21/2022 21:44:03 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.35 on epoch=388
05/21/2022 21:44:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.38 on epoch=389
05/21/2022 21:44:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=391
05/21/2022 21:44:11 - INFO - __main__ - Global step 2350 Train loss 0.36 Classification-F1 0.30156599201302203 on epoch=391
05/21/2022 21:44:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.39 on epoch=393
05/21/2022 21:44:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.34 on epoch=394
05/21/2022 21:44:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.35 on epoch=396
05/21/2022 21:44:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.37 on epoch=398
05/21/2022 21:44:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.34 on epoch=399
05/21/2022 21:44:27 - INFO - __main__ - Global step 2400 Train loss 0.36 Classification-F1 0.33156754955855405 on epoch=399
05/21/2022 21:44:29 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.35 on epoch=401
05/21/2022 21:44:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.33 on epoch=403
05/21/2022 21:44:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.36 on epoch=404
05/21/2022 21:44:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.36 on epoch=406
05/21/2022 21:44:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.36 on epoch=408
05/21/2022 21:44:43 - INFO - __main__ - Global step 2450 Train loss 0.35 Classification-F1 0.3496945221083152 on epoch=408
05/21/2022 21:44:45 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.39 on epoch=409
05/21/2022 21:44:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.39 on epoch=411
05/21/2022 21:44:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.35 on epoch=413
05/21/2022 21:44:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.30 on epoch=414
05/21/2022 21:44:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.34 on epoch=416
05/21/2022 21:44:59 - INFO - __main__ - Global step 2500 Train loss 0.35 Classification-F1 0.3052179132634467 on epoch=416
05/21/2022 21:45:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.34 on epoch=418
05/21/2022 21:45:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.39 on epoch=419
05/21/2022 21:45:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.37 on epoch=421
05/21/2022 21:45:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.33 on epoch=423
05/21/2022 21:45:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.38 on epoch=424
05/21/2022 21:45:15 - INFO - __main__ - Global step 2550 Train loss 0.36 Classification-F1 0.3263845281195329 on epoch=424
05/21/2022 21:45:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.36 on epoch=426
05/21/2022 21:45:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.31 on epoch=428
05/21/2022 21:45:23 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.37 on epoch=429
05/21/2022 21:45:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.33 on epoch=431
05/21/2022 21:45:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.34 on epoch=433
05/21/2022 21:45:31 - INFO - __main__ - Global step 2600 Train loss 0.34 Classification-F1 0.3697478991596639 on epoch=433
05/21/2022 21:45:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.40 on epoch=434
05/21/2022 21:45:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.30 on epoch=436
05/21/2022 21:45:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.40 on epoch=438
05/21/2022 21:45:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.36 on epoch=439
05/21/2022 21:45:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.36 on epoch=441
05/21/2022 21:45:47 - INFO - __main__ - Global step 2650 Train loss 0.36 Classification-F1 0.3663111840306262 on epoch=441
05/21/2022 21:45:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.32 on epoch=443
05/21/2022 21:45:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.34 on epoch=444
05/21/2022 21:45:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.34 on epoch=446
05/21/2022 21:45:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.36 on epoch=448
05/21/2022 21:46:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.40 on epoch=449
05/21/2022 21:46:03 - INFO - __main__ - Global step 2700 Train loss 0.35 Classification-F1 0.3423550016291952 on epoch=449
05/21/2022 21:46:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.32 on epoch=451
05/21/2022 21:46:08 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.36 on epoch=453
05/21/2022 21:46:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.37 on epoch=454
05/21/2022 21:46:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.37 on epoch=456
05/21/2022 21:46:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.33 on epoch=458
05/21/2022 21:46:19 - INFO - __main__ - Global step 2750 Train loss 0.35 Classification-F1 0.27732627732627735 on epoch=458
05/21/2022 21:46:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.37 on epoch=459
05/21/2022 21:46:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.35 on epoch=461
05/21/2022 21:46:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.32 on epoch=463
05/21/2022 21:46:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.33 on epoch=464
05/21/2022 21:46:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.34 on epoch=466
05/21/2022 21:46:35 - INFO - __main__ - Global step 2800 Train loss 0.34 Classification-F1 0.31870494368762664 on epoch=466
05/21/2022 21:46:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.29 on epoch=468
05/21/2022 21:46:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.37 on epoch=469
05/21/2022 21:46:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.34 on epoch=471
05/21/2022 21:46:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.34 on epoch=473
05/21/2022 21:46:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.36 on epoch=474
05/21/2022 21:46:51 - INFO - __main__ - Global step 2850 Train loss 0.34 Classification-F1 0.3426115442305678 on epoch=474
05/21/2022 21:46:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.37 on epoch=476
05/21/2022 21:46:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.31 on epoch=478
05/21/2022 21:46:59 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.38 on epoch=479
05/21/2022 21:47:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.32 on epoch=481
05/21/2022 21:47:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.37 on epoch=483
05/21/2022 21:47:08 - INFO - __main__ - Global step 2900 Train loss 0.35 Classification-F1 0.31826816160118604 on epoch=483
05/21/2022 21:47:10 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.40 on epoch=484
05/21/2022 21:47:13 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.38 on epoch=486
05/21/2022 21:47:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.37 on epoch=488
05/21/2022 21:47:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.32 on epoch=489
05/21/2022 21:47:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.31 on epoch=491
05/21/2022 21:47:24 - INFO - __main__ - Global step 2950 Train loss 0.36 Classification-F1 0.35118608579790145 on epoch=491
05/21/2022 21:47:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.29 on epoch=493
05/21/2022 21:47:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.38 on epoch=494
05/21/2022 21:47:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.35 on epoch=496
05/21/2022 21:47:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.39 on epoch=498
05/21/2022 21:47:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.34 on epoch=499
05/21/2022 21:47:39 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 21:47:39 - INFO - __main__ - Printing 3 examples
05/21/2022 21:47:39 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/21/2022 21:47:39 - INFO - __main__ - ['neutral']
05/21/2022 21:47:39 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/21/2022 21:47:39 - INFO - __main__ - ['neutral']
05/21/2022 21:47:39 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/21/2022 21:47:39 - INFO - __main__ - ['neutral']
05/21/2022 21:47:39 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:47:39 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:47:39 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 21:47:39 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 21:47:39 - INFO - __main__ - Printing 3 examples
05/21/2022 21:47:39 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/21/2022 21:47:39 - INFO - __main__ - ['neutral']
05/21/2022 21:47:39 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/21/2022 21:47:39 - INFO - __main__ - ['neutral']
05/21/2022 21:47:39 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/21/2022 21:47:39 - INFO - __main__ - ['neutral']
05/21/2022 21:47:39 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:47:39 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:47:39 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 21:47:40 - INFO - __main__ - Global step 3000 Train loss 0.35 Classification-F1 0.3401656314699793 on epoch=499
05/21/2022 21:47:40 - INFO - __main__ - save last model!
05/21/2022 21:47:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 21:47:40 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 21:47:40 - INFO - __main__ - Printing 3 examples
05/21/2022 21:47:40 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/21/2022 21:47:40 - INFO - __main__ - ['contradiction']
05/21/2022 21:47:40 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/21/2022 21:47:40 - INFO - __main__ - ['entailment']
05/21/2022 21:47:40 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/21/2022 21:47:40 - INFO - __main__ - ['contradiction']
05/21/2022 21:47:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:47:41 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:47:42 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 21:47:54 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 21:47:54 - INFO - __main__ - task name: anli
05/21/2022 21:47:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 21:47:54 - INFO - __main__ - Starting training!
05/21/2022 21:48:13 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_100_0.5_8_predictions.txt
05/21/2022 21:48:13 - INFO - __main__ - Classification-F1 on test data: 0.3197
05/21/2022 21:48:13 - INFO - __main__ - prefix=anli_32_100, lr=0.5, bsz=8, dev_performance=0.4357622861351113, test_performance=0.3196917941759082
05/21/2022 21:48:13 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.4, bsz=8 ...
05/21/2022 21:48:14 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 21:48:14 - INFO - __main__ - Printing 3 examples
05/21/2022 21:48:14 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/21/2022 21:48:14 - INFO - __main__ - ['neutral']
05/21/2022 21:48:14 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/21/2022 21:48:14 - INFO - __main__ - ['neutral']
05/21/2022 21:48:14 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/21/2022 21:48:14 - INFO - __main__ - ['neutral']
05/21/2022 21:48:14 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:48:14 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:48:14 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 21:48:14 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 21:48:14 - INFO - __main__ - Printing 3 examples
05/21/2022 21:48:14 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/21/2022 21:48:14 - INFO - __main__ - ['neutral']
05/21/2022 21:48:14 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/21/2022 21:48:14 - INFO - __main__ - ['neutral']
05/21/2022 21:48:14 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/21/2022 21:48:14 - INFO - __main__ - ['neutral']
05/21/2022 21:48:14 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:48:14 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:48:14 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 21:48:33 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 21:48:33 - INFO - __main__ - task name: anli
05/21/2022 21:48:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 21:48:34 - INFO - __main__ - Starting training!
05/21/2022 21:48:37 - INFO - __main__ - Step 10 Global step 10 Train loss 6.01 on epoch=1
05/21/2022 21:48:39 - INFO - __main__ - Step 20 Global step 20 Train loss 2.39 on epoch=3
05/21/2022 21:48:42 - INFO - __main__ - Step 30 Global step 30 Train loss 1.18 on epoch=4
05/21/2022 21:48:45 - INFO - __main__ - Step 40 Global step 40 Train loss 0.85 on epoch=6
05/21/2022 21:48:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.66 on epoch=8
05/21/2022 21:48:50 - INFO - __main__ - Global step 50 Train loss 2.22 Classification-F1 0.16666666666666666 on epoch=8
05/21/2022 21:48:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/21/2022 21:48:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=9
05/21/2022 21:48:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=11
05/21/2022 21:48:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.59 on epoch=13
05/21/2022 21:49:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=14
05/21/2022 21:49:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=16
05/21/2022 21:49:05 - INFO - __main__ - Global step 100 Train loss 0.62 Classification-F1 0.16666666666666666 on epoch=16
05/21/2022 21:49:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=18
05/21/2022 21:49:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=19
05/21/2022 21:49:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.61 on epoch=21
05/21/2022 21:49:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.56 on epoch=23
05/21/2022 21:49:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=24
05/21/2022 21:49:21 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.2749635188659579 on epoch=24
05/21/2022 21:49:21 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2749635188659579 on epoch=24, global_step=150
05/21/2022 21:49:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=26
05/21/2022 21:49:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=28
05/21/2022 21:49:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=29
05/21/2022 21:49:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=31
05/21/2022 21:49:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=33
05/21/2022 21:49:36 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
05/21/2022 21:49:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=34
05/21/2022 21:49:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=36
05/21/2022 21:49:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=38
05/21/2022 21:49:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=39
05/21/2022 21:49:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=41
05/21/2022 21:49:52 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=41
05/21/2022 21:49:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=43
05/21/2022 21:49:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=44
05/21/2022 21:50:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=46
05/21/2022 21:50:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=48
05/21/2022 21:50:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=49
05/21/2022 21:50:08 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=49
05/21/2022 21:50:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=51
05/21/2022 21:50:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=53
05/21/2022 21:50:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=54
05/21/2022 21:50:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=56
05/21/2022 21:50:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=58
05/21/2022 21:50:24 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.22505925447101918 on epoch=58
05/21/2022 21:50:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=59
05/21/2022 21:50:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=61
05/21/2022 21:50:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=63
05/21/2022 21:50:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=64
05/21/2022 21:50:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=66
05/21/2022 21:50:40 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/21/2022 21:50:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
05/21/2022 21:50:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=69
05/21/2022 21:50:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=71
05/21/2022 21:50:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=73
05/21/2022 21:50:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=74
05/21/2022 21:50:56 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.2516835016835017 on epoch=74
05/21/2022 21:50:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=76
05/21/2022 21:51:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=78
05/21/2022 21:51:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=79
05/21/2022 21:51:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=81
05/21/2022 21:51:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=83
05/21/2022 21:51:13 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=83
05/21/2022 21:51:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=84
05/21/2022 21:51:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=86
05/21/2022 21:51:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=88
05/21/2022 21:51:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=89
05/21/2022 21:51:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=91
05/21/2022 21:51:29 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.1679790026246719 on epoch=91
05/21/2022 21:51:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=93
05/21/2022 21:51:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=94
05/21/2022 21:51:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=96
05/21/2022 21:51:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=98
05/21/2022 21:51:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=99
05/21/2022 21:51:45 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=99
05/21/2022 21:51:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=101
05/21/2022 21:51:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=103
05/21/2022 21:51:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=104
05/21/2022 21:51:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=106
05/21/2022 21:51:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=108
05/21/2022 21:52:01 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.2153991497401984 on epoch=108
05/21/2022 21:52:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=109
05/21/2022 21:52:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=111
05/21/2022 21:52:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=113
05/21/2022 21:52:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=114
05/21/2022 21:52:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=116
05/21/2022 21:52:17 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=116
05/21/2022 21:52:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=118
05/21/2022 21:52:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=119
05/21/2022 21:52:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=121
05/21/2022 21:52:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=123
05/21/2022 21:52:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=124
05/21/2022 21:52:33 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.21256038647342998 on epoch=124
05/21/2022 21:52:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=126
05/21/2022 21:52:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=128
05/21/2022 21:52:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=129
05/21/2022 21:52:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=131
05/21/2022 21:52:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=133
05/21/2022 21:52:49 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=133
05/21/2022 21:52:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=134
05/21/2022 21:52:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=136
05/21/2022 21:52:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=138
05/21/2022 21:53:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=139
05/21/2022 21:53:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=141
05/21/2022 21:53:05 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=141
05/21/2022 21:53:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=143
05/21/2022 21:53:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=144
05/21/2022 21:53:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=146
05/21/2022 21:53:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=148
05/21/2022 21:53:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=149
05/21/2022 21:53:21 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.23317230273752013 on epoch=149
05/21/2022 21:53:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=151
05/21/2022 21:53:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=153
05/21/2022 21:53:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=154
05/21/2022 21:53:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=156
05/21/2022 21:53:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=158
05/21/2022 21:53:37 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.1731229076996652 on epoch=158
05/21/2022 21:53:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=159
05/21/2022 21:53:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=161
05/21/2022 21:53:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=163
05/21/2022 21:53:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=164
05/21/2022 21:53:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=166
05/21/2022 21:53:54 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=166
05/21/2022 21:53:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=168
05/21/2022 21:53:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=169
05/21/2022 21:54:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=171
05/21/2022 21:54:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=173
05/21/2022 21:54:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=174
05/21/2022 21:54:10 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=174
05/21/2022 21:54:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=176
05/21/2022 21:54:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=178
05/21/2022 21:54:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=179
05/21/2022 21:54:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=181
05/21/2022 21:54:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=183
05/21/2022 21:54:26 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.23075087733251154 on epoch=183
05/21/2022 21:54:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=184
05/21/2022 21:54:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=186
05/21/2022 21:54:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=188
05/21/2022 21:54:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=189
05/21/2022 21:54:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=191
05/21/2022 21:54:42 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=191
05/21/2022 21:54:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=193
05/21/2022 21:54:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=194
05/21/2022 21:54:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=196
05/21/2022 21:54:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=198
05/21/2022 21:54:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=199
05/21/2022 21:54:58 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.16402116402116398 on epoch=199
05/21/2022 21:55:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=201
05/21/2022 21:55:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=203
05/21/2022 21:55:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=204
05/21/2022 21:55:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=206
05/21/2022 21:55:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=208
05/21/2022 21:55:14 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.2187738787985658 on epoch=208
05/21/2022 21:55:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=209
05/21/2022 21:55:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=211
05/21/2022 21:55:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=213
05/21/2022 21:55:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=214
05/21/2022 21:55:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=216
05/21/2022 21:55:30 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.16666666666666666 on epoch=216
05/21/2022 21:55:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=218
05/21/2022 21:55:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=219
05/21/2022 21:55:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=221
05/21/2022 21:55:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=223
05/21/2022 21:55:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=224
05/21/2022 21:55:46 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.16402116402116398 on epoch=224
05/21/2022 21:55:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=226
05/21/2022 21:55:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=228
05/21/2022 21:55:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=229
05/21/2022 21:55:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=231
05/21/2022 21:56:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=233
05/21/2022 21:56:02 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.28463080785919753 on epoch=233
05/21/2022 21:56:02 - INFO - __main__ - Saving model with best Classification-F1: 0.2749635188659579 -> 0.28463080785919753 on epoch=233, global_step=1400
05/21/2022 21:56:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=234
05/21/2022 21:56:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=236
05/21/2022 21:56:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=238
05/21/2022 21:56:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=239
05/21/2022 21:56:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=241
05/21/2022 21:56:19 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.22444444444444445 on epoch=241
05/21/2022 21:56:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=243
05/21/2022 21:56:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=244
05/21/2022 21:56:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=246
05/21/2022 21:56:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=248
05/21/2022 21:56:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.41 on epoch=249
05/21/2022 21:56:35 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.26270414993306557 on epoch=249
05/21/2022 21:56:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=251
05/21/2022 21:56:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=253
05/21/2022 21:56:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=254
05/21/2022 21:56:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=256
05/21/2022 21:56:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=258
05/21/2022 21:56:51 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.24139188414856486 on epoch=258
05/21/2022 21:56:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=259
05/21/2022 21:56:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=261
05/21/2022 21:56:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=263
05/21/2022 21:57:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=264
05/21/2022 21:57:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=266
05/21/2022 21:57:07 - INFO - __main__ - Global step 1600 Train loss 0.38 Classification-F1 0.29028017862287525 on epoch=266
05/21/2022 21:57:07 - INFO - __main__ - Saving model with best Classification-F1: 0.28463080785919753 -> 0.29028017862287525 on epoch=266, global_step=1600
05/21/2022 21:57:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=268
05/21/2022 21:57:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=269
05/21/2022 21:57:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=271
05/21/2022 21:57:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=273
05/21/2022 21:57:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=274
05/21/2022 21:57:23 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.2739300218681662 on epoch=274
05/21/2022 21:57:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=276
05/21/2022 21:57:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=278
05/21/2022 21:57:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=279
05/21/2022 21:57:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=281
05/21/2022 21:57:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=283
05/21/2022 21:57:40 - INFO - __main__ - Global step 1700 Train loss 0.37 Classification-F1 0.29008604564160123 on epoch=283
05/21/2022 21:57:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=284
05/21/2022 21:57:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=286
05/21/2022 21:57:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=288
05/21/2022 21:57:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=289
05/21/2022 21:57:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=291
05/21/2022 21:57:56 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.2124098124098124 on epoch=291
05/21/2022 21:57:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=293
05/21/2022 21:58:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=294
05/21/2022 21:58:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=296
05/21/2022 21:58:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=298
05/21/2022 21:58:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=299
05/21/2022 21:58:12 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.32623509618901325 on epoch=299
05/21/2022 21:58:12 - INFO - __main__ - Saving model with best Classification-F1: 0.29028017862287525 -> 0.32623509618901325 on epoch=299, global_step=1800
05/21/2022 21:58:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=301
05/21/2022 21:58:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=303
05/21/2022 21:58:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=304
05/21/2022 21:58:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=306
05/21/2022 21:58:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=308
05/21/2022 21:58:28 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.3826470464401499 on epoch=308
05/21/2022 21:58:28 - INFO - __main__ - Saving model with best Classification-F1: 0.32623509618901325 -> 0.3826470464401499 on epoch=308, global_step=1850
05/21/2022 21:58:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=309
05/21/2022 21:58:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=311
05/21/2022 21:58:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=313
05/21/2022 21:58:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=314
05/21/2022 21:58:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.33 on epoch=316
05/21/2022 21:58:44 - INFO - __main__ - Global step 1900 Train loss 0.32 Classification-F1 0.339759778784169 on epoch=316
05/21/2022 21:58:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.28 on epoch=318
05/21/2022 21:58:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=319
05/21/2022 21:58:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.32 on epoch=321
05/21/2022 21:58:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=323
05/21/2022 21:58:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=324
05/21/2022 21:59:00 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.3030052128074297 on epoch=324
05/21/2022 21:59:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=326
05/21/2022 21:59:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=328
05/21/2022 21:59:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=329
05/21/2022 21:59:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=331
05/21/2022 21:59:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=333
05/21/2022 21:59:16 - INFO - __main__ - Global step 2000 Train loss 0.29 Classification-F1 0.34216381588096234 on epoch=333
05/21/2022 21:59:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.32 on epoch=334
05/21/2022 21:59:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.28 on epoch=336
05/21/2022 21:59:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=338
05/21/2022 21:59:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=339
05/21/2022 21:59:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.28 on epoch=341
05/21/2022 21:59:32 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.2971272830607869 on epoch=341
05/21/2022 21:59:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=343
05/21/2022 21:59:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.33 on epoch=344
05/21/2022 21:59:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=346
05/21/2022 21:59:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.25 on epoch=348
05/21/2022 21:59:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.27 on epoch=349
05/21/2022 21:59:48 - INFO - __main__ - Global step 2100 Train loss 0.28 Classification-F1 0.293984260370815 on epoch=349
05/21/2022 21:59:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.26 on epoch=351
05/21/2022 21:59:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.34 on epoch=353
05/21/2022 21:59:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.26 on epoch=354
05/21/2022 21:59:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.22 on epoch=356
05/21/2022 22:00:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.24 on epoch=358
05/21/2022 22:00:04 - INFO - __main__ - Global step 2150 Train loss 0.27 Classification-F1 0.2757575757575758 on epoch=358
05/21/2022 22:00:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.33 on epoch=359
05/21/2022 22:00:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=361
05/21/2022 22:00:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=363
05/21/2022 22:00:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.24 on epoch=364
05/21/2022 22:00:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=366
05/21/2022 22:00:20 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.2635992578849722 on epoch=366
05/21/2022 22:00:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.24 on epoch=368
05/21/2022 22:00:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=369
05/21/2022 22:00:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.25 on epoch=371
05/21/2022 22:00:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=373
05/21/2022 22:00:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.25 on epoch=374
05/21/2022 22:00:36 - INFO - __main__ - Global step 2250 Train loss 0.24 Classification-F1 0.30970009372071233 on epoch=374
05/21/2022 22:00:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.25 on epoch=376
05/21/2022 22:00:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.23 on epoch=378
05/21/2022 22:00:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.24 on epoch=379
05/21/2022 22:00:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.22 on epoch=381
05/21/2022 22:00:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=383
05/21/2022 22:00:52 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.3858318636096414 on epoch=383
05/21/2022 22:00:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3826470464401499 -> 0.3858318636096414 on epoch=383, global_step=2300
05/21/2022 22:00:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=384
05/21/2022 22:00:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=386
05/21/2022 22:01:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=388
05/21/2022 22:01:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.27 on epoch=389
05/21/2022 22:01:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=391
05/21/2022 22:01:08 - INFO - __main__ - Global step 2350 Train loss 0.22 Classification-F1 0.25891925869131244 on epoch=391
05/21/2022 22:01:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=393
05/21/2022 22:01:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=394
05/21/2022 22:01:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=396
05/21/2022 22:01:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=398
05/21/2022 22:01:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.24 on epoch=399
05/21/2022 22:01:24 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.3528631775928602 on epoch=399
05/21/2022 22:01:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.24 on epoch=401
05/21/2022 22:01:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.21 on epoch=403
05/21/2022 22:01:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=404
05/21/2022 22:01:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=406
05/21/2022 22:01:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=408
05/21/2022 22:01:40 - INFO - __main__ - Global step 2450 Train loss 0.21 Classification-F1 0.37105478379063284 on epoch=408
05/21/2022 22:01:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=409
05/21/2022 22:01:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=411
05/21/2022 22:01:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=413
05/21/2022 22:01:51 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=414
05/21/2022 22:01:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=416
05/21/2022 22:01:56 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.31681722509239957 on epoch=416
05/21/2022 22:01:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=418
05/21/2022 22:02:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.22 on epoch=419
05/21/2022 22:02:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=421
05/21/2022 22:02:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=423
05/21/2022 22:02:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.11 on epoch=424
05/21/2022 22:02:12 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.37474489136893235 on epoch=424
05/21/2022 22:02:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=426
05/21/2022 22:02:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=428
05/21/2022 22:02:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=429
05/21/2022 22:02:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=431
05/21/2022 22:02:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=433
05/21/2022 22:02:28 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.32451014392231664 on epoch=433
05/21/2022 22:02:30 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=434
05/21/2022 22:02:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=436
05/21/2022 22:02:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.16 on epoch=438
05/21/2022 22:02:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=439
05/21/2022 22:02:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.18 on epoch=441
05/21/2022 22:02:44 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.2785405071119356 on epoch=441
05/21/2022 22:02:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=443
05/21/2022 22:02:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=444
05/21/2022 22:02:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=446
05/21/2022 22:02:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.22 on epoch=448
05/21/2022 22:02:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=449
05/21/2022 22:03:00 - INFO - __main__ - Global step 2700 Train loss 0.16 Classification-F1 0.3142701525054466 on epoch=449
05/21/2022 22:03:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.17 on epoch=451
05/21/2022 22:03:05 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=453
05/21/2022 22:03:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.13 on epoch=454
05/21/2022 22:03:10 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=456
05/21/2022 22:03:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=458
05/21/2022 22:03:16 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.29682017543859646 on epoch=458
05/21/2022 22:03:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.20 on epoch=459
05/21/2022 22:03:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.15 on epoch=461
05/21/2022 22:03:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=463
05/21/2022 22:03:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.15 on epoch=464
05/21/2022 22:03:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=466
05/21/2022 22:03:32 - INFO - __main__ - Global step 2800 Train loss 0.16 Classification-F1 0.3328433424818967 on epoch=466
05/21/2022 22:03:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=468
05/21/2022 22:03:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.14 on epoch=469
05/21/2022 22:03:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=471
05/21/2022 22:03:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=473
05/21/2022 22:03:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=474
05/21/2022 22:03:48 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.30059026617663437 on epoch=474
05/21/2022 22:03:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=476
05/21/2022 22:03:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=478
05/21/2022 22:03:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=479
05/21/2022 22:03:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.10 on epoch=481
05/21/2022 22:04:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=483
05/21/2022 22:04:04 - INFO - __main__ - Global step 2900 Train loss 0.13 Classification-F1 0.24688346883468837 on epoch=483
05/21/2022 22:04:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=484
05/21/2022 22:04:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=486
05/21/2022 22:04:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=488
05/21/2022 22:04:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=489
05/21/2022 22:04:17 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=491
05/21/2022 22:04:20 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.28506062685884237 on epoch=491
05/21/2022 22:04:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.12 on epoch=493
05/21/2022 22:04:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=494
05/21/2022 22:04:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=496
05/21/2022 22:04:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=498
05/21/2022 22:04:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=499
05/21/2022 22:04:34 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:04:34 - INFO - __main__ - Printing 3 examples
05/21/2022 22:04:34 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/21/2022 22:04:34 - INFO - __main__ - ['neutral']
05/21/2022 22:04:34 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/21/2022 22:04:34 - INFO - __main__ - ['neutral']
05/21/2022 22:04:34 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/21/2022 22:04:34 - INFO - __main__ - ['neutral']
05/21/2022 22:04:34 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:04:34 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:04:34 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 22:04:34 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:04:34 - INFO - __main__ - Printing 3 examples
05/21/2022 22:04:34 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/21/2022 22:04:34 - INFO - __main__ - ['neutral']
05/21/2022 22:04:34 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/21/2022 22:04:34 - INFO - __main__ - ['neutral']
05/21/2022 22:04:34 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/21/2022 22:04:34 - INFO - __main__ - ['neutral']
05/21/2022 22:04:34 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:04:34 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:04:34 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 22:04:36 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.16260891588288132 on epoch=499
05/21/2022 22:04:36 - INFO - __main__ - save last model!
05/21/2022 22:04:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 22:04:36 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 22:04:36 - INFO - __main__ - Printing 3 examples
05/21/2022 22:04:36 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/21/2022 22:04:36 - INFO - __main__ - ['contradiction']
05/21/2022 22:04:36 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/21/2022 22:04:36 - INFO - __main__ - ['entailment']
05/21/2022 22:04:36 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/21/2022 22:04:36 - INFO - __main__ - ['contradiction']
05/21/2022 22:04:36 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:04:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:04:37 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 22:04:53 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 22:04:53 - INFO - __main__ - task name: anli
05/21/2022 22:04:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 22:04:54 - INFO - __main__ - Starting training!
05/21/2022 22:05:07 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_100_0.4_8_predictions.txt
05/21/2022 22:05:07 - INFO - __main__ - Classification-F1 on test data: 0.0564
05/21/2022 22:05:07 - INFO - __main__ - prefix=anli_32_100, lr=0.4, bsz=8, dev_performance=0.3858318636096414, test_performance=0.05643341328614131
05/21/2022 22:05:07 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.3, bsz=8 ...
05/21/2022 22:05:08 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:05:08 - INFO - __main__ - Printing 3 examples
05/21/2022 22:05:08 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/21/2022 22:05:08 - INFO - __main__ - ['neutral']
05/21/2022 22:05:08 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/21/2022 22:05:08 - INFO - __main__ - ['neutral']
05/21/2022 22:05:08 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/21/2022 22:05:08 - INFO - __main__ - ['neutral']
05/21/2022 22:05:08 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:05:08 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:05:08 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 22:05:08 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:05:08 - INFO - __main__ - Printing 3 examples
05/21/2022 22:05:08 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/21/2022 22:05:08 - INFO - __main__ - ['neutral']
05/21/2022 22:05:08 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/21/2022 22:05:08 - INFO - __main__ - ['neutral']
05/21/2022 22:05:08 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/21/2022 22:05:08 - INFO - __main__ - ['neutral']
05/21/2022 22:05:08 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:05:08 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:05:08 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 22:05:27 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 22:05:27 - INFO - __main__ - task name: anli
05/21/2022 22:05:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 22:05:28 - INFO - __main__ - Starting training!
05/21/2022 22:05:31 - INFO - __main__ - Step 10 Global step 10 Train loss 6.44 on epoch=1
05/21/2022 22:05:34 - INFO - __main__ - Step 20 Global step 20 Train loss 3.42 on epoch=3
05/21/2022 22:05:36 - INFO - __main__ - Step 30 Global step 30 Train loss 1.73 on epoch=4
05/21/2022 22:05:39 - INFO - __main__ - Step 40 Global step 40 Train loss 1.17 on epoch=6
05/21/2022 22:05:42 - INFO - __main__ - Step 50 Global step 50 Train loss 0.83 on epoch=8
05/21/2022 22:05:44 - INFO - __main__ - Global step 50 Train loss 2.72 Classification-F1 0.16666666666666666 on epoch=8
05/21/2022 22:05:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/21/2022 22:05:46 - INFO - __main__ - Step 60 Global step 60 Train loss 0.68 on epoch=9
05/21/2022 22:05:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=11
05/21/2022 22:05:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=13
05/21/2022 22:05:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=14
05/21/2022 22:05:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=16
05/21/2022 22:05:59 - INFO - __main__ - Global step 100 Train loss 0.61 Classification-F1 0.16666666666666666 on epoch=16
05/21/2022 22:06:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.57 on epoch=18
05/21/2022 22:06:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=19
05/21/2022 22:06:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=21
05/21/2022 22:06:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=23
05/21/2022 22:06:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=24
05/21/2022 22:06:15 - INFO - __main__ - Global step 150 Train loss 0.54 Classification-F1 0.24635349635349638 on epoch=24
05/21/2022 22:06:15 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.24635349635349638 on epoch=24, global_step=150
05/21/2022 22:06:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=26
05/21/2022 22:06:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=28
05/21/2022 22:06:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=29
05/21/2022 22:06:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=31
05/21/2022 22:06:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=33
05/21/2022 22:06:31 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.1693121693121693 on epoch=33
05/21/2022 22:06:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=34
05/21/2022 22:06:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=36
05/21/2022 22:06:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=38
05/21/2022 22:06:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=39
05/21/2022 22:06:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=41
05/21/2022 22:06:46 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=41
05/21/2022 22:06:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=43
05/21/2022 22:06:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=44
05/21/2022 22:06:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=46
05/21/2022 22:06:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=48
05/21/2022 22:07:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=49
05/21/2022 22:07:03 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.2489362489362489 on epoch=49
05/21/2022 22:07:03 - INFO - __main__ - Saving model with best Classification-F1: 0.24635349635349638 -> 0.2489362489362489 on epoch=49, global_step=300
05/21/2022 22:07:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=51
05/21/2022 22:07:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=53
05/21/2022 22:07:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=54
05/21/2022 22:07:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=56
05/21/2022 22:07:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=58
05/21/2022 22:07:19 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.26566416040100255 on epoch=58
05/21/2022 22:07:19 - INFO - __main__ - Saving model with best Classification-F1: 0.2489362489362489 -> 0.26566416040100255 on epoch=58, global_step=350
05/21/2022 22:07:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=59
05/21/2022 22:07:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=61
05/21/2022 22:07:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=63
05/21/2022 22:07:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=64
05/21/2022 22:07:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=66
05/21/2022 22:07:35 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
05/21/2022 22:07:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=68
05/21/2022 22:07:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=69
05/21/2022 22:07:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=71
05/21/2022 22:07:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=73
05/21/2022 22:07:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=74
05/21/2022 22:07:50 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=74
05/21/2022 22:07:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=76
05/21/2022 22:07:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=78
05/21/2022 22:07:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=79
05/21/2022 22:08:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=81
05/21/2022 22:08:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=83
05/21/2022 22:08:06 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.3022197660495533 on epoch=83
05/21/2022 22:08:06 - INFO - __main__ - Saving model with best Classification-F1: 0.26566416040100255 -> 0.3022197660495533 on epoch=83, global_step=500
05/21/2022 22:08:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=84
05/21/2022 22:08:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=86
05/21/2022 22:08:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=88
05/21/2022 22:08:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=89
05/21/2022 22:08:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=91
05/21/2022 22:08:22 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=91
05/21/2022 22:08:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=93
05/21/2022 22:08:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=94
05/21/2022 22:08:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=96
05/21/2022 22:08:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=98
05/21/2022 22:08:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=99
05/21/2022 22:08:38 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.2476579799012093 on epoch=99
05/21/2022 22:08:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=101
05/21/2022 22:08:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=103
05/21/2022 22:08:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=104
05/21/2022 22:08:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=106
05/21/2022 22:08:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=108
05/21/2022 22:08:54 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=108
05/21/2022 22:08:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=109
05/21/2022 22:08:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=111
05/21/2022 22:09:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=113
05/21/2022 22:09:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=114
05/21/2022 22:09:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=116
05/21/2022 22:09:10 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=116
05/21/2022 22:09:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=118
05/21/2022 22:09:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=119
05/21/2022 22:09:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=121
05/21/2022 22:09:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=123
05/21/2022 22:09:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=124
05/21/2022 22:09:25 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.2570356472795497 on epoch=124
05/21/2022 22:09:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=126
05/21/2022 22:09:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=128
05/21/2022 22:09:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=129
05/21/2022 22:09:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=131
05/21/2022 22:09:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=133
05/21/2022 22:09:41 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.26733143399810066 on epoch=133
05/21/2022 22:09:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=134
05/21/2022 22:09:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=136
05/21/2022 22:09:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=138
05/21/2022 22:09:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=139
05/21/2022 22:09:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=141
05/21/2022 22:09:57 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=141
05/21/2022 22:09:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=143
05/21/2022 22:10:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=144
05/21/2022 22:10:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=146
05/21/2022 22:10:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=148
05/21/2022 22:10:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=149
05/21/2022 22:10:13 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.2187738787985658 on epoch=149
05/21/2022 22:10:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=151
05/21/2022 22:10:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=153
05/21/2022 22:10:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=154
05/21/2022 22:10:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=156
05/21/2022 22:10:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=158
05/21/2022 22:10:28 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.2718639385306052 on epoch=158
05/21/2022 22:10:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=159
05/21/2022 22:10:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=161
05/21/2022 22:10:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=163
05/21/2022 22:10:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=164
05/21/2022 22:10:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=166
05/21/2022 22:10:44 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=166
05/21/2022 22:10:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=168
05/21/2022 22:10:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=169
05/21/2022 22:10:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=171
05/21/2022 22:10:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=173
05/21/2022 22:10:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=174
05/21/2022 22:11:00 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.23489377227694985 on epoch=174
05/21/2022 22:11:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=176
05/21/2022 22:11:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=178
05/21/2022 22:11:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=179
05/21/2022 22:11:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=181
05/21/2022 22:11:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=183
05/21/2022 22:11:15 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.30620240229818574 on epoch=183
05/21/2022 22:11:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3022197660495533 -> 0.30620240229818574 on epoch=183, global_step=1100
05/21/2022 22:11:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=184
05/21/2022 22:11:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=186
05/21/2022 22:11:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=188
05/21/2022 22:11:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=189
05/21/2022 22:11:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=191
05/21/2022 22:11:31 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.2747304266291608 on epoch=191
05/21/2022 22:11:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=193
05/21/2022 22:11:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=194
05/21/2022 22:11:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=196
05/21/2022 22:11:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=198
05/21/2022 22:11:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=199
05/21/2022 22:11:47 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.22222222222222218 on epoch=199
05/21/2022 22:11:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=201
05/21/2022 22:11:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=203
05/21/2022 22:11:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=204
05/21/2022 22:11:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=206
05/21/2022 22:12:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=208
05/21/2022 22:12:03 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.2040577162528382 on epoch=208
05/21/2022 22:12:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=209
05/21/2022 22:12:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=211
05/21/2022 22:12:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=213
05/21/2022 22:12:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=214
05/21/2022 22:12:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=216
05/21/2022 22:12:19 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.21779367512632716 on epoch=216
05/21/2022 22:12:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=218
05/21/2022 22:12:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=219
05/21/2022 22:12:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=221
05/21/2022 22:12:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=223
05/21/2022 22:12:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=224
05/21/2022 22:12:34 - INFO - __main__ - Global step 1350 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=224
05/21/2022 22:12:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=226
05/21/2022 22:12:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=228
05/21/2022 22:12:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=229
05/21/2022 22:12:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=231
05/21/2022 22:12:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=233
05/21/2022 22:12:50 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.3150714020279238 on epoch=233
05/21/2022 22:12:50 - INFO - __main__ - Saving model with best Classification-F1: 0.30620240229818574 -> 0.3150714020279238 on epoch=233, global_step=1400
05/21/2022 22:12:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=234
05/21/2022 22:12:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=236
05/21/2022 22:12:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.39 on epoch=238
05/21/2022 22:13:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=239
05/21/2022 22:13:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=241
05/21/2022 22:13:06 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.2951025280686643 on epoch=241
05/21/2022 22:13:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=243
05/21/2022 22:13:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=244
05/21/2022 22:13:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=246
05/21/2022 22:13:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=248
05/21/2022 22:13:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=249
05/21/2022 22:13:22 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.26666666666666666 on epoch=249
05/21/2022 22:13:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=251
05/21/2022 22:13:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=253
05/21/2022 22:13:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=254
05/21/2022 22:13:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=256
05/21/2022 22:13:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=258
05/21/2022 22:13:38 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.23470755823697 on epoch=258
05/21/2022 22:13:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=259
05/21/2022 22:13:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=261
05/21/2022 22:13:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=263
05/21/2022 22:13:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=264
05/21/2022 22:13:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=266
05/21/2022 22:13:54 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.2124388539482879 on epoch=266
05/21/2022 22:13:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=268
05/21/2022 22:13:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=269
05/21/2022 22:14:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=271
05/21/2022 22:14:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.38 on epoch=273
05/21/2022 22:14:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=274
05/21/2022 22:14:09 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.2215780998389694 on epoch=274
05/21/2022 22:14:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=276
05/21/2022 22:14:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=278
05/21/2022 22:14:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=279
05/21/2022 22:14:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=281
05/21/2022 22:14:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=283
05/21/2022 22:14:25 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.34130471956224345 on epoch=283
05/21/2022 22:14:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3150714020279238 -> 0.34130471956224345 on epoch=283, global_step=1700
05/21/2022 22:14:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=284
05/21/2022 22:14:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=286
05/21/2022 22:14:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=288
05/21/2022 22:14:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=289
05/21/2022 22:14:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.40 on epoch=291
05/21/2022 22:14:41 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.16191842590694813 on epoch=291
05/21/2022 22:14:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=293
05/21/2022 22:14:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=294
05/21/2022 22:14:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=296
05/21/2022 22:14:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=298
05/21/2022 22:14:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=299
05/21/2022 22:14:57 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.20342857142857143 on epoch=299
05/21/2022 22:14:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=301
05/21/2022 22:15:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=303
05/21/2022 22:15:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=304
05/21/2022 22:15:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=306
05/21/2022 22:15:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=308
05/21/2022 22:15:12 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.22777777777777777 on epoch=308
05/21/2022 22:15:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=309
05/21/2022 22:15:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=311
05/21/2022 22:15:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=313
05/21/2022 22:15:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=314
05/21/2022 22:15:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=316
05/21/2022 22:15:28 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.2525778331257783 on epoch=316
05/21/2022 22:15:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=318
05/21/2022 22:15:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=319
05/21/2022 22:15:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.32 on epoch=321
05/21/2022 22:15:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.38 on epoch=323
05/21/2022 22:15:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=324
05/21/2022 22:15:44 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.2252607374558594 on epoch=324
05/21/2022 22:15:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=326
05/21/2022 22:15:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=328
05/21/2022 22:15:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=329
05/21/2022 22:15:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.34 on epoch=331
05/21/2022 22:15:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=333
05/21/2022 22:16:00 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.2762016675060153 on epoch=333
05/21/2022 22:16:02 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.32 on epoch=334
05/21/2022 22:16:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.29 on epoch=336
05/21/2022 22:16:07 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.31 on epoch=338
05/21/2022 22:16:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.31 on epoch=339
05/21/2022 22:16:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.34 on epoch=341
05/21/2022 22:16:15 - INFO - __main__ - Global step 2050 Train loss 0.31 Classification-F1 0.2538923829489867 on epoch=341
05/21/2022 22:16:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.33 on epoch=343
05/21/2022 22:16:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.34 on epoch=344
05/21/2022 22:16:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.29 on epoch=346
05/21/2022 22:16:26 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.33 on epoch=348
05/21/2022 22:16:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.32 on epoch=349
05/21/2022 22:16:31 - INFO - __main__ - Global step 2100 Train loss 0.32 Classification-F1 0.24074074074074073 on epoch=349
05/21/2022 22:16:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.27 on epoch=351
05/21/2022 22:16:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.27 on epoch=353
05/21/2022 22:16:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.28 on epoch=354
05/21/2022 22:16:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.27 on epoch=356
05/21/2022 22:16:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.25 on epoch=358
05/21/2022 22:16:47 - INFO - __main__ - Global step 2150 Train loss 0.27 Classification-F1 0.25136943403354245 on epoch=358
05/21/2022 22:16:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.28 on epoch=359
05/21/2022 22:16:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.30 on epoch=361
05/21/2022 22:16:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.25 on epoch=363
05/21/2022 22:16:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.28 on epoch=364
05/21/2022 22:17:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=366
05/21/2022 22:17:02 - INFO - __main__ - Global step 2200 Train loss 0.26 Classification-F1 0.2604793580403337 on epoch=366
05/21/2022 22:17:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=368
05/21/2022 22:17:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.23 on epoch=369
05/21/2022 22:17:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=371
05/21/2022 22:17:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.28 on epoch=373
05/21/2022 22:17:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.29 on epoch=374
05/21/2022 22:17:18 - INFO - __main__ - Global step 2250 Train loss 0.26 Classification-F1 0.35714285714285715 on epoch=374
05/21/2022 22:17:18 - INFO - __main__ - Saving model with best Classification-F1: 0.34130471956224345 -> 0.35714285714285715 on epoch=374, global_step=2250
05/21/2022 22:17:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.25 on epoch=376
05/21/2022 22:17:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.28 on epoch=378
05/21/2022 22:17:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=379
05/21/2022 22:17:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=381
05/21/2022 22:17:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.32 on epoch=383
05/21/2022 22:17:34 - INFO - __main__ - Global step 2300 Train loss 0.26 Classification-F1 0.3006493506493506 on epoch=383
05/21/2022 22:17:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.25 on epoch=384
05/21/2022 22:17:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=386
05/21/2022 22:17:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.28 on epoch=388
05/21/2022 22:17:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.26 on epoch=389
05/21/2022 22:17:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=391
05/21/2022 22:17:50 - INFO - __main__ - Global step 2350 Train loss 0.25 Classification-F1 0.27946127946127947 on epoch=391
05/21/2022 22:17:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.25 on epoch=393
05/21/2022 22:17:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=394
05/21/2022 22:17:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=396
05/21/2022 22:18:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.28 on epoch=398
05/21/2022 22:18:03 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.24 on epoch=399
05/21/2022 22:18:05 - INFO - __main__ - Global step 2400 Train loss 0.24 Classification-F1 0.21031746031746035 on epoch=399
05/21/2022 22:18:08 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.23 on epoch=401
05/21/2022 22:18:11 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.21 on epoch=403
05/21/2022 22:18:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.30 on epoch=404
05/21/2022 22:18:16 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=406
05/21/2022 22:18:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.23 on epoch=408
05/21/2022 22:18:21 - INFO - __main__ - Global step 2450 Train loss 0.23 Classification-F1 0.231336571325778 on epoch=408
05/21/2022 22:18:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.23 on epoch=409
05/21/2022 22:18:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=411
05/21/2022 22:18:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.20 on epoch=413
05/21/2022 22:18:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=414
05/21/2022 22:18:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=416
05/21/2022 22:18:37 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.23981640849110727 on epoch=416
05/21/2022 22:18:39 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.22 on epoch=418
05/21/2022 22:18:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=419
05/21/2022 22:18:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=421
05/21/2022 22:18:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=423
05/21/2022 22:18:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=424
05/21/2022 22:18:52 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.2598039215686274 on epoch=424
05/21/2022 22:18:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.27 on epoch=426
05/21/2022 22:18:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.21 on epoch=428
05/21/2022 22:19:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.23 on epoch=429
05/21/2022 22:19:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.17 on epoch=431
05/21/2022 22:19:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.21 on epoch=433
05/21/2022 22:19:08 - INFO - __main__ - Global step 2600 Train loss 0.22 Classification-F1 0.3079393398751115 on epoch=433
05/21/2022 22:19:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.25 on epoch=434
05/21/2022 22:19:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.21 on epoch=436
05/21/2022 22:19:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=438
05/21/2022 22:19:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.17 on epoch=439
05/21/2022 22:19:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=441
05/21/2022 22:19:24 - INFO - __main__ - Global step 2650 Train loss 0.20 Classification-F1 0.3381091617933723 on epoch=441
05/21/2022 22:19:26 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=443
05/21/2022 22:19:29 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=444
05/21/2022 22:19:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.21 on epoch=446
05/21/2022 22:19:34 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=448
05/21/2022 22:19:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=449
05/21/2022 22:19:39 - INFO - __main__ - Global step 2700 Train loss 0.18 Classification-F1 0.3218083503797789 on epoch=449
05/21/2022 22:19:42 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.17 on epoch=451
05/21/2022 22:19:45 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.23 on epoch=453
05/21/2022 22:19:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=454
05/21/2022 22:19:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=456
05/21/2022 22:19:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=458
05/21/2022 22:19:55 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.2769781820343618 on epoch=458
05/21/2022 22:19:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.17 on epoch=459
05/21/2022 22:20:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=461
05/21/2022 22:20:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.19 on epoch=463
05/21/2022 22:20:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.18 on epoch=464
05/21/2022 22:20:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=466
05/21/2022 22:20:11 - INFO - __main__ - Global step 2800 Train loss 0.17 Classification-F1 0.1822201666802695 on epoch=466
05/21/2022 22:20:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=468
05/21/2022 22:20:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=469
05/21/2022 22:20:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.21 on epoch=471
05/21/2022 22:20:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.14 on epoch=473
05/21/2022 22:20:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=474
05/21/2022 22:20:27 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.25745257452574527 on epoch=474
05/21/2022 22:20:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=476
05/21/2022 22:20:32 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.17 on epoch=478
05/21/2022 22:20:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.17 on epoch=479
05/21/2022 22:20:37 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=481
05/21/2022 22:20:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=483
05/21/2022 22:20:42 - INFO - __main__ - Global step 2900 Train loss 0.16 Classification-F1 0.2588042588042588 on epoch=483
05/21/2022 22:20:45 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=484
05/21/2022 22:20:48 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=486
05/21/2022 22:20:50 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=488
05/21/2022 22:20:53 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=489
05/21/2022 22:20:55 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.20 on epoch=491
05/21/2022 22:20:58 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.21416818475642005 on epoch=491
05/21/2022 22:21:01 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.14 on epoch=493
05/21/2022 22:21:03 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.17 on epoch=494
05/21/2022 22:21:06 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.17 on epoch=496
05/21/2022 22:21:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.15 on epoch=498
05/21/2022 22:21:11 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.12 on epoch=499
05/21/2022 22:21:13 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:21:13 - INFO - __main__ - Printing 3 examples
05/21/2022 22:21:13 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/21/2022 22:21:13 - INFO - __main__ - ['neutral']
05/21/2022 22:21:13 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/21/2022 22:21:13 - INFO - __main__ - ['neutral']
05/21/2022 22:21:13 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/21/2022 22:21:13 - INFO - __main__ - ['neutral']
05/21/2022 22:21:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:21:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:21:13 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 22:21:13 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:21:13 - INFO - __main__ - Printing 3 examples
05/21/2022 22:21:13 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/21/2022 22:21:13 - INFO - __main__ - ['neutral']
05/21/2022 22:21:13 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/21/2022 22:21:13 - INFO - __main__ - ['neutral']
05/21/2022 22:21:13 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/21/2022 22:21:13 - INFO - __main__ - ['neutral']
05/21/2022 22:21:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:21:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:21:13 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 22:21:14 - INFO - __main__ - Global step 3000 Train loss 0.15 Classification-F1 0.27309663753153895 on epoch=499
05/21/2022 22:21:14 - INFO - __main__ - save last model!
05/21/2022 22:21:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 22:21:14 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 22:21:14 - INFO - __main__ - Printing 3 examples
05/21/2022 22:21:14 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/21/2022 22:21:14 - INFO - __main__ - ['contradiction']
05/21/2022 22:21:14 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/21/2022 22:21:14 - INFO - __main__ - ['entailment']
05/21/2022 22:21:14 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/21/2022 22:21:14 - INFO - __main__ - ['contradiction']
05/21/2022 22:21:14 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:21:15 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:21:16 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 22:21:28 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 22:21:28 - INFO - __main__ - task name: anli
05/21/2022 22:21:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 22:21:29 - INFO - __main__ - Starting training!
05/21/2022 22:21:45 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_100_0.3_8_predictions.txt
05/21/2022 22:21:45 - INFO - __main__ - Classification-F1 on test data: 0.2370
05/21/2022 22:21:46 - INFO - __main__ - prefix=anli_32_100, lr=0.3, bsz=8, dev_performance=0.35714285714285715, test_performance=0.23703410584103426
05/21/2022 22:21:46 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.2, bsz=8 ...
05/21/2022 22:21:47 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:21:47 - INFO - __main__ - Printing 3 examples
05/21/2022 22:21:47 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/21/2022 22:21:47 - INFO - __main__ - ['neutral']
05/21/2022 22:21:47 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/21/2022 22:21:47 - INFO - __main__ - ['neutral']
05/21/2022 22:21:47 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/21/2022 22:21:47 - INFO - __main__ - ['neutral']
05/21/2022 22:21:47 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:21:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:21:47 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 22:21:47 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:21:47 - INFO - __main__ - Printing 3 examples
05/21/2022 22:21:47 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/21/2022 22:21:47 - INFO - __main__ - ['neutral']
05/21/2022 22:21:47 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/21/2022 22:21:47 - INFO - __main__ - ['neutral']
05/21/2022 22:21:47 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/21/2022 22:21:47 - INFO - __main__ - ['neutral']
05/21/2022 22:21:47 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:21:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:21:47 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 22:22:05 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 22:22:05 - INFO - __main__ - task name: anli
05/21/2022 22:22:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 22:22:06 - INFO - __main__ - Starting training!
05/21/2022 22:22:10 - INFO - __main__ - Step 10 Global step 10 Train loss 6.97 on epoch=1
05/21/2022 22:22:12 - INFO - __main__ - Step 20 Global step 20 Train loss 4.80 on epoch=3
05/21/2022 22:22:15 - INFO - __main__ - Step 30 Global step 30 Train loss 2.92 on epoch=4
05/21/2022 22:22:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.90 on epoch=6
05/21/2022 22:22:20 - INFO - __main__ - Step 50 Global step 50 Train loss 1.39 on epoch=8
05/21/2022 22:22:22 - INFO - __main__ - Global step 50 Train loss 3.60 Classification-F1 0.18892001244942422 on epoch=8
05/21/2022 22:22:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18892001244942422 on epoch=8, global_step=50
05/21/2022 22:22:24 - INFO - __main__ - Step 60 Global step 60 Train loss 1.24 on epoch=9
05/21/2022 22:22:27 - INFO - __main__ - Step 70 Global step 70 Train loss 1.03 on epoch=11
05/21/2022 22:22:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.83 on epoch=13
05/21/2022 22:22:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.76 on epoch=14
05/21/2022 22:22:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=16
05/21/2022 22:22:37 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.16666666666666666 on epoch=16
05/21/2022 22:22:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=18
05/21/2022 22:22:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=19
05/21/2022 22:22:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.62 on epoch=21
05/21/2022 22:22:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.66 on epoch=23
05/21/2022 22:22:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.66 on epoch=24
05/21/2022 22:22:52 - INFO - __main__ - Global step 150 Train loss 0.64 Classification-F1 0.1881810228266921 on epoch=24
05/21/2022 22:22:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=26
05/21/2022 22:22:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=28
05/21/2022 22:23:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.59 on epoch=29
05/21/2022 22:23:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=31
05/21/2022 22:23:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=33
05/21/2022 22:23:08 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=33
05/21/2022 22:23:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=34
05/21/2022 22:23:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=36
05/21/2022 22:23:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=38
05/21/2022 22:23:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.59 on epoch=39
05/21/2022 22:23:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=41
05/21/2022 22:23:23 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=41
05/21/2022 22:23:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=43
05/21/2022 22:23:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=44
05/21/2022 22:23:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=46
05/21/2022 22:23:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=48
05/21/2022 22:23:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=49
05/21/2022 22:23:38 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=49
05/21/2022 22:23:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=51
05/21/2022 22:23:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=53
05/21/2022 22:23:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.57 on epoch=54
05/21/2022 22:23:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=56
05/21/2022 22:23:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=58
05/21/2022 22:23:53 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=58
05/21/2022 22:23:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=59
05/21/2022 22:23:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=61
05/21/2022 22:24:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=63
05/21/2022 22:24:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=64
05/21/2022 22:24:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=66
05/21/2022 22:24:08 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
05/21/2022 22:24:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=68
05/21/2022 22:24:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=69
05/21/2022 22:24:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=71
05/21/2022 22:24:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=73
05/21/2022 22:24:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=74
05/21/2022 22:24:24 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=74
05/21/2022 22:24:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=76
05/21/2022 22:24:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=78
05/21/2022 22:24:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=79
05/21/2022 22:24:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=81
05/21/2022 22:24:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=83
05/21/2022 22:24:39 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=83
05/21/2022 22:24:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.49 on epoch=84
05/21/2022 22:24:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=86
05/21/2022 22:24:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=88
05/21/2022 22:24:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=89
05/21/2022 22:24:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=91
05/21/2022 22:24:55 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=91
05/21/2022 22:24:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=93
05/21/2022 22:25:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=94
05/21/2022 22:25:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=96
05/21/2022 22:25:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=98
05/21/2022 22:25:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=99
05/21/2022 22:25:10 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=99
05/21/2022 22:25:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=101
05/21/2022 22:25:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=103
05/21/2022 22:25:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.49 on epoch=104
05/21/2022 22:25:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=106
05/21/2022 22:25:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=108
05/21/2022 22:25:25 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=108
05/21/2022 22:25:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=109
05/21/2022 22:25:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=111
05/21/2022 22:25:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=113
05/21/2022 22:25:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.49 on epoch=114
05/21/2022 22:25:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=116
05/21/2022 22:25:40 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=116
05/21/2022 22:25:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=118
05/21/2022 22:25:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=119
05/21/2022 22:25:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=121
05/21/2022 22:25:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=123
05/21/2022 22:25:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=124
05/21/2022 22:25:56 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=124
05/21/2022 22:25:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=126
05/21/2022 22:26:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=128
05/21/2022 22:26:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=129
05/21/2022 22:26:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=131
05/21/2022 22:26:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=133
05/21/2022 22:26:11 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.29024300635070593 on epoch=133
05/21/2022 22:26:11 - INFO - __main__ - Saving model with best Classification-F1: 0.18892001244942422 -> 0.29024300635070593 on epoch=133, global_step=800
05/21/2022 22:26:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=134
05/21/2022 22:26:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=136
05/21/2022 22:26:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=138
05/21/2022 22:26:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=139
05/21/2022 22:26:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=141
05/21/2022 22:26:27 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=141
05/21/2022 22:26:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=143
05/21/2022 22:26:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=144
05/21/2022 22:26:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=146
05/21/2022 22:26:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=148
05/21/2022 22:26:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=149
05/21/2022 22:26:42 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.25120772946859904 on epoch=149
05/21/2022 22:26:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=151
05/21/2022 22:26:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=153
05/21/2022 22:26:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=154
05/21/2022 22:26:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=156
05/21/2022 22:26:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=158
05/21/2022 22:26:58 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.3110070257611241 on epoch=158
05/21/2022 22:26:58 - INFO - __main__ - Saving model with best Classification-F1: 0.29024300635070593 -> 0.3110070257611241 on epoch=158, global_step=950
05/21/2022 22:27:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=159
05/21/2022 22:27:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=161
05/21/2022 22:27:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=163
05/21/2022 22:27:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=164
05/21/2022 22:27:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=166
05/21/2022 22:27:14 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=166
05/21/2022 22:27:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=168
05/21/2022 22:27:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=169
05/21/2022 22:27:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=171
05/21/2022 22:27:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=173
05/21/2022 22:27:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=174
05/21/2022 22:27:28 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.1881810228266921 on epoch=174
05/21/2022 22:27:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=176
05/21/2022 22:27:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=178
05/21/2022 22:27:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=179
05/21/2022 22:27:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=181
05/21/2022 22:27:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=183
05/21/2022 22:27:44 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.2768199233716475 on epoch=183
05/21/2022 22:27:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=184
05/21/2022 22:27:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=186
05/21/2022 22:27:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=188
05/21/2022 22:27:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=189
05/21/2022 22:27:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=191
05/21/2022 22:27:59 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=191
05/21/2022 22:28:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=193
05/21/2022 22:28:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=194
05/21/2022 22:28:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=196
05/21/2022 22:28:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=198
05/21/2022 22:28:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=199
05/21/2022 22:28:13 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=199
05/21/2022 22:28:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=201
05/21/2022 22:28:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=203
05/21/2022 22:28:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=204
05/21/2022 22:28:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=206
05/21/2022 22:28:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=208
05/21/2022 22:28:29 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.2555555555555556 on epoch=208
05/21/2022 22:28:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=209
05/21/2022 22:28:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=211
05/21/2022 22:28:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=213
05/21/2022 22:28:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=214
05/21/2022 22:28:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=216
05/21/2022 22:28:44 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=216
05/21/2022 22:28:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=218
05/21/2022 22:28:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=219
05/21/2022 22:28:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=221
05/21/2022 22:28:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=223
05/21/2022 22:28:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=224
05/21/2022 22:29:00 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.26188866351138157 on epoch=224
05/21/2022 22:29:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=226
05/21/2022 22:29:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=228
05/21/2022 22:29:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=229
05/21/2022 22:29:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=231
05/21/2022 22:29:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=233
05/21/2022 22:29:16 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.35866934970325276 on epoch=233
05/21/2022 22:29:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3110070257611241 -> 0.35866934970325276 on epoch=233, global_step=1400
05/21/2022 22:29:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=234
05/21/2022 22:29:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=236
05/21/2022 22:29:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=238
05/21/2022 22:29:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=239
05/21/2022 22:29:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.45 on epoch=241
05/21/2022 22:29:32 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=241
05/21/2022 22:29:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=243
05/21/2022 22:29:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=244
05/21/2022 22:29:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=246
05/21/2022 22:29:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=248
05/21/2022 22:29:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=249
05/21/2022 22:29:48 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.1881810228266921 on epoch=249
05/21/2022 22:29:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=251
05/21/2022 22:29:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=253
05/21/2022 22:29:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.45 on epoch=254
05/21/2022 22:29:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.39 on epoch=256
05/21/2022 22:30:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=258
05/21/2022 22:30:05 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.3890094289081753 on epoch=258
05/21/2022 22:30:05 - INFO - __main__ - Saving model with best Classification-F1: 0.35866934970325276 -> 0.3890094289081753 on epoch=258, global_step=1550
05/21/2022 22:30:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=259
05/21/2022 22:30:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=261
05/21/2022 22:30:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=263
05/21/2022 22:30:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=264
05/21/2022 22:30:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=266
05/21/2022 22:30:21 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.1881810228266921 on epoch=266
05/21/2022 22:30:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=268
05/21/2022 22:30:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=269
05/21/2022 22:30:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=271
05/21/2022 22:30:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=273
05/21/2022 22:30:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=274
05/21/2022 22:30:37 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.3962563258337906 on epoch=274
05/21/2022 22:30:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3890094289081753 -> 0.3962563258337906 on epoch=274, global_step=1650
05/21/2022 22:30:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=276
05/21/2022 22:30:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=278
05/21/2022 22:30:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=279
05/21/2022 22:30:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.34 on epoch=281
05/21/2022 22:30:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=283
05/21/2022 22:30:53 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.23368338121270935 on epoch=283
05/21/2022 22:30:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=284
05/21/2022 22:30:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=286
05/21/2022 22:31:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=288
05/21/2022 22:31:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=289
05/21/2022 22:31:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=291
05/21/2022 22:31:09 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=291
05/21/2022 22:31:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=293
05/21/2022 22:31:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=294
05/21/2022 22:31:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=296
05/21/2022 22:31:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=298
05/21/2022 22:31:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=299
05/21/2022 22:31:25 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.32355464708405884 on epoch=299
05/21/2022 22:31:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=301
05/21/2022 22:31:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=303
05/21/2022 22:31:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=304
05/21/2022 22:31:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=306
05/21/2022 22:31:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=308
05/21/2022 22:31:41 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.35893461468909044 on epoch=308
05/21/2022 22:31:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=309
05/21/2022 22:31:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=311
05/21/2022 22:31:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=313
05/21/2022 22:31:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=314
05/21/2022 22:31:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=316
05/21/2022 22:31:57 - INFO - __main__ - Global step 1900 Train loss 0.39 Classification-F1 0.22152560083594564 on epoch=316
05/21/2022 22:32:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=318
05/21/2022 22:32:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=319
05/21/2022 22:32:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=321
05/21/2022 22:32:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=323
05/21/2022 22:32:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=324
05/21/2022 22:32:13 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.22208094935367664 on epoch=324
05/21/2022 22:32:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=326
05/21/2022 22:32:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=328
05/21/2022 22:32:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=329
05/21/2022 22:32:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=331
05/21/2022 22:32:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=333
05/21/2022 22:32:30 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.3983019564414913 on epoch=333
05/21/2022 22:32:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3962563258337906 -> 0.3983019564414913 on epoch=333, global_step=2000
05/21/2022 22:32:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=334
05/21/2022 22:32:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.36 on epoch=336
05/21/2022 22:32:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.37 on epoch=338
05/21/2022 22:32:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.40 on epoch=339
05/21/2022 22:32:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.36 on epoch=341
05/21/2022 22:32:46 - INFO - __main__ - Global step 2050 Train loss 0.37 Classification-F1 0.21818181818181817 on epoch=341
05/21/2022 22:32:49 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.38 on epoch=343
05/21/2022 22:32:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.40 on epoch=344
05/21/2022 22:32:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.39 on epoch=346
05/21/2022 22:32:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.39 on epoch=348
05/21/2022 22:32:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.36 on epoch=349
05/21/2022 22:33:02 - INFO - __main__ - Global step 2100 Train loss 0.38 Classification-F1 0.3231364796968948 on epoch=349
05/21/2022 22:33:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.32 on epoch=351
05/21/2022 22:33:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.30 on epoch=353
05/21/2022 22:33:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.40 on epoch=354
05/21/2022 22:33:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.36 on epoch=356
05/21/2022 22:33:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.36 on epoch=358
05/21/2022 22:33:19 - INFO - __main__ - Global step 2150 Train loss 0.35 Classification-F1 0.41455431828516437 on epoch=358
05/21/2022 22:33:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3983019564414913 -> 0.41455431828516437 on epoch=358, global_step=2150
05/21/2022 22:33:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.38 on epoch=359
05/21/2022 22:33:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.34 on epoch=361
05/21/2022 22:33:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.31 on epoch=363
05/21/2022 22:33:29 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.40 on epoch=364
05/21/2022 22:33:32 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.34 on epoch=366
05/21/2022 22:33:35 - INFO - __main__ - Global step 2200 Train loss 0.35 Classification-F1 0.27121543373064155 on epoch=366
05/21/2022 22:33:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=368
05/21/2022 22:33:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.38 on epoch=369
05/21/2022 22:33:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.32 on epoch=371
05/21/2022 22:33:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.31 on epoch=373
05/21/2022 22:33:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.43 on epoch=374
05/21/2022 22:33:51 - INFO - __main__ - Global step 2250 Train loss 0.36 Classification-F1 0.3513865195898875 on epoch=374
05/21/2022 22:33:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.31 on epoch=376
05/21/2022 22:33:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.35 on epoch=378
05/21/2022 22:33:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.32 on epoch=379
05/21/2022 22:34:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.31 on epoch=381
05/21/2022 22:34:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.29 on epoch=383
05/21/2022 22:34:07 - INFO - __main__ - Global step 2300 Train loss 0.32 Classification-F1 0.3709245977902695 on epoch=383
05/21/2022 22:34:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.32 on epoch=384
05/21/2022 22:34:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.34 on epoch=386
05/21/2022 22:34:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.31 on epoch=388
05/21/2022 22:34:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.36 on epoch=389
05/21/2022 22:34:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.34 on epoch=391
05/21/2022 22:34:23 - INFO - __main__ - Global step 2350 Train loss 0.34 Classification-F1 0.1881810228266921 on epoch=391
05/21/2022 22:34:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=393
05/21/2022 22:34:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.41 on epoch=394
05/21/2022 22:34:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.38 on epoch=396
05/21/2022 22:34:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.35 on epoch=398
05/21/2022 22:34:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.36 on epoch=399
05/21/2022 22:34:40 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.35180537035584486 on epoch=399
05/21/2022 22:34:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.41 on epoch=401
05/21/2022 22:34:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.33 on epoch=403
05/21/2022 22:34:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.40 on epoch=404
05/21/2022 22:34:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.38 on epoch=406
05/21/2022 22:34:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=408
05/21/2022 22:34:56 - INFO - __main__ - Global step 2450 Train loss 0.37 Classification-F1 0.19027450980392158 on epoch=408
05/21/2022 22:34:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.35 on epoch=409
05/21/2022 22:35:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.32 on epoch=411
05/21/2022 22:35:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.33 on epoch=413
05/21/2022 22:35:07 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.39 on epoch=414
05/21/2022 22:35:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.34 on epoch=416
05/21/2022 22:35:12 - INFO - __main__ - Global step 2500 Train loss 0.35 Classification-F1 0.2550654320565825 on epoch=416
05/21/2022 22:35:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.34 on epoch=418
05/21/2022 22:35:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.34 on epoch=419
05/21/2022 22:35:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.37 on epoch=421
05/21/2022 22:35:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.37 on epoch=423
05/21/2022 22:35:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.34 on epoch=424
05/21/2022 22:35:29 - INFO - __main__ - Global step 2550 Train loss 0.35 Classification-F1 0.4072216311158399 on epoch=424
05/21/2022 22:35:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.36 on epoch=426
05/21/2022 22:35:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.33 on epoch=428
05/21/2022 22:35:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.35 on epoch=429
05/21/2022 22:35:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.35 on epoch=431
05/21/2022 22:35:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.31 on epoch=433
05/21/2022 22:35:45 - INFO - __main__ - Global step 2600 Train loss 0.34 Classification-F1 0.32422616692279616 on epoch=433
05/21/2022 22:35:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.34 on epoch=434
05/21/2022 22:35:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.32 on epoch=436
05/21/2022 22:35:53 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.37 on epoch=438
05/21/2022 22:35:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.26 on epoch=439
05/21/2022 22:35:58 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.29 on epoch=441
05/21/2022 22:36:01 - INFO - __main__ - Global step 2650 Train loss 0.32 Classification-F1 0.3385925868195372 on epoch=441
05/21/2022 22:36:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.24 on epoch=443
05/21/2022 22:36:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.31 on epoch=444
05/21/2022 22:36:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.30 on epoch=446
05/21/2022 22:36:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.32 on epoch=448
05/21/2022 22:36:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.26 on epoch=449
05/21/2022 22:36:18 - INFO - __main__ - Global step 2700 Train loss 0.29 Classification-F1 0.3597600656424185 on epoch=449
05/21/2022 22:36:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.31 on epoch=451
05/21/2022 22:36:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.29 on epoch=453
05/21/2022 22:36:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.24 on epoch=454
05/21/2022 22:36:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.26 on epoch=456
05/21/2022 22:36:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.29 on epoch=458
05/21/2022 22:36:34 - INFO - __main__ - Global step 2750 Train loss 0.28 Classification-F1 0.34414503665543883 on epoch=458
05/21/2022 22:36:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.27 on epoch=459
05/21/2022 22:36:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.28 on epoch=461
05/21/2022 22:36:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.23 on epoch=463
05/21/2022 22:36:45 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.25 on epoch=464
05/21/2022 22:36:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.26 on epoch=466
05/21/2022 22:36:50 - INFO - __main__ - Global step 2800 Train loss 0.26 Classification-F1 0.30140291806958475 on epoch=466
05/21/2022 22:36:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.26 on epoch=468
05/21/2022 22:36:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.28 on epoch=469
05/21/2022 22:36:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.29 on epoch=471
05/21/2022 22:37:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.27 on epoch=473
05/21/2022 22:37:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.21 on epoch=474
05/21/2022 22:37:07 - INFO - __main__ - Global step 2850 Train loss 0.26 Classification-F1 0.3367832594902331 on epoch=474
05/21/2022 22:37:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.27 on epoch=476
05/21/2022 22:37:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.22 on epoch=478
05/21/2022 22:37:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.27 on epoch=479
05/21/2022 22:37:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.25 on epoch=481
05/21/2022 22:37:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.21 on epoch=483
05/21/2022 22:37:23 - INFO - __main__ - Global step 2900 Train loss 0.24 Classification-F1 0.28952380952380957 on epoch=483
05/21/2022 22:37:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.23 on epoch=484
05/21/2022 22:37:28 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.23 on epoch=486
05/21/2022 22:37:31 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.17 on epoch=488
05/21/2022 22:37:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=489
05/21/2022 22:37:36 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.24 on epoch=491
05/21/2022 22:37:39 - INFO - __main__ - Global step 2950 Train loss 0.21 Classification-F1 0.3323775388291517 on epoch=491
05/21/2022 22:37:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.17 on epoch=493
05/21/2022 22:37:45 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=494
05/21/2022 22:37:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.19 on epoch=496
05/21/2022 22:37:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.21 on epoch=498
05/21/2022 22:37:53 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.26 on epoch=499
05/21/2022 22:37:54 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:37:54 - INFO - __main__ - Printing 3 examples
05/21/2022 22:37:54 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/21/2022 22:37:54 - INFO - __main__ - ['contradiction']
05/21/2022 22:37:54 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/21/2022 22:37:54 - INFO - __main__ - ['contradiction']
05/21/2022 22:37:54 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/21/2022 22:37:54 - INFO - __main__ - ['contradiction']
05/21/2022 22:37:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:37:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:37:54 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 22:37:54 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:37:54 - INFO - __main__ - Printing 3 examples
05/21/2022 22:37:54 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/21/2022 22:37:54 - INFO - __main__ - ['contradiction']
05/21/2022 22:37:54 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/21/2022 22:37:54 - INFO - __main__ - ['contradiction']
05/21/2022 22:37:54 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/21/2022 22:37:54 - INFO - __main__ - ['contradiction']
05/21/2022 22:37:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:37:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:37:54 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 22:37:55 - INFO - __main__ - Global step 3000 Train loss 0.20 Classification-F1 0.38048847609219133 on epoch=499
05/21/2022 22:37:55 - INFO - __main__ - save last model!
05/21/2022 22:37:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 22:37:56 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 22:37:56 - INFO - __main__ - Printing 3 examples
05/21/2022 22:37:56 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/21/2022 22:37:56 - INFO - __main__ - ['contradiction']
05/21/2022 22:37:56 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/21/2022 22:37:56 - INFO - __main__ - ['entailment']
05/21/2022 22:37:56 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/21/2022 22:37:56 - INFO - __main__ - ['contradiction']
05/21/2022 22:37:56 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:37:56 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:37:57 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 22:38:13 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 22:38:13 - INFO - __main__ - task name: anli
05/21/2022 22:38:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 22:38:13 - INFO - __main__ - Starting training!
05/21/2022 22:38:27 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_100_0.2_8_predictions.txt
05/21/2022 22:38:27 - INFO - __main__ - Classification-F1 on test data: 0.2554
05/21/2022 22:38:28 - INFO - __main__ - prefix=anli_32_100, lr=0.2, bsz=8, dev_performance=0.41455431828516437, test_performance=0.255417308104183
05/21/2022 22:38:28 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.5, bsz=8 ...
05/21/2022 22:38:29 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:38:29 - INFO - __main__ - Printing 3 examples
05/21/2022 22:38:29 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/21/2022 22:38:29 - INFO - __main__ - ['contradiction']
05/21/2022 22:38:29 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/21/2022 22:38:29 - INFO - __main__ - ['contradiction']
05/21/2022 22:38:29 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/21/2022 22:38:29 - INFO - __main__ - ['contradiction']
05/21/2022 22:38:29 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:38:29 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:38:29 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 22:38:29 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:38:29 - INFO - __main__ - Printing 3 examples
05/21/2022 22:38:29 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/21/2022 22:38:29 - INFO - __main__ - ['contradiction']
05/21/2022 22:38:29 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/21/2022 22:38:29 - INFO - __main__ - ['contradiction']
05/21/2022 22:38:29 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/21/2022 22:38:29 - INFO - __main__ - ['contradiction']
05/21/2022 22:38:29 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:38:29 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:38:29 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 22:38:47 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 22:38:47 - INFO - __main__ - task name: anli
05/21/2022 22:38:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 22:38:48 - INFO - __main__ - Starting training!
05/21/2022 22:38:51 - INFO - __main__ - Step 10 Global step 10 Train loss 5.43 on epoch=1
05/21/2022 22:38:54 - INFO - __main__ - Step 20 Global step 20 Train loss 2.10 on epoch=3
05/21/2022 22:38:57 - INFO - __main__ - Step 30 Global step 30 Train loss 1.06 on epoch=4
05/21/2022 22:38:59 - INFO - __main__ - Step 40 Global step 40 Train loss 0.86 on epoch=6
05/21/2022 22:39:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.69 on epoch=8
05/21/2022 22:39:05 - INFO - __main__ - Global step 50 Train loss 2.03 Classification-F1 0.1836290071584189 on epoch=8
05/21/2022 22:39:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1836290071584189 on epoch=8, global_step=50
05/21/2022 22:39:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.70 on epoch=9
05/21/2022 22:39:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=11
05/21/2022 22:39:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.59 on epoch=13
05/21/2022 22:39:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=14
05/21/2022 22:39:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=16
05/21/2022 22:39:21 - INFO - __main__ - Global step 100 Train loss 0.61 Classification-F1 0.16666666666666666 on epoch=16
05/21/2022 22:39:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=18
05/21/2022 22:39:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=19
05/21/2022 22:39:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=21
05/21/2022 22:39:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=23
05/21/2022 22:39:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=24
05/21/2022 22:39:36 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.1679790026246719 on epoch=24
05/21/2022 22:39:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=26
05/21/2022 22:39:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.55 on epoch=28
05/21/2022 22:39:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=29
05/21/2022 22:39:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=31
05/21/2022 22:39:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=33
05/21/2022 22:39:52 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16272965879265092 on epoch=33
05/21/2022 22:39:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=34
05/21/2022 22:39:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.55 on epoch=36
05/21/2022 22:39:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=38
05/21/2022 22:40:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=39
05/21/2022 22:40:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=41
05/21/2022 22:40:07 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=41
05/21/2022 22:40:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=43
05/21/2022 22:40:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=44
05/21/2022 22:40:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=46
05/21/2022 22:40:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=48
05/21/2022 22:40:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=49
05/21/2022 22:40:23 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.2366771159874608 on epoch=49
05/21/2022 22:40:23 - INFO - __main__ - Saving model with best Classification-F1: 0.1836290071584189 -> 0.2366771159874608 on epoch=49, global_step=300
05/21/2022 22:40:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=51
05/21/2022 22:40:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=53
05/21/2022 22:40:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=54
05/21/2022 22:40:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=56
05/21/2022 22:40:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=58
05/21/2022 22:40:39 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.3692073212220181 on epoch=58
05/21/2022 22:40:39 - INFO - __main__ - Saving model with best Classification-F1: 0.2366771159874608 -> 0.3692073212220181 on epoch=58, global_step=350
05/21/2022 22:40:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=59
05/21/2022 22:40:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=61
05/21/2022 22:40:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=63
05/21/2022 22:40:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=64
05/21/2022 22:40:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=66
05/21/2022 22:40:55 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=66
05/21/2022 22:40:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=68
05/21/2022 22:41:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=69
05/21/2022 22:41:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=71
05/21/2022 22:41:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=73
05/21/2022 22:41:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=74
05/21/2022 22:41:10 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=74
05/21/2022 22:41:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=76
05/21/2022 22:41:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=78
05/21/2022 22:41:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=79
05/21/2022 22:41:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=81
05/21/2022 22:41:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=83
05/21/2022 22:41:26 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.3563066649199007 on epoch=83
05/21/2022 22:41:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=84
05/21/2022 22:41:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=86
05/21/2022 22:41:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=88
05/21/2022 22:41:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=89
05/21/2022 22:41:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=91
05/21/2022 22:41:42 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=91
05/21/2022 22:41:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=93
05/21/2022 22:41:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=94
05/21/2022 22:41:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=96
05/21/2022 22:41:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=98
05/21/2022 22:41:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=99
05/21/2022 22:41:58 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.358474753823591 on epoch=99
05/21/2022 22:42:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=101
05/21/2022 22:42:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=103
05/21/2022 22:42:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=104
05/21/2022 22:42:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=106
05/21/2022 22:42:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=108
05/21/2022 22:42:14 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.2673611111111111 on epoch=108
05/21/2022 22:42:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=109
05/21/2022 22:42:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=111
05/21/2022 22:42:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=113
05/21/2022 22:42:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=114
05/21/2022 22:42:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=116
05/21/2022 22:42:29 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=116
05/21/2022 22:42:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=118
05/21/2022 22:42:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=119
05/21/2022 22:42:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=121
05/21/2022 22:42:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=123
05/21/2022 22:42:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=124
05/21/2022 22:42:45 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.2818347408511343 on epoch=124
05/21/2022 22:42:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=126
05/21/2022 22:42:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=128
05/21/2022 22:42:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=129
05/21/2022 22:42:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=131
05/21/2022 22:42:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=133
05/21/2022 22:43:00 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.18089816571790004 on epoch=133
05/21/2022 22:43:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=134
05/21/2022 22:43:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=136
05/21/2022 22:43:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=138
05/21/2022 22:43:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=139
05/21/2022 22:43:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=141
05/21/2022 22:43:16 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.3278375149342891 on epoch=141
05/21/2022 22:43:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=143
05/21/2022 22:43:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=144
05/21/2022 22:43:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=146
05/21/2022 22:43:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=148
05/21/2022 22:43:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=149
05/21/2022 22:43:32 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.35185185185185186 on epoch=149
05/21/2022 22:43:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=151
05/21/2022 22:43:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=153
05/21/2022 22:43:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=154
05/21/2022 22:43:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=156
05/21/2022 22:43:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=158
05/21/2022 22:43:48 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.2552887949776661 on epoch=158
05/21/2022 22:43:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=159
05/21/2022 22:43:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=161
05/21/2022 22:43:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=163
05/21/2022 22:43:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=164
05/21/2022 22:44:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=166
05/21/2022 22:44:03 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=166
05/21/2022 22:44:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=168
05/21/2022 22:44:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=169
05/21/2022 22:44:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=171
05/21/2022 22:44:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=173
05/21/2022 22:44:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=174
05/21/2022 22:44:19 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.2051431966726084 on epoch=174
05/21/2022 22:44:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=176
05/21/2022 22:44:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=178
05/21/2022 22:44:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=179
05/21/2022 22:44:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=181
05/21/2022 22:44:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=183
05/21/2022 22:44:35 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.2905633586614475 on epoch=183
05/21/2022 22:44:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=184
05/21/2022 22:44:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=186
05/21/2022 22:44:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=188
05/21/2022 22:44:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=189
05/21/2022 22:44:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=191
05/21/2022 22:44:50 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.25408062930186825 on epoch=191
05/21/2022 22:44:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=193
05/21/2022 22:44:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=194
05/21/2022 22:44:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=196
05/21/2022 22:45:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=198
05/21/2022 22:45:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=199
05/21/2022 22:45:08 - INFO - __main__ - Global step 1200 Train loss 0.35 Classification-F1 0.22420634920634921 on epoch=199
05/21/2022 22:45:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=201
05/21/2022 22:45:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=203
05/21/2022 22:45:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=204
05/21/2022 22:45:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=206
05/21/2022 22:45:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.30 on epoch=208
05/21/2022 22:45:26 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.40344894133944553 on epoch=208
05/21/2022 22:45:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3692073212220181 -> 0.40344894133944553 on epoch=208, global_step=1250
05/21/2022 22:45:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=209
05/21/2022 22:45:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=211
05/21/2022 22:45:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=213
05/21/2022 22:45:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=214
05/21/2022 22:45:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=216
05/21/2022 22:45:42 - INFO - __main__ - Global step 1300 Train loss 0.26 Classification-F1 0.2826345294375985 on epoch=216
05/21/2022 22:45:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=218
05/21/2022 22:45:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.24 on epoch=219
05/21/2022 22:45:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=221
05/21/2022 22:45:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=223
05/21/2022 22:45:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=224
05/21/2022 22:45:58 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.3867724867724867 on epoch=224
05/21/2022 22:46:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=226
05/21/2022 22:46:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=228
05/21/2022 22:46:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=229
05/21/2022 22:46:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=231
05/21/2022 22:46:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=233
05/21/2022 22:46:16 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.4006583309652363 on epoch=233
05/21/2022 22:46:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=234
05/21/2022 22:46:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=236
05/21/2022 22:46:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=238
05/21/2022 22:46:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=239
05/21/2022 22:46:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=241
05/21/2022 22:46:34 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.3650970213817822 on epoch=241
05/21/2022 22:46:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=243
05/21/2022 22:46:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=244
05/21/2022 22:46:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=246
05/21/2022 22:46:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=248
05/21/2022 22:46:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.15 on epoch=249
05/21/2022 22:46:50 - INFO - __main__ - Global step 1500 Train loss 0.18 Classification-F1 0.3888407888407888 on epoch=249
05/21/2022 22:46:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=251
05/21/2022 22:46:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=253
05/21/2022 22:46:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=254
05/21/2022 22:47:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=256
05/21/2022 22:47:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=258
05/21/2022 22:47:06 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.449766262881017 on epoch=258
05/21/2022 22:47:06 - INFO - __main__ - Saving model with best Classification-F1: 0.40344894133944553 -> 0.449766262881017 on epoch=258, global_step=1550
05/21/2022 22:47:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=259
05/21/2022 22:47:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=261
05/21/2022 22:47:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=263
05/21/2022 22:47:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=264
05/21/2022 22:47:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=266
05/21/2022 22:47:24 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.38572394655224684 on epoch=266
05/21/2022 22:47:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=268
05/21/2022 22:47:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=269
05/21/2022 22:47:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=271
05/21/2022 22:47:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=273
05/21/2022 22:47:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=274
05/21/2022 22:47:40 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.46774766244525984 on epoch=274
05/21/2022 22:47:40 - INFO - __main__ - Saving model with best Classification-F1: 0.449766262881017 -> 0.46774766244525984 on epoch=274, global_step=1650
05/21/2022 22:47:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=276
05/21/2022 22:47:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=278
05/21/2022 22:47:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=279
05/21/2022 22:47:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=281
05/21/2022 22:47:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=283
05/21/2022 22:47:56 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.3529885384105333 on epoch=283
05/21/2022 22:47:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=284
05/21/2022 22:48:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=286
05/21/2022 22:48:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=288
05/21/2022 22:48:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=289
05/21/2022 22:48:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=291
05/21/2022 22:48:12 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.33629016538467305 on epoch=291
05/21/2022 22:48:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=293
05/21/2022 22:48:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=294
05/21/2022 22:48:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=296
05/21/2022 22:48:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=298
05/21/2022 22:48:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=299
05/21/2022 22:48:34 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.31739496809919343 on epoch=299
05/21/2022 22:48:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=301
05/21/2022 22:48:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=303
05/21/2022 22:48:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=304
05/21/2022 22:48:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=306
05/21/2022 22:48:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=308
05/21/2022 22:48:58 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.2793960607498814 on epoch=308
05/21/2022 22:49:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=309
05/21/2022 22:49:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=311
05/21/2022 22:49:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=313
05/21/2022 22:49:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=314
05/21/2022 22:49:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=316
05/21/2022 22:49:19 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.31879917383931655 on epoch=316
05/21/2022 22:49:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=318
05/21/2022 22:49:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=319
05/21/2022 22:49:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=321
05/21/2022 22:49:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=323
05/21/2022 22:49:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=324
05/21/2022 22:49:35 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.3103492552645095 on epoch=324
05/21/2022 22:49:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=326
05/21/2022 22:49:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=328
05/21/2022 22:49:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=329
05/21/2022 22:49:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=331
05/21/2022 22:49:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=333
05/21/2022 22:49:51 - INFO - __main__ - Global step 2000 Train loss 0.09 Classification-F1 0.16518550133573606 on epoch=333
05/21/2022 22:49:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=334
05/21/2022 22:49:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=336
05/21/2022 22:49:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=338
05/21/2022 22:50:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=339
05/21/2022 22:50:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=341
05/21/2022 22:50:07 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.3633848747391839 on epoch=341
05/21/2022 22:50:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=343
05/21/2022 22:50:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=344
05/21/2022 22:50:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=346
05/21/2022 22:50:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=348
05/21/2022 22:50:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=349
05/21/2022 22:50:23 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.3694513658176448 on epoch=349
05/21/2022 22:50:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=351
05/21/2022 22:50:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.17 on epoch=353
05/21/2022 22:50:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=354
05/21/2022 22:50:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=356
05/21/2022 22:50:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=358
05/21/2022 22:50:39 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.23449947312961011 on epoch=358
05/21/2022 22:50:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=359
05/21/2022 22:50:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=361
05/21/2022 22:50:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=363
05/21/2022 22:50:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=364
05/21/2022 22:50:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=366
05/21/2022 22:50:55 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.2452833869866354 on epoch=366
05/21/2022 22:50:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.10 on epoch=368
05/21/2022 22:51:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=369
05/21/2022 22:51:03 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=371
05/21/2022 22:51:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=373
05/21/2022 22:51:08 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=374
05/21/2022 22:51:11 - INFO - __main__ - Global step 2250 Train loss 0.08 Classification-F1 0.40079309721253464 on epoch=374
05/21/2022 22:51:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=376
05/21/2022 22:51:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=378
05/21/2022 22:51:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=379
05/21/2022 22:51:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=381
05/21/2022 22:51:24 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=383
05/21/2022 22:51:27 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.2074791498520312 on epoch=383
05/21/2022 22:51:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=384
05/21/2022 22:51:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=386
05/21/2022 22:51:35 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=388
05/21/2022 22:51:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=389
05/21/2022 22:51:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=391
05/21/2022 22:51:43 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.18752276867030968 on epoch=391
05/21/2022 22:51:46 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=393
05/21/2022 22:51:48 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=394
05/21/2022 22:51:51 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=396
05/21/2022 22:51:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=398
05/21/2022 22:51:56 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=399
05/21/2022 22:51:59 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.33182527854659005 on epoch=399
05/21/2022 22:52:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=401
05/21/2022 22:52:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=403
05/21/2022 22:52:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=404
05/21/2022 22:52:09 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=406
05/21/2022 22:52:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.09 on epoch=408
05/21/2022 22:52:15 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.16108297499601848 on epoch=408
05/21/2022 22:52:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=409
05/21/2022 22:52:20 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=411
05/21/2022 22:52:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=413
05/21/2022 22:52:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=414
05/21/2022 22:52:28 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=416
05/21/2022 22:52:31 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.23315315315315313 on epoch=416
05/21/2022 22:52:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=418
05/21/2022 22:52:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=419
05/21/2022 22:52:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=421
05/21/2022 22:52:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=423
05/21/2022 22:52:44 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=424
05/21/2022 22:52:47 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.22022936612548047 on epoch=424
05/21/2022 22:52:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=426
05/21/2022 22:52:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=428
05/21/2022 22:52:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=429
05/21/2022 22:52:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=431
05/21/2022 22:53:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=433
05/21/2022 22:53:03 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.17532730334637114 on epoch=433
05/21/2022 22:53:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=434
05/21/2022 22:53:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=436
05/21/2022 22:53:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=438
05/21/2022 22:53:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=439
05/21/2022 22:53:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=441
05/21/2022 22:53:18 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.11788978876574784 on epoch=441
05/21/2022 22:53:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=443
05/21/2022 22:53:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=444
05/21/2022 22:53:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=446
05/21/2022 22:53:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=448
05/21/2022 22:53:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=449
05/21/2022 22:53:34 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.24334522842101544 on epoch=449
05/21/2022 22:53:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=451
05/21/2022 22:53:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=453
05/21/2022 22:53:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=454
05/21/2022 22:53:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=456
05/21/2022 22:53:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=458
05/21/2022 22:53:51 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.28201046972269384 on epoch=458
05/21/2022 22:53:53 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=459
05/21/2022 22:53:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=461
05/21/2022 22:53:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=463
05/21/2022 22:54:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=464
05/21/2022 22:54:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=466
05/21/2022 22:54:07 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.22735357832132025 on epoch=466
05/21/2022 22:54:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=468
05/21/2022 22:54:12 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=469
05/21/2022 22:54:15 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=471
05/21/2022 22:54:17 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=473
05/21/2022 22:54:20 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=474
05/21/2022 22:54:23 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.22184153125995637 on epoch=474
05/21/2022 22:54:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=476
05/21/2022 22:54:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=478
05/21/2022 22:54:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=479
05/21/2022 22:54:33 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=481
05/21/2022 22:54:36 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=483
05/21/2022 22:54:39 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.25691841579210395 on epoch=483
05/21/2022 22:54:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=484
05/21/2022 22:54:44 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=486
05/21/2022 22:54:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=488
05/21/2022 22:54:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=489
05/21/2022 22:54:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=491
05/21/2022 22:54:56 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.3857483214208361 on epoch=491
05/21/2022 22:54:58 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=493
05/21/2022 22:55:01 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=494
05/21/2022 22:55:03 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=496
05/21/2022 22:55:06 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=498
05/21/2022 22:55:09 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=499
05/21/2022 22:55:10 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:55:10 - INFO - __main__ - Printing 3 examples
05/21/2022 22:55:10 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/21/2022 22:55:10 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:10 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/21/2022 22:55:10 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:10 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/21/2022 22:55:10 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:55:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:55:10 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 22:55:10 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:55:10 - INFO - __main__ - Printing 3 examples
05/21/2022 22:55:10 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/21/2022 22:55:10 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:10 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/21/2022 22:55:10 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:10 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/21/2022 22:55:10 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:55:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:55:10 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 22:55:12 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.28272351380870653 on epoch=499
05/21/2022 22:55:12 - INFO - __main__ - save last model!
05/21/2022 22:55:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 22:55:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 22:55:12 - INFO - __main__ - Printing 3 examples
05/21/2022 22:55:12 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/21/2022 22:55:12 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:12 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/21/2022 22:55:12 - INFO - __main__ - ['entailment']
05/21/2022 22:55:12 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/21/2022 22:55:12 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:12 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:55:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:55:14 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 22:55:29 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 22:55:29 - INFO - __main__ - task name: anli
05/21/2022 22:55:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 22:55:29 - INFO - __main__ - Starting training!
05/21/2022 22:55:48 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_13_0.5_8_predictions.txt
05/21/2022 22:55:48 - INFO - __main__ - Classification-F1 on test data: 0.2055
05/21/2022 22:55:48 - INFO - __main__ - prefix=anli_32_13, lr=0.5, bsz=8, dev_performance=0.46774766244525984, test_performance=0.20552873944371514
05/21/2022 22:55:48 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.4, bsz=8 ...
05/21/2022 22:55:49 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:55:49 - INFO - __main__ - Printing 3 examples
05/21/2022 22:55:49 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/21/2022 22:55:49 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:49 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/21/2022 22:55:49 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:49 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/21/2022 22:55:49 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:55:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:55:49 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 22:55:49 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 22:55:49 - INFO - __main__ - Printing 3 examples
05/21/2022 22:55:49 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/21/2022 22:55:49 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:49 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/21/2022 22:55:49 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:49 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/21/2022 22:55:49 - INFO - __main__ - ['contradiction']
05/21/2022 22:55:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:55:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:55:50 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 22:56:08 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 22:56:08 - INFO - __main__ - task name: anli
05/21/2022 22:56:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 22:56:09 - INFO - __main__ - Starting training!
05/21/2022 22:56:12 - INFO - __main__ - Step 10 Global step 10 Train loss 6.03 on epoch=1
05/21/2022 22:56:15 - INFO - __main__ - Step 20 Global step 20 Train loss 2.57 on epoch=3
05/21/2022 22:56:17 - INFO - __main__ - Step 30 Global step 30 Train loss 0.97 on epoch=4
05/21/2022 22:56:20 - INFO - __main__ - Step 40 Global step 40 Train loss 0.75 on epoch=6
05/21/2022 22:56:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.66 on epoch=8
05/21/2022 22:56:25 - INFO - __main__ - Global step 50 Train loss 2.20 Classification-F1 0.16666666666666666 on epoch=8
05/21/2022 22:56:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/21/2022 22:56:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=9
05/21/2022 22:56:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.57 on epoch=11
05/21/2022 22:56:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=13
05/21/2022 22:56:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=14
05/21/2022 22:56:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=16
05/21/2022 22:56:40 - INFO - __main__ - Global step 100 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=16
05/21/2022 22:56:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=18
05/21/2022 22:56:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=19
05/21/2022 22:56:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=21
05/21/2022 22:56:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=23
05/21/2022 22:56:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=24
05/21/2022 22:56:56 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=24
05/21/2022 22:56:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=26
05/21/2022 22:57:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=28
05/21/2022 22:57:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=29
05/21/2022 22:57:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=31
05/21/2022 22:57:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=33
05/21/2022 22:57:12 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.2252607374558594 on epoch=33
05/21/2022 22:57:12 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2252607374558594 on epoch=33, global_step=200
05/21/2022 22:57:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=34
05/21/2022 22:57:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=36
05/21/2022 22:57:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=38
05/21/2022 22:57:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=39
05/21/2022 22:57:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=41
05/21/2022 22:57:27 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=41
05/21/2022 22:57:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=43
05/21/2022 22:57:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=44
05/21/2022 22:57:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=46
05/21/2022 22:57:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=48
05/21/2022 22:57:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=49
05/21/2022 22:57:43 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
05/21/2022 22:57:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=51
05/21/2022 22:57:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=53
05/21/2022 22:57:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=54
05/21/2022 22:57:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=56
05/21/2022 22:57:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=58
05/21/2022 22:57:59 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.31324985439720443 on epoch=58
05/21/2022 22:57:59 - INFO - __main__ - Saving model with best Classification-F1: 0.2252607374558594 -> 0.31324985439720443 on epoch=58, global_step=350
05/21/2022 22:58:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=59
05/21/2022 22:58:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=61
05/21/2022 22:58:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=63
05/21/2022 22:58:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=64
05/21/2022 22:58:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=66
05/21/2022 22:58:15 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.34372338613524106 on epoch=66
05/21/2022 22:58:15 - INFO - __main__ - Saving model with best Classification-F1: 0.31324985439720443 -> 0.34372338613524106 on epoch=66, global_step=400
05/21/2022 22:58:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=68
05/21/2022 22:58:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=69
05/21/2022 22:58:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=71
05/21/2022 22:58:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=73
05/21/2022 22:58:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=74
05/21/2022 22:58:31 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.302514591174385 on epoch=74
05/21/2022 22:58:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=76
05/21/2022 22:58:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=78
05/21/2022 22:58:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=79
05/21/2022 22:58:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=81
05/21/2022 22:58:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=83
05/21/2022 22:58:47 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.2513641542243809 on epoch=83
05/21/2022 22:58:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=84
05/21/2022 22:58:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=86
05/21/2022 22:58:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=88
05/21/2022 22:58:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=89
05/21/2022 22:59:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=91
05/21/2022 22:59:03 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=91
05/21/2022 22:59:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=93
05/21/2022 22:59:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=94
05/21/2022 22:59:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=96
05/21/2022 22:59:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=98
05/21/2022 22:59:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=99
05/21/2022 22:59:19 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.2085278555866791 on epoch=99
05/21/2022 22:59:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=101
05/21/2022 22:59:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=103
05/21/2022 22:59:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=104
05/21/2022 22:59:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=106
05/21/2022 22:59:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=108
05/21/2022 22:59:35 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.1975254941762119 on epoch=108
05/21/2022 22:59:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=109
05/21/2022 22:59:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=111
05/21/2022 22:59:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=113
05/21/2022 22:59:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=114
05/21/2022 22:59:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=116
05/21/2022 22:59:51 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=116
05/21/2022 22:59:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=118
05/21/2022 22:59:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=119
05/21/2022 22:59:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=121
05/21/2022 23:00:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=123
05/21/2022 23:00:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=124
05/21/2022 23:00:07 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.2754036087369421 on epoch=124
05/21/2022 23:00:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=126
05/21/2022 23:00:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=128
05/21/2022 23:00:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=129
05/21/2022 23:00:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=131
05/21/2022 23:00:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=133
05/21/2022 23:00:23 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.3919237641453246 on epoch=133
05/21/2022 23:00:24 - INFO - __main__ - Saving model with best Classification-F1: 0.34372338613524106 -> 0.3919237641453246 on epoch=133, global_step=800
05/21/2022 23:00:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=134
05/21/2022 23:00:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=136
05/21/2022 23:00:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=138
05/21/2022 23:00:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=139
05/21/2022 23:00:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=141
05/21/2022 23:00:40 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.1851851851851852 on epoch=141
05/21/2022 23:00:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=143
05/21/2022 23:00:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=144
05/21/2022 23:00:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=146
05/21/2022 23:00:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=148
05/21/2022 23:00:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=149
05/21/2022 23:00:56 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.2863802863802864 on epoch=149
05/21/2022 23:00:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=151
05/21/2022 23:01:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=153
05/21/2022 23:01:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=154
05/21/2022 23:01:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=156
05/21/2022 23:01:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=158
05/21/2022 23:01:12 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.33575757575757575 on epoch=158
05/21/2022 23:01:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=159
05/21/2022 23:01:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=161
05/21/2022 23:01:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=163
05/21/2022 23:01:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=164
05/21/2022 23:01:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=166
05/21/2022 23:01:28 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.1990221455277538 on epoch=166
05/21/2022 23:01:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=168
05/21/2022 23:01:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=169
05/21/2022 23:01:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=171
05/21/2022 23:01:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=173
05/21/2022 23:01:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=174
05/21/2022 23:01:45 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.4000625097671511 on epoch=174
05/21/2022 23:01:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3919237641453246 -> 0.4000625097671511 on epoch=174, global_step=1050
05/21/2022 23:01:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=176
05/21/2022 23:01:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=178
05/21/2022 23:01:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=179
05/21/2022 23:01:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=181
05/21/2022 23:01:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=183
05/21/2022 23:02:01 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.3110340286810875 on epoch=183
05/21/2022 23:02:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=184
05/21/2022 23:02:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=186
05/21/2022 23:02:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=188
05/21/2022 23:02:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=189
05/21/2022 23:02:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=191
05/21/2022 23:02:17 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.2040577162528382 on epoch=191
05/21/2022 23:02:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=193
05/21/2022 23:02:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=194
05/21/2022 23:02:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=196
05/21/2022 23:02:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=198
05/21/2022 23:02:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=199
05/21/2022 23:02:33 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.29212454212454214 on epoch=199
05/21/2022 23:02:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=201
05/21/2022 23:02:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=203
05/21/2022 23:02:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=204
05/21/2022 23:02:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=206
05/21/2022 23:02:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=208
05/21/2022 23:02:49 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.1881810228266921 on epoch=208
05/21/2022 23:02:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=209
05/21/2022 23:02:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=211
05/21/2022 23:02:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=213
05/21/2022 23:03:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=214
05/21/2022 23:03:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.43 on epoch=216
05/21/2022 23:03:05 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=216
05/21/2022 23:03:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=218
05/21/2022 23:03:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=219
05/21/2022 23:03:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=221
05/21/2022 23:03:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=223
05/21/2022 23:03:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=224
05/21/2022 23:03:22 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.2272576691181342 on epoch=224
05/21/2022 23:03:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=226
05/21/2022 23:03:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=228
05/21/2022 23:03:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=229
05/21/2022 23:03:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=231
05/21/2022 23:03:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=233
05/21/2022 23:03:38 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.21825396825396826 on epoch=233
05/21/2022 23:03:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=234
05/21/2022 23:03:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=236
05/21/2022 23:03:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=238
05/21/2022 23:03:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=239
05/21/2022 23:03:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.35 on epoch=241
05/21/2022 23:03:54 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.16666666666666666 on epoch=241
05/21/2022 23:03:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=243
05/21/2022 23:03:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=244
05/21/2022 23:04:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=246
05/21/2022 23:04:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=248
05/21/2022 23:04:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.41 on epoch=249
05/21/2022 23:04:10 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.28279030910609854 on epoch=249
05/21/2022 23:04:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=251
05/21/2022 23:04:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=253
05/21/2022 23:04:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=254
05/21/2022 23:04:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=256
05/21/2022 23:04:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=258
05/21/2022 23:04:26 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.3037783145188459 on epoch=258
05/21/2022 23:04:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=259
05/21/2022 23:04:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=261
05/21/2022 23:04:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=263
05/21/2022 23:04:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=264
05/21/2022 23:04:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=266
05/21/2022 23:04:42 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.16666666666666666 on epoch=266
05/21/2022 23:04:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=268
05/21/2022 23:04:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=269
05/21/2022 23:04:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=271
05/21/2022 23:04:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=273
05/21/2022 23:04:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=274
05/21/2022 23:04:59 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.33844581857900397 on epoch=274
05/21/2022 23:05:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=276
05/21/2022 23:05:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=278
05/21/2022 23:05:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.34 on epoch=279
05/21/2022 23:05:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=281
05/21/2022 23:05:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=283
05/21/2022 23:05:15 - INFO - __main__ - Global step 1700 Train loss 0.37 Classification-F1 0.3355001259763164 on epoch=283
05/21/2022 23:05:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=284
05/21/2022 23:05:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=286
05/21/2022 23:05:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=288
05/21/2022 23:05:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.33 on epoch=289
05/21/2022 23:05:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=291
05/21/2022 23:05:31 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.31258947168454404 on epoch=291
05/21/2022 23:05:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=293
05/21/2022 23:05:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=294
05/21/2022 23:05:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=296
05/21/2022 23:05:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=298
05/21/2022 23:05:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=299
05/21/2022 23:05:48 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.37692141293836207 on epoch=299
05/21/2022 23:05:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=301
05/21/2022 23:05:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=303
05/21/2022 23:05:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=304
05/21/2022 23:05:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=306
05/21/2022 23:06:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=308
05/21/2022 23:06:06 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.4295984005108317 on epoch=308
05/21/2022 23:06:06 - INFO - __main__ - Saving model with best Classification-F1: 0.4000625097671511 -> 0.4295984005108317 on epoch=308, global_step=1850
05/21/2022 23:06:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=309
05/21/2022 23:06:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=311
05/21/2022 23:06:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=313
05/21/2022 23:06:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.30 on epoch=314
05/21/2022 23:06:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=316
05/21/2022 23:06:22 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.36977062365625946 on epoch=316
05/21/2022 23:06:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=318
05/21/2022 23:06:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=319
05/21/2022 23:06:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.32 on epoch=321
05/21/2022 23:06:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=323
05/21/2022 23:06:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=324
05/21/2022 23:06:39 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.32504450151508973 on epoch=324
05/21/2022 23:06:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.30 on epoch=326
05/21/2022 23:06:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=328
05/21/2022 23:06:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=329
05/21/2022 23:06:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=331
05/21/2022 23:06:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=333
05/21/2022 23:06:57 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.41413053376523695 on epoch=333
05/21/2022 23:06:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=334
05/21/2022 23:07:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.30 on epoch=336
05/21/2022 23:07:05 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=338
05/21/2022 23:07:07 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.31 on epoch=339
05/21/2022 23:07:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.34 on epoch=341
05/21/2022 23:07:13 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.3939597419036152 on epoch=341
05/21/2022 23:07:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.28 on epoch=343
05/21/2022 23:07:18 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.25 on epoch=344
05/21/2022 23:07:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.32 on epoch=346
05/21/2022 23:07:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.33 on epoch=348
05/21/2022 23:07:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.23 on epoch=349
05/21/2022 23:07:29 - INFO - __main__ - Global step 2100 Train loss 0.28 Classification-F1 0.3617461984744814 on epoch=349
05/21/2022 23:07:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.28 on epoch=351
05/21/2022 23:07:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.27 on epoch=353
05/21/2022 23:07:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.31 on epoch=354
05/21/2022 23:07:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.21 on epoch=356
05/21/2022 23:07:42 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.25 on epoch=358
05/21/2022 23:07:45 - INFO - __main__ - Global step 2150 Train loss 0.26 Classification-F1 0.3966577886723077 on epoch=358
05/21/2022 23:07:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.24 on epoch=359
05/21/2022 23:07:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.24 on epoch=361
05/21/2022 23:07:53 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=363
05/21/2022 23:07:56 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.28 on epoch=364
05/21/2022 23:07:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=366
05/21/2022 23:08:02 - INFO - __main__ - Global step 2200 Train loss 0.25 Classification-F1 0.4433862433862434 on epoch=366
05/21/2022 23:08:02 - INFO - __main__ - Saving model with best Classification-F1: 0.4295984005108317 -> 0.4433862433862434 on epoch=366, global_step=2200
05/21/2022 23:08:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.29 on epoch=368
05/21/2022 23:08:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=369
05/21/2022 23:08:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.27 on epoch=371
05/21/2022 23:08:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.24 on epoch=373
05/21/2022 23:08:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=374
05/21/2022 23:08:18 - INFO - __main__ - Global step 2250 Train loss 0.26 Classification-F1 0.2731554160125589 on epoch=374
05/21/2022 23:08:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=376
05/21/2022 23:08:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.23 on epoch=378
05/21/2022 23:08:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=379
05/21/2022 23:08:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.25 on epoch=381
05/21/2022 23:08:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=383
05/21/2022 23:08:34 - INFO - __main__ - Global step 2300 Train loss 0.22 Classification-F1 0.4036330049261084 on epoch=383
05/21/2022 23:08:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.29 on epoch=384
05/21/2022 23:08:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.17 on epoch=386
05/21/2022 23:08:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.21 on epoch=388
05/21/2022 23:08:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.24 on epoch=389
05/21/2022 23:08:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.24 on epoch=391
05/21/2022 23:08:51 - INFO - __main__ - Global step 2350 Train loss 0.23 Classification-F1 0.37830848020496094 on epoch=391
05/21/2022 23:08:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.25 on epoch=393
05/21/2022 23:08:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=394
05/21/2022 23:08:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=396
05/21/2022 23:09:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=398
05/21/2022 23:09:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.21 on epoch=399
05/21/2022 23:09:07 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.382694988627192 on epoch=399
05/21/2022 23:09:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.21 on epoch=401
05/21/2022 23:09:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.21 on epoch=403
05/21/2022 23:09:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=404
05/21/2022 23:09:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=406
05/21/2022 23:09:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.19 on epoch=408
05/21/2022 23:09:23 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.36350108225108224 on epoch=408
05/21/2022 23:09:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.20 on epoch=409
05/21/2022 23:09:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=411
05/21/2022 23:09:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=413
05/21/2022 23:09:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=414
05/21/2022 23:09:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=416
05/21/2022 23:09:40 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.41282051282051274 on epoch=416
05/21/2022 23:09:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=418
05/21/2022 23:09:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=419
05/21/2022 23:09:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.23 on epoch=421
05/21/2022 23:09:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=423
05/21/2022 23:09:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=424
05/21/2022 23:09:56 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.3899470899470899 on epoch=424
05/21/2022 23:09:59 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=426
05/21/2022 23:10:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=428
05/21/2022 23:10:04 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.13 on epoch=429
05/21/2022 23:10:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=431
05/21/2022 23:10:09 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=433
05/21/2022 23:10:12 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.4648902295961119 on epoch=433
05/21/2022 23:10:12 - INFO - __main__ - Saving model with best Classification-F1: 0.4433862433862434 -> 0.4648902295961119 on epoch=433, global_step=2600
05/21/2022 23:10:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=434
05/21/2022 23:10:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=436
05/21/2022 23:10:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=438
05/21/2022 23:10:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=439
05/21/2022 23:10:26 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=441
05/21/2022 23:10:28 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.3020938872002702 on epoch=441
05/21/2022 23:10:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=443
05/21/2022 23:10:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=444
05/21/2022 23:10:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=446
05/21/2022 23:10:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=448
05/21/2022 23:10:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.14 on epoch=449
05/21/2022 23:10:45 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.40784954243028365 on epoch=449
05/21/2022 23:10:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=451
05/21/2022 23:10:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=453
05/21/2022 23:10:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=454
05/21/2022 23:10:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=456
05/21/2022 23:10:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.15 on epoch=458
05/21/2022 23:11:01 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.46775481681142056 on epoch=458
05/21/2022 23:11:01 - INFO - __main__ - Saving model with best Classification-F1: 0.4648902295961119 -> 0.46775481681142056 on epoch=458, global_step=2750
05/21/2022 23:11:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.21 on epoch=459
05/21/2022 23:11:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=461
05/21/2022 23:11:09 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.12 on epoch=463
05/21/2022 23:11:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=464
05/21/2022 23:11:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=466
05/21/2022 23:11:18 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.3032570422535211 on epoch=466
05/21/2022 23:11:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.17 on epoch=468
05/21/2022 23:11:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=469
05/21/2022 23:11:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=471
05/21/2022 23:11:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=473
05/21/2022 23:11:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=474
05/21/2022 23:11:34 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.2558840579710145 on epoch=474
05/21/2022 23:11:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=476
05/21/2022 23:11:40 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=478
05/21/2022 23:11:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=479
05/21/2022 23:11:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=481
05/21/2022 23:11:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=483
05/21/2022 23:11:51 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.44114452798663323 on epoch=483
05/21/2022 23:11:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=484
05/21/2022 23:11:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=486
05/21/2022 23:11:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=488
05/21/2022 23:12:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=489
05/21/2022 23:12:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=491
05/21/2022 23:12:09 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.4084578696343402 on epoch=491
05/21/2022 23:12:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.16 on epoch=493
05/21/2022 23:12:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=494
05/21/2022 23:12:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=496
05/21/2022 23:12:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=498
05/21/2022 23:12:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=499
05/21/2022 23:12:24 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:12:24 - INFO - __main__ - Printing 3 examples
05/21/2022 23:12:24 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/21/2022 23:12:24 - INFO - __main__ - ['contradiction']
05/21/2022 23:12:24 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/21/2022 23:12:24 - INFO - __main__ - ['contradiction']
05/21/2022 23:12:24 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/21/2022 23:12:24 - INFO - __main__ - ['contradiction']
05/21/2022 23:12:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:12:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:12:24 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 23:12:24 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:12:24 - INFO - __main__ - Printing 3 examples
05/21/2022 23:12:24 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/21/2022 23:12:24 - INFO - __main__ - ['contradiction']
05/21/2022 23:12:24 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/21/2022 23:12:24 - INFO - __main__ - ['contradiction']
05/21/2022 23:12:24 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/21/2022 23:12:24 - INFO - __main__ - ['contradiction']
05/21/2022 23:12:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:12:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:12:24 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 23:12:34 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.4422578728461081 on epoch=499
05/21/2022 23:12:34 - INFO - __main__ - save last model!
05/21/2022 23:12:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 23:12:34 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 23:12:34 - INFO - __main__ - Printing 3 examples
05/21/2022 23:12:34 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/21/2022 23:12:34 - INFO - __main__ - ['contradiction']
05/21/2022 23:12:34 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/21/2022 23:12:34 - INFO - __main__ - ['entailment']
05/21/2022 23:12:34 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/21/2022 23:12:34 - INFO - __main__ - ['contradiction']
05/21/2022 23:12:34 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:12:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:12:36 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 23:12:43 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 23:12:43 - INFO - __main__ - task name: anli
05/21/2022 23:12:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 23:12:43 - INFO - __main__ - Starting training!
05/21/2022 23:13:23 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_13_0.4_8_predictions.txt
05/21/2022 23:13:23 - INFO - __main__ - Classification-F1 on test data: 0.0951
05/21/2022 23:13:24 - INFO - __main__ - prefix=anli_32_13, lr=0.4, bsz=8, dev_performance=0.46775481681142056, test_performance=0.09507406004397709
05/21/2022 23:13:24 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.3, bsz=8 ...
05/21/2022 23:13:25 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:13:25 - INFO - __main__ - Printing 3 examples
05/21/2022 23:13:25 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/21/2022 23:13:25 - INFO - __main__ - ['contradiction']
05/21/2022 23:13:25 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/21/2022 23:13:25 - INFO - __main__ - ['contradiction']
05/21/2022 23:13:25 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/21/2022 23:13:25 - INFO - __main__ - ['contradiction']
05/21/2022 23:13:25 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:13:25 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:13:25 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 23:13:25 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:13:25 - INFO - __main__ - Printing 3 examples
05/21/2022 23:13:25 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/21/2022 23:13:25 - INFO - __main__ - ['contradiction']
05/21/2022 23:13:25 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/21/2022 23:13:25 - INFO - __main__ - ['contradiction']
05/21/2022 23:13:25 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/21/2022 23:13:25 - INFO - __main__ - ['contradiction']
05/21/2022 23:13:25 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:13:25 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:13:25 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 23:13:40 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 23:13:40 - INFO - __main__ - task name: anli
05/21/2022 23:13:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 23:13:40 - INFO - __main__ - Starting training!
05/21/2022 23:13:43 - INFO - __main__ - Step 10 Global step 10 Train loss 6.09 on epoch=1
05/21/2022 23:13:46 - INFO - __main__ - Step 20 Global step 20 Train loss 3.02 on epoch=3
05/21/2022 23:13:49 - INFO - __main__ - Step 30 Global step 30 Train loss 1.45 on epoch=4
05/21/2022 23:13:51 - INFO - __main__ - Step 40 Global step 40 Train loss 1.04 on epoch=6
05/21/2022 23:13:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.81 on epoch=8
05/21/2022 23:13:56 - INFO - __main__ - Global step 50 Train loss 2.48 Classification-F1 0.16666666666666666 on epoch=8
05/21/2022 23:13:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/21/2022 23:13:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.68 on epoch=9
05/21/2022 23:14:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.62 on epoch=11
05/21/2022 23:14:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=13
05/21/2022 23:14:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=14
05/21/2022 23:14:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.57 on epoch=16
05/21/2022 23:14:12 - INFO - __main__ - Global step 100 Train loss 0.62 Classification-F1 0.27336300063572794 on epoch=16
05/21/2022 23:14:12 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.27336300063572794 on epoch=16, global_step=100
05/21/2022 23:14:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=18
05/21/2022 23:14:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.60 on epoch=19
05/21/2022 23:14:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.61 on epoch=21
05/21/2022 23:14:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=23
05/21/2022 23:14:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.62 on epoch=24
05/21/2022 23:14:28 - INFO - __main__ - Global step 150 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=24
05/21/2022 23:14:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=26
05/21/2022 23:14:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=28
05/21/2022 23:14:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=29
05/21/2022 23:14:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=31
05/21/2022 23:14:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=33
05/21/2022 23:14:43 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=33
05/21/2022 23:14:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.59 on epoch=34
05/21/2022 23:14:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=36
05/21/2022 23:14:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=38
05/21/2022 23:14:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=39
05/21/2022 23:14:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=41
05/21/2022 23:14:59 - INFO - __main__ - Global step 250 Train loss 0.52 Classification-F1 0.3358124859139058 on epoch=41
05/21/2022 23:14:59 - INFO - __main__ - Saving model with best Classification-F1: 0.27336300063572794 -> 0.3358124859139058 on epoch=41, global_step=250
05/21/2022 23:15:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=43
05/21/2022 23:15:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=44
05/21/2022 23:15:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=46
05/21/2022 23:15:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=48
05/21/2022 23:15:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=49
05/21/2022 23:15:15 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=49
05/21/2022 23:15:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=51
05/21/2022 23:15:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=53
05/21/2022 23:15:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=54
05/21/2022 23:15:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=56
05/21/2022 23:15:28 - INFO - __main__ - Step 350 Global step 350 Train loss 1.78 on epoch=58
05/21/2022 23:15:31 - INFO - __main__ - Global step 350 Train loss 0.73 Classification-F1 0.1881810228266921 on epoch=58
05/21/2022 23:15:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.59 on epoch=59
05/21/2022 23:15:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=61
05/21/2022 23:15:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=63
05/21/2022 23:15:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=64
05/21/2022 23:15:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=66
05/21/2022 23:15:47 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.4386574074074074 on epoch=66
05/21/2022 23:15:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3358124859139058 -> 0.4386574074074074 on epoch=66, global_step=400
05/21/2022 23:15:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=68
05/21/2022 23:15:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=69
05/21/2022 23:15:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=71
05/21/2022 23:15:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=73
05/21/2022 23:16:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=74
05/21/2022 23:16:04 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.38548447437336325 on epoch=74
05/21/2022 23:16:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=76
05/21/2022 23:16:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=78
05/21/2022 23:16:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=79
05/21/2022 23:16:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=81
05/21/2022 23:16:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=83
05/21/2022 23:16:20 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.32783250540259884 on epoch=83
05/21/2022 23:16:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=84
05/21/2022 23:16:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=86
05/21/2022 23:16:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=88
05/21/2022 23:16:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=89
05/21/2022 23:16:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=91
05/21/2022 23:16:36 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.3606060606060606 on epoch=91
05/21/2022 23:16:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=93
05/21/2022 23:16:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=94
05/21/2022 23:16:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=96
05/21/2022 23:16:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=98
05/21/2022 23:16:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=99
05/21/2022 23:16:52 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.3596758422074878 on epoch=99
05/21/2022 23:16:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=101
05/21/2022 23:16:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=103
05/21/2022 23:17:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=104
05/21/2022 23:17:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=106
05/21/2022 23:17:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=108
05/21/2022 23:17:09 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.3656696734715408 on epoch=108
05/21/2022 23:17:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=109
05/21/2022 23:17:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=111
05/21/2022 23:17:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=113
05/21/2022 23:17:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=114
05/21/2022 23:17:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=116
05/21/2022 23:17:24 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.2439311115029683 on epoch=116
05/21/2022 23:17:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=118
05/21/2022 23:17:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=119
05/21/2022 23:17:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=121
05/21/2022 23:17:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=123
05/21/2022 23:17:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=124
05/21/2022 23:17:40 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.3783776971163541 on epoch=124
05/21/2022 23:17:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=126
05/21/2022 23:17:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=128
05/21/2022 23:17:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=129
05/21/2022 23:17:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=131
05/21/2022 23:17:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=133
05/21/2022 23:17:56 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.3669702538969437 on epoch=133
05/21/2022 23:17:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=134
05/21/2022 23:18:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=136
05/21/2022 23:18:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.50 on epoch=138
05/21/2022 23:18:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=139
05/21/2022 23:18:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.49 on epoch=141
05/21/2022 23:18:13 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.35775482883916615 on epoch=141
05/21/2022 23:18:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=143
05/21/2022 23:18:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=144
05/21/2022 23:18:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=146
05/21/2022 23:18:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=148
05/21/2022 23:18:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=149
05/21/2022 23:18:29 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.3933681235051098 on epoch=149
05/21/2022 23:18:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.51 on epoch=151
05/21/2022 23:18:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=153
05/21/2022 23:18:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=154
05/21/2022 23:18:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=156
05/21/2022 23:18:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=158
05/21/2022 23:18:45 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.2838753387533875 on epoch=158
05/21/2022 23:18:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.50 on epoch=159
05/21/2022 23:18:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=161
05/21/2022 23:18:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=163
05/21/2022 23:18:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=164
05/21/2022 23:18:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=166
05/21/2022 23:19:02 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=166
05/21/2022 23:19:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=168
05/21/2022 23:19:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=169
05/21/2022 23:19:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=171
05/21/2022 23:19:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=173
05/21/2022 23:19:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=174
05/21/2022 23:19:18 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.26136360313575496 on epoch=174
05/21/2022 23:19:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=176
05/21/2022 23:19:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=178
05/21/2022 23:19:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=179
05/21/2022 23:19:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.47 on epoch=181
05/21/2022 23:19:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=183
05/21/2022 23:19:33 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.2476579799012093 on epoch=183
05/21/2022 23:19:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=184
05/21/2022 23:19:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=186
05/21/2022 23:19:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=188
05/21/2022 23:19:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=189
05/21/2022 23:19:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=191
05/21/2022 23:19:50 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=191
05/21/2022 23:19:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=193
05/21/2022 23:19:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=194
05/21/2022 23:19:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=196
05/21/2022 23:20:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=198
05/21/2022 23:20:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=199
05/21/2022 23:20:06 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.38405178546023616 on epoch=199
05/21/2022 23:20:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=201
05/21/2022 23:20:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=203
05/21/2022 23:20:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=204
05/21/2022 23:20:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=206
05/21/2022 23:20:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=208
05/21/2022 23:20:22 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.25626898354171085 on epoch=208
05/21/2022 23:20:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=209
05/21/2022 23:20:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.50 on epoch=211
05/21/2022 23:20:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=213
05/21/2022 23:20:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.49 on epoch=214
05/21/2022 23:20:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=216
05/21/2022 23:20:38 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=216
05/21/2022 23:20:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=218
05/21/2022 23:20:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=219
05/21/2022 23:20:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=221
05/21/2022 23:20:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.50 on epoch=223
05/21/2022 23:20:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=224
05/21/2022 23:20:54 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.23310787729726035 on epoch=224
05/21/2022 23:20:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.45 on epoch=226
05/21/2022 23:21:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=228
05/21/2022 23:21:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=229
05/21/2022 23:21:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=231
05/21/2022 23:21:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=233
05/21/2022 23:21:11 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.2961983815642352 on epoch=233
05/21/2022 23:21:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=234
05/21/2022 23:21:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=236
05/21/2022 23:21:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=238
05/21/2022 23:21:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.46 on epoch=239
05/21/2022 23:21:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=241
05/21/2022 23:21:27 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=241
05/21/2022 23:21:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.38 on epoch=243
05/21/2022 23:21:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=244
05/21/2022 23:21:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=246
05/21/2022 23:21:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=248
05/21/2022 23:21:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=249
05/21/2022 23:21:43 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.45085687190950346 on epoch=249
05/21/2022 23:21:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4386574074074074 -> 0.45085687190950346 on epoch=249, global_step=1500
05/21/2022 23:21:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.46 on epoch=251
05/21/2022 23:21:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=253
05/21/2022 23:21:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=254
05/21/2022 23:21:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=256
05/21/2022 23:21:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.44 on epoch=258
05/21/2022 23:21:59 - INFO - __main__ - Global step 1550 Train loss 0.44 Classification-F1 0.3550951617544292 on epoch=258
05/21/2022 23:22:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=259
05/21/2022 23:22:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=261
05/21/2022 23:22:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=263
05/21/2022 23:22:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=264
05/21/2022 23:22:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=266
05/21/2022 23:22:15 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=266
05/21/2022 23:22:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=268
05/21/2022 23:22:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.46 on epoch=269
05/21/2022 23:22:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=271
05/21/2022 23:22:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=273
05/21/2022 23:22:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=274
05/21/2022 23:22:32 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.44939060939060943 on epoch=274
05/21/2022 23:22:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=276
05/21/2022 23:22:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=278
05/21/2022 23:22:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=279
05/21/2022 23:22:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.46 on epoch=281
05/21/2022 23:22:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=283
05/21/2022 23:22:48 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=283
05/21/2022 23:22:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=284
05/21/2022 23:22:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.39 on epoch=286
05/21/2022 23:22:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=288
05/21/2022 23:22:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=289
05/21/2022 23:23:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.40 on epoch=291
05/21/2022 23:23:04 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=291
05/21/2022 23:23:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=293
05/21/2022 23:23:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=294
05/21/2022 23:23:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=296
05/21/2022 23:23:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=298
05/21/2022 23:23:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=299
05/21/2022 23:23:20 - INFO - __main__ - Global step 1800 Train loss 0.40 Classification-F1 0.33984586996635197 on epoch=299
05/21/2022 23:23:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.47 on epoch=301
05/21/2022 23:23:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=303
05/21/2022 23:23:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=304
05/21/2022 23:23:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=306
05/21/2022 23:23:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=308
05/21/2022 23:23:37 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.23417147823927487 on epoch=308
05/21/2022 23:23:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=309
05/21/2022 23:23:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=311
05/21/2022 23:23:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=313
05/21/2022 23:23:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=314
05/21/2022 23:23:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.40 on epoch=316
05/21/2022 23:23:53 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=316
05/21/2022 23:23:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=318
05/21/2022 23:23:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=319
05/21/2022 23:24:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=321
05/21/2022 23:24:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=323
05/21/2022 23:24:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=324
05/21/2022 23:24:09 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.32036541889483067 on epoch=324
05/21/2022 23:24:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=326
05/21/2022 23:24:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=328
05/21/2022 23:24:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=329
05/21/2022 23:24:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=331
05/21/2022 23:24:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=333
05/21/2022 23:24:25 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.20474709915057807 on epoch=333
05/21/2022 23:24:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.40 on epoch=334
05/21/2022 23:24:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.37 on epoch=336
05/21/2022 23:24:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.42 on epoch=338
05/21/2022 23:24:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.38 on epoch=339
05/21/2022 23:24:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.41 on epoch=341
05/21/2022 23:24:41 - INFO - __main__ - Global step 2050 Train loss 0.40 Classification-F1 0.1881810228266921 on epoch=341
05/21/2022 23:24:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.39 on epoch=343
05/21/2022 23:24:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=344
05/21/2022 23:24:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.43 on epoch=346
05/21/2022 23:24:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.39 on epoch=348
05/21/2022 23:24:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.39 on epoch=349
05/21/2022 23:24:58 - INFO - __main__ - Global step 2100 Train loss 0.40 Classification-F1 0.38610460577673694 on epoch=349
05/21/2022 23:25:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.39 on epoch=351
05/21/2022 23:25:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.39 on epoch=353
05/21/2022 23:25:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.46 on epoch=354
05/21/2022 23:25:08 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.37 on epoch=356
05/21/2022 23:25:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.41 on epoch=358
05/21/2022 23:25:14 - INFO - __main__ - Global step 2150 Train loss 0.40 Classification-F1 0.2973856209150327 on epoch=358
05/21/2022 23:25:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.40 on epoch=359
05/21/2022 23:25:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.36 on epoch=361
05/21/2022 23:25:22 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.35 on epoch=363
05/21/2022 23:25:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.45 on epoch=364
05/21/2022 23:25:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.37 on epoch=366
05/21/2022 23:25:30 - INFO - __main__ - Global step 2200 Train loss 0.38 Classification-F1 0.3255157820490213 on epoch=366
05/21/2022 23:25:33 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.38 on epoch=368
05/21/2022 23:25:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.41 on epoch=369
05/21/2022 23:25:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.41 on epoch=371
05/21/2022 23:25:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.40 on epoch=373
05/21/2022 23:25:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.35 on epoch=374
05/21/2022 23:25:46 - INFO - __main__ - Global step 2250 Train loss 0.39 Classification-F1 0.22549019607843138 on epoch=374
05/21/2022 23:25:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.36 on epoch=376
05/21/2022 23:25:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.35 on epoch=378
05/21/2022 23:25:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.38 on epoch=379
05/21/2022 23:25:57 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.40 on epoch=381
05/21/2022 23:26:00 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=383
05/21/2022 23:26:02 - INFO - __main__ - Global step 2300 Train loss 0.37 Classification-F1 0.26728586171310625 on epoch=383
05/21/2022 23:26:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.40 on epoch=384
05/21/2022 23:26:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.39 on epoch=386
05/21/2022 23:26:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.39 on epoch=388
05/21/2022 23:26:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.33 on epoch=389
05/21/2022 23:26:16 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.40 on epoch=391
05/21/2022 23:26:18 - INFO - __main__ - Global step 2350 Train loss 0.38 Classification-F1 0.16666666666666666 on epoch=391
05/21/2022 23:26:21 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=393
05/21/2022 23:26:23 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.44 on epoch=394
05/21/2022 23:26:26 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=396
05/21/2022 23:26:29 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.37 on epoch=398
05/21/2022 23:26:31 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.39 on epoch=399
05/21/2022 23:26:34 - INFO - __main__ - Global step 2400 Train loss 0.40 Classification-F1 0.297698975571316 on epoch=399
05/21/2022 23:26:37 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.37 on epoch=401
05/21/2022 23:26:40 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.38 on epoch=403
05/21/2022 23:26:42 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.34 on epoch=404
05/21/2022 23:26:45 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.42 on epoch=406
05/21/2022 23:26:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.37 on epoch=408
05/21/2022 23:26:50 - INFO - __main__ - Global step 2450 Train loss 0.37 Classification-F1 0.2735093547823046 on epoch=408
05/21/2022 23:26:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.38 on epoch=409
05/21/2022 23:26:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.35 on epoch=411
05/21/2022 23:26:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.36 on epoch=413
05/21/2022 23:27:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.36 on epoch=414
05/21/2022 23:27:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.39 on epoch=416
05/21/2022 23:27:07 - INFO - __main__ - Global step 2500 Train loss 0.37 Classification-F1 0.25605113840407956 on epoch=416
05/21/2022 23:27:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.37 on epoch=418
05/21/2022 23:27:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.36 on epoch=419
05/21/2022 23:27:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.34 on epoch=421
05/21/2022 23:27:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.32 on epoch=423
05/21/2022 23:27:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.37 on epoch=424
05/21/2022 23:27:23 - INFO - __main__ - Global step 2550 Train loss 0.35 Classification-F1 0.29401046258743313 on epoch=424
05/21/2022 23:27:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.35 on epoch=426
05/21/2022 23:27:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.36 on epoch=428
05/21/2022 23:27:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.39 on epoch=429
05/21/2022 23:27:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.35 on epoch=431
05/21/2022 23:27:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.35 on epoch=433
05/21/2022 23:27:39 - INFO - __main__ - Global step 2600 Train loss 0.36 Classification-F1 0.36059650213988476 on epoch=433
05/21/2022 23:27:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.35 on epoch=434
05/21/2022 23:27:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.36 on epoch=436
05/21/2022 23:27:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.32 on epoch=438
05/21/2022 23:27:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.34 on epoch=439
05/21/2022 23:27:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.31 on epoch=441
05/21/2022 23:27:55 - INFO - __main__ - Global step 2650 Train loss 0.34 Classification-F1 0.2630579297245964 on epoch=441
05/21/2022 23:27:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.33 on epoch=443
05/21/2022 23:28:00 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.32 on epoch=444
05/21/2022 23:28:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.35 on epoch=446
05/21/2022 23:28:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.31 on epoch=448
05/21/2022 23:28:08 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.34 on epoch=449
05/21/2022 23:28:11 - INFO - __main__ - Global step 2700 Train loss 0.33 Classification-F1 0.31047337173863254 on epoch=449
05/21/2022 23:28:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.33 on epoch=451
05/21/2022 23:28:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.35 on epoch=453
05/21/2022 23:28:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.35 on epoch=454
05/21/2022 23:28:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.32 on epoch=456
05/21/2022 23:28:24 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.30 on epoch=458
05/21/2022 23:28:27 - INFO - __main__ - Global step 2750 Train loss 0.33 Classification-F1 0.2843859056625014 on epoch=458
05/21/2022 23:28:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.33 on epoch=459
05/21/2022 23:28:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.31 on epoch=461
05/21/2022 23:28:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.36 on epoch=463
05/21/2022 23:28:38 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.38 on epoch=464
05/21/2022 23:28:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.31 on epoch=466
05/21/2022 23:28:43 - INFO - __main__ - Global step 2800 Train loss 0.34 Classification-F1 0.2818347408511343 on epoch=466
05/21/2022 23:28:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.34 on epoch=468
05/21/2022 23:28:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.30 on epoch=469
05/21/2022 23:28:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.27 on epoch=471
05/21/2022 23:28:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.26 on epoch=473
05/21/2022 23:28:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.37 on epoch=474
05/21/2022 23:28:59 - INFO - __main__ - Global step 2850 Train loss 0.31 Classification-F1 0.31943606892896953 on epoch=474
05/21/2022 23:29:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.29 on epoch=476
05/21/2022 23:29:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.31 on epoch=478
05/21/2022 23:29:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.32 on epoch=479
05/21/2022 23:29:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.26 on epoch=481
05/21/2022 23:29:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.35 on epoch=483
05/21/2022 23:29:15 - INFO - __main__ - Global step 2900 Train loss 0.31 Classification-F1 0.3563991488068095 on epoch=483
05/21/2022 23:29:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.35 on epoch=484
05/21/2022 23:29:20 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.28 on epoch=486
05/21/2022 23:29:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.32 on epoch=488
05/21/2022 23:29:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.29 on epoch=489
05/21/2022 23:29:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.28 on epoch=491
05/21/2022 23:29:31 - INFO - __main__ - Global step 2950 Train loss 0.30 Classification-F1 0.2858900935965156 on epoch=491
05/21/2022 23:29:33 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.35 on epoch=493
05/21/2022 23:29:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.29 on epoch=494
05/21/2022 23:29:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.28 on epoch=496
05/21/2022 23:29:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.28 on epoch=498
05/21/2022 23:29:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.32 on epoch=499
05/21/2022 23:29:45 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:29:45 - INFO - __main__ - Printing 3 examples
05/21/2022 23:29:45 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/21/2022 23:29:45 - INFO - __main__ - ['contradiction']
05/21/2022 23:29:45 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/21/2022 23:29:45 - INFO - __main__ - ['contradiction']
05/21/2022 23:29:45 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/21/2022 23:29:45 - INFO - __main__ - ['contradiction']
05/21/2022 23:29:45 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:29:45 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:29:45 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 23:29:45 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:29:45 - INFO - __main__ - Printing 3 examples
05/21/2022 23:29:45 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/21/2022 23:29:45 - INFO - __main__ - ['contradiction']
05/21/2022 23:29:45 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/21/2022 23:29:45 - INFO - __main__ - ['contradiction']
05/21/2022 23:29:45 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/21/2022 23:29:45 - INFO - __main__ - ['contradiction']
05/21/2022 23:29:45 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:29:45 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:29:46 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 23:29:47 - INFO - __main__ - Global step 3000 Train loss 0.30 Classification-F1 0.3292697085800534 on epoch=499
05/21/2022 23:29:47 - INFO - __main__ - save last model!
05/21/2022 23:29:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 23:29:47 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 23:29:47 - INFO - __main__ - Printing 3 examples
05/21/2022 23:29:47 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/21/2022 23:29:47 - INFO - __main__ - ['contradiction']
05/21/2022 23:29:47 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/21/2022 23:29:47 - INFO - __main__ - ['entailment']
05/21/2022 23:29:47 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/21/2022 23:29:47 - INFO - __main__ - ['contradiction']
05/21/2022 23:29:47 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:29:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:29:48 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 23:30:04 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 23:30:04 - INFO - __main__ - task name: anli
05/21/2022 23:30:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 23:30:05 - INFO - __main__ - Starting training!
05/21/2022 23:30:17 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_13_0.3_8_predictions.txt
05/21/2022 23:30:17 - INFO - __main__ - Classification-F1 on test data: 0.2754
05/21/2022 23:30:17 - INFO - __main__ - prefix=anli_32_13, lr=0.3, bsz=8, dev_performance=0.45085687190950346, test_performance=0.2754157646096622
05/21/2022 23:30:17 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.2, bsz=8 ...
05/21/2022 23:30:18 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:30:18 - INFO - __main__ - Printing 3 examples
05/21/2022 23:30:18 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/21/2022 23:30:18 - INFO - __main__ - ['contradiction']
05/21/2022 23:30:18 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/21/2022 23:30:18 - INFO - __main__ - ['contradiction']
05/21/2022 23:30:18 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/21/2022 23:30:18 - INFO - __main__ - ['contradiction']
05/21/2022 23:30:18 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:30:18 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:30:18 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 23:30:18 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:30:18 - INFO - __main__ - Printing 3 examples
05/21/2022 23:30:18 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/21/2022 23:30:18 - INFO - __main__ - ['contradiction']
05/21/2022 23:30:18 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/21/2022 23:30:18 - INFO - __main__ - ['contradiction']
05/21/2022 23:30:18 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/21/2022 23:30:18 - INFO - __main__ - ['contradiction']
05/21/2022 23:30:18 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:30:18 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:30:19 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 23:30:33 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 23:30:33 - INFO - __main__ - task name: anli
05/21/2022 23:30:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 23:30:34 - INFO - __main__ - Starting training!
05/21/2022 23:30:37 - INFO - __main__ - Step 10 Global step 10 Train loss 6.79 on epoch=1
05/21/2022 23:30:39 - INFO - __main__ - Step 20 Global step 20 Train loss 4.91 on epoch=3
05/21/2022 23:30:42 - INFO - __main__ - Step 30 Global step 30 Train loss 2.84 on epoch=4
05/21/2022 23:30:45 - INFO - __main__ - Step 40 Global step 40 Train loss 1.57 on epoch=6
05/21/2022 23:30:47 - INFO - __main__ - Step 50 Global step 50 Train loss 1.24 on epoch=8
05/21/2022 23:30:50 - INFO - __main__ - Global step 50 Train loss 3.47 Classification-F1 0.16666666666666666 on epoch=8
05/21/2022 23:30:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/21/2022 23:30:53 - INFO - __main__ - Step 60 Global step 60 Train loss 1.04 on epoch=9
05/21/2022 23:30:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.80 on epoch=11
05/21/2022 23:30:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.76 on epoch=13
05/21/2022 23:31:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.71 on epoch=14
05/21/2022 23:31:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.66 on epoch=16
05/21/2022 23:31:06 - INFO - __main__ - Global step 100 Train loss 0.80 Classification-F1 0.16666666666666666 on epoch=16
05/21/2022 23:31:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.66 on epoch=18
05/21/2022 23:31:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=19
05/21/2022 23:31:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=21
05/21/2022 23:31:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.56 on epoch=23
05/21/2022 23:31:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=24
05/21/2022 23:31:22 - INFO - __main__ - Global step 150 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=24
05/21/2022 23:31:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=26
05/21/2022 23:31:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=28
05/21/2022 23:31:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=29
05/21/2022 23:31:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=31
05/21/2022 23:31:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.60 on epoch=33
05/21/2022 23:31:38 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.24007936507936503 on epoch=33
05/21/2022 23:31:38 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.24007936507936503 on epoch=33, global_step=200
05/21/2022 23:31:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.56 on epoch=34
05/21/2022 23:31:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=36
05/21/2022 23:31:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=38
05/21/2022 23:31:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.54 on epoch=39
05/21/2022 23:31:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=41
05/21/2022 23:31:54 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=41
05/21/2022 23:31:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=43
05/21/2022 23:32:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=44
05/21/2022 23:32:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=46
05/21/2022 23:32:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=48
05/21/2022 23:32:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=49
05/21/2022 23:32:10 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.24947589098532497 on epoch=49
05/21/2022 23:32:10 - INFO - __main__ - Saving model with best Classification-F1: 0.24007936507936503 -> 0.24947589098532497 on epoch=49, global_step=300
05/21/2022 23:32:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=51
05/21/2022 23:32:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=53
05/21/2022 23:32:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.58 on epoch=54
05/21/2022 23:32:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.55 on epoch=56
05/21/2022 23:32:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=58
05/21/2022 23:32:26 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.15873015873015875 on epoch=58
05/21/2022 23:32:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=59
05/21/2022 23:32:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=61
05/21/2022 23:32:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=63
05/21/2022 23:32:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=64
05/21/2022 23:32:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=66
05/21/2022 23:32:43 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=66
05/21/2022 23:32:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=68
05/21/2022 23:32:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=69
05/21/2022 23:32:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=71
05/21/2022 23:32:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=73
05/21/2022 23:32:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=74
05/21/2022 23:32:59 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=74
05/21/2022 23:33:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=76
05/21/2022 23:33:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=78
05/21/2022 23:33:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=79
05/21/2022 23:33:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.53 on epoch=81
05/21/2022 23:33:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=83
05/21/2022 23:33:15 - INFO - __main__ - Global step 500 Train loss 0.49 Classification-F1 0.26188866351138157 on epoch=83
05/21/2022 23:33:15 - INFO - __main__ - Saving model with best Classification-F1: 0.24947589098532497 -> 0.26188866351138157 on epoch=83, global_step=500
05/21/2022 23:33:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=84
05/21/2022 23:33:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=86
05/21/2022 23:33:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=88
05/21/2022 23:33:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=89
05/21/2022 23:33:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=91
05/21/2022 23:33:30 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=91
05/21/2022 23:33:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=93
05/21/2022 23:33:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=94
05/21/2022 23:33:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=96
05/21/2022 23:33:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=98
05/21/2022 23:33:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=99
05/21/2022 23:33:46 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=99
05/21/2022 23:33:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=101
05/21/2022 23:33:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=103
05/21/2022 23:33:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=104
05/21/2022 23:33:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=106
05/21/2022 23:33:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=108
05/21/2022 23:34:02 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=108
05/21/2022 23:34:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=109
05/21/2022 23:34:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=111
05/21/2022 23:34:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=113
05/21/2022 23:34:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=114
05/21/2022 23:34:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=116
05/21/2022 23:34:18 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.1836290071584189 on epoch=116
05/21/2022 23:34:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=118
05/21/2022 23:34:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=119
05/21/2022 23:34:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=121
05/21/2022 23:34:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=123
05/21/2022 23:34:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=124
05/21/2022 23:34:34 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.3864173651407694 on epoch=124
05/21/2022 23:34:34 - INFO - __main__ - Saving model with best Classification-F1: 0.26188866351138157 -> 0.3864173651407694 on epoch=124, global_step=750
05/21/2022 23:34:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=126
05/21/2022 23:34:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=128
05/21/2022 23:34:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=129
05/21/2022 23:34:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=131
05/21/2022 23:34:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=133
05/21/2022 23:34:50 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=133
05/21/2022 23:34:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=134
05/21/2022 23:34:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=136
05/21/2022 23:34:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=138
05/21/2022 23:35:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=139
05/21/2022 23:35:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=141
05/21/2022 23:35:06 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.20370370370370372 on epoch=141
05/21/2022 23:35:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=143
05/21/2022 23:35:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=144
05/21/2022 23:35:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=146
05/21/2022 23:35:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=148
05/21/2022 23:35:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=149
05/21/2022 23:35:22 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.20342857142857143 on epoch=149
05/21/2022 23:35:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=151
05/21/2022 23:35:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=153
05/21/2022 23:35:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=154
05/21/2022 23:35:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=156
05/21/2022 23:35:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=158
05/21/2022 23:35:39 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.26137566137566143 on epoch=158
05/21/2022 23:35:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=159
05/21/2022 23:35:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=161
05/21/2022 23:35:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.49 on epoch=163
05/21/2022 23:35:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=164
05/21/2022 23:35:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=166
05/21/2022 23:35:55 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=166
05/21/2022 23:35:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=168
05/21/2022 23:36:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=169
05/21/2022 23:36:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=171
05/21/2022 23:36:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=173
05/21/2022 23:36:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=174
05/21/2022 23:36:11 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.1881810228266921 on epoch=174
05/21/2022 23:36:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=176
05/21/2022 23:36:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=178
05/21/2022 23:36:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=179
05/21/2022 23:36:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=181
05/21/2022 23:36:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=183
05/21/2022 23:36:27 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.2515869073246122 on epoch=183
05/21/2022 23:36:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=184
05/21/2022 23:36:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=186
05/21/2022 23:36:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.47 on epoch=188
05/21/2022 23:36:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=189
05/21/2022 23:36:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=191
05/21/2022 23:36:43 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=191
05/21/2022 23:36:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=193
05/21/2022 23:36:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=194
05/21/2022 23:36:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=196
05/21/2022 23:36:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=198
05/21/2022 23:36:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=199
05/21/2022 23:36:59 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.33891922425155335 on epoch=199
05/21/2022 23:37:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=201
05/21/2022 23:37:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=203
05/21/2022 23:37:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=204
05/21/2022 23:37:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=206
05/21/2022 23:37:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=208
05/21/2022 23:37:15 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.28225974567437984 on epoch=208
05/21/2022 23:37:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=209
05/21/2022 23:37:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=211
05/21/2022 23:37:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=213
05/21/2022 23:37:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=214
05/21/2022 23:37:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=216
05/21/2022 23:37:31 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.1986376620522962 on epoch=216
05/21/2022 23:37:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=218
05/21/2022 23:37:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=219
05/21/2022 23:37:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=221
05/21/2022 23:37:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=223
05/21/2022 23:37:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=224
05/21/2022 23:37:47 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.2899931477967531 on epoch=224
05/21/2022 23:37:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=226
05/21/2022 23:37:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=228
05/21/2022 23:37:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=229
05/21/2022 23:37:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=231
05/21/2022 23:38:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=233
05/21/2022 23:38:03 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.22676432180775563 on epoch=233
05/21/2022 23:38:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=234
05/21/2022 23:38:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=236
05/21/2022 23:38:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=238
05/21/2022 23:38:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=239
05/21/2022 23:38:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=241
05/21/2022 23:38:19 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=241
05/21/2022 23:38:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=243
05/21/2022 23:38:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=244
05/21/2022 23:38:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=246
05/21/2022 23:38:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=248
05/21/2022 23:38:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=249
05/21/2022 23:38:35 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.3267188892188892 on epoch=249
05/21/2022 23:38:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=251
05/21/2022 23:38:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=253
05/21/2022 23:38:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=254
05/21/2022 23:38:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=256
05/21/2022 23:38:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=258
05/21/2022 23:38:52 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.3813041948635169 on epoch=258
05/21/2022 23:38:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=259
05/21/2022 23:38:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=261
05/21/2022 23:39:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=263
05/21/2022 23:39:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=264
05/21/2022 23:39:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.41 on epoch=266
05/21/2022 23:39:08 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.24228395061728394 on epoch=266
05/21/2022 23:39:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.44 on epoch=268
05/21/2022 23:39:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=269
05/21/2022 23:39:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=271
05/21/2022 23:39:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=273
05/21/2022 23:39:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=274
05/21/2022 23:39:24 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3583127135619182 on epoch=274
05/21/2022 23:39:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=276
05/21/2022 23:39:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=278
05/21/2022 23:39:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=279
05/21/2022 23:39:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=281
05/21/2022 23:39:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=283
05/21/2022 23:39:40 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.26208621100261964 on epoch=283
05/21/2022 23:39:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=284
05/21/2022 23:39:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=286
05/21/2022 23:39:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=288
05/21/2022 23:39:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=289
05/21/2022 23:39:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=291
05/21/2022 23:39:56 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.2798777373792809 on epoch=291
05/21/2022 23:39:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=293
05/21/2022 23:40:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=294
05/21/2022 23:40:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=296
05/21/2022 23:40:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=298
05/21/2022 23:40:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=299
05/21/2022 23:40:12 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.28908576350436815 on epoch=299
05/21/2022 23:40:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=301
05/21/2022 23:40:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=303
05/21/2022 23:40:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=304
05/21/2022 23:40:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=306
05/21/2022 23:40:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=308
05/21/2022 23:40:28 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.3044330390209689 on epoch=308
05/21/2022 23:40:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=309
05/21/2022 23:40:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=311
05/21/2022 23:40:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=313
05/21/2022 23:40:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=314
05/21/2022 23:40:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=316
05/21/2022 23:40:44 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.3226526856889893 on epoch=316
05/21/2022 23:40:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=318
05/21/2022 23:40:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=319
05/21/2022 23:40:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=321
05/21/2022 23:40:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=323
05/21/2022 23:40:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=324
05/21/2022 23:41:00 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.30914725827733286 on epoch=324
05/21/2022 23:41:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.30 on epoch=326
05/21/2022 23:41:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.31 on epoch=328
05/21/2022 23:41:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=329
05/21/2022 23:41:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=331
05/21/2022 23:41:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=333
05/21/2022 23:41:18 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.38284887220360525 on epoch=333
05/21/2022 23:41:20 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.34 on epoch=334
05/21/2022 23:41:23 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.31 on epoch=336
05/21/2022 23:41:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.35 on epoch=338
05/21/2022 23:41:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.29 on epoch=339
05/21/2022 23:41:31 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.34 on epoch=341
05/21/2022 23:41:33 - INFO - __main__ - Global step 2050 Train loss 0.33 Classification-F1 0.32890559732665 on epoch=341
05/21/2022 23:41:36 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.30 on epoch=343
05/21/2022 23:41:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.29 on epoch=344
05/21/2022 23:41:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=346
05/21/2022 23:41:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.26 on epoch=348
05/21/2022 23:41:47 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.29 on epoch=349
05/21/2022 23:41:50 - INFO - __main__ - Global step 2100 Train loss 0.29 Classification-F1 0.31987012987012986 on epoch=349
05/21/2022 23:41:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.28 on epoch=351
05/21/2022 23:41:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.28 on epoch=353
05/21/2022 23:41:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.28 on epoch=354
05/21/2022 23:42:00 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=356
05/21/2022 23:42:03 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.28 on epoch=358
05/21/2022 23:42:06 - INFO - __main__ - Global step 2150 Train loss 0.27 Classification-F1 0.37777777777777777 on epoch=358
05/21/2022 23:42:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=359
05/21/2022 23:42:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=361
05/21/2022 23:42:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.32 on epoch=363
05/21/2022 23:42:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.26 on epoch=364
05/21/2022 23:42:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.37 on epoch=366
05/21/2022 23:42:22 - INFO - __main__ - Global step 2200 Train loss 0.30 Classification-F1 0.2700396825396825 on epoch=366
05/21/2022 23:42:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.29 on epoch=368
05/21/2022 23:42:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=369
05/21/2022 23:42:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=371
05/21/2022 23:42:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.29 on epoch=373
05/21/2022 23:42:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.28 on epoch=374
05/21/2022 23:42:40 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.2852332852332852 on epoch=374
05/21/2022 23:42:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.23 on epoch=376
05/21/2022 23:42:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.29 on epoch=378
05/21/2022 23:42:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.28 on epoch=379
05/21/2022 23:42:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.24 on epoch=381
05/21/2022 23:42:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.30 on epoch=383
05/21/2022 23:42:56 - INFO - __main__ - Global step 2300 Train loss 0.27 Classification-F1 0.2955516373237892 on epoch=383
05/21/2022 23:42:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.23 on epoch=384
05/21/2022 23:43:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.24 on epoch=386
05/21/2022 23:43:04 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=388
05/21/2022 23:43:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.29 on epoch=389
05/21/2022 23:43:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=391
05/21/2022 23:43:12 - INFO - __main__ - Global step 2350 Train loss 0.23 Classification-F1 0.3555771713666451 on epoch=391
05/21/2022 23:43:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.25 on epoch=393
05/21/2022 23:43:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.24 on epoch=394
05/21/2022 23:43:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.27 on epoch=396
05/21/2022 23:43:22 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.20 on epoch=398
05/21/2022 23:43:25 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=399
05/21/2022 23:43:29 - INFO - __main__ - Global step 2400 Train loss 0.24 Classification-F1 0.37225737225737227 on epoch=399
05/21/2022 23:43:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.26 on epoch=401
05/21/2022 23:43:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=403
05/21/2022 23:43:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=404
05/21/2022 23:43:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.24 on epoch=406
05/21/2022 23:43:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.23 on epoch=408
05/21/2022 23:43:46 - INFO - __main__ - Global step 2450 Train loss 0.24 Classification-F1 0.3834603834603835 on epoch=408
05/21/2022 23:43:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.26 on epoch=409
05/21/2022 23:43:52 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.20 on epoch=411
05/21/2022 23:43:54 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.23 on epoch=413
05/21/2022 23:43:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=414
05/21/2022 23:44:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.23 on epoch=416
05/21/2022 23:44:02 - INFO - __main__ - Global step 2500 Train loss 0.23 Classification-F1 0.2485803293279929 on epoch=416
05/21/2022 23:44:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=418
05/21/2022 23:44:08 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.23 on epoch=419
05/21/2022 23:44:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.25 on epoch=421
05/21/2022 23:44:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.22 on epoch=423
05/21/2022 23:44:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.27 on epoch=424
05/21/2022 23:44:20 - INFO - __main__ - Global step 2550 Train loss 0.24 Classification-F1 0.3033334226453628 on epoch=424
05/21/2022 23:44:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.21 on epoch=426
05/21/2022 23:44:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.22 on epoch=428
05/21/2022 23:44:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.18 on epoch=429
05/21/2022 23:44:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=431
05/21/2022 23:44:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=433
05/21/2022 23:44:37 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.3565836200583122 on epoch=433
05/21/2022 23:44:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.18 on epoch=434
05/21/2022 23:44:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.22 on epoch=436
05/21/2022 23:44:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.22 on epoch=438
05/21/2022 23:44:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.19 on epoch=439
05/21/2022 23:44:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=441
05/21/2022 23:44:56 - INFO - __main__ - Global step 2650 Train loss 0.20 Classification-F1 0.16959421592337712 on epoch=441
05/21/2022 23:44:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=443
05/21/2022 23:45:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.26 on epoch=444
05/21/2022 23:45:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=446
05/21/2022 23:45:06 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=448
05/21/2022 23:45:09 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.18 on epoch=449
05/21/2022 23:45:12 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.21875150959896722 on epoch=449
05/21/2022 23:45:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=451
05/21/2022 23:45:17 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=453
05/21/2022 23:45:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=454
05/21/2022 23:45:22 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.25 on epoch=456
05/21/2022 23:45:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.23 on epoch=458
05/21/2022 23:45:28 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.34484665661136243 on epoch=458
05/21/2022 23:45:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=459
05/21/2022 23:45:33 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=461
05/21/2022 23:45:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.21 on epoch=463
05/21/2022 23:45:38 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.16 on epoch=464
05/21/2022 23:45:41 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=466
05/21/2022 23:45:44 - INFO - __main__ - Global step 2800 Train loss 0.19 Classification-F1 0.28864139975251085 on epoch=466
05/21/2022 23:45:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=468
05/21/2022 23:45:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.22 on epoch=469
05/21/2022 23:45:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=471
05/21/2022 23:45:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.21 on epoch=473
05/21/2022 23:45:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=474
05/21/2022 23:46:00 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.32712609028398504 on epoch=474
05/21/2022 23:46:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=476
05/21/2022 23:46:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=478
05/21/2022 23:46:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=479
05/21/2022 23:46:11 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=481
05/21/2022 23:46:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.17 on epoch=483
05/21/2022 23:46:16 - INFO - __main__ - Global step 2900 Train loss 0.15 Classification-F1 0.2720557491289199 on epoch=483
05/21/2022 23:46:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=484
05/21/2022 23:46:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=486
05/21/2022 23:46:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.18 on epoch=488
05/21/2022 23:46:27 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=489
05/21/2022 23:46:29 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.21 on epoch=491
05/21/2022 23:46:32 - INFO - __main__ - Global step 2950 Train loss 0.17 Classification-F1 0.2716266583261657 on epoch=491
05/21/2022 23:46:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=493
05/21/2022 23:46:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.20 on epoch=494
05/21/2022 23:46:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.17 on epoch=496
05/21/2022 23:46:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.15 on epoch=498
05/21/2022 23:46:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=499
05/21/2022 23:46:47 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:46:47 - INFO - __main__ - Printing 3 examples
05/21/2022 23:46:47 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/21/2022 23:46:47 - INFO - __main__ - ['entailment']
05/21/2022 23:46:47 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/21/2022 23:46:47 - INFO - __main__ - ['entailment']
05/21/2022 23:46:47 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/21/2022 23:46:47 - INFO - __main__ - ['entailment']
05/21/2022 23:46:47 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:46:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:46:47 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 23:46:47 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:46:47 - INFO - __main__ - Printing 3 examples
05/21/2022 23:46:47 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/21/2022 23:46:47 - INFO - __main__ - ['entailment']
05/21/2022 23:46:47 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/21/2022 23:46:47 - INFO - __main__ - ['entailment']
05/21/2022 23:46:47 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/21/2022 23:46:47 - INFO - __main__ - ['entailment']
05/21/2022 23:46:47 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:46:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:46:47 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 23:46:48 - INFO - __main__ - Global step 3000 Train loss 0.15 Classification-F1 0.1401136278687299 on epoch=499
05/21/2022 23:46:48 - INFO - __main__ - save last model!
05/21/2022 23:46:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 23:46:48 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 23:46:48 - INFO - __main__ - Printing 3 examples
05/21/2022 23:46:48 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/21/2022 23:46:48 - INFO - __main__ - ['contradiction']
05/21/2022 23:46:48 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/21/2022 23:46:48 - INFO - __main__ - ['entailment']
05/21/2022 23:46:48 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/21/2022 23:46:48 - INFO - __main__ - ['contradiction']
05/21/2022 23:46:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:46:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:46:50 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 23:47:05 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 23:47:05 - INFO - __main__ - task name: anli
05/21/2022 23:47:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 23:47:06 - INFO - __main__ - Starting training!
05/21/2022 23:47:44 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_13_0.2_8_predictions.txt
05/21/2022 23:47:44 - INFO - __main__ - Classification-F1 on test data: 0.0810
05/21/2022 23:47:44 - INFO - __main__ - prefix=anli_32_13, lr=0.2, bsz=8, dev_performance=0.3864173651407694, test_performance=0.08098551067678834
05/21/2022 23:47:44 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.5, bsz=8 ...
05/21/2022 23:47:45 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:47:45 - INFO - __main__ - Printing 3 examples
05/21/2022 23:47:45 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/21/2022 23:47:45 - INFO - __main__ - ['entailment']
05/21/2022 23:47:45 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/21/2022 23:47:45 - INFO - __main__ - ['entailment']
05/21/2022 23:47:45 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/21/2022 23:47:45 - INFO - __main__ - ['entailment']
05/21/2022 23:47:45 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:47:45 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:47:45 - INFO - __main__ - Loaded 96 examples from train data
05/21/2022 23:47:45 - INFO - __main__ - Start tokenizing ... 96 instances
05/21/2022 23:47:45 - INFO - __main__ - Printing 3 examples
05/21/2022 23:47:45 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/21/2022 23:47:45 - INFO - __main__ - ['entailment']
05/21/2022 23:47:45 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/21/2022 23:47:45 - INFO - __main__ - ['entailment']
05/21/2022 23:47:45 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/21/2022 23:47:45 - INFO - __main__ - ['entailment']
05/21/2022 23:47:45 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:47:45 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:47:46 - INFO - __main__ - Loaded 96 examples from dev data
05/21/2022 23:48:00 - INFO - __main__ - try to initialize prompt embeddings
05/21/2022 23:48:00 - INFO - __main__ - task name: anli
05/21/2022 23:48:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/21/2022 23:48:01 - INFO - __main__ - Starting training!
05/21/2022 23:48:04 - INFO - __main__ - Step 10 Global step 10 Train loss 5.56 on epoch=1
05/21/2022 23:48:07 - INFO - __main__ - Step 20 Global step 20 Train loss 1.97 on epoch=3
05/21/2022 23:48:09 - INFO - __main__ - Step 30 Global step 30 Train loss 1.06 on epoch=4
05/21/2022 23:48:12 - INFO - __main__ - Step 40 Global step 40 Train loss 0.79 on epoch=6
05/21/2022 23:48:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.73 on epoch=8
05/21/2022 23:48:17 - INFO - __main__ - Global step 50 Train loss 2.02 Classification-F1 0.16666666666666666 on epoch=8
05/21/2022 23:48:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/21/2022 23:48:19 - INFO - __main__ - Step 60 Global step 60 Train loss 0.66 on epoch=9
05/21/2022 23:48:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.62 on epoch=11
05/21/2022 23:48:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=13
05/21/2022 23:48:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=14
05/21/2022 23:48:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=16
05/21/2022 23:48:32 - INFO - __main__ - Global step 100 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=16
05/21/2022 23:48:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=18
05/21/2022 23:48:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=19
05/21/2022 23:48:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=21
05/21/2022 23:48:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=23
05/21/2022 23:48:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=24
05/21/2022 23:48:48 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.18971428571428572 on epoch=24
05/21/2022 23:48:48 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.18971428571428572 on epoch=24, global_step=150
05/21/2022 23:48:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=26
05/21/2022 23:48:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=28
05/21/2022 23:48:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=29
05/21/2022 23:48:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=31
05/21/2022 23:49:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=33
05/21/2022 23:49:03 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=33
05/21/2022 23:49:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=34
05/21/2022 23:49:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=36
05/21/2022 23:49:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=38
05/21/2022 23:49:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=39
05/21/2022 23:49:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=41
05/21/2022 23:49:19 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=41
05/21/2022 23:49:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=43
05/21/2022 23:49:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=44
05/21/2022 23:49:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=46
05/21/2022 23:49:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=48
05/21/2022 23:49:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=49
05/21/2022 23:49:35 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.21132323897300856 on epoch=49
05/21/2022 23:49:35 - INFO - __main__ - Saving model with best Classification-F1: 0.18971428571428572 -> 0.21132323897300856 on epoch=49, global_step=300
05/21/2022 23:49:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=51
05/21/2022 23:49:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=53
05/21/2022 23:49:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=54
05/21/2022 23:49:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=56
05/21/2022 23:49:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=58
05/21/2022 23:49:50 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=58
05/21/2022 23:49:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=59
05/21/2022 23:49:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=61
05/21/2022 23:49:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=63
05/21/2022 23:50:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=64
05/21/2022 23:50:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=66
05/21/2022 23:50:05 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
05/21/2022 23:50:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
05/21/2022 23:50:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=69
05/21/2022 23:50:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=71
05/21/2022 23:50:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=73
05/21/2022 23:50:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=74
05/21/2022 23:50:20 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.2304970918030821 on epoch=74
05/21/2022 23:50:20 - INFO - __main__ - Saving model with best Classification-F1: 0.21132323897300856 -> 0.2304970918030821 on epoch=74, global_step=450
05/21/2022 23:50:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=76
05/21/2022 23:50:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=78
05/21/2022 23:50:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=79
05/21/2022 23:50:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=81
05/21/2022 23:50:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=83
05/21/2022 23:50:36 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=83
05/21/2022 23:50:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=84
05/21/2022 23:50:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=86
05/21/2022 23:50:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=88
05/21/2022 23:50:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=89
05/21/2022 23:50:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=91
05/21/2022 23:50:51 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=91
05/21/2022 23:50:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=93
05/21/2022 23:50:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=94
05/21/2022 23:50:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=96
05/21/2022 23:51:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=98
05/21/2022 23:51:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=99
05/21/2022 23:51:06 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.2481203007518797 on epoch=99
05/21/2022 23:51:06 - INFO - __main__ - Saving model with best Classification-F1: 0.2304970918030821 -> 0.2481203007518797 on epoch=99, global_step=600
05/21/2022 23:51:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=101
05/21/2022 23:51:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=103
05/21/2022 23:51:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=104
05/21/2022 23:51:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=106
05/21/2022 23:51:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=108
05/21/2022 23:51:22 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.20342857142857143 on epoch=108
05/21/2022 23:51:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=109
05/21/2022 23:51:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=111
05/21/2022 23:51:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=113
05/21/2022 23:51:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=114
05/21/2022 23:51:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=116
05/21/2022 23:51:38 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=116
05/21/2022 23:51:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=118
05/21/2022 23:51:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=119
05/21/2022 23:51:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=121
05/21/2022 23:51:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=123
05/21/2022 23:51:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=124
05/21/2022 23:51:54 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.2761794052116633 on epoch=124
05/21/2022 23:51:54 - INFO - __main__ - Saving model with best Classification-F1: 0.2481203007518797 -> 0.2761794052116633 on epoch=124, global_step=750
05/21/2022 23:51:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=126
05/21/2022 23:52:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=128
05/21/2022 23:52:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=129
05/21/2022 23:52:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=131
05/21/2022 23:52:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=133
05/21/2022 23:52:10 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=133
05/21/2022 23:52:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=134
05/21/2022 23:52:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=136
05/21/2022 23:52:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=138
05/21/2022 23:52:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=139
05/21/2022 23:52:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=141
05/21/2022 23:52:26 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=141
05/21/2022 23:52:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=143
05/21/2022 23:52:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=144
05/21/2022 23:52:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=146
05/21/2022 23:52:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=148
05/21/2022 23:52:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=149
05/21/2022 23:52:42 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.2842921784098254 on epoch=149
05/21/2022 23:52:42 - INFO - __main__ - Saving model with best Classification-F1: 0.2761794052116633 -> 0.2842921784098254 on epoch=149, global_step=900
05/21/2022 23:52:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=151
05/21/2022 23:52:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=153
05/21/2022 23:52:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=154
05/21/2022 23:52:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=156
05/21/2022 23:52:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=158
05/21/2022 23:52:58 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=158
05/21/2022 23:53:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=159
05/21/2022 23:53:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=161
05/21/2022 23:53:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=163
05/21/2022 23:53:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=164
05/21/2022 23:53:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=166
05/21/2022 23:53:14 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=166
05/21/2022 23:53:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=168
05/21/2022 23:53:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=169
05/21/2022 23:53:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=171
05/21/2022 23:53:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=173
05/21/2022 23:53:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=174
05/21/2022 23:53:30 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.26029482910725976 on epoch=174
05/21/2022 23:53:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=176
05/21/2022 23:53:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=178
05/21/2022 23:53:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=179
05/21/2022 23:53:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=181
05/21/2022 23:53:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=183
05/21/2022 23:53:46 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.2640984483681113 on epoch=183
05/21/2022 23:53:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=184
05/21/2022 23:53:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=186
05/21/2022 23:53:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=188
05/21/2022 23:53:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=189
05/21/2022 23:54:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=191
05/21/2022 23:54:03 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.18494117647058825 on epoch=191
05/21/2022 23:54:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=193
05/21/2022 23:54:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=194
05/21/2022 23:54:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=196
05/21/2022 23:54:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=198
05/21/2022 23:54:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=199
05/21/2022 23:54:19 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.33737604881769645 on epoch=199
05/21/2022 23:54:19 - INFO - __main__ - Saving model with best Classification-F1: 0.2842921784098254 -> 0.33737604881769645 on epoch=199, global_step=1200
05/21/2022 23:54:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=201
05/21/2022 23:54:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=203
05/21/2022 23:54:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=204
05/21/2022 23:54:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=206
05/21/2022 23:54:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=208
05/21/2022 23:54:35 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.29637142213207124 on epoch=208
05/21/2022 23:54:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=209
05/21/2022 23:54:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=211
05/21/2022 23:54:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=213
05/21/2022 23:54:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=214
05/21/2022 23:54:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=216
05/21/2022 23:54:51 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.2566408826527553 on epoch=216
05/21/2022 23:54:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.31 on epoch=218
05/21/2022 23:54:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=219
05/21/2022 23:54:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.32 on epoch=221
05/21/2022 23:55:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.31 on epoch=223
05/21/2022 23:55:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=224
05/21/2022 23:55:08 - INFO - __main__ - Global step 1350 Train loss 0.34 Classification-F1 0.3874877066082929 on epoch=224
05/21/2022 23:55:08 - INFO - __main__ - Saving model with best Classification-F1: 0.33737604881769645 -> 0.3874877066082929 on epoch=224, global_step=1350
05/21/2022 23:55:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=226
05/21/2022 23:55:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.30 on epoch=228
05/21/2022 23:55:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=229
05/21/2022 23:55:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=231
05/21/2022 23:55:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=233
05/21/2022 23:55:23 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.284277537045953 on epoch=233
05/21/2022 23:55:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=234
05/21/2022 23:55:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=236
05/21/2022 23:55:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=238
05/21/2022 23:55:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.29 on epoch=239
05/21/2022 23:55:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=241
05/21/2022 23:55:39 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.21934331025240114 on epoch=241
05/21/2022 23:55:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=243
05/21/2022 23:55:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=244
05/21/2022 23:55:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.38 on epoch=246
05/21/2022 23:55:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=248
05/21/2022 23:55:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=249
05/21/2022 23:55:55 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.3577725612936881 on epoch=249
05/21/2022 23:55:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=251
05/21/2022 23:56:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.27 on epoch=253
05/21/2022 23:56:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=254
05/21/2022 23:56:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=256
05/21/2022 23:56:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=258
05/21/2022 23:56:10 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.26997942712454503 on epoch=258
05/21/2022 23:56:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=259
05/21/2022 23:56:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=261
05/21/2022 23:56:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=263
05/21/2022 23:56:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=264
05/21/2022 23:56:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=266
05/21/2022 23:56:26 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.28597036753065125 on epoch=266
05/21/2022 23:56:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=268
05/21/2022 23:56:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=269
05/21/2022 23:56:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=271
05/21/2022 23:56:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=273
05/21/2022 23:56:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.28 on epoch=274
05/21/2022 23:56:42 - INFO - __main__ - Global step 1650 Train loss 0.25 Classification-F1 0.2637430536366706 on epoch=274
05/21/2022 23:56:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=276
05/21/2022 23:56:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=278
05/21/2022 23:56:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=279
05/21/2022 23:56:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=281
05/21/2022 23:56:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.24 on epoch=283
05/21/2022 23:56:58 - INFO - __main__ - Global step 1700 Train loss 0.26 Classification-F1 0.32094314934978235 on epoch=283
05/21/2022 23:57:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=284
05/21/2022 23:57:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=286
05/21/2022 23:57:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=288
05/21/2022 23:57:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.27 on epoch=289
05/21/2022 23:57:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=291
05/21/2022 23:57:14 - INFO - __main__ - Global step 1750 Train loss 0.24 Classification-F1 0.28649293135752 on epoch=291
05/21/2022 23:57:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=293
05/21/2022 23:57:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=294
05/21/2022 23:57:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=296
05/21/2022 23:57:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=298
05/21/2022 23:57:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.24 on epoch=299
05/21/2022 23:57:30 - INFO - __main__ - Global step 1800 Train loss 0.24 Classification-F1 0.31856540084388185 on epoch=299
05/21/2022 23:57:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=301
05/21/2022 23:57:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=303
05/21/2022 23:57:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=304
05/21/2022 23:57:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.17 on epoch=306
05/21/2022 23:57:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.23 on epoch=308
05/21/2022 23:57:46 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.21261261261261258 on epoch=308
05/21/2022 23:57:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=309
05/21/2022 23:57:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=311
05/21/2022 23:57:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=313
05/21/2022 23:57:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=314
05/21/2022 23:57:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=316
05/21/2022 23:58:02 - INFO - __main__ - Global step 1900 Train loss 0.22 Classification-F1 0.2976932367149759 on epoch=316
05/21/2022 23:58:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=318
05/21/2022 23:58:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=319
05/21/2022 23:58:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=321
05/21/2022 23:58:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=323
05/21/2022 23:58:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=324
05/21/2022 23:58:18 - INFO - __main__ - Global step 1950 Train loss 0.22 Classification-F1 0.23885984565596216 on epoch=324
05/21/2022 23:58:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.22 on epoch=326
05/21/2022 23:58:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=328
05/21/2022 23:58:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=329
05/21/2022 23:58:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=331
05/21/2022 23:58:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=333
05/21/2022 23:58:34 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.2182346109175377 on epoch=333
05/21/2022 23:58:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.18 on epoch=334
05/21/2022 23:58:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=336
05/21/2022 23:58:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=338
05/21/2022 23:58:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=339
05/21/2022 23:58:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=341
05/21/2022 23:58:50 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.2411156095366622 on epoch=341
05/21/2022 23:58:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=343
05/21/2022 23:58:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.19 on epoch=344
05/21/2022 23:58:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=346
05/21/2022 23:59:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=348
05/21/2022 23:59:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=349
05/21/2022 23:59:06 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.35891690009337074 on epoch=349
05/21/2022 23:59:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=351
05/21/2022 23:59:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=353
05/21/2022 23:59:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=354
05/21/2022 23:59:17 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=356
05/21/2022 23:59:20 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=358
05/21/2022 23:59:22 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.3210607163068963 on epoch=358
05/21/2022 23:59:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=359
05/21/2022 23:59:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=361
05/21/2022 23:59:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=363
05/21/2022 23:59:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.13 on epoch=364
05/21/2022 23:59:36 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=366
05/21/2022 23:59:38 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.2699430199430199 on epoch=366
05/21/2022 23:59:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.14 on epoch=368
05/21/2022 23:59:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=369
05/21/2022 23:59:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=371
05/21/2022 23:59:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=373
05/21/2022 23:59:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.13 on epoch=374
05/21/2022 23:59:54 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.25489987595250757 on epoch=374
05/21/2022 23:59:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=376
05/21/2022 23:59:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=378
05/22/2022 00:00:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=379
05/22/2022 00:00:04 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=381
05/22/2022 00:00:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=383
05/22/2022 00:00:10 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.2155470038482683 on epoch=383
05/22/2022 00:00:12 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=384
05/22/2022 00:00:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=386
05/22/2022 00:00:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.13 on epoch=388
05/22/2022 00:00:20 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=389
05/22/2022 00:00:23 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=391
05/22/2022 00:00:25 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.12408455886716757 on epoch=391
05/22/2022 00:00:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=393
05/22/2022 00:00:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=394
05/22/2022 00:00:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=396
05/22/2022 00:00:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=398
05/22/2022 00:00:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=399
05/22/2022 00:00:41 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.22325340538106492 on epoch=399
05/22/2022 00:00:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=401
05/22/2022 00:00:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=403
05/22/2022 00:00:49 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=404
05/22/2022 00:00:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=406
05/22/2022 00:00:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=408
05/22/2022 00:00:57 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.23686560729243655 on epoch=408
05/22/2022 00:01:00 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=409
05/22/2022 00:01:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=411
05/22/2022 00:01:05 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=413
05/22/2022 00:01:08 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=414
05/22/2022 00:01:10 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=416
05/22/2022 00:01:13 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.3205840688678217 on epoch=416
05/22/2022 00:01:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=418
05/22/2022 00:01:19 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=419
05/22/2022 00:01:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=421
05/22/2022 00:01:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=423
05/22/2022 00:01:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=424
05/22/2022 00:01:29 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.19393939393939394 on epoch=424
05/22/2022 00:01:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=426
05/22/2022 00:01:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.13 on epoch=428
05/22/2022 00:01:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=429
05/22/2022 00:01:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=431
05/22/2022 00:01:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=433
05/22/2022 00:01:46 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.15714285714285714 on epoch=433
05/22/2022 00:01:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=434
05/22/2022 00:01:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=436
05/22/2022 00:01:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=438
05/22/2022 00:01:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=439
05/22/2022 00:01:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=441
05/22/2022 00:02:02 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.20235478806907378 on epoch=441
05/22/2022 00:02:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=443
05/22/2022 00:02:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=444
05/22/2022 00:02:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=446
05/22/2022 00:02:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=448
05/22/2022 00:02:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=449
05/22/2022 00:02:18 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.24215609608868033 on epoch=449
05/22/2022 00:02:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=451
05/22/2022 00:02:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=453
05/22/2022 00:02:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=454
05/22/2022 00:02:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=456
05/22/2022 00:02:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=458
05/22/2022 00:02:34 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.32109919066440806 on epoch=458
05/22/2022 00:02:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=459
05/22/2022 00:02:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=461
05/22/2022 00:02:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=463
05/22/2022 00:02:45 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.08 on epoch=464
05/22/2022 00:02:48 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=466
05/22/2022 00:02:50 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.18800539083557952 on epoch=466
05/22/2022 00:02:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=468
05/22/2022 00:02:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=469
05/22/2022 00:02:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=471
05/22/2022 00:03:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=473
05/22/2022 00:03:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=474
05/22/2022 00:03:06 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.11842105263157895 on epoch=474
05/22/2022 00:03:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=476
05/22/2022 00:03:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=478
05/22/2022 00:03:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=479
05/22/2022 00:03:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=481
05/22/2022 00:03:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=483
05/22/2022 00:03:22 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.1620585634784418 on epoch=483
05/22/2022 00:03:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=484
05/22/2022 00:03:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=486
05/22/2022 00:03:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=488
05/22/2022 00:03:33 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=489
05/22/2022 00:03:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=491
05/22/2022 00:03:38 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.13665788896203643 on epoch=491
05/22/2022 00:03:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=493
05/22/2022 00:03:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=494
05/22/2022 00:03:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=496
05/22/2022 00:03:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=498
05/22/2022 00:03:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=499
05/22/2022 00:03:53 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:03:53 - INFO - __main__ - Printing 3 examples
05/22/2022 00:03:53 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/22/2022 00:03:53 - INFO - __main__ - ['entailment']
05/22/2022 00:03:53 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/22/2022 00:03:53 - INFO - __main__ - ['entailment']
05/22/2022 00:03:53 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/22/2022 00:03:53 - INFO - __main__ - ['entailment']
05/22/2022 00:03:53 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:03:53 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:03:53 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 00:03:53 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:03:53 - INFO - __main__ - Printing 3 examples
05/22/2022 00:03:53 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/22/2022 00:03:53 - INFO - __main__ - ['entailment']
05/22/2022 00:03:53 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/22/2022 00:03:53 - INFO - __main__ - ['entailment']
05/22/2022 00:03:53 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/22/2022 00:03:53 - INFO - __main__ - ['entailment']
05/22/2022 00:03:53 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:03:53 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:03:53 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 00:03:55 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.14057871952608794 on epoch=499
05/22/2022 00:03:55 - INFO - __main__ - save last model!
05/22/2022 00:03:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 00:03:55 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 00:03:55 - INFO - __main__ - Printing 3 examples
05/22/2022 00:03:55 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 00:03:55 - INFO - __main__ - ['contradiction']
05/22/2022 00:03:55 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 00:03:55 - INFO - __main__ - ['entailment']
05/22/2022 00:03:55 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 00:03:55 - INFO - __main__ - ['contradiction']
05/22/2022 00:03:55 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:03:55 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:03:56 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 00:04:12 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 00:04:12 - INFO - __main__ - task name: anli
05/22/2022 00:04:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 00:04:13 - INFO - __main__ - Starting training!
05/22/2022 00:04:25 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_21_0.5_8_predictions.txt
05/22/2022 00:04:25 - INFO - __main__ - Classification-F1 on test data: 0.0604
05/22/2022 00:04:26 - INFO - __main__ - prefix=anli_32_21, lr=0.5, bsz=8, dev_performance=0.3874877066082929, test_performance=0.06041630869539189
05/22/2022 00:04:26 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.4, bsz=8 ...
05/22/2022 00:04:27 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:04:27 - INFO - __main__ - Printing 3 examples
05/22/2022 00:04:27 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/22/2022 00:04:27 - INFO - __main__ - ['entailment']
05/22/2022 00:04:27 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/22/2022 00:04:27 - INFO - __main__ - ['entailment']
05/22/2022 00:04:27 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/22/2022 00:04:27 - INFO - __main__ - ['entailment']
05/22/2022 00:04:27 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:04:27 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:04:27 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 00:04:27 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:04:27 - INFO - __main__ - Printing 3 examples
05/22/2022 00:04:27 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/22/2022 00:04:27 - INFO - __main__ - ['entailment']
05/22/2022 00:04:27 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/22/2022 00:04:27 - INFO - __main__ - ['entailment']
05/22/2022 00:04:27 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/22/2022 00:04:27 - INFO - __main__ - ['entailment']
05/22/2022 00:04:27 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:04:27 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:04:27 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 00:04:41 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 00:04:41 - INFO - __main__ - task name: anli
05/22/2022 00:04:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 00:04:42 - INFO - __main__ - Starting training!
05/22/2022 00:04:45 - INFO - __main__ - Step 10 Global step 10 Train loss 6.10 on epoch=1
05/22/2022 00:04:48 - INFO - __main__ - Step 20 Global step 20 Train loss 2.39 on epoch=3
05/22/2022 00:04:50 - INFO - __main__ - Step 30 Global step 30 Train loss 1.30 on epoch=4
05/22/2022 00:04:53 - INFO - __main__ - Step 40 Global step 40 Train loss 1.03 on epoch=6
05/22/2022 00:04:56 - INFO - __main__ - Step 50 Global step 50 Train loss 0.77 on epoch=8
05/22/2022 00:04:59 - INFO - __main__ - Global step 50 Train loss 2.32 Classification-F1 0.16666666666666666 on epoch=8
05/22/2022 00:04:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/22/2022 00:05:01 - INFO - __main__ - Step 60 Global step 60 Train loss 0.73 on epoch=9
05/22/2022 00:05:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.70 on epoch=11
05/22/2022 00:05:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.62 on epoch=13
05/22/2022 00:05:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=14
05/22/2022 00:05:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.61 on epoch=16
05/22/2022 00:05:14 - INFO - __main__ - Global step 100 Train loss 0.65 Classification-F1 0.16666666666666666 on epoch=16
05/22/2022 00:05:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=18
05/22/2022 00:05:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=19
05/22/2022 00:05:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=21
05/22/2022 00:05:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=23
05/22/2022 00:05:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=24
05/22/2022 00:05:30 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.24835339541221893 on epoch=24
05/22/2022 00:05:30 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.24835339541221893 on epoch=24, global_step=150
05/22/2022 00:05:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=26
05/22/2022 00:05:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=28
05/22/2022 00:05:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=29
05/22/2022 00:05:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=31
05/22/2022 00:05:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=33
05/22/2022 00:05:45 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=33
05/22/2022 00:05:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=34
05/22/2022 00:05:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=36
05/22/2022 00:05:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=38
05/22/2022 00:05:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=39
05/22/2022 00:05:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=41
05/22/2022 00:06:01 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=41
05/22/2022 00:06:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=43
05/22/2022 00:06:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=44
05/22/2022 00:06:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=46
05/22/2022 00:06:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=48
05/22/2022 00:06:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=49
05/22/2022 00:06:17 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.28154487601492206 on epoch=49
05/22/2022 00:06:17 - INFO - __main__ - Saving model with best Classification-F1: 0.24835339541221893 -> 0.28154487601492206 on epoch=49, global_step=300
05/22/2022 00:06:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=51
05/22/2022 00:06:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=53
05/22/2022 00:06:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=54
05/22/2022 00:06:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=56
05/22/2022 00:06:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=58
05/22/2022 00:06:33 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=58
05/22/2022 00:06:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=59
05/22/2022 00:06:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=61
05/22/2022 00:06:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=63
05/22/2022 00:06:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=64
05/22/2022 00:06:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=66
05/22/2022 00:06:50 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=66
05/22/2022 00:06:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
05/22/2022 00:06:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=69
05/22/2022 00:06:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=71
05/22/2022 00:07:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=73
05/22/2022 00:07:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=74
05/22/2022 00:07:06 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=74
05/22/2022 00:07:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=76
05/22/2022 00:07:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=78
05/22/2022 00:07:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=79
05/22/2022 00:07:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=81
05/22/2022 00:07:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=83
05/22/2022 00:07:22 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=83
05/22/2022 00:07:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.49 on epoch=84
05/22/2022 00:07:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=86
05/22/2022 00:07:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=88
05/22/2022 00:07:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=89
05/22/2022 00:07:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=91
05/22/2022 00:07:38 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=91
05/22/2022 00:07:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=93
05/22/2022 00:07:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=94
05/22/2022 00:07:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=96
05/22/2022 00:07:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=98
05/22/2022 00:07:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=99
05/22/2022 00:07:54 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.26704219475303814 on epoch=99
05/22/2022 00:07:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=101
05/22/2022 00:08:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=103
05/22/2022 00:08:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=104
05/22/2022 00:08:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=106
05/22/2022 00:08:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=108
05/22/2022 00:08:11 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=108
05/22/2022 00:08:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=109
05/22/2022 00:08:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=111
05/22/2022 00:08:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=113
05/22/2022 00:08:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=114
05/22/2022 00:08:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.55 on epoch=116
05/22/2022 00:08:27 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=116
05/22/2022 00:08:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=118
05/22/2022 00:08:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=119
05/22/2022 00:08:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=121
05/22/2022 00:08:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=123
05/22/2022 00:08:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=124
05/22/2022 00:08:43 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.18892001244942422 on epoch=124
05/22/2022 00:08:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=126
05/22/2022 00:08:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=128
05/22/2022 00:08:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=129
05/22/2022 00:08:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=131
05/22/2022 00:08:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=133
05/22/2022 00:08:59 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.2457368006304176 on epoch=133
05/22/2022 00:09:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=134
05/22/2022 00:09:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=136
05/22/2022 00:09:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=138
05/22/2022 00:09:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=139
05/22/2022 00:09:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=141
05/22/2022 00:09:15 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=141
05/22/2022 00:09:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=143
05/22/2022 00:09:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=144
05/22/2022 00:09:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=146
05/22/2022 00:09:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=148
05/22/2022 00:09:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=149
05/22/2022 00:09:31 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.26115942028985506 on epoch=149
05/22/2022 00:09:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=151
05/22/2022 00:09:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=153
05/22/2022 00:09:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=154
05/22/2022 00:09:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=156
05/22/2022 00:09:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=158
05/22/2022 00:09:48 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.2630751406818934 on epoch=158
05/22/2022 00:09:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=159
05/22/2022 00:09:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=161
05/22/2022 00:09:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=163
05/22/2022 00:09:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=164
05/22/2022 00:10:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=166
05/22/2022 00:10:04 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=166
05/22/2022 00:10:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=168
05/22/2022 00:10:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=169
05/22/2022 00:10:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=171
05/22/2022 00:10:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=173
05/22/2022 00:10:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=174
05/22/2022 00:10:20 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.3408369408369409 on epoch=174
05/22/2022 00:10:20 - INFO - __main__ - Saving model with best Classification-F1: 0.28154487601492206 -> 0.3408369408369409 on epoch=174, global_step=1050
05/22/2022 00:10:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=176
05/22/2022 00:10:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=178
05/22/2022 00:10:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=179
05/22/2022 00:10:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=181
05/22/2022 00:10:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=183
05/22/2022 00:10:36 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.29614310039841957 on epoch=183
05/22/2022 00:10:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=184
05/22/2022 00:10:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=186
05/22/2022 00:10:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=188
05/22/2022 00:10:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=189
05/22/2022 00:10:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=191
05/22/2022 00:10:52 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=191
05/22/2022 00:10:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=193
05/22/2022 00:10:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=194
05/22/2022 00:11:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=196
05/22/2022 00:11:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=198
05/22/2022 00:11:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=199
05/22/2022 00:11:08 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.2601083127398917 on epoch=199
05/22/2022 00:11:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=201
05/22/2022 00:11:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=203
05/22/2022 00:11:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=204
05/22/2022 00:11:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=206
05/22/2022 00:11:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=208
05/22/2022 00:11:24 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.18496047067475638 on epoch=208
05/22/2022 00:11:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=209
05/22/2022 00:11:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=211
05/22/2022 00:11:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=213
05/22/2022 00:11:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=214
05/22/2022 00:11:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=216
05/22/2022 00:11:40 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.1679790026246719 on epoch=216
05/22/2022 00:11:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=218
05/22/2022 00:11:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=219
05/22/2022 00:11:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=221
05/22/2022 00:11:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=223
05/22/2022 00:11:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=224
05/22/2022 00:11:56 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.23147804576376005 on epoch=224
05/22/2022 00:11:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=226
05/22/2022 00:12:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=228
05/22/2022 00:12:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.44 on epoch=229
05/22/2022 00:12:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=231
05/22/2022 00:12:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=233
05/22/2022 00:12:13 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.1983273596176822 on epoch=233
05/22/2022 00:12:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=234
05/22/2022 00:12:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=236
05/22/2022 00:12:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=238
05/22/2022 00:12:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=239
05/22/2022 00:12:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=241
05/22/2022 00:12:29 - INFO - __main__ - Global step 1450 Train loss 0.40 Classification-F1 0.20201185062698088 on epoch=241
05/22/2022 00:12:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=243
05/22/2022 00:12:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=244
05/22/2022 00:12:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=246
05/22/2022 00:12:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=248
05/22/2022 00:12:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=249
05/22/2022 00:12:45 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.31287647537647534 on epoch=249
05/22/2022 00:12:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=251
05/22/2022 00:12:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=253
05/22/2022 00:12:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=254
05/22/2022 00:12:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=256
05/22/2022 00:12:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=258
05/22/2022 00:13:01 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.224400871459695 on epoch=258
05/22/2022 00:13:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=259
05/22/2022 00:13:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=261
05/22/2022 00:13:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=263
05/22/2022 00:13:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=264
05/22/2022 00:13:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=266
05/22/2022 00:13:17 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.25377929563976076 on epoch=266
05/22/2022 00:13:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=268
05/22/2022 00:13:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=269
05/22/2022 00:13:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=271
05/22/2022 00:13:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=273
05/22/2022 00:13:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=274
05/22/2022 00:13:34 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.2692307692307692 on epoch=274
05/22/2022 00:13:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=276
05/22/2022 00:13:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=278
05/22/2022 00:13:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=279
05/22/2022 00:13:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=281
05/22/2022 00:13:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=283
05/22/2022 00:13:50 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.22431077694235588 on epoch=283
05/22/2022 00:13:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=284
05/22/2022 00:13:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=286
05/22/2022 00:13:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=288
05/22/2022 00:14:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=289
05/22/2022 00:14:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=291
05/22/2022 00:14:06 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.16 on epoch=291
05/22/2022 00:14:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=293
05/22/2022 00:14:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=294
05/22/2022 00:14:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.36 on epoch=296
05/22/2022 00:14:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=298
05/22/2022 00:14:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=299
05/22/2022 00:14:22 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.2623269734380846 on epoch=299
05/22/2022 00:14:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=301
05/22/2022 00:14:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=303
05/22/2022 00:14:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=304
05/22/2022 00:14:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=306
05/22/2022 00:14:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.33 on epoch=308
05/22/2022 00:14:39 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.3060954272473637 on epoch=308
05/22/2022 00:14:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=309
05/22/2022 00:14:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=311
05/22/2022 00:14:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=313
05/22/2022 00:14:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=314
05/22/2022 00:14:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=316
05/22/2022 00:14:55 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.21087621326951203 on epoch=316
05/22/2022 00:14:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=318
05/22/2022 00:15:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=319
05/22/2022 00:15:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=321
05/22/2022 00:15:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.33 on epoch=323
05/22/2022 00:15:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=324
05/22/2022 00:15:11 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.2550057059860981 on epoch=324
05/22/2022 00:15:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.33 on epoch=326
05/22/2022 00:15:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=328
05/22/2022 00:15:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=329
05/22/2022 00:15:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=331
05/22/2022 00:15:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=333
05/22/2022 00:15:27 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.31230206237211 on epoch=333
05/22/2022 00:15:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=334
05/22/2022 00:15:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.32 on epoch=336
05/22/2022 00:15:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.33 on epoch=338
05/22/2022 00:15:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.33 on epoch=339
05/22/2022 00:15:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.32 on epoch=341
05/22/2022 00:15:44 - INFO - __main__ - Global step 2050 Train loss 0.32 Classification-F1 0.22130753927419908 on epoch=341
05/22/2022 00:15:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.33 on epoch=343
05/22/2022 00:15:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.36 on epoch=344
05/22/2022 00:15:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.29 on epoch=346
05/22/2022 00:15:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=348
05/22/2022 00:15:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.33 on epoch=349
05/22/2022 00:16:00 - INFO - __main__ - Global step 2100 Train loss 0.33 Classification-F1 0.2744708994708995 on epoch=349
05/22/2022 00:16:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.37 on epoch=351
05/22/2022 00:16:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.30 on epoch=353
05/22/2022 00:16:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.30 on epoch=354
05/22/2022 00:16:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.31 on epoch=356
05/22/2022 00:16:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.28 on epoch=358
05/22/2022 00:16:16 - INFO - __main__ - Global step 2150 Train loss 0.31 Classification-F1 0.2778831184383857 on epoch=358
05/22/2022 00:16:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.33 on epoch=359
05/22/2022 00:16:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.30 on epoch=361
05/22/2022 00:16:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=363
05/22/2022 00:16:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.30 on epoch=364
05/22/2022 00:16:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.31 on epoch=366
05/22/2022 00:16:32 - INFO - __main__ - Global step 2200 Train loss 0.30 Classification-F1 0.20286604361370716 on epoch=366
05/22/2022 00:16:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.29 on epoch=368
05/22/2022 00:16:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.29 on epoch=369
05/22/2022 00:16:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.26 on epoch=371
05/22/2022 00:16:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.29 on epoch=373
05/22/2022 00:16:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.31 on epoch=374
05/22/2022 00:16:49 - INFO - __main__ - Global step 2250 Train loss 0.29 Classification-F1 0.31527418109127253 on epoch=374
05/22/2022 00:16:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.26 on epoch=376
05/22/2022 00:16:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.24 on epoch=378
05/22/2022 00:16:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.27 on epoch=379
05/22/2022 00:16:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.29 on epoch=381
05/22/2022 00:17:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.24 on epoch=383
05/22/2022 00:17:04 - INFO - __main__ - Global step 2300 Train loss 0.26 Classification-F1 0.2006734619585002 on epoch=383
05/22/2022 00:17:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.30 on epoch=384
05/22/2022 00:17:09 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.27 on epoch=386
05/22/2022 00:17:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.26 on epoch=388
05/22/2022 00:17:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=389
05/22/2022 00:17:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.28 on epoch=391
05/22/2022 00:17:20 - INFO - __main__ - Global step 2350 Train loss 0.27 Classification-F1 0.3080209388720027 on epoch=391
05/22/2022 00:17:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.25 on epoch=393
05/22/2022 00:17:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.28 on epoch=394
05/22/2022 00:17:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=396
05/22/2022 00:17:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.23 on epoch=398
05/22/2022 00:17:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.24 on epoch=399
05/22/2022 00:17:36 - INFO - __main__ - Global step 2400 Train loss 0.24 Classification-F1 0.22096837290481777 on epoch=399
05/22/2022 00:17:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.28 on epoch=401
05/22/2022 00:17:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.22 on epoch=403
05/22/2022 00:17:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.25 on epoch=404
05/22/2022 00:17:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=406
05/22/2022 00:17:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.24 on epoch=408
05/22/2022 00:17:53 - INFO - __main__ - Global step 2450 Train loss 0.23 Classification-F1 0.25539906103286386 on epoch=408
05/22/2022 00:17:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.24 on epoch=409
05/22/2022 00:17:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=411
05/22/2022 00:18:01 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=413
05/22/2022 00:18:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.20 on epoch=414
05/22/2022 00:18:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.24 on epoch=416
05/22/2022 00:18:09 - INFO - __main__ - Global step 2500 Train loss 0.21 Classification-F1 0.30491218918448965 on epoch=416
05/22/2022 00:18:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=418
05/22/2022 00:18:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=419
05/22/2022 00:18:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.26 on epoch=421
05/22/2022 00:18:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=423
05/22/2022 00:18:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.25 on epoch=424
05/22/2022 00:18:25 - INFO - __main__ - Global step 2550 Train loss 0.21 Classification-F1 0.25906905096778515 on epoch=424
05/22/2022 00:18:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=426
05/22/2022 00:18:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.17 on epoch=428
05/22/2022 00:18:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.21 on epoch=429
05/22/2022 00:18:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=431
05/22/2022 00:18:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=433
05/22/2022 00:18:41 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.26672677945724477 on epoch=433
05/22/2022 00:18:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=434
05/22/2022 00:18:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=436
05/22/2022 00:18:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=438
05/22/2022 00:18:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.19 on epoch=439
05/22/2022 00:18:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.29 on epoch=441
05/22/2022 00:18:57 - INFO - __main__ - Global step 2650 Train loss 0.20 Classification-F1 0.2948717948717949 on epoch=441
05/22/2022 00:19:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.21 on epoch=443
05/22/2022 00:19:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.19 on epoch=444
05/22/2022 00:19:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=446
05/22/2022 00:19:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.17 on epoch=448
05/22/2022 00:19:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.23 on epoch=449
05/22/2022 00:19:13 - INFO - __main__ - Global step 2700 Train loss 0.20 Classification-F1 0.2964759751927201 on epoch=449
05/22/2022 00:19:16 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=451
05/22/2022 00:19:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=453
05/22/2022 00:19:21 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=454
05/22/2022 00:19:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.16 on epoch=456
05/22/2022 00:19:26 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=458
05/22/2022 00:19:29 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.3327144962584785 on epoch=458
05/22/2022 00:19:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=459
05/22/2022 00:19:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=461
05/22/2022 00:19:37 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.16 on epoch=463
05/22/2022 00:19:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=464
05/22/2022 00:19:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=466
05/22/2022 00:19:46 - INFO - __main__ - Global step 2800 Train loss 0.16 Classification-F1 0.21924019607843137 on epoch=466
05/22/2022 00:19:48 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=468
05/22/2022 00:19:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=469
05/22/2022 00:19:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=471
05/22/2022 00:19:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=473
05/22/2022 00:19:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=474
05/22/2022 00:20:02 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.2315319089512638 on epoch=474
05/22/2022 00:20:04 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=476
05/22/2022 00:20:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=478
05/22/2022 00:20:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=479
05/22/2022 00:20:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=481
05/22/2022 00:20:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=483
05/22/2022 00:20:18 - INFO - __main__ - Global step 2900 Train loss 0.13 Classification-F1 0.2872139219965307 on epoch=483
05/22/2022 00:20:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=484
05/22/2022 00:20:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=486
05/22/2022 00:20:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=488
05/22/2022 00:20:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=489
05/22/2022 00:20:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=491
05/22/2022 00:20:35 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.12390115863697078 on epoch=491
05/22/2022 00:20:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=493
05/22/2022 00:20:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=494
05/22/2022 00:20:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.10 on epoch=496
05/22/2022 00:20:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=498
05/22/2022 00:20:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=499
05/22/2022 00:20:49 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:20:49 - INFO - __main__ - Printing 3 examples
05/22/2022 00:20:49 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/22/2022 00:20:49 - INFO - __main__ - ['entailment']
05/22/2022 00:20:49 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/22/2022 00:20:49 - INFO - __main__ - ['entailment']
05/22/2022 00:20:49 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/22/2022 00:20:49 - INFO - __main__ - ['entailment']
05/22/2022 00:20:49 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:20:49 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:20:50 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 00:20:50 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:20:50 - INFO - __main__ - Printing 3 examples
05/22/2022 00:20:50 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/22/2022 00:20:50 - INFO - __main__ - ['entailment']
05/22/2022 00:20:50 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/22/2022 00:20:50 - INFO - __main__ - ['entailment']
05/22/2022 00:20:50 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/22/2022 00:20:50 - INFO - __main__ - ['entailment']
05/22/2022 00:20:50 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:20:50 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:20:50 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 00:20:51 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.20996873977681654 on epoch=499
05/22/2022 00:20:51 - INFO - __main__ - save last model!
05/22/2022 00:20:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 00:20:51 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 00:20:51 - INFO - __main__ - Printing 3 examples
05/22/2022 00:20:51 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 00:20:51 - INFO - __main__ - ['contradiction']
05/22/2022 00:20:51 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 00:20:51 - INFO - __main__ - ['entailment']
05/22/2022 00:20:51 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 00:20:51 - INFO - __main__ - ['contradiction']
05/22/2022 00:20:51 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:20:52 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:20:53 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 00:21:08 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 00:21:08 - INFO - __main__ - task name: anli
05/22/2022 00:21:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 00:21:09 - INFO - __main__ - Starting training!
05/22/2022 00:21:24 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_21_0.4_8_predictions.txt
05/22/2022 00:21:24 - INFO - __main__ - Classification-F1 on test data: 0.1505
05/22/2022 00:21:24 - INFO - __main__ - prefix=anli_32_21, lr=0.4, bsz=8, dev_performance=0.3408369408369409, test_performance=0.1504947690139613
05/22/2022 00:21:24 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.3, bsz=8 ...
05/22/2022 00:21:25 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:21:25 - INFO - __main__ - Printing 3 examples
05/22/2022 00:21:25 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/22/2022 00:21:25 - INFO - __main__ - ['entailment']
05/22/2022 00:21:25 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/22/2022 00:21:25 - INFO - __main__ - ['entailment']
05/22/2022 00:21:25 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/22/2022 00:21:25 - INFO - __main__ - ['entailment']
05/22/2022 00:21:25 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:21:25 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:21:25 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 00:21:25 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:21:25 - INFO - __main__ - Printing 3 examples
05/22/2022 00:21:25 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/22/2022 00:21:25 - INFO - __main__ - ['entailment']
05/22/2022 00:21:25 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/22/2022 00:21:25 - INFO - __main__ - ['entailment']
05/22/2022 00:21:25 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/22/2022 00:21:25 - INFO - __main__ - ['entailment']
05/22/2022 00:21:25 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:21:25 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:21:25 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 00:21:44 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 00:21:44 - INFO - __main__ - task name: anli
05/22/2022 00:21:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 00:21:45 - INFO - __main__ - Starting training!
05/22/2022 00:21:48 - INFO - __main__ - Step 10 Global step 10 Train loss 6.45 on epoch=1
05/22/2022 00:21:50 - INFO - __main__ - Step 20 Global step 20 Train loss 3.66 on epoch=3
05/22/2022 00:21:53 - INFO - __main__ - Step 30 Global step 30 Train loss 1.70 on epoch=4
05/22/2022 00:21:56 - INFO - __main__ - Step 40 Global step 40 Train loss 1.18 on epoch=6
05/22/2022 00:21:58 - INFO - __main__ - Step 50 Global step 50 Train loss 0.85 on epoch=8
05/22/2022 00:22:00 - INFO - __main__ - Global step 50 Train loss 2.77 Classification-F1 0.16666666666666666 on epoch=8
05/22/2022 00:22:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/22/2022 00:22:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.81 on epoch=9
05/22/2022 00:22:06 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=11
05/22/2022 00:22:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=13
05/22/2022 00:22:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.63 on epoch=14
05/22/2022 00:22:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=16
05/22/2022 00:22:15 - INFO - __main__ - Global step 100 Train loss 0.68 Classification-F1 0.16666666666666666 on epoch=16
05/22/2022 00:22:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=18
05/22/2022 00:22:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=19
05/22/2022 00:22:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.68 on epoch=21
05/22/2022 00:22:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=23
05/22/2022 00:22:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=24
05/22/2022 00:22:30 - INFO - __main__ - Global step 150 Train loss 0.57 Classification-F1 0.25615705636865926 on epoch=24
05/22/2022 00:22:30 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.25615705636865926 on epoch=24, global_step=150
05/22/2022 00:22:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=26
05/22/2022 00:22:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=28
05/22/2022 00:22:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=29
05/22/2022 00:22:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=31
05/22/2022 00:22:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=33
05/22/2022 00:22:45 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=33
05/22/2022 00:22:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=34
05/22/2022 00:22:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=36
05/22/2022 00:22:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=38
05/22/2022 00:22:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.53 on epoch=39
05/22/2022 00:22:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=41
05/22/2022 00:23:01 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=41
05/22/2022 00:23:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=43
05/22/2022 00:23:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=44
05/22/2022 00:23:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=46
05/22/2022 00:23:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=48
05/22/2022 00:23:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=49
05/22/2022 00:23:16 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.2779016671976213 on epoch=49
05/22/2022 00:23:16 - INFO - __main__ - Saving model with best Classification-F1: 0.25615705636865926 -> 0.2779016671976213 on epoch=49, global_step=300
05/22/2022 00:23:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=51
05/22/2022 00:23:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=53
05/22/2022 00:23:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=54
05/22/2022 00:23:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=56
05/22/2022 00:23:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=58
05/22/2022 00:23:32 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=58
05/22/2022 00:23:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=59
05/22/2022 00:23:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=61
05/22/2022 00:23:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=63
05/22/2022 00:23:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=64
05/22/2022 00:23:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=66
05/22/2022 00:23:47 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=66
05/22/2022 00:23:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=68
05/22/2022 00:23:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=69
05/22/2022 00:23:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=71
05/22/2022 00:23:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=73
05/22/2022 00:24:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=74
05/22/2022 00:24:03 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.23956799214531177 on epoch=74
05/22/2022 00:24:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=76
05/22/2022 00:24:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=78
05/22/2022 00:24:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.52 on epoch=79
05/22/2022 00:24:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=81
05/22/2022 00:24:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=83
05/22/2022 00:24:19 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.17687363141908596 on epoch=83
05/22/2022 00:24:21 - INFO - __main__ - Step 510 Global step 510 Train loss 1.20 on epoch=84
05/22/2022 00:24:24 - INFO - __main__ - Step 520 Global step 520 Train loss 3.25 on epoch=86
05/22/2022 00:24:27 - INFO - __main__ - Step 530 Global step 530 Train loss 2.46 on epoch=88
05/22/2022 00:24:29 - INFO - __main__ - Step 540 Global step 540 Train loss 1.10 on epoch=89
05/22/2022 00:24:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=91
05/22/2022 00:24:35 - INFO - __main__ - Global step 550 Train loss 1.70 Classification-F1 0.3393188960532963 on epoch=91
05/22/2022 00:24:35 - INFO - __main__ - Saving model with best Classification-F1: 0.2779016671976213 -> 0.3393188960532963 on epoch=91, global_step=550
05/22/2022 00:24:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=93
05/22/2022 00:24:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=94
05/22/2022 00:24:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=96
05/22/2022 00:24:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=98
05/22/2022 00:24:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=99
05/22/2022 00:24:50 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.24885052136514121 on epoch=99
05/22/2022 00:24:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=101
05/22/2022 00:24:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=103
05/22/2022 00:24:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=104
05/22/2022 00:25:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=106
05/22/2022 00:25:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=108
05/22/2022 00:25:06 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.2761734280721622 on epoch=108
05/22/2022 00:25:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=109
05/22/2022 00:25:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=111
05/22/2022 00:25:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=113
05/22/2022 00:25:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=114
05/22/2022 00:25:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=116
05/22/2022 00:25:22 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.26890668630519504 on epoch=116
05/22/2022 00:25:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=118
05/22/2022 00:25:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=119
05/22/2022 00:25:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=121
05/22/2022 00:25:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=123
05/22/2022 00:25:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=124
05/22/2022 00:25:37 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.2919359760326223 on epoch=124
05/22/2022 00:25:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=126
05/22/2022 00:25:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=128
05/22/2022 00:25:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=129
05/22/2022 00:25:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=131
05/22/2022 00:25:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=133
05/22/2022 00:25:53 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.32742240629953473 on epoch=133
05/22/2022 00:25:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=134
05/22/2022 00:25:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=136
05/22/2022 00:26:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=138
05/22/2022 00:26:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=139
05/22/2022 00:26:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=141
05/22/2022 00:26:09 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.24788359788359793 on epoch=141
05/22/2022 00:26:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=143
05/22/2022 00:26:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=144
05/22/2022 00:26:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=146
05/22/2022 00:26:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=148
05/22/2022 00:26:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=149
05/22/2022 00:26:25 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.23778880921738063 on epoch=149
05/22/2022 00:26:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=151
05/22/2022 00:26:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=153
05/22/2022 00:26:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=154
05/22/2022 00:26:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=156
05/22/2022 00:26:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=158
05/22/2022 00:26:41 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.26755651755651755 on epoch=158
05/22/2022 00:26:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=159
05/22/2022 00:26:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=161
05/22/2022 00:26:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=163
05/22/2022 00:26:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=164
05/22/2022 00:26:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=166
05/22/2022 00:26:57 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.24775599128540304 on epoch=166
05/22/2022 00:27:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=168
05/22/2022 00:27:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=169
05/22/2022 00:27:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=171
05/22/2022 00:27:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=173
05/22/2022 00:27:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=174
05/22/2022 00:27:13 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.2403087766403759 on epoch=174
05/22/2022 00:27:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=176
05/22/2022 00:27:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=178
05/22/2022 00:27:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.49 on epoch=179
05/22/2022 00:27:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=181
05/22/2022 00:27:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=183
05/22/2022 00:27:30 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.20498693225965955 on epoch=183
05/22/2022 00:27:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=184
05/22/2022 00:27:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=186
05/22/2022 00:27:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=188
05/22/2022 00:27:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=189
05/22/2022 00:27:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=191
05/22/2022 00:27:46 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.24895572263993318 on epoch=191
05/22/2022 00:27:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=193
05/22/2022 00:27:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=194
05/22/2022 00:27:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=196
05/22/2022 00:27:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=198
05/22/2022 00:27:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.47 on epoch=199
05/22/2022 00:28:02 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.19841269841269837 on epoch=199
05/22/2022 00:28:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=201
05/22/2022 00:28:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=203
05/22/2022 00:28:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.48 on epoch=204
05/22/2022 00:28:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=206
05/22/2022 00:28:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=208
05/22/2022 00:28:18 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.21794871794871795 on epoch=208
05/22/2022 00:28:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=209
05/22/2022 00:28:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=211
05/22/2022 00:28:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=213
05/22/2022 00:28:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.43 on epoch=214
05/22/2022 00:28:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=216
05/22/2022 00:28:33 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.30530849695139 on epoch=216
05/22/2022 00:28:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=218
05/22/2022 00:28:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=219
05/22/2022 00:28:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=221
05/22/2022 00:28:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=223
05/22/2022 00:28:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=224
05/22/2022 00:28:49 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.22293504410585405 on epoch=224
05/22/2022 00:28:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.45 on epoch=226
05/22/2022 00:28:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=228
05/22/2022 00:28:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=229
05/22/2022 00:29:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.45 on epoch=231
05/22/2022 00:29:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=233
05/22/2022 00:29:06 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.18971428571428572 on epoch=233
05/22/2022 00:29:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=234
05/22/2022 00:29:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=236
05/22/2022 00:29:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=238
05/22/2022 00:29:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.52 on epoch=239
05/22/2022 00:29:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=241
05/22/2022 00:29:21 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.22777777777777777 on epoch=241
05/22/2022 00:29:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=243
05/22/2022 00:29:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=244
05/22/2022 00:29:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=246
05/22/2022 00:29:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=248
05/22/2022 00:29:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=249
05/22/2022 00:29:37 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.2359620734684608 on epoch=249
05/22/2022 00:29:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=251
05/22/2022 00:29:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=253
05/22/2022 00:29:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=254
05/22/2022 00:29:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=256
05/22/2022 00:29:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=258
05/22/2022 00:29:54 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.23909782492231865 on epoch=258
05/22/2022 00:29:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=259
05/22/2022 00:29:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.45 on epoch=261
05/22/2022 00:30:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.47 on epoch=263
05/22/2022 00:30:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=264
05/22/2022 00:30:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=266
05/22/2022 00:30:10 - INFO - __main__ - Global step 1600 Train loss 0.45 Classification-F1 0.23410986482599946 on epoch=266
05/22/2022 00:30:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=268
05/22/2022 00:30:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=269
05/22/2022 00:30:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=271
05/22/2022 00:30:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=273
05/22/2022 00:30:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=274
05/22/2022 00:30:26 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.2281464309861876 on epoch=274
05/22/2022 00:30:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=276
05/22/2022 00:30:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=278
05/22/2022 00:30:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.45 on epoch=279
05/22/2022 00:30:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=281
05/22/2022 00:30:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=283
05/22/2022 00:30:42 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.21246498599439775 on epoch=283
05/22/2022 00:30:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=284
05/22/2022 00:30:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=286
05/22/2022 00:30:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=288
05/22/2022 00:30:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=289
05/22/2022 00:30:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.40 on epoch=291
05/22/2022 00:30:59 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.25777777777777783 on epoch=291
05/22/2022 00:31:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=293
05/22/2022 00:31:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.51 on epoch=294
05/22/2022 00:31:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=296
05/22/2022 00:31:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=298
05/22/2022 00:31:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=299
05/22/2022 00:31:15 - INFO - __main__ - Global step 1800 Train loss 0.44 Classification-F1 0.22207573427085622 on epoch=299
05/22/2022 00:31:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=301
05/22/2022 00:31:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=303
05/22/2022 00:31:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=304
05/22/2022 00:31:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=306
05/22/2022 00:31:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.45 on epoch=308
05/22/2022 00:31:31 - INFO - __main__ - Global step 1850 Train loss 0.42 Classification-F1 0.28264208909370203 on epoch=308
05/22/2022 00:31:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=309
05/22/2022 00:31:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=311
05/22/2022 00:31:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=313
05/22/2022 00:31:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=314
05/22/2022 00:31:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.40 on epoch=316
05/22/2022 00:31:47 - INFO - __main__ - Global step 1900 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=316
05/22/2022 00:31:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.42 on epoch=318
05/22/2022 00:31:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=319
05/22/2022 00:31:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=321
05/22/2022 00:31:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=323
05/22/2022 00:32:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=324
05/22/2022 00:32:03 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.25 on epoch=324
05/22/2022 00:32:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=326
05/22/2022 00:32:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=328
05/22/2022 00:32:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=329
05/22/2022 00:32:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=331
05/22/2022 00:32:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=333
05/22/2022 00:32:19 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.2263630089717046 on epoch=333
05/22/2022 00:32:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.42 on epoch=334
05/22/2022 00:32:24 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.41 on epoch=336
05/22/2022 00:32:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.44 on epoch=338
05/22/2022 00:32:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=339
05/22/2022 00:32:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.43 on epoch=341
05/22/2022 00:32:35 - INFO - __main__ - Global step 2050 Train loss 0.43 Classification-F1 0.1836290071584189 on epoch=341
05/22/2022 00:32:38 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.42 on epoch=343
05/22/2022 00:32:41 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.44 on epoch=344
05/22/2022 00:32:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.39 on epoch=346
05/22/2022 00:32:46 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.42 on epoch=348
05/22/2022 00:32:49 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.42 on epoch=349
05/22/2022 00:32:51 - INFO - __main__ - Global step 2100 Train loss 0.42 Classification-F1 0.346262372677467 on epoch=349
05/22/2022 00:32:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3393188960532963 -> 0.346262372677467 on epoch=349, global_step=2100
05/22/2022 00:32:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.40 on epoch=351
05/22/2022 00:32:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.44 on epoch=353
05/22/2022 00:32:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.43 on epoch=354
05/22/2022 00:33:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.47 on epoch=356
05/22/2022 00:33:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.40 on epoch=358
05/22/2022 00:33:08 - INFO - __main__ - Global step 2150 Train loss 0.43 Classification-F1 0.21603443637341938 on epoch=358
05/22/2022 00:33:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.49 on epoch=359
05/22/2022 00:33:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.41 on epoch=361
05/22/2022 00:33:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.41 on epoch=363
05/22/2022 00:33:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.41 on epoch=364
05/22/2022 00:33:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=366
05/22/2022 00:33:24 - INFO - __main__ - Global step 2200 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=366
05/22/2022 00:33:26 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.37 on epoch=368
05/22/2022 00:33:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.45 on epoch=369
05/22/2022 00:33:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=371
05/22/2022 00:33:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.41 on epoch=373
05/22/2022 00:33:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.44 on epoch=374
05/22/2022 00:33:40 - INFO - __main__ - Global step 2250 Train loss 0.41 Classification-F1 0.33418693982074266 on epoch=374
05/22/2022 00:33:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.41 on epoch=376
05/22/2022 00:33:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.46 on epoch=378
05/22/2022 00:33:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.40 on epoch=379
05/22/2022 00:33:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.42 on epoch=381
05/22/2022 00:33:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.48 on epoch=383
05/22/2022 00:33:56 - INFO - __main__ - Global step 2300 Train loss 0.43 Classification-F1 0.21057792486363913 on epoch=383
05/22/2022 00:33:59 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.36 on epoch=384
05/22/2022 00:34:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.37 on epoch=386
05/22/2022 00:34:04 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.37 on epoch=388
05/22/2022 00:34:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.44 on epoch=389
05/22/2022 00:34:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=391
05/22/2022 00:34:12 - INFO - __main__ - Global step 2350 Train loss 0.39 Classification-F1 0.2354646947948383 on epoch=391
05/22/2022 00:34:15 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=393
05/22/2022 00:34:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.43 on epoch=394
05/22/2022 00:34:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=396
05/22/2022 00:34:23 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.38 on epoch=398
05/22/2022 00:34:25 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.41 on epoch=399
05/22/2022 00:34:28 - INFO - __main__ - Global step 2400 Train loss 0.40 Classification-F1 0.3245848877185283 on epoch=399
05/22/2022 00:34:31 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.41 on epoch=401
05/22/2022 00:34:33 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=403
05/22/2022 00:34:36 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.45 on epoch=404
05/22/2022 00:34:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.41 on epoch=406
05/22/2022 00:34:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.43 on epoch=408
05/22/2022 00:34:44 - INFO - __main__ - Global step 2450 Train loss 0.41 Classification-F1 0.30587064676616915 on epoch=408
05/22/2022 00:34:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=409
05/22/2022 00:34:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.44 on epoch=411
05/22/2022 00:34:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.41 on epoch=413
05/22/2022 00:34:55 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=414
05/22/2022 00:34:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.39 on epoch=416
05/22/2022 00:35:00 - INFO - __main__ - Global step 2500 Train loss 0.41 Classification-F1 0.1983273596176822 on epoch=416
05/22/2022 00:35:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.42 on epoch=418
05/22/2022 00:35:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.45 on epoch=419
05/22/2022 00:35:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.45 on epoch=421
05/22/2022 00:35:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.41 on epoch=423
05/22/2022 00:35:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.42 on epoch=424
05/22/2022 00:35:16 - INFO - __main__ - Global step 2550 Train loss 0.43 Classification-F1 0.32183027982863915 on epoch=424
05/22/2022 00:35:19 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.37 on epoch=426
05/22/2022 00:35:22 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.40 on epoch=428
05/22/2022 00:35:24 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.39 on epoch=429
05/22/2022 00:35:27 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.40 on epoch=431
05/22/2022 00:35:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.37 on epoch=433
05/22/2022 00:35:33 - INFO - __main__ - Global step 2600 Train loss 0.39 Classification-F1 0.2255821653239157 on epoch=433
05/22/2022 00:35:35 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.44 on epoch=434
05/22/2022 00:35:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.36 on epoch=436
05/22/2022 00:35:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.40 on epoch=438
05/22/2022 00:35:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.37 on epoch=439
05/22/2022 00:35:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.39 on epoch=441
05/22/2022 00:35:48 - INFO - __main__ - Global step 2650 Train loss 0.39 Classification-F1 0.1679790026246719 on epoch=441
05/22/2022 00:35:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.40 on epoch=443
05/22/2022 00:35:54 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.35 on epoch=444
05/22/2022 00:35:56 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.39 on epoch=446
05/22/2022 00:35:59 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.37 on epoch=448
05/22/2022 00:36:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.40 on epoch=449
05/22/2022 00:36:04 - INFO - __main__ - Global step 2700 Train loss 0.38 Classification-F1 0.2867760259064607 on epoch=449
05/22/2022 00:36:07 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.39 on epoch=451
05/22/2022 00:36:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.39 on epoch=453
05/22/2022 00:36:12 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.35 on epoch=454
05/22/2022 00:36:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.36 on epoch=456
05/22/2022 00:36:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.36 on epoch=458
05/22/2022 00:36:20 - INFO - __main__ - Global step 2750 Train loss 0.37 Classification-F1 0.2802636291997994 on epoch=458
05/22/2022 00:36:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.39 on epoch=459
05/22/2022 00:36:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.35 on epoch=461
05/22/2022 00:36:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.36 on epoch=463
05/22/2022 00:36:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.35 on epoch=464
05/22/2022 00:36:34 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.38 on epoch=466
05/22/2022 00:36:36 - INFO - __main__ - Global step 2800 Train loss 0.37 Classification-F1 0.21317829457364337 on epoch=466
05/22/2022 00:36:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.37 on epoch=468
05/22/2022 00:36:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.34 on epoch=469
05/22/2022 00:36:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.36 on epoch=471
05/22/2022 00:36:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.36 on epoch=473
05/22/2022 00:36:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.39 on epoch=474
05/22/2022 00:36:52 - INFO - __main__ - Global step 2850 Train loss 0.37 Classification-F1 0.31271644794767517 on epoch=474
05/22/2022 00:36:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.39 on epoch=476
05/22/2022 00:36:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.35 on epoch=478
05/22/2022 00:37:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.39 on epoch=479
05/22/2022 00:37:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.35 on epoch=481
05/22/2022 00:37:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.35 on epoch=483
05/22/2022 00:37:08 - INFO - __main__ - Global step 2900 Train loss 0.37 Classification-F1 0.29132122016069445 on epoch=483
05/22/2022 00:37:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.38 on epoch=484
05/22/2022 00:37:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.37 on epoch=486
05/22/2022 00:37:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.36 on epoch=488
05/22/2022 00:37:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.39 on epoch=489
05/22/2022 00:37:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.35 on epoch=491
05/22/2022 00:37:24 - INFO - __main__ - Global step 2950 Train loss 0.37 Classification-F1 0.30952380952380953 on epoch=491
05/22/2022 00:37:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.33 on epoch=493
05/22/2022 00:37:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.38 on epoch=494
05/22/2022 00:37:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.41 on epoch=496
05/22/2022 00:37:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.40 on epoch=498
05/22/2022 00:37:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.37 on epoch=499
05/22/2022 00:37:39 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:37:39 - INFO - __main__ - Printing 3 examples
05/22/2022 00:37:39 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/22/2022 00:37:39 - INFO - __main__ - ['entailment']
05/22/2022 00:37:39 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/22/2022 00:37:39 - INFO - __main__ - ['entailment']
05/22/2022 00:37:39 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/22/2022 00:37:39 - INFO - __main__ - ['entailment']
05/22/2022 00:37:39 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:37:39 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:37:39 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 00:37:39 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:37:39 - INFO - __main__ - Printing 3 examples
05/22/2022 00:37:39 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/22/2022 00:37:39 - INFO - __main__ - ['entailment']
05/22/2022 00:37:39 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/22/2022 00:37:39 - INFO - __main__ - ['entailment']
05/22/2022 00:37:39 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/22/2022 00:37:39 - INFO - __main__ - ['entailment']
05/22/2022 00:37:39 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:37:39 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:37:39 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 00:37:41 - INFO - __main__ - Global step 3000 Train loss 0.38 Classification-F1 0.31233092052350664 on epoch=499
05/22/2022 00:37:41 - INFO - __main__ - save last model!
05/22/2022 00:37:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 00:37:41 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 00:37:41 - INFO - __main__ - Printing 3 examples
05/22/2022 00:37:41 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 00:37:41 - INFO - __main__ - ['contradiction']
05/22/2022 00:37:41 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 00:37:41 - INFO - __main__ - ['entailment']
05/22/2022 00:37:41 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 00:37:41 - INFO - __main__ - ['contradiction']
05/22/2022 00:37:41 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:37:41 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:37:42 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 00:37:58 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 00:37:58 - INFO - __main__ - task name: anli
05/22/2022 00:37:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 00:37:59 - INFO - __main__ - Starting training!
05/22/2022 00:38:12 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_21_0.3_8_predictions.txt
05/22/2022 00:38:12 - INFO - __main__ - Classification-F1 on test data: 0.2711
05/22/2022 00:38:13 - INFO - __main__ - prefix=anli_32_21, lr=0.3, bsz=8, dev_performance=0.346262372677467, test_performance=0.2710753739999631
05/22/2022 00:38:13 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.2, bsz=8 ...
05/22/2022 00:38:14 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:38:14 - INFO - __main__ - Printing 3 examples
05/22/2022 00:38:14 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/22/2022 00:38:14 - INFO - __main__ - ['entailment']
05/22/2022 00:38:14 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/22/2022 00:38:14 - INFO - __main__ - ['entailment']
05/22/2022 00:38:14 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/22/2022 00:38:14 - INFO - __main__ - ['entailment']
05/22/2022 00:38:14 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:38:14 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:38:14 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 00:38:14 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:38:14 - INFO - __main__ - Printing 3 examples
05/22/2022 00:38:14 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/22/2022 00:38:14 - INFO - __main__ - ['entailment']
05/22/2022 00:38:14 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/22/2022 00:38:14 - INFO - __main__ - ['entailment']
05/22/2022 00:38:14 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/22/2022 00:38:14 - INFO - __main__ - ['entailment']
05/22/2022 00:38:14 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:38:14 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:38:14 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 00:38:33 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 00:38:33 - INFO - __main__ - task name: anli
05/22/2022 00:38:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 00:38:34 - INFO - __main__ - Starting training!
05/22/2022 00:38:37 - INFO - __main__ - Step 10 Global step 10 Train loss 6.77 on epoch=1
05/22/2022 00:38:39 - INFO - __main__ - Step 20 Global step 20 Train loss 3.97 on epoch=3
05/22/2022 00:38:42 - INFO - __main__ - Step 30 Global step 30 Train loss 2.35 on epoch=4
05/22/2022 00:38:45 - INFO - __main__ - Step 40 Global step 40 Train loss 1.42 on epoch=6
05/22/2022 00:38:47 - INFO - __main__ - Step 50 Global step 50 Train loss 1.13 on epoch=8
05/22/2022 00:38:50 - INFO - __main__ - Global step 50 Train loss 3.13 Classification-F1 0.16666666666666666 on epoch=8
05/22/2022 00:38:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/22/2022 00:38:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=9
05/22/2022 00:38:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.86 on epoch=11
05/22/2022 00:38:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.70 on epoch=13
05/22/2022 00:39:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.77 on epoch=14
05/22/2022 00:39:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.62 on epoch=16
05/22/2022 00:39:05 - INFO - __main__ - Global step 100 Train loss 0.79 Classification-F1 0.16666666666666666 on epoch=16
05/22/2022 00:39:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.67 on epoch=18
05/22/2022 00:39:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=19
05/22/2022 00:39:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=21
05/22/2022 00:39:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=23
05/22/2022 00:39:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.63 on epoch=24
05/22/2022 00:39:20 - INFO - __main__ - Global step 150 Train loss 0.60 Classification-F1 0.1679790026246719 on epoch=24
05/22/2022 00:39:20 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1679790026246719 on epoch=24, global_step=150
05/22/2022 00:39:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=26
05/22/2022 00:39:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=28
05/22/2022 00:39:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.59 on epoch=29
05/22/2022 00:39:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=31
05/22/2022 00:39:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=33
05/22/2022 00:39:36 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=33
05/22/2022 00:39:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=34
05/22/2022 00:39:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.64 on epoch=36
05/22/2022 00:39:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.58 on epoch=38
05/22/2022 00:39:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.53 on epoch=39
05/22/2022 00:39:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=41
05/22/2022 00:39:50 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=41
05/22/2022 00:39:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=43
05/22/2022 00:39:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=44
05/22/2022 00:39:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=46
05/22/2022 00:40:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=48
05/22/2022 00:40:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=49
05/22/2022 00:40:06 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=49
05/22/2022 00:40:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.53 on epoch=51
05/22/2022 00:40:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=53
05/22/2022 00:40:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=54
05/22/2022 00:40:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=56
05/22/2022 00:40:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=58
05/22/2022 00:40:21 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.20908004778972522 on epoch=58
05/22/2022 00:40:21 - INFO - __main__ - Saving model with best Classification-F1: 0.1679790026246719 -> 0.20908004778972522 on epoch=58, global_step=350
05/22/2022 00:40:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=59
05/22/2022 00:40:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=61
05/22/2022 00:40:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=63
05/22/2022 00:40:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=64
05/22/2022 00:40:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.53 on epoch=66
05/22/2022 00:40:37 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=66
05/22/2022 00:40:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=68
05/22/2022 00:40:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=69
05/22/2022 00:40:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=71
05/22/2022 00:40:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=73
05/22/2022 00:40:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=74
05/22/2022 00:40:53 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.2979594756514119 on epoch=74
05/22/2022 00:40:53 - INFO - __main__ - Saving model with best Classification-F1: 0.20908004778972522 -> 0.2979594756514119 on epoch=74, global_step=450
05/22/2022 00:40:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=76
05/22/2022 00:40:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=78
05/22/2022 00:41:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.51 on epoch=79
05/22/2022 00:41:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=81
05/22/2022 00:41:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=83
05/22/2022 00:41:09 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.21106166560712014 on epoch=83
05/22/2022 00:41:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.49 on epoch=84
05/22/2022 00:41:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=86
05/22/2022 00:41:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=88
05/22/2022 00:41:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=89
05/22/2022 00:41:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=91
05/22/2022 00:41:24 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=91
05/22/2022 00:41:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=93
05/22/2022 00:41:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=94
05/22/2022 00:41:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=96
05/22/2022 00:41:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=98
05/22/2022 00:41:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=99
05/22/2022 00:41:39 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.15873015873015875 on epoch=99
05/22/2022 00:41:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=101
05/22/2022 00:41:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=103
05/22/2022 00:41:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=104
05/22/2022 00:41:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=106
05/22/2022 00:41:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=108
05/22/2022 00:41:54 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=108
05/22/2022 00:41:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=109
05/22/2022 00:42:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=111
05/22/2022 00:42:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=113
05/22/2022 00:42:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.93 on epoch=114
05/22/2022 00:42:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=116
05/22/2022 00:42:10 - INFO - __main__ - Global step 700 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=116
05/22/2022 00:42:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=118
05/22/2022 00:42:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.50 on epoch=119
05/22/2022 00:42:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=121
05/22/2022 00:42:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=123
05/22/2022 00:42:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.51 on epoch=124
05/22/2022 00:42:25 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=124
05/22/2022 00:42:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=126
05/22/2022 00:42:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=128
05/22/2022 00:42:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=129
05/22/2022 00:42:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=131
05/22/2022 00:42:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=133
05/22/2022 00:42:40 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=133
05/22/2022 00:42:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=134
05/22/2022 00:42:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=136
05/22/2022 00:42:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=138
05/22/2022 00:42:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=139
05/22/2022 00:42:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=141
05/22/2022 00:42:56 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=141
05/22/2022 00:42:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=143
05/22/2022 00:43:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=144
05/22/2022 00:43:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=146
05/22/2022 00:43:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=148
05/22/2022 00:43:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=149
05/22/2022 00:43:12 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=149
05/22/2022 00:43:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=151
05/22/2022 00:43:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=153
05/22/2022 00:43:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=154
05/22/2022 00:43:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=156
05/22/2022 00:43:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=158
05/22/2022 00:43:27 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=158
05/22/2022 00:43:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=159
05/22/2022 00:43:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=161
05/22/2022 00:43:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=163
05/22/2022 00:43:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=164
05/22/2022 00:43:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=166
05/22/2022 00:43:43 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=166
05/22/2022 00:43:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=168
05/22/2022 00:43:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=169
05/22/2022 00:43:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=171
05/22/2022 00:43:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=173
05/22/2022 00:43:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=174
05/22/2022 00:43:58 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.22190339550454327 on epoch=174
05/22/2022 00:44:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=176
05/22/2022 00:44:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=178
05/22/2022 00:44:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=179
05/22/2022 00:44:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=181
05/22/2022 00:44:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=183
05/22/2022 00:44:14 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.16272965879265092 on epoch=183
05/22/2022 00:44:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.48 on epoch=184
05/22/2022 00:44:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=186
05/22/2022 00:44:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=188
05/22/2022 00:44:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=189
05/22/2022 00:44:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=191
05/22/2022 00:44:29 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=191
05/22/2022 00:44:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=193
05/22/2022 00:44:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=194
05/22/2022 00:44:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=196
05/22/2022 00:44:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=198
05/22/2022 00:44:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=199
05/22/2022 00:44:45 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=199
05/22/2022 00:44:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=201
05/22/2022 00:44:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=203
05/22/2022 00:44:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=204
05/22/2022 00:44:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=206
05/22/2022 00:44:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=208
05/22/2022 00:45:00 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=208
05/22/2022 00:45:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=209
05/22/2022 00:45:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=211
05/22/2022 00:45:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=213
05/22/2022 00:45:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=214
05/22/2022 00:45:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=216
05/22/2022 00:45:16 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=216
05/22/2022 00:45:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=218
05/22/2022 00:45:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=219
05/22/2022 00:45:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=221
05/22/2022 00:45:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=223
05/22/2022 00:45:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=224
05/22/2022 00:45:32 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=224
05/22/2022 00:45:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=226
05/22/2022 00:45:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=228
05/22/2022 00:45:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=229
05/22/2022 00:45:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=231
05/22/2022 00:45:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=233
05/22/2022 00:45:47 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=233
05/22/2022 00:45:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=234
05/22/2022 00:45:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=236
05/22/2022 00:45:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=238
05/22/2022 00:45:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=239
05/22/2022 00:46:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=241
05/22/2022 00:46:03 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.21216227313788294 on epoch=241
05/22/2022 00:46:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=243
05/22/2022 00:46:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=244
05/22/2022 00:46:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=246
05/22/2022 00:46:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=248
05/22/2022 00:46:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.47 on epoch=249
05/22/2022 00:46:19 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=249
05/22/2022 00:46:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=251
05/22/2022 00:46:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=253
05/22/2022 00:46:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=254
05/22/2022 00:46:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=256
05/22/2022 00:46:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=258
05/22/2022 00:46:34 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=258
05/22/2022 00:46:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=259
05/22/2022 00:46:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=261
05/22/2022 00:46:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=263
05/22/2022 00:46:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=264
05/22/2022 00:46:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=266
05/22/2022 00:46:50 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=266
05/22/2022 00:46:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=268
05/22/2022 00:46:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=269
05/22/2022 00:46:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=271
05/22/2022 00:47:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=273
05/22/2022 00:47:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=274
05/22/2022 00:47:05 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=274
05/22/2022 00:47:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=276
05/22/2022 00:47:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=278
05/22/2022 00:47:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=279
05/22/2022 00:47:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=281
05/22/2022 00:47:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=283
05/22/2022 00:47:21 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.21129265000232741 on epoch=283
05/22/2022 00:47:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=284
05/22/2022 00:47:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=286
05/22/2022 00:47:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=288
05/22/2022 00:47:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=289
05/22/2022 00:47:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=291
05/22/2022 00:47:36 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=291
05/22/2022 00:47:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=293
05/22/2022 00:47:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=294
05/22/2022 00:47:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=296
05/22/2022 00:47:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=298
05/22/2022 00:47:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=299
05/22/2022 00:47:52 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.21657130748039835 on epoch=299
05/22/2022 00:47:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=301
05/22/2022 00:47:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=303
05/22/2022 00:47:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=304
05/22/2022 00:48:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=306
05/22/2022 00:48:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=308
05/22/2022 00:48:07 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.2664999739542637 on epoch=308
05/22/2022 00:48:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=309
05/22/2022 00:48:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=311
05/22/2022 00:48:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=313
05/22/2022 00:48:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=314
05/22/2022 00:48:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=316
05/22/2022 00:48:22 - INFO - __main__ - Global step 1900 Train loss 0.38 Classification-F1 0.206848357791754 on epoch=316
05/22/2022 00:48:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=318
05/22/2022 00:48:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.33 on epoch=319
05/22/2022 00:48:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.33 on epoch=321
05/22/2022 00:48:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.38 on epoch=323
05/22/2022 00:48:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=324
05/22/2022 00:48:38 - INFO - __main__ - Global step 1950 Train loss 0.36 Classification-F1 0.2822222222222222 on epoch=324
05/22/2022 00:48:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=326
05/22/2022 00:48:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.36 on epoch=328
05/22/2022 00:48:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=329
05/22/2022 00:48:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=331
05/22/2022 00:48:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=333
05/22/2022 00:48:53 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.2675424192665572 on epoch=333
05/22/2022 00:48:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=334
05/22/2022 00:48:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.30 on epoch=336
05/22/2022 00:49:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=338
05/22/2022 00:49:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.37 on epoch=339
05/22/2022 00:49:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.36 on epoch=341
05/22/2022 00:49:09 - INFO - __main__ - Global step 2050 Train loss 0.35 Classification-F1 0.16666666666666666 on epoch=341
05/22/2022 00:49:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.29 on epoch=343
05/22/2022 00:49:14 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.35 on epoch=344
05/22/2022 00:49:16 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.36 on epoch=346
05/22/2022 00:49:19 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=348
05/22/2022 00:49:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.34 on epoch=349
05/22/2022 00:49:24 - INFO - __main__ - Global step 2100 Train loss 0.34 Classification-F1 0.23848499438292406 on epoch=349
05/22/2022 00:49:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=351
05/22/2022 00:49:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.37 on epoch=353
05/22/2022 00:49:32 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.35 on epoch=354
05/22/2022 00:49:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.32 on epoch=356
05/22/2022 00:49:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=358
05/22/2022 00:49:39 - INFO - __main__ - Global step 2150 Train loss 0.36 Classification-F1 0.2659995830727538 on epoch=358
05/22/2022 00:49:42 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.35 on epoch=359
05/22/2022 00:49:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.29 on epoch=361
05/22/2022 00:49:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.36 on epoch=363
05/22/2022 00:49:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.29 on epoch=364
05/22/2022 00:49:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.30 on epoch=366
05/22/2022 00:49:54 - INFO - __main__ - Global step 2200 Train loss 0.32 Classification-F1 0.27272240945808596 on epoch=366
05/22/2022 00:49:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=368
05/22/2022 00:50:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=369
05/22/2022 00:50:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.33 on epoch=371
05/22/2022 00:50:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.32 on epoch=373
05/22/2022 00:50:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.34 on epoch=374
05/22/2022 00:50:10 - INFO - __main__ - Global step 2250 Train loss 0.34 Classification-F1 0.24979114452798665 on epoch=374
05/22/2022 00:50:12 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.32 on epoch=376
05/22/2022 00:50:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=378
05/22/2022 00:50:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.26 on epoch=379
05/22/2022 00:50:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.31 on epoch=381
05/22/2022 00:50:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.30 on epoch=383
05/22/2022 00:50:25 - INFO - __main__ - Global step 2300 Train loss 0.29 Classification-F1 0.2786676485930836 on epoch=383
05/22/2022 00:50:28 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.26 on epoch=384
05/22/2022 00:50:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=386
05/22/2022 00:50:33 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.31 on epoch=388
05/22/2022 00:50:36 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.30 on epoch=389
05/22/2022 00:50:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.30 on epoch=391
05/22/2022 00:50:41 - INFO - __main__ - Global step 2350 Train loss 0.28 Classification-F1 0.26206997511345337 on epoch=391
05/22/2022 00:50:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.27 on epoch=393
05/22/2022 00:50:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.30 on epoch=394
05/22/2022 00:50:49 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.31 on epoch=396
05/22/2022 00:50:51 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.26 on epoch=398
05/22/2022 00:50:54 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.27 on epoch=399
05/22/2022 00:50:57 - INFO - __main__ - Global step 2400 Train loss 0.28 Classification-F1 0.2822222222222222 on epoch=399
05/22/2022 00:50:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.33 on epoch=401
05/22/2022 00:51:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.27 on epoch=403
05/22/2022 00:51:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.29 on epoch=404
05/22/2022 00:51:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.25 on epoch=406
05/22/2022 00:51:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.25 on epoch=408
05/22/2022 00:51:13 - INFO - __main__ - Global step 2450 Train loss 0.28 Classification-F1 0.2508714596949891 on epoch=408
05/22/2022 00:51:16 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.30 on epoch=409
05/22/2022 00:51:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.28 on epoch=411
05/22/2022 00:51:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.25 on epoch=413
05/22/2022 00:51:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.24 on epoch=414
05/22/2022 00:51:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.25 on epoch=416
05/22/2022 00:51:29 - INFO - __main__ - Global step 2500 Train loss 0.27 Classification-F1 0.22199062011464302 on epoch=416
05/22/2022 00:51:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.22 on epoch=418
05/22/2022 00:51:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.27 on epoch=419
05/22/2022 00:51:37 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.27 on epoch=421
05/22/2022 00:51:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.25 on epoch=423
05/22/2022 00:51:42 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.25 on epoch=424
05/22/2022 00:51:44 - INFO - __main__ - Global step 2550 Train loss 0.25 Classification-F1 0.2541239254785415 on epoch=424
05/22/2022 00:51:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.29 on epoch=426
05/22/2022 00:51:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.26 on epoch=428
05/22/2022 00:51:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.28 on epoch=429
05/22/2022 00:51:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.24 on epoch=431
05/22/2022 00:51:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.22 on epoch=433
05/22/2022 00:52:00 - INFO - __main__ - Global step 2600 Train loss 0.25 Classification-F1 0.26824894514767933 on epoch=433
05/22/2022 00:52:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.25 on epoch=434
05/22/2022 00:52:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.28 on epoch=436
05/22/2022 00:52:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.22 on epoch=438
05/22/2022 00:52:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.24 on epoch=439
05/22/2022 00:52:13 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.27 on epoch=441
05/22/2022 00:52:15 - INFO - __main__ - Global step 2650 Train loss 0.25 Classification-F1 0.24547433243085415 on epoch=441
05/22/2022 00:52:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.21 on epoch=443
05/22/2022 00:52:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.26 on epoch=444
05/22/2022 00:52:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.26 on epoch=446
05/22/2022 00:52:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.26 on epoch=448
05/22/2022 00:52:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.23 on epoch=449
05/22/2022 00:52:31 - INFO - __main__ - Global step 2700 Train loss 0.24 Classification-F1 0.2736373780850108 on epoch=449
05/22/2022 00:52:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=451
05/22/2022 00:52:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.21 on epoch=453
05/22/2022 00:52:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.22 on epoch=454
05/22/2022 00:52:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.26 on epoch=456
05/22/2022 00:52:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.22 on epoch=458
05/22/2022 00:52:46 - INFO - __main__ - Global step 2750 Train loss 0.22 Classification-F1 0.2708245113381816 on epoch=458
05/22/2022 00:52:49 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=459
05/22/2022 00:52:51 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.20 on epoch=461
05/22/2022 00:52:54 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.19 on epoch=463
05/22/2022 00:52:57 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=464
05/22/2022 00:52:59 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=466
05/22/2022 00:53:02 - INFO - __main__ - Global step 2800 Train loss 0.19 Classification-F1 0.23597883597883595 on epoch=466
05/22/2022 00:53:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.17 on epoch=468
05/22/2022 00:53:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.23 on epoch=469
05/22/2022 00:53:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=471
05/22/2022 00:53:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.17 on epoch=473
05/22/2022 00:53:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.20 on epoch=474
05/22/2022 00:53:17 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.2806784919798619 on epoch=474
05/22/2022 00:53:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=476
05/22/2022 00:53:22 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=478
05/22/2022 00:53:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.25 on epoch=479
05/22/2022 00:53:27 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=481
05/22/2022 00:53:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=483
05/22/2022 00:53:32 - INFO - __main__ - Global step 2900 Train loss 0.20 Classification-F1 0.18309971328448552 on epoch=483
05/22/2022 00:53:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.13 on epoch=484
05/22/2022 00:53:37 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.23 on epoch=486
05/22/2022 00:53:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.21 on epoch=488
05/22/2022 00:53:43 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=489
05/22/2022 00:53:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.23 on epoch=491
05/22/2022 00:53:48 - INFO - __main__ - Global step 2950 Train loss 0.19 Classification-F1 0.22645502645502644 on epoch=491
05/22/2022 00:53:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=493
05/22/2022 00:53:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=494
05/22/2022 00:53:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.18 on epoch=496
05/22/2022 00:53:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.22 on epoch=498
05/22/2022 00:54:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=499
05/22/2022 00:54:02 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:54:02 - INFO - __main__ - Printing 3 examples
05/22/2022 00:54:02 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/22/2022 00:54:02 - INFO - __main__ - ['neutral']
05/22/2022 00:54:02 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/22/2022 00:54:02 - INFO - __main__ - ['neutral']
05/22/2022 00:54:02 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/22/2022 00:54:02 - INFO - __main__ - ['neutral']
05/22/2022 00:54:02 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:54:02 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:54:02 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 00:54:02 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:54:02 - INFO - __main__ - Printing 3 examples
05/22/2022 00:54:02 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/22/2022 00:54:02 - INFO - __main__ - ['neutral']
05/22/2022 00:54:02 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/22/2022 00:54:02 - INFO - __main__ - ['neutral']
05/22/2022 00:54:02 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/22/2022 00:54:02 - INFO - __main__ - ['neutral']
05/22/2022 00:54:02 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:54:02 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:54:02 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 00:54:03 - INFO - __main__ - Global step 3000 Train loss 0.18 Classification-F1 0.2328857245854362 on epoch=499
05/22/2022 00:54:03 - INFO - __main__ - save last model!
05/22/2022 00:54:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 00:54:03 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 00:54:03 - INFO - __main__ - Printing 3 examples
05/22/2022 00:54:03 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 00:54:03 - INFO - __main__ - ['contradiction']
05/22/2022 00:54:03 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 00:54:03 - INFO - __main__ - ['entailment']
05/22/2022 00:54:03 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 00:54:03 - INFO - __main__ - ['contradiction']
05/22/2022 00:54:03 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:54:04 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:54:05 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 00:54:21 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 00:54:21 - INFO - __main__ - task name: anli
05/22/2022 00:54:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 00:54:21 - INFO - __main__ - Starting training!
05/22/2022 00:54:34 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_21_0.2_8_predictions.txt
05/22/2022 00:54:34 - INFO - __main__ - Classification-F1 on test data: 0.2243
05/22/2022 00:54:34 - INFO - __main__ - prefix=anli_32_21, lr=0.2, bsz=8, dev_performance=0.2979594756514119, test_performance=0.2242947867139329
05/22/2022 00:54:34 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.5, bsz=8 ...
05/22/2022 00:54:35 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:54:35 - INFO - __main__ - Printing 3 examples
05/22/2022 00:54:35 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/22/2022 00:54:35 - INFO - __main__ - ['neutral']
05/22/2022 00:54:35 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/22/2022 00:54:35 - INFO - __main__ - ['neutral']
05/22/2022 00:54:35 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/22/2022 00:54:35 - INFO - __main__ - ['neutral']
05/22/2022 00:54:35 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:54:35 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:54:35 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 00:54:35 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 00:54:35 - INFO - __main__ - Printing 3 examples
05/22/2022 00:54:35 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/22/2022 00:54:35 - INFO - __main__ - ['neutral']
05/22/2022 00:54:35 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/22/2022 00:54:35 - INFO - __main__ - ['neutral']
05/22/2022 00:54:35 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/22/2022 00:54:35 - INFO - __main__ - ['neutral']
05/22/2022 00:54:35 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:54:36 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:54:36 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 00:54:54 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 00:54:54 - INFO - __main__ - task name: anli
05/22/2022 00:54:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 00:54:55 - INFO - __main__ - Starting training!
05/22/2022 00:54:59 - INFO - __main__ - Step 10 Global step 10 Train loss 6.54 on epoch=1
05/22/2022 00:55:01 - INFO - __main__ - Step 20 Global step 20 Train loss 2.21 on epoch=3
05/22/2022 00:55:04 - INFO - __main__ - Step 30 Global step 30 Train loss 1.15 on epoch=4
05/22/2022 00:55:06 - INFO - __main__ - Step 40 Global step 40 Train loss 0.87 on epoch=6
05/22/2022 00:55:09 - INFO - __main__ - Step 50 Global step 50 Train loss 0.82 on epoch=8
05/22/2022 00:55:11 - INFO - __main__ - Global step 50 Train loss 2.32 Classification-F1 0.16666666666666666 on epoch=8
05/22/2022 00:55:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/22/2022 00:55:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.68 on epoch=9
05/22/2022 00:55:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=11
05/22/2022 00:55:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=13
05/22/2022 00:55:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.64 on epoch=14
05/22/2022 00:55:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=16
05/22/2022 00:55:26 - INFO - __main__ - Global step 100 Train loss 0.64 Classification-F1 0.16666666666666666 on epoch=16
05/22/2022 00:55:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.64 on epoch=18
05/22/2022 00:55:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=19
05/22/2022 00:55:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=21
05/22/2022 00:55:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.67 on epoch=23
05/22/2022 00:55:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=24
05/22/2022 00:55:42 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.1679790026246719 on epoch=24
05/22/2022 00:55:42 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1679790026246719 on epoch=24, global_step=150
05/22/2022 00:55:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=26
05/22/2022 00:55:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=28
05/22/2022 00:55:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.61 on epoch=29
05/22/2022 00:55:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=31
05/22/2022 00:55:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.58 on epoch=33
05/22/2022 00:55:57 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.1842231842231842 on epoch=33
05/22/2022 00:55:57 - INFO - __main__ - Saving model with best Classification-F1: 0.1679790026246719 -> 0.1842231842231842 on epoch=33, global_step=200
05/22/2022 00:56:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=34
05/22/2022 00:56:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=36
05/22/2022 00:56:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.61 on epoch=38
05/22/2022 00:56:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=39
05/22/2022 00:56:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.58 on epoch=41
05/22/2022 00:56:12 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=41
05/22/2022 00:56:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=43
05/22/2022 00:56:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=44
05/22/2022 00:56:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=46
05/22/2022 00:56:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.55 on epoch=48
05/22/2022 00:56:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=49
05/22/2022 00:56:28 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.17066666666666666 on epoch=49
05/22/2022 00:56:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.53 on epoch=51
05/22/2022 00:56:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=53
05/22/2022 00:56:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=54
05/22/2022 00:56:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=56
05/22/2022 00:56:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=58
05/22/2022 00:56:44 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.2816738816738817 on epoch=58
05/22/2022 00:56:44 - INFO - __main__ - Saving model with best Classification-F1: 0.1842231842231842 -> 0.2816738816738817 on epoch=58, global_step=350
05/22/2022 00:56:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=59
05/22/2022 00:56:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.58 on epoch=61
05/22/2022 00:56:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=63
05/22/2022 00:56:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=64
05/22/2022 00:56:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.58 on epoch=66
05/22/2022 00:57:00 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=66
05/22/2022 00:57:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=68
05/22/2022 00:57:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=69
05/22/2022 00:57:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=71
05/22/2022 00:57:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.52 on epoch=73
05/22/2022 00:57:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=74
05/22/2022 00:57:16 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.24552429667519182 on epoch=74
05/22/2022 00:57:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=76
05/22/2022 00:57:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=78
05/22/2022 00:57:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=79
05/22/2022 00:57:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=81
05/22/2022 00:57:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=83
05/22/2022 00:57:32 - INFO - __main__ - Global step 500 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=83
05/22/2022 00:57:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=84
05/22/2022 00:57:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=86
05/22/2022 00:57:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=88
05/22/2022 00:57:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=89
05/22/2022 00:57:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=91
05/22/2022 00:57:48 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=91
05/22/2022 00:57:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=93
05/22/2022 00:57:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=94
05/22/2022 00:57:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=96
05/22/2022 00:57:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=98
05/22/2022 00:58:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=99
05/22/2022 00:58:04 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.24 on epoch=99
05/22/2022 00:58:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=101
05/22/2022 00:58:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=103
05/22/2022 00:58:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=104
05/22/2022 00:58:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=106
05/22/2022 00:58:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=108
05/22/2022 00:58:20 - INFO - __main__ - Global step 650 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=108
05/22/2022 00:58:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=109
05/22/2022 00:58:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=111
05/22/2022 00:58:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=113
05/22/2022 00:58:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=114
05/22/2022 00:58:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=116
05/22/2022 00:58:35 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=116
05/22/2022 00:58:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=118
05/22/2022 00:58:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=119
05/22/2022 00:58:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=121
05/22/2022 00:58:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=123
05/22/2022 00:58:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=124
05/22/2022 00:58:51 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.24937343358395989 on epoch=124
05/22/2022 00:58:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=126
05/22/2022 00:58:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=128
05/22/2022 00:58:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=129
05/22/2022 00:59:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.50 on epoch=131
05/22/2022 00:59:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=133
05/22/2022 00:59:07 - INFO - __main__ - Global step 800 Train loss 0.49 Classification-F1 0.23544374195059123 on epoch=133
05/22/2022 00:59:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=134
05/22/2022 00:59:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=136
05/22/2022 00:59:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=138
05/22/2022 00:59:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=139
05/22/2022 00:59:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.51 on epoch=141
05/22/2022 00:59:23 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=141
05/22/2022 00:59:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=143
05/22/2022 00:59:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=144
05/22/2022 00:59:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=146
05/22/2022 00:59:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.52 on epoch=148
05/22/2022 00:59:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=149
05/22/2022 00:59:38 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.1881810228266921 on epoch=149
05/22/2022 00:59:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=151
05/22/2022 00:59:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=153
05/22/2022 00:59:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=154
05/22/2022 00:59:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=156
05/22/2022 00:59:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=158
05/22/2022 00:59:54 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.16272965879265092 on epoch=158
05/22/2022 00:59:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=159
05/22/2022 00:59:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=161
05/22/2022 01:00:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.50 on epoch=163
05/22/2022 01:00:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=164
05/22/2022 01:00:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=166
05/22/2022 01:00:10 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.1881810228266921 on epoch=166
05/22/2022 01:00:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=168
05/22/2022 01:00:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=169
05/22/2022 01:00:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=171
05/22/2022 01:00:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.50 on epoch=173
05/22/2022 01:00:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=174
05/22/2022 01:00:26 - INFO - __main__ - Global step 1050 Train loss 0.47 Classification-F1 0.21179832821224434 on epoch=174
05/22/2022 01:00:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=176
05/22/2022 01:00:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=178
05/22/2022 01:00:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=179
05/22/2022 01:00:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=181
05/22/2022 01:00:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=183
05/22/2022 01:00:41 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.31560592850915437 on epoch=183
05/22/2022 01:00:41 - INFO - __main__ - Saving model with best Classification-F1: 0.2816738816738817 -> 0.31560592850915437 on epoch=183, global_step=1100
05/22/2022 01:00:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.48 on epoch=184
05/22/2022 01:00:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=186
05/22/2022 01:00:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=188
05/22/2022 01:00:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=189
05/22/2022 01:00:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=191
05/22/2022 01:00:57 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=191
05/22/2022 01:01:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=193
05/22/2022 01:01:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=194
05/22/2022 01:01:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=196
05/22/2022 01:01:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=198
05/22/2022 01:01:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.69 on epoch=199
05/22/2022 01:01:15 - INFO - __main__ - Global step 1200 Train loss 0.51 Classification-F1 0.265993265993266 on epoch=199
05/22/2022 01:01:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.86 on epoch=201
05/22/2022 01:01:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=203
05/22/2022 01:01:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=204
05/22/2022 01:01:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.50 on epoch=206
05/22/2022 01:01:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.51 on epoch=208
05/22/2022 01:01:31 - INFO - __main__ - Global step 1250 Train loss 0.76 Classification-F1 0.2608003108003108 on epoch=208
05/22/2022 01:01:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=209
05/22/2022 01:01:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=211
05/22/2022 01:01:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=213
05/22/2022 01:01:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=214
05/22/2022 01:01:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=216
05/22/2022 01:01:47 - INFO - __main__ - Global step 1300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=216
05/22/2022 01:01:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=218
05/22/2022 01:01:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.48 on epoch=219
05/22/2022 01:01:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=221
05/22/2022 01:01:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=223
05/22/2022 01:02:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=224
05/22/2022 01:02:03 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.17904761904761904 on epoch=224
05/22/2022 01:02:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=226
05/22/2022 01:02:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=228
05/22/2022 01:02:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.46 on epoch=229
05/22/2022 01:02:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=231
05/22/2022 01:02:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.49 on epoch=233
05/22/2022 01:02:19 - INFO - __main__ - Global step 1400 Train loss 0.48 Classification-F1 0.1881810228266921 on epoch=233
05/22/2022 01:02:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=234
05/22/2022 01:02:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=236
05/22/2022 01:02:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=238
05/22/2022 01:02:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.46 on epoch=239
05/22/2022 01:02:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=241
05/22/2022 01:02:34 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=241
05/22/2022 01:02:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.48 on epoch=243
05/22/2022 01:02:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=244
05/22/2022 01:02:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.47 on epoch=246
05/22/2022 01:02:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.48 on epoch=248
05/22/2022 01:02:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=249
05/22/2022 01:02:50 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.2021544644495464 on epoch=249
05/22/2022 01:02:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.45 on epoch=251
05/22/2022 01:02:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=253
05/22/2022 01:02:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=254
05/22/2022 01:03:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=256
05/22/2022 01:03:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.44 on epoch=258
05/22/2022 01:03:06 - INFO - __main__ - Global step 1550 Train loss 0.44 Classification-F1 0.22407242210988873 on epoch=258
05/22/2022 01:03:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=259
05/22/2022 01:03:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=261
05/22/2022 01:03:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=263
05/22/2022 01:03:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=264
05/22/2022 01:03:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.50 on epoch=266
05/22/2022 01:03:22 - INFO - __main__ - Global step 1600 Train loss 0.46 Classification-F1 0.2184039709812906 on epoch=266
05/22/2022 01:03:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=268
05/22/2022 01:03:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=269
05/22/2022 01:03:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=271
05/22/2022 01:03:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.48 on epoch=273
05/22/2022 01:03:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=274
05/22/2022 01:03:39 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.1922495175507224 on epoch=274
05/22/2022 01:03:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=276
05/22/2022 01:03:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.49 on epoch=278
05/22/2022 01:03:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=279
05/22/2022 01:03:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.49 on epoch=281
05/22/2022 01:03:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=283
05/22/2022 01:03:55 - INFO - __main__ - Global step 1700 Train loss 0.46 Classification-F1 0.20478568456096544 on epoch=283
05/22/2022 01:03:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=284
05/22/2022 01:04:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.46 on epoch=286
05/22/2022 01:04:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=288
05/22/2022 01:04:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=289
05/22/2022 01:04:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=291
05/22/2022 01:04:11 - INFO - __main__ - Global step 1750 Train loss 0.44 Classification-F1 0.20212343155623688 on epoch=291
05/22/2022 01:04:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.52 on epoch=293
05/22/2022 01:04:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=294
05/22/2022 01:04:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=296
05/22/2022 01:04:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.49 on epoch=298
05/22/2022 01:04:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=299
05/22/2022 01:04:27 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.24468934091172154 on epoch=299
05/22/2022 01:04:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=301
05/22/2022 01:04:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=303
05/22/2022 01:04:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.45 on epoch=304
05/22/2022 01:04:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.47 on epoch=306
05/22/2022 01:04:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.47 on epoch=308
05/22/2022 01:04:43 - INFO - __main__ - Global step 1850 Train loss 0.45 Classification-F1 0.24045715847314533 on epoch=308
05/22/2022 01:04:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=309
05/22/2022 01:04:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=311
05/22/2022 01:04:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.49 on epoch=313
05/22/2022 01:04:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.47 on epoch=314
05/22/2022 01:04:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=316
05/22/2022 01:04:59 - INFO - __main__ - Global step 1900 Train loss 0.46 Classification-F1 0.22962497381102032 on epoch=316
05/22/2022 01:05:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.48 on epoch=318
05/22/2022 01:05:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=319
05/22/2022 01:05:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.51 on epoch=321
05/22/2022 01:05:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.49 on epoch=323
05/22/2022 01:05:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=324
05/22/2022 01:05:15 - INFO - __main__ - Global step 1950 Train loss 0.46 Classification-F1 0.2372092589483894 on epoch=324
05/22/2022 01:05:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=326
05/22/2022 01:05:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=328
05/22/2022 01:05:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=329
05/22/2022 01:05:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.47 on epoch=331
05/22/2022 01:05:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.47 on epoch=333
05/22/2022 01:05:31 - INFO - __main__ - Global step 2000 Train loss 0.46 Classification-F1 0.23426363940439823 on epoch=333
05/22/2022 01:05:34 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.45 on epoch=334
05/22/2022 01:05:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=336
05/22/2022 01:05:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.47 on epoch=338
05/22/2022 01:05:42 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.40 on epoch=339
05/22/2022 01:05:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.43 on epoch=341
05/22/2022 01:05:47 - INFO - __main__ - Global step 2050 Train loss 0.44 Classification-F1 0.23748516967206132 on epoch=341
05/22/2022 01:05:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.44 on epoch=343
05/22/2022 01:05:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.45 on epoch=344
05/22/2022 01:05:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.47 on epoch=346
05/22/2022 01:05:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.46 on epoch=348
05/22/2022 01:06:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.46 on epoch=349
05/22/2022 01:06:03 - INFO - __main__ - Global step 2100 Train loss 0.46 Classification-F1 0.22200094831673778 on epoch=349
05/22/2022 01:06:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.46 on epoch=351
05/22/2022 01:06:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.47 on epoch=353
05/22/2022 01:06:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.43 on epoch=354
05/22/2022 01:06:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.49 on epoch=356
05/22/2022 01:06:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.46 on epoch=358
05/22/2022 01:06:19 - INFO - __main__ - Global step 2150 Train loss 0.46 Classification-F1 0.2600819442924706 on epoch=358
05/22/2022 01:06:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.41 on epoch=359
05/22/2022 01:06:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.47 on epoch=361
05/22/2022 01:06:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.44 on epoch=363
05/22/2022 01:06:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.43 on epoch=364
05/22/2022 01:06:32 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=366
05/22/2022 01:06:35 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.24184447605500237 on epoch=366
05/22/2022 01:06:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.45 on epoch=368
05/22/2022 01:06:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.47 on epoch=369
05/22/2022 01:06:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.48 on epoch=371
05/22/2022 01:06:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.46 on epoch=373
05/22/2022 01:06:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.46 on epoch=374
05/22/2022 01:06:51 - INFO - __main__ - Global step 2250 Train loss 0.46 Classification-F1 0.21355111917725347 on epoch=374
05/22/2022 01:06:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.47 on epoch=376
05/22/2022 01:06:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.49 on epoch=378
05/22/2022 01:06:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.41 on epoch=379
05/22/2022 01:07:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.46 on epoch=381
05/22/2022 01:07:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.47 on epoch=383
05/22/2022 01:07:07 - INFO - __main__ - Global step 2300 Train loss 0.46 Classification-F1 0.20367965367965368 on epoch=383
05/22/2022 01:07:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.45 on epoch=384
05/22/2022 01:07:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.47 on epoch=386
05/22/2022 01:07:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.50 on epoch=388
05/22/2022 01:07:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.46 on epoch=389
05/22/2022 01:07:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.50 on epoch=391
05/22/2022 01:07:23 - INFO - __main__ - Global step 2350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=391
05/22/2022 01:07:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.40 on epoch=393
05/22/2022 01:07:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.43 on epoch=394
05/22/2022 01:07:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=396
05/22/2022 01:07:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.42 on epoch=398
05/22/2022 01:07:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.41 on epoch=399
05/22/2022 01:07:39 - INFO - __main__ - Global step 2400 Train loss 0.42 Classification-F1 0.2180672268907563 on epoch=399
05/22/2022 01:07:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.48 on epoch=401
05/22/2022 01:07:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.47 on epoch=403
05/22/2022 01:07:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.43 on epoch=404
05/22/2022 01:07:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.44 on epoch=406
05/22/2022 01:07:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.44 on epoch=408
05/22/2022 01:07:55 - INFO - __main__ - Global step 2450 Train loss 0.46 Classification-F1 0.2449131817070188 on epoch=408
05/22/2022 01:07:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.40 on epoch=409
05/22/2022 01:08:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.47 on epoch=411
05/22/2022 01:08:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.47 on epoch=413
05/22/2022 01:08:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=414
05/22/2022 01:08:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.44 on epoch=416
05/22/2022 01:08:11 - INFO - __main__ - Global step 2500 Train loss 0.44 Classification-F1 0.21566640433812354 on epoch=416
05/22/2022 01:08:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.45 on epoch=418
05/22/2022 01:08:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.45 on epoch=419
05/22/2022 01:08:19 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.41 on epoch=421
05/22/2022 01:08:22 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.47 on epoch=423
05/22/2022 01:08:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.41 on epoch=424
05/22/2022 01:08:27 - INFO - __main__ - Global step 2550 Train loss 0.44 Classification-F1 0.23135110503531556 on epoch=424
05/22/2022 01:08:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.44 on epoch=426
05/22/2022 01:08:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.44 on epoch=428
05/22/2022 01:08:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.43 on epoch=429
05/22/2022 01:08:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.46 on epoch=431
05/22/2022 01:08:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.45 on epoch=433
05/22/2022 01:08:43 - INFO - __main__ - Global step 2600 Train loss 0.44 Classification-F1 0.20448662640207071 on epoch=433
05/22/2022 01:08:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.40 on epoch=434
05/22/2022 01:08:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.45 on epoch=436
05/22/2022 01:08:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.46 on epoch=438
05/22/2022 01:08:54 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.45 on epoch=439
05/22/2022 01:08:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.47 on epoch=441
05/22/2022 01:09:00 - INFO - __main__ - Global step 2650 Train loss 0.45 Classification-F1 0.21167502088554724 on epoch=441
05/22/2022 01:09:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.47 on epoch=443
05/22/2022 01:09:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.43 on epoch=444
05/22/2022 01:09:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.45 on epoch=446
05/22/2022 01:09:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.46 on epoch=448
05/22/2022 01:09:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.42 on epoch=449
05/22/2022 01:09:16 - INFO - __main__ - Global step 2700 Train loss 0.45 Classification-F1 0.3968537098971881 on epoch=449
05/22/2022 01:09:16 - INFO - __main__ - Saving model with best Classification-F1: 0.31560592850915437 -> 0.3968537098971881 on epoch=449, global_step=2700
05/22/2022 01:09:18 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.47 on epoch=451
05/22/2022 01:09:21 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.41 on epoch=453
05/22/2022 01:09:24 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.42 on epoch=454
05/22/2022 01:09:26 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.42 on epoch=456
05/22/2022 01:09:29 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.43 on epoch=458
05/22/2022 01:09:32 - INFO - __main__ - Global step 2750 Train loss 0.43 Classification-F1 0.3860102232983589 on epoch=458
05/22/2022 01:09:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.45 on epoch=459
05/22/2022 01:09:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.42 on epoch=461
05/22/2022 01:09:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.48 on epoch=463
05/22/2022 01:09:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.42 on epoch=464
05/22/2022 01:09:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.42 on epoch=466
05/22/2022 01:09:48 - INFO - __main__ - Global step 2800 Train loss 0.44 Classification-F1 0.18115558690413214 on epoch=466
05/22/2022 01:09:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.42 on epoch=468
05/22/2022 01:09:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.42 on epoch=469
05/22/2022 01:09:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.42 on epoch=471
05/22/2022 01:09:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.43 on epoch=473
05/22/2022 01:10:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.41 on epoch=474
05/22/2022 01:10:04 - INFO - __main__ - Global step 2850 Train loss 0.42 Classification-F1 0.22344877344877345 on epoch=474
05/22/2022 01:10:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.48 on epoch=476
05/22/2022 01:10:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.45 on epoch=478
05/22/2022 01:10:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.43 on epoch=479
05/22/2022 01:10:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.45 on epoch=481
05/22/2022 01:10:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.43 on epoch=483
05/22/2022 01:10:20 - INFO - __main__ - Global step 2900 Train loss 0.45 Classification-F1 0.28325244289134793 on epoch=483
05/22/2022 01:10:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.43 on epoch=484
05/22/2022 01:10:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.49 on epoch=486
05/22/2022 01:10:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.45 on epoch=488
05/22/2022 01:10:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.41 on epoch=489
05/22/2022 01:10:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.42 on epoch=491
05/22/2022 01:10:36 - INFO - __main__ - Global step 2950 Train loss 0.44 Classification-F1 0.2709675148699539 on epoch=491
05/22/2022 01:10:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.46 on epoch=493
05/22/2022 01:10:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.42 on epoch=494
05/22/2022 01:10:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.44 on epoch=496
05/22/2022 01:10:47 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.40 on epoch=498
05/22/2022 01:10:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.39 on epoch=499
05/22/2022 01:10:51 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:10:51 - INFO - __main__ - Printing 3 examples
05/22/2022 01:10:51 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/22/2022 01:10:51 - INFO - __main__ - ['neutral']
05/22/2022 01:10:51 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/22/2022 01:10:51 - INFO - __main__ - ['neutral']
05/22/2022 01:10:51 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/22/2022 01:10:51 - INFO - __main__ - ['neutral']
05/22/2022 01:10:51 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:10:51 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:10:51 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 01:10:51 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:10:51 - INFO - __main__ - Printing 3 examples
05/22/2022 01:10:51 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/22/2022 01:10:51 - INFO - __main__ - ['neutral']
05/22/2022 01:10:51 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/22/2022 01:10:51 - INFO - __main__ - ['neutral']
05/22/2022 01:10:51 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/22/2022 01:10:51 - INFO - __main__ - ['neutral']
05/22/2022 01:10:51 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:10:51 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:10:51 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 01:10:54 - INFO - __main__ - Global step 3000 Train loss 0.43 Classification-F1 0.17146494320407363 on epoch=499
05/22/2022 01:10:54 - INFO - __main__ - save last model!
05/22/2022 01:10:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 01:10:54 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 01:10:54 - INFO - __main__ - Printing 3 examples
05/22/2022 01:10:54 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 01:10:54 - INFO - __main__ - ['contradiction']
05/22/2022 01:10:54 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 01:10:54 - INFO - __main__ - ['entailment']
05/22/2022 01:10:54 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 01:10:54 - INFO - __main__ - ['contradiction']
05/22/2022 01:10:54 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:10:55 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:10:56 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 01:11:09 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 01:11:09 - INFO - __main__ - task name: anli
05/22/2022 01:11:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 01:11:10 - INFO - __main__ - Starting training!
05/22/2022 01:11:23 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_42_0.5_8_predictions.txt
05/22/2022 01:11:23 - INFO - __main__ - Classification-F1 on test data: 0.2180
05/22/2022 01:11:24 - INFO - __main__ - prefix=anli_32_42, lr=0.5, bsz=8, dev_performance=0.3968537098971881, test_performance=0.21801723915521157
05/22/2022 01:11:24 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.4, bsz=8 ...
05/22/2022 01:11:24 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:11:24 - INFO - __main__ - Printing 3 examples
05/22/2022 01:11:24 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/22/2022 01:11:24 - INFO - __main__ - ['neutral']
05/22/2022 01:11:24 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/22/2022 01:11:24 - INFO - __main__ - ['neutral']
05/22/2022 01:11:24 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/22/2022 01:11:24 - INFO - __main__ - ['neutral']
05/22/2022 01:11:24 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:11:25 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:11:25 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 01:11:25 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:11:25 - INFO - __main__ - Printing 3 examples
05/22/2022 01:11:25 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/22/2022 01:11:25 - INFO - __main__ - ['neutral']
05/22/2022 01:11:25 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/22/2022 01:11:25 - INFO - __main__ - ['neutral']
05/22/2022 01:11:25 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/22/2022 01:11:25 - INFO - __main__ - ['neutral']
05/22/2022 01:11:25 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:11:25 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:11:25 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 01:11:43 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 01:11:43 - INFO - __main__ - task name: anli
05/22/2022 01:11:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 01:11:44 - INFO - __main__ - Starting training!
05/22/2022 01:11:47 - INFO - __main__ - Step 10 Global step 10 Train loss 6.39 on epoch=1
05/22/2022 01:11:50 - INFO - __main__ - Step 20 Global step 20 Train loss 2.47 on epoch=3
05/22/2022 01:11:52 - INFO - __main__ - Step 30 Global step 30 Train loss 1.10 on epoch=4
05/22/2022 01:11:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.91 on epoch=6
05/22/2022 01:11:58 - INFO - __main__ - Step 50 Global step 50 Train loss 0.78 on epoch=8
05/22/2022 01:12:00 - INFO - __main__ - Global step 50 Train loss 2.33 Classification-F1 0.16666666666666666 on epoch=8
05/22/2022 01:12:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/22/2022 01:12:02 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=9
05/22/2022 01:12:05 - INFO - __main__ - Step 70 Global step 70 Train loss 0.68 on epoch=11
05/22/2022 01:12:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.70 on epoch=13
05/22/2022 01:12:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=14
05/22/2022 01:12:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=16
05/22/2022 01:12:15 - INFO - __main__ - Global step 100 Train loss 0.65 Classification-F1 0.16666666666666666 on epoch=16
05/22/2022 01:12:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=18
05/22/2022 01:12:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=19
05/22/2022 01:12:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=21
05/22/2022 01:12:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.67 on epoch=23
05/22/2022 01:12:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=24
05/22/2022 01:12:31 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=24
05/22/2022 01:12:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=26
05/22/2022 01:12:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.57 on epoch=28
05/22/2022 01:12:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=29
05/22/2022 01:12:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=31
05/22/2022 01:12:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=33
05/22/2022 01:12:47 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.21256038647342998 on epoch=33
05/22/2022 01:12:47 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.21256038647342998 on epoch=33, global_step=200
05/22/2022 01:12:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=34
05/22/2022 01:12:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=36
05/22/2022 01:12:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=38
05/22/2022 01:12:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.58 on epoch=39
05/22/2022 01:13:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.52 on epoch=41
05/22/2022 01:13:03 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.2402755696873344 on epoch=41
05/22/2022 01:13:03 - INFO - __main__ - Saving model with best Classification-F1: 0.21256038647342998 -> 0.2402755696873344 on epoch=41, global_step=250
05/22/2022 01:13:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=43
05/22/2022 01:13:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=44
05/22/2022 01:13:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=46
05/22/2022 01:13:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=48
05/22/2022 01:13:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=49
05/22/2022 01:13:18 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.19594552187284253 on epoch=49
05/22/2022 01:13:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=51
05/22/2022 01:13:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=53
05/22/2022 01:13:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=54
05/22/2022 01:13:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=56
05/22/2022 01:13:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=58
05/22/2022 01:13:34 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.18238327329236417 on epoch=58
05/22/2022 01:13:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=59
05/22/2022 01:13:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=61
05/22/2022 01:13:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=63
05/22/2022 01:13:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=64
05/22/2022 01:13:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.55 on epoch=66
05/22/2022 01:13:50 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=66
05/22/2022 01:13:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=68
05/22/2022 01:13:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=69
05/22/2022 01:13:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=71
05/22/2022 01:14:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=73
05/22/2022 01:14:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=74
05/22/2022 01:14:06 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=74
05/22/2022 01:14:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.56 on epoch=76
05/22/2022 01:14:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.58 on epoch=78
05/22/2022 01:14:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=79
05/22/2022 01:14:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=81
05/22/2022 01:14:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=83
05/22/2022 01:14:22 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.26666666666666666 on epoch=83
05/22/2022 01:14:22 - INFO - __main__ - Saving model with best Classification-F1: 0.2402755696873344 -> 0.26666666666666666 on epoch=83, global_step=500
05/22/2022 01:14:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=84
05/22/2022 01:14:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=86
05/22/2022 01:14:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=88
05/22/2022 01:14:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=89
05/22/2022 01:14:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=91
05/22/2022 01:14:38 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.18344798832603715 on epoch=91
05/22/2022 01:14:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=93
05/22/2022 01:14:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=94
05/22/2022 01:14:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=96
05/22/2022 01:14:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.54 on epoch=98
05/22/2022 01:14:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=99
05/22/2022 01:14:54 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.28126813314342985 on epoch=99
05/22/2022 01:14:54 - INFO - __main__ - Saving model with best Classification-F1: 0.26666666666666666 -> 0.28126813314342985 on epoch=99, global_step=600
05/22/2022 01:14:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=101
05/22/2022 01:14:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=103
05/22/2022 01:15:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=104
05/22/2022 01:15:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=106
05/22/2022 01:15:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.52 on epoch=108
05/22/2022 01:15:10 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.2578273236643369 on epoch=108
05/22/2022 01:15:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=109
05/22/2022 01:15:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=111
05/22/2022 01:15:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=113
05/22/2022 01:15:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=114
05/22/2022 01:15:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=116
05/22/2022 01:15:25 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.18971428571428572 on epoch=116
05/22/2022 01:15:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=118
05/22/2022 01:15:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.53 on epoch=119
05/22/2022 01:15:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=121
05/22/2022 01:15:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=123
05/22/2022 01:15:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=124
05/22/2022 01:15:41 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.19734575189120643 on epoch=124
05/22/2022 01:15:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=126
05/22/2022 01:15:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=128
05/22/2022 01:15:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=129
05/22/2022 01:15:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=131
05/22/2022 01:15:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=133
05/22/2022 01:15:57 - INFO - __main__ - Global step 800 Train loss 0.47 Classification-F1 0.227633188338882 on epoch=133
05/22/2022 01:16:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.64 on epoch=134
05/22/2022 01:16:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=136
05/22/2022 01:16:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.73 on epoch=138
05/22/2022 01:16:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=139
05/22/2022 01:16:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=141
05/22/2022 01:16:13 - INFO - __main__ - Global step 850 Train loss 0.56 Classification-F1 0.1679790026246719 on epoch=141
05/22/2022 01:16:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=143
05/22/2022 01:16:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=144
05/22/2022 01:16:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=146
05/22/2022 01:16:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=148
05/22/2022 01:16:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=149
05/22/2022 01:16:29 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.29224498788279357 on epoch=149
05/22/2022 01:16:29 - INFO - __main__ - Saving model with best Classification-F1: 0.28126813314342985 -> 0.29224498788279357 on epoch=149, global_step=900
05/22/2022 01:16:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.49 on epoch=151
05/22/2022 01:16:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=153
05/22/2022 01:16:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=154
05/22/2022 01:16:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=156
05/22/2022 01:16:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.49 on epoch=158
05/22/2022 01:16:45 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.2847444035360062 on epoch=158
05/22/2022 01:16:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=159
05/22/2022 01:16:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=161
05/22/2022 01:16:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=163
05/22/2022 01:16:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=164
05/22/2022 01:16:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=166
05/22/2022 01:17:01 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=166
05/22/2022 01:17:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=168
05/22/2022 01:17:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=169
05/22/2022 01:17:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=171
05/22/2022 01:17:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=173
05/22/2022 01:17:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=174
05/22/2022 01:17:17 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.25505804311774466 on epoch=174
05/22/2022 01:17:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=176
05/22/2022 01:17:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=178
05/22/2022 01:17:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=179
05/22/2022 01:17:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=181
05/22/2022 01:17:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=183
05/22/2022 01:17:33 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.17066666666666666 on epoch=183
05/22/2022 01:17:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=184
05/22/2022 01:17:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=186
05/22/2022 01:17:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=188
05/22/2022 01:17:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=189
05/22/2022 01:17:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=191
05/22/2022 01:17:49 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.18892001244942422 on epoch=191
05/22/2022 01:17:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=193
05/22/2022 01:17:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=194
05/22/2022 01:17:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=196
05/22/2022 01:18:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=198
05/22/2022 01:18:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=199
05/22/2022 01:18:05 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.212719298245614 on epoch=199
05/22/2022 01:18:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=201
05/22/2022 01:18:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=203
05/22/2022 01:18:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=204
05/22/2022 01:18:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=206
05/22/2022 01:18:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=208
05/22/2022 01:18:21 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.2359620734684608 on epoch=208
05/22/2022 01:18:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=209
05/22/2022 01:18:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=211
05/22/2022 01:18:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=213
05/22/2022 01:18:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=214
05/22/2022 01:18:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=216
05/22/2022 01:18:37 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.2274949669255105 on epoch=216
05/22/2022 01:18:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=218
05/22/2022 01:18:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=219
05/22/2022 01:18:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=221
05/22/2022 01:18:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=223
05/22/2022 01:18:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=224
05/22/2022 01:18:53 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.2792479065978156 on epoch=224
05/22/2022 01:18:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=226
05/22/2022 01:18:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=228
05/22/2022 01:19:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=229
05/22/2022 01:19:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=231
05/22/2022 01:19:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=233
05/22/2022 01:19:10 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.33139074040713384 on epoch=233
05/22/2022 01:19:10 - INFO - __main__ - Saving model with best Classification-F1: 0.29224498788279357 -> 0.33139074040713384 on epoch=233, global_step=1400
05/22/2022 01:19:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=234
05/22/2022 01:19:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=236
05/22/2022 01:19:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=238
05/22/2022 01:19:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=239
05/22/2022 01:19:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=241
05/22/2022 01:19:26 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.27192982456140347 on epoch=241
05/22/2022 01:19:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=243
05/22/2022 01:19:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=244
05/22/2022 01:19:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=246
05/22/2022 01:19:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=248
05/22/2022 01:19:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=249
05/22/2022 01:19:42 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.3833333333333333 on epoch=249
05/22/2022 01:19:42 - INFO - __main__ - Saving model with best Classification-F1: 0.33139074040713384 -> 0.3833333333333333 on epoch=249, global_step=1500
05/22/2022 01:19:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=251
05/22/2022 01:19:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=253
05/22/2022 01:19:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=254
05/22/2022 01:19:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=256
05/22/2022 01:19:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=258
05/22/2022 01:19:58 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.323403720462544 on epoch=258
05/22/2022 01:20:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=259
05/22/2022 01:20:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=261
05/22/2022 01:20:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=263
05/22/2022 01:20:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=264
05/22/2022 01:20:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.34 on epoch=266
05/22/2022 01:20:13 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.3360811667723526 on epoch=266
05/22/2022 01:20:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=268
05/22/2022 01:20:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=269
05/22/2022 01:20:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=271
05/22/2022 01:20:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=273
05/22/2022 01:20:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=274
05/22/2022 01:20:29 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.28853615520282183 on epoch=274
05/22/2022 01:20:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=276
05/22/2022 01:20:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=278
05/22/2022 01:20:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=279
05/22/2022 01:20:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=281
05/22/2022 01:20:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=283
05/22/2022 01:20:44 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.3080889848294058 on epoch=283
05/22/2022 01:20:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=284
05/22/2022 01:20:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=286
05/22/2022 01:20:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=288
05/22/2022 01:20:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=289
05/22/2022 01:20:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=291
05/22/2022 01:21:00 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.3105339105339105 on epoch=291
05/22/2022 01:21:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=293
05/22/2022 01:21:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=294
05/22/2022 01:21:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=296
05/22/2022 01:21:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.28 on epoch=298
05/22/2022 01:21:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=299
05/22/2022 01:21:16 - INFO - __main__ - Global step 1800 Train loss 0.31 Classification-F1 0.1994772903863813 on epoch=299
05/22/2022 01:21:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=301
05/22/2022 01:21:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=303
05/22/2022 01:21:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=304
05/22/2022 01:21:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.26 on epoch=306
05/22/2022 01:21:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=308
05/22/2022 01:21:32 - INFO - __main__ - Global step 1850 Train loss 0.29 Classification-F1 0.4493766937669377 on epoch=308
05/22/2022 01:21:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3833333333333333 -> 0.4493766937669377 on epoch=308, global_step=1850
05/22/2022 01:21:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=309
05/22/2022 01:21:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=311
05/22/2022 01:21:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.32 on epoch=313
05/22/2022 01:21:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.28 on epoch=314
05/22/2022 01:21:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.33 on epoch=316
05/22/2022 01:21:48 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.31309523809523815 on epoch=316
05/22/2022 01:21:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=318
05/22/2022 01:21:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=319
05/22/2022 01:21:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=321
05/22/2022 01:21:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=323
05/22/2022 01:22:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=324
05/22/2022 01:22:04 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.3190548014077426 on epoch=324
05/22/2022 01:22:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=326
05/22/2022 01:22:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=328
05/22/2022 01:22:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=329
05/22/2022 01:22:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.25 on epoch=331
05/22/2022 01:22:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=333
05/22/2022 01:22:20 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.3996448028415773 on epoch=333
05/22/2022 01:22:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=334
05/22/2022 01:22:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=336
05/22/2022 01:22:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=338
05/22/2022 01:22:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=339
05/22/2022 01:22:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.23 on epoch=341
05/22/2022 01:22:36 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.3773109243697479 on epoch=341
05/22/2022 01:22:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=343
05/22/2022 01:22:42 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=344
05/22/2022 01:22:44 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.21 on epoch=346
05/22/2022 01:22:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=348
05/22/2022 01:22:50 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=349
05/22/2022 01:22:52 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.3472026056702939 on epoch=349
05/22/2022 01:22:55 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=351
05/22/2022 01:22:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=353
05/22/2022 01:23:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.26 on epoch=354
05/22/2022 01:23:03 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=356
05/22/2022 01:23:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=358
05/22/2022 01:23:08 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.35254594924759847 on epoch=358
05/22/2022 01:23:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=359
05/22/2022 01:23:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=361
05/22/2022 01:23:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=363
05/22/2022 01:23:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.18 on epoch=364
05/22/2022 01:23:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=366
05/22/2022 01:23:24 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.34734340307095724 on epoch=366
05/22/2022 01:23:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=368
05/22/2022 01:23:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=369
05/22/2022 01:23:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.18 on epoch=371
05/22/2022 01:23:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.13 on epoch=373
05/22/2022 01:23:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=374
05/22/2022 01:23:39 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.23873409012727895 on epoch=374
05/22/2022 01:23:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.16 on epoch=376
05/22/2022 01:23:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.16 on epoch=378
05/22/2022 01:23:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=379
05/22/2022 01:23:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=381
05/22/2022 01:23:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=383
05/22/2022 01:23:56 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.33109483109483107 on epoch=383
05/22/2022 01:23:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=384
05/22/2022 01:24:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=386
05/22/2022 01:24:03 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=388
05/22/2022 01:24:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=389
05/22/2022 01:24:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=391
05/22/2022 01:24:12 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.36886724386724384 on epoch=391
05/22/2022 01:24:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=393
05/22/2022 01:24:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=394
05/22/2022 01:24:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.13 on epoch=396
05/22/2022 01:24:22 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=398
05/22/2022 01:24:25 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=399
05/22/2022 01:24:27 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.29058441558441556 on epoch=399
05/22/2022 01:24:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=401
05/22/2022 01:24:33 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=403
05/22/2022 01:24:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=404
05/22/2022 01:24:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=406
05/22/2022 01:24:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.15 on epoch=408
05/22/2022 01:24:43 - INFO - __main__ - Global step 2450 Train loss 0.14 Classification-F1 0.3005291005291005 on epoch=408
05/22/2022 01:24:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=409
05/22/2022 01:24:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=411
05/22/2022 01:24:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=413
05/22/2022 01:24:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.18 on epoch=414
05/22/2022 01:24:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=416
05/22/2022 01:24:59 - INFO - __main__ - Global step 2500 Train loss 0.12 Classification-F1 0.28825783186837955 on epoch=416
05/22/2022 01:25:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=418
05/22/2022 01:25:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=419
05/22/2022 01:25:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=421
05/22/2022 01:25:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=423
05/22/2022 01:25:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.11 on epoch=424
05/22/2022 01:25:14 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.26704219475303814 on epoch=424
05/22/2022 01:25:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=426
05/22/2022 01:25:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=428
05/22/2022 01:25:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.12 on epoch=429
05/22/2022 01:25:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=431
05/22/2022 01:25:27 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=433
05/22/2022 01:25:30 - INFO - __main__ - Global step 2600 Train loss 0.10 Classification-F1 0.21028496560759907 on epoch=433
05/22/2022 01:25:32 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=434
05/22/2022 01:25:35 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=436
05/22/2022 01:25:38 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.10 on epoch=438
05/22/2022 01:25:40 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=439
05/22/2022 01:25:43 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=441
05/22/2022 01:25:45 - INFO - __main__ - Global step 2650 Train loss 0.12 Classification-F1 0.29259259259259257 on epoch=441
05/22/2022 01:25:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=443
05/22/2022 01:25:51 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=444
05/22/2022 01:25:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=446
05/22/2022 01:25:56 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.20 on epoch=448
05/22/2022 01:25:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=449
05/22/2022 01:26:01 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.27779537535635096 on epoch=449
05/22/2022 01:26:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=451
05/22/2022 01:26:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=453
05/22/2022 01:26:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=454
05/22/2022 01:26:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=456
05/22/2022 01:26:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=458
05/22/2022 01:26:16 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.31848852901484476 on epoch=458
05/22/2022 01:26:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.12 on epoch=459
05/22/2022 01:26:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=461
05/22/2022 01:26:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=463
05/22/2022 01:26:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=464
05/22/2022 01:26:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=466
05/22/2022 01:26:32 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.11878448349036584 on epoch=466
05/22/2022 01:26:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=468
05/22/2022 01:26:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=469
05/22/2022 01:26:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=471
05/22/2022 01:26:43 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=473
05/22/2022 01:26:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=474
05/22/2022 01:26:47 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.22027584020291693 on epoch=474
05/22/2022 01:26:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=476
05/22/2022 01:26:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=478
05/22/2022 01:26:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=479
05/22/2022 01:26:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=481
05/22/2022 01:27:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=483
05/22/2022 01:27:03 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.28703703703703703 on epoch=483
05/22/2022 01:27:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=484
05/22/2022 01:27:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=486
05/22/2022 01:27:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=488
05/22/2022 01:27:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=489
05/22/2022 01:27:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=491
05/22/2022 01:27:19 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.2118043247075505 on epoch=491
05/22/2022 01:27:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=493
05/22/2022 01:27:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=494
05/22/2022 01:27:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=496
05/22/2022 01:27:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=498
05/22/2022 01:27:32 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=499
05/22/2022 01:27:33 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:27:33 - INFO - __main__ - Printing 3 examples
05/22/2022 01:27:33 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/22/2022 01:27:33 - INFO - __main__ - ['neutral']
05/22/2022 01:27:33 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/22/2022 01:27:33 - INFO - __main__ - ['neutral']
05/22/2022 01:27:33 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/22/2022 01:27:33 - INFO - __main__ - ['neutral']
05/22/2022 01:27:33 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:27:33 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:27:33 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 01:27:33 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:27:33 - INFO - __main__ - Printing 3 examples
05/22/2022 01:27:33 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/22/2022 01:27:33 - INFO - __main__ - ['neutral']
05/22/2022 01:27:33 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/22/2022 01:27:33 - INFO - __main__ - ['neutral']
05/22/2022 01:27:33 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/22/2022 01:27:33 - INFO - __main__ - ['neutral']
05/22/2022 01:27:33 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:27:33 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:27:34 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 01:27:34 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.21072974644403214 on epoch=499
05/22/2022 01:27:34 - INFO - __main__ - save last model!
05/22/2022 01:27:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 01:27:34 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 01:27:34 - INFO - __main__ - Printing 3 examples
05/22/2022 01:27:34 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 01:27:34 - INFO - __main__ - ['contradiction']
05/22/2022 01:27:34 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 01:27:34 - INFO - __main__ - ['entailment']
05/22/2022 01:27:34 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 01:27:34 - INFO - __main__ - ['contradiction']
05/22/2022 01:27:34 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:27:35 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:27:36 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 01:27:52 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 01:27:52 - INFO - __main__ - task name: anli
05/22/2022 01:27:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 01:27:53 - INFO - __main__ - Starting training!
05/22/2022 01:27:59 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_42_0.4_8_predictions.txt
05/22/2022 01:27:59 - INFO - __main__ - Classification-F1 on test data: 0.1783
05/22/2022 01:27:59 - INFO - __main__ - prefix=anli_32_42, lr=0.4, bsz=8, dev_performance=0.4493766937669377, test_performance=0.17825528054500017
05/22/2022 01:27:59 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.3, bsz=8 ...
05/22/2022 01:28:00 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:28:00 - INFO - __main__ - Printing 3 examples
05/22/2022 01:28:00 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/22/2022 01:28:00 - INFO - __main__ - ['neutral']
05/22/2022 01:28:00 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/22/2022 01:28:00 - INFO - __main__ - ['neutral']
05/22/2022 01:28:00 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/22/2022 01:28:00 - INFO - __main__ - ['neutral']
05/22/2022 01:28:00 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:28:00 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:28:00 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 01:28:00 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:28:00 - INFO - __main__ - Printing 3 examples
05/22/2022 01:28:00 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/22/2022 01:28:00 - INFO - __main__ - ['neutral']
05/22/2022 01:28:00 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/22/2022 01:28:00 - INFO - __main__ - ['neutral']
05/22/2022 01:28:00 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/22/2022 01:28:00 - INFO - __main__ - ['neutral']
05/22/2022 01:28:00 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:28:00 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:28:00 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 01:28:19 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 01:28:19 - INFO - __main__ - task name: anli
05/22/2022 01:28:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 01:28:20 - INFO - __main__ - Starting training!
05/22/2022 01:28:23 - INFO - __main__ - Step 10 Global step 10 Train loss 7.26 on epoch=1
05/22/2022 01:28:25 - INFO - __main__ - Step 20 Global step 20 Train loss 3.74 on epoch=3
05/22/2022 01:28:28 - INFO - __main__ - Step 30 Global step 30 Train loss 1.66 on epoch=4
05/22/2022 01:28:31 - INFO - __main__ - Step 40 Global step 40 Train loss 1.07 on epoch=6
05/22/2022 01:28:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.91 on epoch=8
05/22/2022 01:28:35 - INFO - __main__ - Global step 50 Train loss 2.93 Classification-F1 0.16666666666666666 on epoch=8
05/22/2022 01:28:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/22/2022 01:28:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.77 on epoch=9
05/22/2022 01:28:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.70 on epoch=11
05/22/2022 01:28:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=13
05/22/2022 01:28:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=14
05/22/2022 01:28:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=16
05/22/2022 01:28:51 - INFO - __main__ - Global step 100 Train loss 0.67 Classification-F1 0.16666666666666666 on epoch=16
05/22/2022 01:28:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=18
05/22/2022 01:28:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=19
05/22/2022 01:28:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.64 on epoch=21
05/22/2022 01:29:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=23
05/22/2022 01:29:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.61 on epoch=24
05/22/2022 01:29:07 - INFO - __main__ - Global step 150 Train loss 0.60 Classification-F1 0.16666666666666666 on epoch=24
05/22/2022 01:29:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=26
05/22/2022 01:29:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=28
05/22/2022 01:29:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=29
05/22/2022 01:29:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.59 on epoch=31
05/22/2022 01:29:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.61 on epoch=33
05/22/2022 01:29:23 - INFO - __main__ - Global step 200 Train loss 0.57 Classification-F1 0.2681828950485667 on epoch=33
05/22/2022 01:29:23 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2681828950485667 on epoch=33, global_step=200
05/22/2022 01:29:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=34
05/22/2022 01:29:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=36
05/22/2022 01:29:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=38
05/22/2022 01:29:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=39
05/22/2022 01:29:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=41
05/22/2022 01:29:39 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.1881810228266921 on epoch=41
05/22/2022 01:29:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.62 on epoch=43
05/22/2022 01:29:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=44
05/22/2022 01:29:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=46
05/22/2022 01:29:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=48
05/22/2022 01:29:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=49
05/22/2022 01:29:55 - INFO - __main__ - Global step 300 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=49
05/22/2022 01:29:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.56 on epoch=51
05/22/2022 01:30:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=53
05/22/2022 01:30:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=54
05/22/2022 01:30:05 - INFO - __main__ - Step 340 Global step 340 Train loss 1.83 on epoch=56
05/22/2022 01:30:08 - INFO - __main__ - Step 350 Global step 350 Train loss 2.60 on epoch=58
05/22/2022 01:30:11 - INFO - __main__ - Global step 350 Train loss 1.20 Classification-F1 0.24321285140562246 on epoch=58
05/22/2022 01:30:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.89 on epoch=59
05/22/2022 01:30:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.65 on epoch=61
05/22/2022 01:30:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.74 on epoch=63
05/22/2022 01:30:21 - INFO - __main__ - Step 390 Global step 390 Train loss 1.77 on epoch=64
05/22/2022 01:30:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.70 on epoch=66
05/22/2022 01:30:26 - INFO - __main__ - Global step 400 Train loss 0.95 Classification-F1 0.16666666666666666 on epoch=66
05/22/2022 01:30:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.65 on epoch=68
05/22/2022 01:30:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=69
05/22/2022 01:30:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=71
05/22/2022 01:30:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=73
05/22/2022 01:30:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=74
05/22/2022 01:30:42 - INFO - __main__ - Global step 450 Train loss 0.55 Classification-F1 0.1679790026246719 on epoch=74
05/22/2022 01:30:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=76
05/22/2022 01:30:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=78
05/22/2022 01:30:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=79
05/22/2022 01:30:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.56 on epoch=81
05/22/2022 01:30:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.55 on epoch=83
05/22/2022 01:30:58 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.1679790026246719 on epoch=83
05/22/2022 01:31:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=84
05/22/2022 01:31:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=86
05/22/2022 01:31:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.51 on epoch=88
05/22/2022 01:31:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=89
05/22/2022 01:31:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.56 on epoch=91
05/22/2022 01:31:14 - INFO - __main__ - Global step 550 Train loss 0.51 Classification-F1 0.1679790026246719 on epoch=91
05/22/2022 01:31:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.52 on epoch=93
05/22/2022 01:31:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.54 on epoch=94
05/22/2022 01:31:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=96
05/22/2022 01:31:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=98
05/22/2022 01:31:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=99
05/22/2022 01:31:30 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.1679790026246719 on epoch=99
05/22/2022 01:31:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=101
05/22/2022 01:31:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=103
05/22/2022 01:31:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=104
05/22/2022 01:31:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=106
05/22/2022 01:31:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.56 on epoch=108
05/22/2022 01:31:45 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.1679790026246719 on epoch=108
05/22/2022 01:31:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=109
05/22/2022 01:31:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=111
05/22/2022 01:31:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.55 on epoch=113
05/22/2022 01:31:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.51 on epoch=114
05/22/2022 01:31:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.54 on epoch=116
05/22/2022 01:32:01 - INFO - __main__ - Global step 700 Train loss 0.54 Classification-F1 0.1679790026246719 on epoch=116
05/22/2022 01:32:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.57 on epoch=118
05/22/2022 01:32:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=119
05/22/2022 01:32:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=121
05/22/2022 01:32:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=123
05/22/2022 01:32:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=124
05/22/2022 01:32:17 - INFO - __main__ - Global step 750 Train loss 0.52 Classification-F1 0.1679790026246719 on epoch=124
05/22/2022 01:32:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.55 on epoch=126
05/22/2022 01:32:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=128
05/22/2022 01:32:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=129
05/22/2022 01:32:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=131
05/22/2022 01:32:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.55 on epoch=133
05/22/2022 01:32:33 - INFO - __main__ - Global step 800 Train loss 0.50 Classification-F1 0.1679790026246719 on epoch=133
05/22/2022 01:32:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.53 on epoch=134
05/22/2022 01:32:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.50 on epoch=136
05/22/2022 01:32:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.59 on epoch=138
05/22/2022 01:32:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=139
05/22/2022 01:32:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.53 on epoch=141
05/22/2022 01:32:49 - INFO - __main__ - Global step 850 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=141
05/22/2022 01:32:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.57 on epoch=143
05/22/2022 01:32:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.54 on epoch=144
05/22/2022 01:32:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.52 on epoch=146
05/22/2022 01:33:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=148
05/22/2022 01:33:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=149
05/22/2022 01:33:05 - INFO - __main__ - Global step 900 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=149
05/22/2022 01:33:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.57 on epoch=151
05/22/2022 01:33:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=153
05/22/2022 01:33:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=154
05/22/2022 01:33:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=156
05/22/2022 01:33:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.55 on epoch=158
05/22/2022 01:33:21 - INFO - __main__ - Global step 950 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=158
05/22/2022 01:33:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=159
05/22/2022 01:33:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.55 on epoch=161
05/22/2022 01:33:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=163
05/22/2022 01:33:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.50 on epoch=164
05/22/2022 01:33:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.51 on epoch=166
05/22/2022 01:33:38 - INFO - __main__ - Global step 1000 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=166
05/22/2022 01:33:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.59 on epoch=168
05/22/2022 01:33:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.51 on epoch=169
05/22/2022 01:33:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=171
05/22/2022 01:33:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.55 on epoch=173
05/22/2022 01:33:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.53 on epoch=174
05/22/2022 01:33:53 - INFO - __main__ - Global step 1050 Train loss 0.53 Classification-F1 0.1679790026246719 on epoch=174
05/22/2022 01:33:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.50 on epoch=176
05/22/2022 01:33:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.55 on epoch=178
05/22/2022 01:34:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.51 on epoch=179
05/22/2022 01:34:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.52 on epoch=181
05/22/2022 01:34:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.48 on epoch=183
05/22/2022 01:34:09 - INFO - __main__ - Global step 1100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=183
05/22/2022 01:34:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=184
05/22/2022 01:34:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.50 on epoch=186
05/22/2022 01:34:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.50 on epoch=188
05/22/2022 01:34:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=189
05/22/2022 01:34:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=191
05/22/2022 01:34:25 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.1679790026246719 on epoch=191
05/22/2022 01:34:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.54 on epoch=193
05/22/2022 01:34:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=194
05/22/2022 01:34:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=196
05/22/2022 01:34:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.51 on epoch=198
05/22/2022 01:34:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=199
05/22/2022 01:34:41 - INFO - __main__ - Global step 1200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=199
05/22/2022 01:34:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=201
05/22/2022 01:34:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.52 on epoch=203
05/22/2022 01:34:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.51 on epoch=204
05/22/2022 01:34:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=206
05/22/2022 01:34:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=208
05/22/2022 01:34:57 - INFO - __main__ - Global step 1250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=208
05/22/2022 01:35:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.51 on epoch=209
05/22/2022 01:35:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=211
05/22/2022 01:35:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=213
05/22/2022 01:35:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.49 on epoch=214
05/22/2022 01:35:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=216
05/22/2022 01:35:13 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.1679790026246719 on epoch=216
05/22/2022 01:35:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=218
05/22/2022 01:35:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=219
05/22/2022 01:35:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.51 on epoch=221
05/22/2022 01:35:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.48 on epoch=223
05/22/2022 01:35:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=224
05/22/2022 01:35:28 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.1679790026246719 on epoch=224
05/22/2022 01:35:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.52 on epoch=226
05/22/2022 01:35:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.46 on epoch=228
05/22/2022 01:35:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=229
05/22/2022 01:35:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=231
05/22/2022 01:35:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.53 on epoch=233
05/22/2022 01:35:44 - INFO - __main__ - Global step 1400 Train loss 0.50 Classification-F1 0.1679790026246719 on epoch=233
05/22/2022 01:35:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=234
05/22/2022 01:35:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.55 on epoch=236
05/22/2022 01:35:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=238
05/22/2022 01:35:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.46 on epoch=239
05/22/2022 01:35:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.50 on epoch=241
05/22/2022 01:35:59 - INFO - __main__ - Global step 1450 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=241
05/22/2022 01:36:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.49 on epoch=243
05/22/2022 01:36:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.50 on epoch=244
05/22/2022 01:36:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.53 on epoch=246
05/22/2022 01:36:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.50 on epoch=248
05/22/2022 01:36:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.48 on epoch=249
05/22/2022 01:36:15 - INFO - __main__ - Global step 1500 Train loss 0.50 Classification-F1 0.1679790026246719 on epoch=249
05/22/2022 01:36:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.49 on epoch=251
05/22/2022 01:36:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.48 on epoch=253
05/22/2022 01:36:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.55 on epoch=254
05/22/2022 01:36:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.49 on epoch=256
05/22/2022 01:36:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=258
05/22/2022 01:36:31 - INFO - __main__ - Global step 1550 Train loss 0.49 Classification-F1 0.18147828587863102 on epoch=258
05/22/2022 01:36:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=259
05/22/2022 01:36:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.54 on epoch=261
05/22/2022 01:36:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.53 on epoch=263
05/22/2022 01:36:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=264
05/22/2022 01:36:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.51 on epoch=266
05/22/2022 01:36:47 - INFO - __main__ - Global step 1600 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=266
05/22/2022 01:36:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.52 on epoch=268
05/22/2022 01:36:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.52 on epoch=269
05/22/2022 01:36:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.52 on epoch=271
05/22/2022 01:36:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.53 on epoch=273
05/22/2022 01:37:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=274
05/22/2022 01:37:02 - INFO - __main__ - Global step 1650 Train loss 0.51 Classification-F1 0.1679790026246719 on epoch=274
05/22/2022 01:37:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.54 on epoch=276
05/22/2022 01:37:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.50 on epoch=278
05/22/2022 01:37:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=279
05/22/2022 01:37:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=281
05/22/2022 01:37:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.52 on epoch=283
05/22/2022 01:37:18 - INFO - __main__ - Global step 1700 Train loss 0.50 Classification-F1 0.1693121693121693 on epoch=283
05/22/2022 01:37:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=284
05/22/2022 01:37:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=286
05/22/2022 01:37:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.47 on epoch=288
05/22/2022 01:37:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.46 on epoch=289
05/22/2022 01:37:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=291
05/22/2022 01:37:34 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=291
05/22/2022 01:37:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.55 on epoch=293
05/22/2022 01:37:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.50 on epoch=294
05/22/2022 01:37:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.49 on epoch=296
05/22/2022 01:37:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.51 on epoch=298
05/22/2022 01:37:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=299
05/22/2022 01:37:50 - INFO - __main__ - Global step 1800 Train loss 0.50 Classification-F1 0.21212121212121207 on epoch=299
05/22/2022 01:37:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=301
05/22/2022 01:37:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.50 on epoch=303
05/22/2022 01:37:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.47 on epoch=304
05/22/2022 01:38:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.53 on epoch=306
05/22/2022 01:38:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.48 on epoch=308
05/22/2022 01:38:05 - INFO - __main__ - Global step 1850 Train loss 0.48 Classification-F1 0.23963133640552994 on epoch=308
05/22/2022 01:38:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.50 on epoch=309
05/22/2022 01:38:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=311
05/22/2022 01:38:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=313
05/22/2022 01:38:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=314
05/22/2022 01:38:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.48 on epoch=316
05/22/2022 01:38:21 - INFO - __main__ - Global step 1900 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=316
05/22/2022 01:38:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=318
05/22/2022 01:38:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.51 on epoch=319
05/22/2022 01:38:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.50 on epoch=321
05/22/2022 01:38:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=323
05/22/2022 01:38:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=324
05/22/2022 01:38:37 - INFO - __main__ - Global step 1950 Train loss 0.48 Classification-F1 0.260039499670836 on epoch=324
05/22/2022 01:38:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.51 on epoch=326
05/22/2022 01:38:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.48 on epoch=328
05/22/2022 01:38:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.50 on epoch=329
05/22/2022 01:38:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.51 on epoch=331
05/22/2022 01:38:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=333
05/22/2022 01:38:53 - INFO - __main__ - Global step 2000 Train loss 0.49 Classification-F1 0.1949853111976498 on epoch=333
05/22/2022 01:38:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.52 on epoch=334
05/22/2022 01:38:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.48 on epoch=336
05/22/2022 01:39:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.48 on epoch=338
05/22/2022 01:39:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.46 on epoch=339
05/22/2022 01:39:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.50 on epoch=341
05/22/2022 01:39:09 - INFO - __main__ - Global step 2050 Train loss 0.49 Classification-F1 0.18892001244942422 on epoch=341
05/22/2022 01:39:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.46 on epoch=343
05/22/2022 01:39:14 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.46 on epoch=344
05/22/2022 01:39:17 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.42 on epoch=346
05/22/2022 01:39:19 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.46 on epoch=348
05/22/2022 01:39:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.44 on epoch=349
05/22/2022 01:39:24 - INFO - __main__ - Global step 2100 Train loss 0.45 Classification-F1 0.18892001244942422 on epoch=349
05/22/2022 01:39:27 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.48 on epoch=351
05/22/2022 01:39:30 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.51 on epoch=353
05/22/2022 01:39:32 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.46 on epoch=354
05/22/2022 01:39:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.50 on epoch=356
05/22/2022 01:39:38 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.50 on epoch=358
05/22/2022 01:39:40 - INFO - __main__ - Global step 2150 Train loss 0.49 Classification-F1 0.2737593150467734 on epoch=358
05/22/2022 01:39:41 - INFO - __main__ - Saving model with best Classification-F1: 0.2681828950485667 -> 0.2737593150467734 on epoch=358, global_step=2150
05/22/2022 01:39:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.46 on epoch=359
05/22/2022 01:39:46 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.45 on epoch=361
05/22/2022 01:39:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.47 on epoch=363
05/22/2022 01:39:51 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=364
05/22/2022 01:39:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.47 on epoch=366
05/22/2022 01:39:56 - INFO - __main__ - Global step 2200 Train loss 0.46 Classification-F1 0.18892001244942422 on epoch=366
05/22/2022 01:39:59 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.53 on epoch=368
05/22/2022 01:40:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.52 on epoch=369
05/22/2022 01:40:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.45 on epoch=371
05/22/2022 01:40:07 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.47 on epoch=373
05/22/2022 01:40:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.52 on epoch=374
05/22/2022 01:40:12 - INFO - __main__ - Global step 2250 Train loss 0.50 Classification-F1 0.2562310738411367 on epoch=374
05/22/2022 01:40:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.45 on epoch=376
05/22/2022 01:40:17 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.47 on epoch=378
05/22/2022 01:40:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.47 on epoch=379
05/22/2022 01:40:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.52 on epoch=381
05/22/2022 01:40:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.49 on epoch=383
05/22/2022 01:40:28 - INFO - __main__ - Global step 2300 Train loss 0.48 Classification-F1 0.24994627122286697 on epoch=383
05/22/2022 01:40:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.47 on epoch=384
05/22/2022 01:40:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.46 on epoch=386
05/22/2022 01:40:36 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.42 on epoch=388
05/22/2022 01:40:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.43 on epoch=389
05/22/2022 01:40:41 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.50 on epoch=391
05/22/2022 01:40:44 - INFO - __main__ - Global step 2350 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=391
05/22/2022 01:40:46 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.43 on epoch=393
05/22/2022 01:40:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.48 on epoch=394
05/22/2022 01:40:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.46 on epoch=396
05/22/2022 01:40:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.47 on epoch=398
05/22/2022 01:40:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.41 on epoch=399
05/22/2022 01:41:00 - INFO - __main__ - Global step 2400 Train loss 0.45 Classification-F1 0.1994772903863813 on epoch=399
05/22/2022 01:41:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.46 on epoch=401
05/22/2022 01:41:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.46 on epoch=403
05/22/2022 01:41:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.42 on epoch=404
05/22/2022 01:41:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.49 on epoch=406
05/22/2022 01:41:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.48 on epoch=408
05/22/2022 01:41:16 - INFO - __main__ - Global step 2450 Train loss 0.46 Classification-F1 0.2664999739542637 on epoch=408
05/22/2022 01:41:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=409
05/22/2022 01:41:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=411
05/22/2022 01:41:24 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=413
05/22/2022 01:41:26 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.45 on epoch=414
05/22/2022 01:41:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.48 on epoch=416
05/22/2022 01:41:32 - INFO - __main__ - Global step 2500 Train loss 0.44 Classification-F1 0.2040577162528382 on epoch=416
05/22/2022 01:41:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.47 on epoch=418
05/22/2022 01:41:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.48 on epoch=419
05/22/2022 01:41:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.49 on epoch=421
05/22/2022 01:41:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=423
05/22/2022 01:41:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.48 on epoch=424
05/22/2022 01:41:47 - INFO - __main__ - Global step 2550 Train loss 0.47 Classification-F1 0.2528219119456481 on epoch=424
05/22/2022 01:41:50 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.49 on epoch=426
05/22/2022 01:41:53 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.45 on epoch=428
05/22/2022 01:41:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.48 on epoch=429
05/22/2022 01:41:58 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.48 on epoch=431
05/22/2022 01:42:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.43 on epoch=433
05/22/2022 01:42:04 - INFO - __main__ - Global step 2600 Train loss 0.46 Classification-F1 0.2995670995670996 on epoch=433
05/22/2022 01:42:04 - INFO - __main__ - Saving model with best Classification-F1: 0.2737593150467734 -> 0.2995670995670996 on epoch=433, global_step=2600
05/22/2022 01:42:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.44 on epoch=434
05/22/2022 01:42:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.46 on epoch=436
05/22/2022 01:42:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.57 on epoch=438
05/22/2022 01:42:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.44 on epoch=439
05/22/2022 01:42:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.45 on epoch=441
05/22/2022 01:42:20 - INFO - __main__ - Global step 2650 Train loss 0.47 Classification-F1 0.204406364749082 on epoch=441
05/22/2022 01:42:22 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.46 on epoch=443
05/22/2022 01:42:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.44 on epoch=444
05/22/2022 01:42:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.48 on epoch=446
05/22/2022 01:42:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.42 on epoch=448
05/22/2022 01:42:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.41 on epoch=449
05/22/2022 01:42:36 - INFO - __main__ - Global step 2700 Train loss 0.44 Classification-F1 0.2112454737992758 on epoch=449
05/22/2022 01:42:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.42 on epoch=451
05/22/2022 01:42:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.44 on epoch=453
05/22/2022 01:42:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.50 on epoch=454
05/22/2022 01:42:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.47 on epoch=456
05/22/2022 01:42:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.45 on epoch=458
05/22/2022 01:42:52 - INFO - __main__ - Global step 2750 Train loss 0.45 Classification-F1 0.3414465932929107 on epoch=458
05/22/2022 01:42:52 - INFO - __main__ - Saving model with best Classification-F1: 0.2995670995670996 -> 0.3414465932929107 on epoch=458, global_step=2750
05/22/2022 01:42:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.45 on epoch=459
05/22/2022 01:42:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.50 on epoch=461
05/22/2022 01:43:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.44 on epoch=463
05/22/2022 01:43:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.46 on epoch=464
05/22/2022 01:43:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.45 on epoch=466
05/22/2022 01:43:08 - INFO - __main__ - Global step 2800 Train loss 0.46 Classification-F1 0.2119251119251119 on epoch=466
05/22/2022 01:43:11 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.42 on epoch=468
05/22/2022 01:43:13 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.42 on epoch=469
05/22/2022 01:43:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.46 on epoch=471
05/22/2022 01:43:18 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=473
05/22/2022 01:43:21 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.47 on epoch=474
05/22/2022 01:43:24 - INFO - __main__ - Global step 2850 Train loss 0.44 Classification-F1 0.26396342675412443 on epoch=474
05/22/2022 01:43:27 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.46 on epoch=476
05/22/2022 01:43:29 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.48 on epoch=478
05/22/2022 01:43:32 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.39 on epoch=479
05/22/2022 01:43:35 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.45 on epoch=481
05/22/2022 01:43:37 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.46 on epoch=483
05/22/2022 01:43:40 - INFO - __main__ - Global step 2900 Train loss 0.45 Classification-F1 0.23611111111111108 on epoch=483
05/22/2022 01:43:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.48 on epoch=484
05/22/2022 01:43:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.44 on epoch=486
05/22/2022 01:43:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.44 on epoch=488
05/22/2022 01:43:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.45 on epoch=489
05/22/2022 01:43:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.41 on epoch=491
05/22/2022 01:43:56 - INFO - __main__ - Global step 2950 Train loss 0.44 Classification-F1 0.21225071225071224 on epoch=491
05/22/2022 01:43:59 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.44 on epoch=493
05/22/2022 01:44:01 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.45 on epoch=494
05/22/2022 01:44:04 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.43 on epoch=496
05/22/2022 01:44:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.48 on epoch=498
05/22/2022 01:44:09 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.44 on epoch=499
05/22/2022 01:44:10 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:44:10 - INFO - __main__ - Printing 3 examples
05/22/2022 01:44:10 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/22/2022 01:44:10 - INFO - __main__ - ['neutral']
05/22/2022 01:44:10 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/22/2022 01:44:10 - INFO - __main__ - ['neutral']
05/22/2022 01:44:10 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/22/2022 01:44:10 - INFO - __main__ - ['neutral']
05/22/2022 01:44:10 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:44:10 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:44:11 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 01:44:11 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:44:11 - INFO - __main__ - Printing 3 examples
05/22/2022 01:44:11 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/22/2022 01:44:11 - INFO - __main__ - ['neutral']
05/22/2022 01:44:11 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/22/2022 01:44:11 - INFO - __main__ - ['neutral']
05/22/2022 01:44:11 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/22/2022 01:44:11 - INFO - __main__ - ['neutral']
05/22/2022 01:44:11 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:44:11 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:44:11 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 01:44:12 - INFO - __main__ - Global step 3000 Train loss 0.45 Classification-F1 0.22207573427085622 on epoch=499
05/22/2022 01:44:12 - INFO - __main__ - save last model!
05/22/2022 01:44:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 01:44:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 01:44:12 - INFO - __main__ - Printing 3 examples
05/22/2022 01:44:12 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 01:44:12 - INFO - __main__ - ['contradiction']
05/22/2022 01:44:12 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 01:44:12 - INFO - __main__ - ['entailment']
05/22/2022 01:44:12 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 01:44:12 - INFO - __main__ - ['contradiction']
05/22/2022 01:44:12 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:44:13 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:44:14 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 01:44:29 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 01:44:29 - INFO - __main__ - task name: anli
05/22/2022 01:44:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 01:44:30 - INFO - __main__ - Starting training!
05/22/2022 01:44:43 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_42_0.3_8_predictions.txt
05/22/2022 01:44:43 - INFO - __main__ - Classification-F1 on test data: 0.2069
05/22/2022 01:44:43 - INFO - __main__ - prefix=anli_32_42, lr=0.3, bsz=8, dev_performance=0.3414465932929107, test_performance=0.20690506425231758
05/22/2022 01:44:43 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.2, bsz=8 ...
05/22/2022 01:44:44 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:44:44 - INFO - __main__ - Printing 3 examples
05/22/2022 01:44:44 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/22/2022 01:44:44 - INFO - __main__ - ['neutral']
05/22/2022 01:44:44 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/22/2022 01:44:44 - INFO - __main__ - ['neutral']
05/22/2022 01:44:44 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/22/2022 01:44:44 - INFO - __main__ - ['neutral']
05/22/2022 01:44:44 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:44:44 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:44:44 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 01:44:44 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 01:44:44 - INFO - __main__ - Printing 3 examples
05/22/2022 01:44:44 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/22/2022 01:44:44 - INFO - __main__ - ['neutral']
05/22/2022 01:44:44 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/22/2022 01:44:44 - INFO - __main__ - ['neutral']
05/22/2022 01:44:44 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/22/2022 01:44:44 - INFO - __main__ - ['neutral']
05/22/2022 01:44:44 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:44:44 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:44:44 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 01:44:59 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 01:44:59 - INFO - __main__ - task name: anli
05/22/2022 01:44:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 01:44:59 - INFO - __main__ - Starting training!
05/22/2022 01:45:03 - INFO - __main__ - Step 10 Global step 10 Train loss 7.75 on epoch=1
05/22/2022 01:45:05 - INFO - __main__ - Step 20 Global step 20 Train loss 5.37 on epoch=3
05/22/2022 01:45:08 - INFO - __main__ - Step 30 Global step 30 Train loss 3.02 on epoch=4
05/22/2022 01:45:10 - INFO - __main__ - Step 40 Global step 40 Train loss 1.87 on epoch=6
05/22/2022 01:45:13 - INFO - __main__ - Step 50 Global step 50 Train loss 1.49 on epoch=8
05/22/2022 01:45:16 - INFO - __main__ - Global step 50 Train loss 3.90 Classification-F1 0.18971428571428572 on epoch=8
05/22/2022 01:45:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18971428571428572 on epoch=8, global_step=50
05/22/2022 01:45:19 - INFO - __main__ - Step 60 Global step 60 Train loss 1.11 on epoch=9
05/22/2022 01:45:22 - INFO - __main__ - Step 70 Global step 70 Train loss 1.02 on epoch=11
05/22/2022 01:45:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.84 on epoch=13
05/22/2022 01:45:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.79 on epoch=14
05/22/2022 01:45:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.76 on epoch=16
05/22/2022 01:45:32 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.1679790026246719 on epoch=16
05/22/2022 01:45:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.74 on epoch=18
05/22/2022 01:45:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.71 on epoch=19
05/22/2022 01:45:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.70 on epoch=21
05/22/2022 01:45:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.67 on epoch=23
05/22/2022 01:45:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=24
05/22/2022 01:45:47 - INFO - __main__ - Global step 150 Train loss 0.68 Classification-F1 0.1693121693121693 on epoch=24
05/22/2022 01:45:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.68 on epoch=26
05/22/2022 01:45:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=28
05/22/2022 01:45:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=29
05/22/2022 01:45:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.60 on epoch=31
05/22/2022 01:46:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.57 on epoch=33
05/22/2022 01:46:03 - INFO - __main__ - Global step 200 Train loss 0.60 Classification-F1 0.16666666666666666 on epoch=33
05/22/2022 01:46:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=34
05/22/2022 01:46:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.61 on epoch=36
05/22/2022 01:46:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=38
05/22/2022 01:46:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.54 on epoch=39
05/22/2022 01:46:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=41
05/22/2022 01:46:18 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=41
05/22/2022 01:46:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.61 on epoch=43
05/22/2022 01:46:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.57 on epoch=44
05/22/2022 01:46:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.59 on epoch=46
05/22/2022 01:46:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=48
05/22/2022 01:46:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=49
05/22/2022 01:46:34 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=49
05/22/2022 01:46:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.54 on epoch=51
05/22/2022 01:46:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=53
05/22/2022 01:46:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.57 on epoch=54
05/22/2022 01:46:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=56
05/22/2022 01:46:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.53 on epoch=58
05/22/2022 01:46:51 - INFO - __main__ - Global step 350 Train loss 0.54 Classification-F1 0.20370370370370372 on epoch=58
05/22/2022 01:46:51 - INFO - __main__ - Saving model with best Classification-F1: 0.18971428571428572 -> 0.20370370370370372 on epoch=58, global_step=350
05/22/2022 01:46:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=59
05/22/2022 01:46:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.60 on epoch=61
05/22/2022 01:46:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.59 on epoch=63
05/22/2022 01:47:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.55 on epoch=64
05/22/2022 01:47:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.60 on epoch=66
05/22/2022 01:47:07 - INFO - __main__ - Global step 400 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=66
05/22/2022 01:47:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=68
05/22/2022 01:47:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=69
05/22/2022 01:47:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=71
05/22/2022 01:47:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.57 on epoch=73
05/22/2022 01:47:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=74
05/22/2022 01:47:23 - INFO - __main__ - Global step 450 Train loss 0.54 Classification-F1 0.26332784044525775 on epoch=74
05/22/2022 01:47:23 - INFO - __main__ - Saving model with best Classification-F1: 0.20370370370370372 -> 0.26332784044525775 on epoch=74, global_step=450
05/22/2022 01:47:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=76
05/22/2022 01:47:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.55 on epoch=78
05/22/2022 01:47:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=79
05/22/2022 01:47:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=81
05/22/2022 01:47:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.57 on epoch=83
05/22/2022 01:47:39 - INFO - __main__ - Global step 500 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=83
05/22/2022 01:47:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=84
05/22/2022 01:47:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.51 on epoch=86
05/22/2022 01:47:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=88
05/22/2022 01:47:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=89
05/22/2022 01:47:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=91
05/22/2022 01:47:55 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.17904761904761904 on epoch=91
05/22/2022 01:47:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.56 on epoch=93
05/22/2022 01:48:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=94
05/22/2022 01:48:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=96
05/22/2022 01:48:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.59 on epoch=98
05/22/2022 01:48:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=99
05/22/2022 01:48:11 - INFO - __main__ - Global step 600 Train loss 0.53 Classification-F1 0.19878787878787882 on epoch=99
05/22/2022 01:48:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.54 on epoch=101
05/22/2022 01:48:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=103
05/22/2022 01:48:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=104
05/22/2022 01:48:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=106
05/22/2022 01:48:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=108
05/22/2022 01:48:27 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.25333740333740334 on epoch=108
05/22/2022 01:48:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=109
05/22/2022 01:48:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=111
05/22/2022 01:48:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=113
05/22/2022 01:48:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=114
05/22/2022 01:48:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=116
05/22/2022 01:48:43 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.2556862745098039 on epoch=116
05/22/2022 01:48:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=118
05/22/2022 01:48:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=119
05/22/2022 01:48:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.55 on epoch=121
05/22/2022 01:48:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.52 on epoch=123
05/22/2022 01:48:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=124
05/22/2022 01:48:59 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.2703733548213806 on epoch=124
05/22/2022 01:48:59 - INFO - __main__ - Saving model with best Classification-F1: 0.26332784044525775 -> 0.2703733548213806 on epoch=124, global_step=750
05/22/2022 01:49:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.50 on epoch=126
05/22/2022 01:49:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.53 on epoch=128
05/22/2022 01:49:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.54 on epoch=129
05/22/2022 01:49:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=131
05/22/2022 01:49:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=133
05/22/2022 01:49:15 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.31216931216931215 on epoch=133
05/22/2022 01:49:15 - INFO - __main__ - Saving model with best Classification-F1: 0.2703733548213806 -> 0.31216931216931215 on epoch=133, global_step=800
05/22/2022 01:49:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=134
05/22/2022 01:49:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.53 on epoch=136
05/22/2022 01:49:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=138
05/22/2022 01:49:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=139
05/22/2022 01:49:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=141
05/22/2022 01:49:32 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.20836742486227022 on epoch=141
05/22/2022 01:49:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=143
05/22/2022 01:49:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=144
05/22/2022 01:49:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=146
05/22/2022 01:49:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=148
05/22/2022 01:49:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=149
05/22/2022 01:49:48 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.28028375577873427 on epoch=149
05/22/2022 01:49:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.58 on epoch=151
05/22/2022 01:49:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=153
05/22/2022 01:49:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=154
05/22/2022 01:49:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=156
05/22/2022 01:50:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.52 on epoch=158
05/22/2022 01:50:04 - INFO - __main__ - Global step 950 Train loss 0.51 Classification-F1 0.2585136003426858 on epoch=158
05/22/2022 01:50:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=159
05/22/2022 01:50:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.50 on epoch=161
05/22/2022 01:50:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=163
05/22/2022 01:50:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=164
05/22/2022 01:50:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=166
05/22/2022 01:50:20 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.1881810228266921 on epoch=166
05/22/2022 01:50:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.53 on epoch=168
05/22/2022 01:50:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=169
05/22/2022 01:50:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.49 on epoch=171
05/22/2022 01:50:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.47 on epoch=173
05/22/2022 01:50:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=174
05/22/2022 01:50:36 - INFO - __main__ - Global step 1050 Train loss 0.49 Classification-F1 0.33023294788000673 on epoch=174
05/22/2022 01:50:36 - INFO - __main__ - Saving model with best Classification-F1: 0.31216931216931215 -> 0.33023294788000673 on epoch=174, global_step=1050
05/22/2022 01:50:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.52 on epoch=176
05/22/2022 01:50:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.54 on epoch=178
05/22/2022 01:50:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.50 on epoch=179
05/22/2022 01:50:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=181
05/22/2022 01:50:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.48 on epoch=183
05/22/2022 01:50:52 - INFO - __main__ - Global step 1100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=183
05/22/2022 01:50:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=184
05/22/2022 01:50:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=186
05/22/2022 01:51:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.47 on epoch=188
05/22/2022 01:51:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=189
05/22/2022 01:51:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=191
05/22/2022 01:51:08 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=191
05/22/2022 01:51:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.47 on epoch=193
05/22/2022 01:51:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=194
05/22/2022 01:51:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=196
05/22/2022 01:51:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=198
05/22/2022 01:51:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=199
05/22/2022 01:51:24 - INFO - __main__ - Global step 1200 Train loss 0.47 Classification-F1 0.21509656494176618 on epoch=199
05/22/2022 01:51:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=201
05/22/2022 01:51:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=203
05/22/2022 01:51:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=204
05/22/2022 01:51:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=206
05/22/2022 01:51:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.50 on epoch=208
05/22/2022 01:51:40 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.28853019661279605 on epoch=208
05/22/2022 01:51:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=209
05/22/2022 01:51:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=211
05/22/2022 01:51:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=213
05/22/2022 01:51:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=214
05/22/2022 01:51:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=216
05/22/2022 01:51:56 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.21039809863339273 on epoch=216
05/22/2022 01:51:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=218
05/22/2022 01:52:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.47 on epoch=219
05/22/2022 01:52:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=221
05/22/2022 01:52:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=223
05/22/2022 01:52:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=224
05/22/2022 01:52:12 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.2683473389355742 on epoch=224
05/22/2022 01:52:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=226
05/22/2022 01:52:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=228
05/22/2022 01:52:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=229
05/22/2022 01:52:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=231
05/22/2022 01:52:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=233
05/22/2022 01:52:28 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.3011564924608403 on epoch=233
05/22/2022 01:52:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=234
05/22/2022 01:52:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=236
05/22/2022 01:52:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=238
05/22/2022 01:52:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=239
05/22/2022 01:52:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=241
05/22/2022 01:52:45 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.21931789892954942 on epoch=241
05/22/2022 01:52:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=243
05/22/2022 01:52:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=244
05/22/2022 01:52:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=246
05/22/2022 01:52:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=248
05/22/2022 01:52:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=249
05/22/2022 01:53:01 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.2508417508417508 on epoch=249
05/22/2022 01:53:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=251
05/22/2022 01:53:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=253
05/22/2022 01:53:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.45 on epoch=254
05/22/2022 01:53:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=256
05/22/2022 01:53:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=258
05/22/2022 01:53:17 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.28299319727891153 on epoch=258
05/22/2022 01:53:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=259
05/22/2022 01:53:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=261
05/22/2022 01:53:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=263
05/22/2022 01:53:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=264
05/22/2022 01:53:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=266
05/22/2022 01:53:33 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.31816902911585215 on epoch=266
05/22/2022 01:53:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=268
05/22/2022 01:53:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.33 on epoch=269
05/22/2022 01:53:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=271
05/22/2022 01:53:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=273
05/22/2022 01:53:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=274
05/22/2022 01:53:49 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.2959695640616693 on epoch=274
05/22/2022 01:53:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=276
05/22/2022 01:53:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=278
05/22/2022 01:53:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=279
05/22/2022 01:53:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=281
05/22/2022 01:54:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=283
05/22/2022 01:54:05 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.3058974358974359 on epoch=283
05/22/2022 01:54:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=284
05/22/2022 01:54:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=286
05/22/2022 01:54:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=288
05/22/2022 01:54:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=289
05/22/2022 01:54:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=291
05/22/2022 01:54:21 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.3123479694908266 on epoch=291
05/22/2022 01:54:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=293
05/22/2022 01:54:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=294
05/22/2022 01:54:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=296
05/22/2022 01:54:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=298
05/22/2022 01:54:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.31 on epoch=299
05/22/2022 01:54:37 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.265993265993266 on epoch=299
05/22/2022 01:54:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=301
05/22/2022 01:54:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=303
05/22/2022 01:54:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=304
05/22/2022 01:54:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=306
05/22/2022 01:54:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.33 on epoch=308
05/22/2022 01:54:53 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.28291858453601315 on epoch=308
05/22/2022 01:54:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=309
05/22/2022 01:54:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=311
05/22/2022 01:55:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=313
05/22/2022 01:55:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.29 on epoch=314
05/22/2022 01:55:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.29 on epoch=316
05/22/2022 01:55:09 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.3198261821143177 on epoch=316
05/22/2022 01:55:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.44 on epoch=318
05/22/2022 01:55:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=319
05/22/2022 01:55:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.32 on epoch=321
05/22/2022 01:55:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=323
05/22/2022 01:55:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=324
05/22/2022 01:55:25 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3294333807491702 on epoch=324
05/22/2022 01:55:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.31 on epoch=326
05/22/2022 01:55:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=328
05/22/2022 01:55:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=329
05/22/2022 01:55:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=331
05/22/2022 01:55:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=333
05/22/2022 01:55:41 - INFO - __main__ - Global step 2000 Train loss 0.34 Classification-F1 0.30361586883326014 on epoch=333
05/22/2022 01:55:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.36 on epoch=334
05/22/2022 01:55:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.28 on epoch=336
05/22/2022 01:55:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.35 on epoch=338
05/22/2022 01:55:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.32 on epoch=339
05/22/2022 01:55:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.31 on epoch=341
05/22/2022 01:55:57 - INFO - __main__ - Global step 2050 Train loss 0.32 Classification-F1 0.28697306199816414 on epoch=341
05/22/2022 01:55:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=343
05/22/2022 01:56:02 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.32 on epoch=344
05/22/2022 01:56:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.27 on epoch=346
05/22/2022 01:56:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.33 on epoch=348
05/22/2022 01:56:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.32 on epoch=349
05/22/2022 01:56:13 - INFO - __main__ - Global step 2100 Train loss 0.29 Classification-F1 0.31975265497893485 on epoch=349
05/22/2022 01:56:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.32 on epoch=351
05/22/2022 01:56:18 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.32 on epoch=353
05/22/2022 01:56:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.32 on epoch=354
05/22/2022 01:56:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.27 on epoch=356
05/22/2022 01:56:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.28 on epoch=358
05/22/2022 01:56:29 - INFO - __main__ - Global step 2150 Train loss 0.30 Classification-F1 0.3277340717512363 on epoch=358
05/22/2022 01:56:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.31 on epoch=359
05/22/2022 01:56:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.29 on epoch=361
05/22/2022 01:56:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.27 on epoch=363
05/22/2022 01:56:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.28 on epoch=364
05/22/2022 01:56:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.28 on epoch=366
05/22/2022 01:56:45 - INFO - __main__ - Global step 2200 Train loss 0.28 Classification-F1 0.28469560999272375 on epoch=366
05/22/2022 01:56:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.21 on epoch=368
05/22/2022 01:56:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=369
05/22/2022 01:56:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.30 on epoch=371
05/22/2022 01:56:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.25 on epoch=373
05/22/2022 01:56:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.27 on epoch=374
05/22/2022 01:57:01 - INFO - __main__ - Global step 2250 Train loss 0.26 Classification-F1 0.29148992930505535 on epoch=374
05/22/2022 01:57:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.29 on epoch=376
05/22/2022 01:57:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.30 on epoch=378
05/22/2022 01:57:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.34 on epoch=379
05/22/2022 01:57:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.25 on epoch=381
05/22/2022 01:57:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.25 on epoch=383
05/22/2022 01:57:17 - INFO - __main__ - Global step 2300 Train loss 0.29 Classification-F1 0.30749863163656266 on epoch=383
05/22/2022 01:57:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.25 on epoch=384
05/22/2022 01:57:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.27 on epoch=386
05/22/2022 01:57:25 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.23 on epoch=388
05/22/2022 01:57:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.32 on epoch=389
05/22/2022 01:57:30 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.30 on epoch=391
05/22/2022 01:57:33 - INFO - __main__ - Global step 2350 Train loss 0.27 Classification-F1 0.28887182112988563 on epoch=391
05/22/2022 01:57:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.32 on epoch=393
05/22/2022 01:57:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.28 on epoch=394
05/22/2022 01:57:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.26 on epoch=396
05/22/2022 01:57:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.25 on epoch=398
05/22/2022 01:57:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=399
05/22/2022 01:57:49 - INFO - __main__ - Global step 2400 Train loss 0.27 Classification-F1 0.29014517684468427 on epoch=399
05/22/2022 01:57:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=401
05/22/2022 01:57:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.31 on epoch=403
05/22/2022 01:57:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.24 on epoch=404
05/22/2022 01:58:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.26 on epoch=406
05/22/2022 01:58:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.28 on epoch=408
05/22/2022 01:58:05 - INFO - __main__ - Global step 2450 Train loss 0.27 Classification-F1 0.3368370759675107 on epoch=408
05/22/2022 01:58:05 - INFO - __main__ - Saving model with best Classification-F1: 0.33023294788000673 -> 0.3368370759675107 on epoch=408, global_step=2450
05/22/2022 01:58:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=409
05/22/2022 01:58:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.31 on epoch=411
05/22/2022 01:58:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.32 on epoch=413
05/22/2022 01:58:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.30 on epoch=414
05/22/2022 01:58:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.24 on epoch=416
05/22/2022 01:58:21 - INFO - __main__ - Global step 2500 Train loss 0.28 Classification-F1 0.3544218602772917 on epoch=416
05/22/2022 01:58:21 - INFO - __main__ - Saving model with best Classification-F1: 0.3368370759675107 -> 0.3544218602772917 on epoch=416, global_step=2500
05/22/2022 01:58:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.27 on epoch=418
05/22/2022 01:58:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.23 on epoch=419
05/22/2022 01:58:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.27 on epoch=421
05/22/2022 01:58:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.25 on epoch=423
05/22/2022 01:58:34 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.28 on epoch=424
05/22/2022 01:58:37 - INFO - __main__ - Global step 2550 Train loss 0.26 Classification-F1 0.2658617337950307 on epoch=424
05/22/2022 01:58:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.25 on epoch=426
05/22/2022 01:58:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.26 on epoch=428
05/22/2022 01:58:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.24 on epoch=429
05/22/2022 01:58:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.23 on epoch=431
05/22/2022 01:58:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.27 on epoch=433
05/22/2022 01:58:53 - INFO - __main__ - Global step 2600 Train loss 0.25 Classification-F1 0.29057239057239054 on epoch=433
05/22/2022 01:58:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.27 on epoch=434
05/22/2022 01:58:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.26 on epoch=436
05/22/2022 01:59:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.31 on epoch=438
05/22/2022 01:59:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.28 on epoch=439
05/22/2022 01:59:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.26 on epoch=441
05/22/2022 01:59:09 - INFO - __main__ - Global step 2650 Train loss 0.28 Classification-F1 0.2760064412238325 on epoch=441
05/22/2022 01:59:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.23 on epoch=443
05/22/2022 01:59:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.27 on epoch=444
05/22/2022 01:59:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.21 on epoch=446
05/22/2022 01:59:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.27 on epoch=448
05/22/2022 01:59:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.28 on epoch=449
05/22/2022 01:59:25 - INFO - __main__ - Global step 2700 Train loss 0.25 Classification-F1 0.2751007341171276 on epoch=449
05/22/2022 01:59:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.26 on epoch=451
05/22/2022 01:59:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.26 on epoch=453
05/22/2022 01:59:33 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=454
05/22/2022 01:59:36 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=456
05/22/2022 01:59:39 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.24 on epoch=458
05/22/2022 01:59:42 - INFO - __main__ - Global step 2750 Train loss 0.23 Classification-F1 0.33070719881119376 on epoch=458
05/22/2022 01:59:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.27 on epoch=459
05/22/2022 01:59:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.20 on epoch=461
05/22/2022 01:59:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.23 on epoch=463
05/22/2022 01:59:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.22 on epoch=464
05/22/2022 01:59:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.23 on epoch=466
05/22/2022 01:59:58 - INFO - __main__ - Global step 2800 Train loss 0.23 Classification-F1 0.27995642701525053 on epoch=466
05/22/2022 02:00:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=468
05/22/2022 02:00:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.24 on epoch=469
05/22/2022 02:00:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.21 on epoch=471
05/22/2022 02:00:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.25 on epoch=473
05/22/2022 02:00:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.21 on epoch=474
05/22/2022 02:00:14 - INFO - __main__ - Global step 2850 Train loss 0.22 Classification-F1 0.2826854149434795 on epoch=474
05/22/2022 02:00:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.26 on epoch=476
05/22/2022 02:00:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.24 on epoch=478
05/22/2022 02:00:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.25 on epoch=479
05/22/2022 02:00:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.16 on epoch=481
05/22/2022 02:00:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.24 on epoch=483
05/22/2022 02:00:30 - INFO - __main__ - Global step 2900 Train loss 0.23 Classification-F1 0.2740097666847531 on epoch=483
05/22/2022 02:00:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=484
05/22/2022 02:00:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.24 on epoch=486
05/22/2022 02:00:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.26 on epoch=488
05/22/2022 02:00:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.19 on epoch=489
05/22/2022 02:00:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=491
05/22/2022 02:00:46 - INFO - __main__ - Global step 2950 Train loss 0.21 Classification-F1 0.25540194572452635 on epoch=491
05/22/2022 02:00:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.18 on epoch=493
05/22/2022 02:00:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.19 on epoch=494
05/22/2022 02:00:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.25 on epoch=496
05/22/2022 02:00:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=498
05/22/2022 02:00:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=499
05/22/2022 02:01:00 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:01:00 - INFO - __main__ - Printing 3 examples
05/22/2022 02:01:00 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/22/2022 02:01:00 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:00 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/22/2022 02:01:00 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:00 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/22/2022 02:01:00 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:00 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:01:00 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:01:00 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 02:01:00 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:01:00 - INFO - __main__ - Printing 3 examples
05/22/2022 02:01:00 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/22/2022 02:01:00 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:00 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/22/2022 02:01:00 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:00 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/22/2022 02:01:00 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:00 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:01:01 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:01:01 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 02:01:02 - INFO - __main__ - Global step 3000 Train loss 0.19 Classification-F1 0.2689140753656883 on epoch=499
05/22/2022 02:01:02 - INFO - __main__ - save last model!
05/22/2022 02:01:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 02:01:02 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 02:01:02 - INFO - __main__ - Printing 3 examples
05/22/2022 02:01:02 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 02:01:02 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:02 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 02:01:02 - INFO - __main__ - ['entailment']
05/22/2022 02:01:02 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 02:01:02 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:02 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:01:03 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:01:04 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 02:01:19 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 02:01:19 - INFO - __main__ - task name: anli
05/22/2022 02:01:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 02:01:20 - INFO - __main__ - Starting training!
05/22/2022 02:01:33 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_42_0.2_8_predictions.txt
05/22/2022 02:01:33 - INFO - __main__ - Classification-F1 on test data: 0.2641
05/22/2022 02:01:33 - INFO - __main__ - prefix=anli_32_42, lr=0.2, bsz=8, dev_performance=0.3544218602772917, test_performance=0.2641417482037356
05/22/2022 02:01:34 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.5, bsz=8 ...
05/22/2022 02:01:34 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:01:34 - INFO - __main__ - Printing 3 examples
05/22/2022 02:01:34 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/22/2022 02:01:34 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:34 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/22/2022 02:01:34 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:34 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/22/2022 02:01:34 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:34 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:01:35 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:01:35 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 02:01:35 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:01:35 - INFO - __main__ - Printing 3 examples
05/22/2022 02:01:35 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/22/2022 02:01:35 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:35 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/22/2022 02:01:35 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:35 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/22/2022 02:01:35 - INFO - __main__ - ['contradiction']
05/22/2022 02:01:35 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:01:35 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:01:35 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 02:01:49 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 02:01:49 - INFO - __main__ - task name: anli
05/22/2022 02:01:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 02:01:50 - INFO - __main__ - Starting training!
05/22/2022 02:01:53 - INFO - __main__ - Step 10 Global step 10 Train loss 5.82 on epoch=1
05/22/2022 02:01:56 - INFO - __main__ - Step 20 Global step 20 Train loss 2.21 on epoch=3
05/22/2022 02:01:58 - INFO - __main__ - Step 30 Global step 30 Train loss 1.77 on epoch=4
05/22/2022 02:02:01 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=6
05/22/2022 02:02:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.80 on epoch=8
05/22/2022 02:02:06 - INFO - __main__ - Global step 50 Train loss 2.33 Classification-F1 0.16666666666666666 on epoch=8
05/22/2022 02:02:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/22/2022 02:02:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.74 on epoch=9
05/22/2022 02:02:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.64 on epoch=11
05/22/2022 02:02:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=13
05/22/2022 02:02:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=14
05/22/2022 02:02:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=16
05/22/2022 02:02:23 - INFO - __main__ - Global step 100 Train loss 0.63 Classification-F1 0.36550256705927575 on epoch=16
05/22/2022 02:02:23 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.36550256705927575 on epoch=16, global_step=100
05/22/2022 02:02:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.57 on epoch=18
05/22/2022 02:02:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.60 on epoch=19
05/22/2022 02:02:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=21
05/22/2022 02:02:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=23
05/22/2022 02:02:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=24
05/22/2022 02:02:39 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.275797373358349 on epoch=24
05/22/2022 02:02:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=26
05/22/2022 02:02:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=28
05/22/2022 02:02:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=29
05/22/2022 02:02:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=31
05/22/2022 02:02:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=33
05/22/2022 02:02:55 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.2566524956699732 on epoch=33
05/22/2022 02:02:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=34
05/22/2022 02:03:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=36
05/22/2022 02:03:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=38
05/22/2022 02:03:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=39
05/22/2022 02:03:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=41
05/22/2022 02:03:11 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=41
05/22/2022 02:03:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=43
05/22/2022 02:03:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=44
05/22/2022 02:03:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=46
05/22/2022 02:03:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=48
05/22/2022 02:03:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=49
05/22/2022 02:03:27 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=49
05/22/2022 02:03:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=51
05/22/2022 02:03:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=53
05/22/2022 02:03:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=54
05/22/2022 02:03:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=56
05/22/2022 02:03:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=58
05/22/2022 02:03:43 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.2426739926739927 on epoch=58
05/22/2022 02:03:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=59
05/22/2022 02:03:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=61
05/22/2022 02:03:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=63
05/22/2022 02:03:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=64
05/22/2022 02:03:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=66
05/22/2022 02:03:59 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.24560848235480295 on epoch=66
05/22/2022 02:04:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
05/22/2022 02:04:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=69
05/22/2022 02:04:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=71
05/22/2022 02:04:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=73
05/22/2022 02:04:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=74
05/22/2022 02:04:15 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.21665568007031422 on epoch=74
05/22/2022 02:04:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=76
05/22/2022 02:04:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=78
05/22/2022 02:04:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=79
05/22/2022 02:04:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=81
05/22/2022 02:04:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=83
05/22/2022 02:04:31 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.255026455026455 on epoch=83
05/22/2022 02:04:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=84
05/22/2022 02:04:37 - INFO - __main__ - Step 520 Global step 520 Train loss 2.93 on epoch=86
05/22/2022 02:04:40 - INFO - __main__ - Step 530 Global step 530 Train loss 1.36 on epoch=88
05/22/2022 02:04:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=89
05/22/2022 02:04:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=91
05/22/2022 02:04:48 - INFO - __main__ - Global step 550 Train loss 1.14 Classification-F1 0.23410986482599946 on epoch=91
05/22/2022 02:04:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=93
05/22/2022 02:04:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=94
05/22/2022 02:04:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=96
05/22/2022 02:04:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=98
05/22/2022 02:05:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=99
05/22/2022 02:05:04 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.24680135848300164 on epoch=99
05/22/2022 02:05:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=101
05/22/2022 02:05:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=103
05/22/2022 02:05:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=104
05/22/2022 02:05:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=106
05/22/2022 02:05:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=108
05/22/2022 02:05:20 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.31532944516969236 on epoch=108
05/22/2022 02:05:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=109
05/22/2022 02:05:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=111
05/22/2022 02:05:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=113
05/22/2022 02:05:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=114
05/22/2022 02:05:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=116
05/22/2022 02:05:36 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.2899357317961969 on epoch=116
05/22/2022 02:05:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=118
05/22/2022 02:05:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=119
05/22/2022 02:05:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=121
05/22/2022 02:05:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=123
05/22/2022 02:05:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=124
05/22/2022 02:05:53 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.3721203228173147 on epoch=124
05/22/2022 02:05:53 - INFO - __main__ - Saving model with best Classification-F1: 0.36550256705927575 -> 0.3721203228173147 on epoch=124, global_step=750
05/22/2022 02:05:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=126
05/22/2022 02:05:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=128
05/22/2022 02:06:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=129
05/22/2022 02:06:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=131
05/22/2022 02:06:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=133
05/22/2022 02:06:09 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.34584500466853413 on epoch=133
05/22/2022 02:06:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=134
05/22/2022 02:06:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=136
05/22/2022 02:06:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=138
05/22/2022 02:06:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=139
05/22/2022 02:06:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=141
05/22/2022 02:06:25 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.2764695621838479 on epoch=141
05/22/2022 02:06:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=143
05/22/2022 02:06:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=144
05/22/2022 02:06:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=146
05/22/2022 02:06:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=148
05/22/2022 02:06:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=149
05/22/2022 02:06:41 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.3203473659995399 on epoch=149
05/22/2022 02:06:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=151
05/22/2022 02:06:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=153
05/22/2022 02:06:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=154
05/22/2022 02:06:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=156
05/22/2022 02:06:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=158
05/22/2022 02:06:57 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.3473260970755124 on epoch=158
05/22/2022 02:07:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=159
05/22/2022 02:07:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=161
05/22/2022 02:07:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=163
05/22/2022 02:07:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=164
05/22/2022 02:07:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=166
05/22/2022 02:07:13 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.34988301089996005 on epoch=166
05/22/2022 02:07:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=168
05/22/2022 02:07:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=169
05/22/2022 02:07:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=171
05/22/2022 02:07:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=173
05/22/2022 02:07:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=174
05/22/2022 02:07:30 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.3334073777896249 on epoch=174
05/22/2022 02:07:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=176
05/22/2022 02:07:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=178
05/22/2022 02:07:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=179
05/22/2022 02:07:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=181
05/22/2022 02:07:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=183
05/22/2022 02:07:46 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.25453013989599355 on epoch=183
05/22/2022 02:07:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=184
05/22/2022 02:07:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=186
05/22/2022 02:07:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=188
05/22/2022 02:07:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=189
05/22/2022 02:07:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=191
05/22/2022 02:08:02 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.3598991320666198 on epoch=191
05/22/2022 02:08:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=193
05/22/2022 02:08:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=194
05/22/2022 02:08:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=196
05/22/2022 02:08:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=198
05/22/2022 02:08:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=199
05/22/2022 02:08:18 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.303500872175571 on epoch=199
05/22/2022 02:08:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=201
05/22/2022 02:08:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.49 on epoch=203
05/22/2022 02:08:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=204
05/22/2022 02:08:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=206
05/22/2022 02:08:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=208
05/22/2022 02:08:34 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.31526374859708195 on epoch=208
05/22/2022 02:08:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=209
05/22/2022 02:08:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=211
05/22/2022 02:08:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=213
05/22/2022 02:08:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=214
05/22/2022 02:08:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=216
05/22/2022 02:08:50 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.2818102077736966 on epoch=216
05/22/2022 02:08:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=218
05/22/2022 02:08:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=219
05/22/2022 02:08:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=221
05/22/2022 02:09:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=223
05/22/2022 02:09:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=224
05/22/2022 02:09:06 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.4060931899641577 on epoch=224
05/22/2022 02:09:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3721203228173147 -> 0.4060931899641577 on epoch=224, global_step=1350
05/22/2022 02:09:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=226
05/22/2022 02:09:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=228
05/22/2022 02:09:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=229
05/22/2022 02:09:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=231
05/22/2022 02:09:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=233
05/22/2022 02:09:23 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.31875077303648736 on epoch=233
05/22/2022 02:09:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=234
05/22/2022 02:09:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=236
05/22/2022 02:09:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=238
05/22/2022 02:09:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=239
05/22/2022 02:09:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=241
05/22/2022 02:09:39 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.38726551226551226 on epoch=241
05/22/2022 02:09:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=243
05/22/2022 02:09:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=244
05/22/2022 02:09:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=246
05/22/2022 02:09:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=248
05/22/2022 02:09:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=249
05/22/2022 02:09:55 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3572649572649573 on epoch=249
05/22/2022 02:09:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=251
05/22/2022 02:10:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=253
05/22/2022 02:10:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=254
05/22/2022 02:10:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=256
05/22/2022 02:10:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=258
05/22/2022 02:10:11 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.3028923575187801 on epoch=258
05/22/2022 02:10:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=259
05/22/2022 02:10:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=261
05/22/2022 02:10:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=263
05/22/2022 02:10:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=264
05/22/2022 02:10:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=266
05/22/2022 02:10:27 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.3675213675213675 on epoch=266
05/22/2022 02:10:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=268
05/22/2022 02:10:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=269
05/22/2022 02:10:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=271
05/22/2022 02:10:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=273
05/22/2022 02:10:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=274
05/22/2022 02:10:44 - INFO - __main__ - Global step 1650 Train loss 0.39 Classification-F1 0.2860178647618261 on epoch=274
05/22/2022 02:10:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=276
05/22/2022 02:10:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=278
05/22/2022 02:10:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=279
05/22/2022 02:10:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=281
05/22/2022 02:10:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=283
05/22/2022 02:11:00 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.296256038647343 on epoch=283
05/22/2022 02:11:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=284
05/22/2022 02:11:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=286
05/22/2022 02:11:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=288
05/22/2022 02:11:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=289
05/22/2022 02:11:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=291
05/22/2022 02:11:16 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.37891866851962147 on epoch=291
05/22/2022 02:11:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=293
05/22/2022 02:11:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=294
05/22/2022 02:11:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=296
05/22/2022 02:11:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=298
05/22/2022 02:11:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=299
05/22/2022 02:11:32 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.3571428571428572 on epoch=299
05/22/2022 02:11:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=301
05/22/2022 02:11:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=303
05/22/2022 02:11:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=304
05/22/2022 02:11:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=306
05/22/2022 02:11:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=308
05/22/2022 02:11:48 - INFO - __main__ - Global step 1850 Train loss 0.37 Classification-F1 0.3363258777700384 on epoch=308
05/22/2022 02:11:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.37 on epoch=309
05/22/2022 02:11:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=311
05/22/2022 02:11:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=313
05/22/2022 02:11:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=314
05/22/2022 02:12:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=316
05/22/2022 02:12:05 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.3964287123274368 on epoch=316
05/22/2022 02:12:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=318
05/22/2022 02:12:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=319
05/22/2022 02:12:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=321
05/22/2022 02:12:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=323
05/22/2022 02:12:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=324
05/22/2022 02:12:21 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.38667547142123415 on epoch=324
05/22/2022 02:12:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=326
05/22/2022 02:12:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=328
05/22/2022 02:12:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=329
05/22/2022 02:12:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=331
05/22/2022 02:12:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.43 on epoch=333
05/22/2022 02:12:38 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.3978508132736988 on epoch=333
05/22/2022 02:12:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.35 on epoch=334
05/22/2022 02:12:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=336
05/22/2022 02:12:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.37 on epoch=338
05/22/2022 02:12:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.37 on epoch=339
05/22/2022 02:12:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.37 on epoch=341
05/22/2022 02:12:54 - INFO - __main__ - Global step 2050 Train loss 0.37 Classification-F1 0.2842495126705653 on epoch=341
05/22/2022 02:12:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.39 on epoch=343
05/22/2022 02:12:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.37 on epoch=344
05/22/2022 02:13:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.33 on epoch=346
05/22/2022 02:13:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.39 on epoch=348
05/22/2022 02:13:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.38 on epoch=349
05/22/2022 02:13:10 - INFO - __main__ - Global step 2100 Train loss 0.37 Classification-F1 0.38309729871492265 on epoch=349
05/22/2022 02:13:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.31 on epoch=351
05/22/2022 02:13:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.38 on epoch=353
05/22/2022 02:13:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.36 on epoch=354
05/22/2022 02:13:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.35 on epoch=356
05/22/2022 02:13:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=358
05/22/2022 02:13:26 - INFO - __main__ - Global step 2150 Train loss 0.36 Classification-F1 0.3995925731913161 on epoch=358
05/22/2022 02:13:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.36 on epoch=359
05/22/2022 02:13:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.35 on epoch=361
05/22/2022 02:13:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.36 on epoch=363
05/22/2022 02:13:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.34 on epoch=364
05/22/2022 02:13:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.36 on epoch=366
05/22/2022 02:13:42 - INFO - __main__ - Global step 2200 Train loss 0.35 Classification-F1 0.35600148643626905 on epoch=366
05/22/2022 02:13:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.38 on epoch=368
05/22/2022 02:13:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.38 on epoch=369
05/22/2022 02:13:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.33 on epoch=371
05/22/2022 02:13:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.31 on epoch=373
05/22/2022 02:13:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=374
05/22/2022 02:13:58 - INFO - __main__ - Global step 2250 Train loss 0.35 Classification-F1 0.31388696315903725 on epoch=374
05/22/2022 02:14:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.31 on epoch=376
05/22/2022 02:14:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.33 on epoch=378
05/22/2022 02:14:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.31 on epoch=379
05/22/2022 02:14:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.37 on epoch=381
05/22/2022 02:14:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=383
05/22/2022 02:14:14 - INFO - __main__ - Global step 2300 Train loss 0.34 Classification-F1 0.40374707259953163 on epoch=383
05/22/2022 02:14:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.27 on epoch=384
05/22/2022 02:14:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.40 on epoch=386
05/22/2022 02:14:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.34 on epoch=388
05/22/2022 02:14:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.36 on epoch=389
05/22/2022 02:14:27 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.34 on epoch=391
05/22/2022 02:14:30 - INFO - __main__ - Global step 2350 Train loss 0.34 Classification-F1 0.3540860137245679 on epoch=391
05/22/2022 02:14:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.30 on epoch=393
05/22/2022 02:14:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.34 on epoch=394
05/22/2022 02:14:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.35 on epoch=396
05/22/2022 02:14:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.37 on epoch=398
05/22/2022 02:14:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.37 on epoch=399
05/22/2022 02:14:46 - INFO - __main__ - Global step 2400 Train loss 0.35 Classification-F1 0.41486651157638194 on epoch=399
05/22/2022 02:14:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4060931899641577 -> 0.41486651157638194 on epoch=399, global_step=2400
05/22/2022 02:14:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.29 on epoch=401
05/22/2022 02:14:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.31 on epoch=403
05/22/2022 02:14:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.31 on epoch=404
05/22/2022 02:14:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.33 on epoch=406
05/22/2022 02:14:59 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.32 on epoch=408
05/22/2022 02:15:02 - INFO - __main__ - Global step 2450 Train loss 0.31 Classification-F1 0.30068842993559847 on epoch=408
05/22/2022 02:15:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.39 on epoch=409
05/22/2022 02:15:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.26 on epoch=411
05/22/2022 02:15:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.30 on epoch=413
05/22/2022 02:15:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.30 on epoch=414
05/22/2022 02:15:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.31 on epoch=416
05/22/2022 02:15:17 - INFO - __main__ - Global step 2500 Train loss 0.31 Classification-F1 0.3193705673758865 on epoch=416
05/22/2022 02:15:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.29 on epoch=418
05/22/2022 02:15:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.31 on epoch=419
05/22/2022 02:15:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.26 on epoch=421
05/22/2022 02:15:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.27 on epoch=423
05/22/2022 02:15:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.30 on epoch=424
05/22/2022 02:15:33 - INFO - __main__ - Global step 2550 Train loss 0.28 Classification-F1 0.35611647644132277 on epoch=424
05/22/2022 02:15:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.26 on epoch=426
05/22/2022 02:15:38 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.27 on epoch=428
05/22/2022 02:15:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.31 on epoch=429
05/22/2022 02:15:44 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.32 on epoch=431
05/22/2022 02:15:46 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.32 on epoch=433
05/22/2022 02:15:49 - INFO - __main__ - Global step 2600 Train loss 0.30 Classification-F1 0.45094785131215004 on epoch=433
05/22/2022 02:15:49 - INFO - __main__ - Saving model with best Classification-F1: 0.41486651157638194 -> 0.45094785131215004 on epoch=433, global_step=2600
05/22/2022 02:15:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.28 on epoch=434
05/22/2022 02:15:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.30 on epoch=436
05/22/2022 02:15:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.24 on epoch=438
05/22/2022 02:16:00 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.24 on epoch=439
05/22/2022 02:16:02 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.23 on epoch=441
05/22/2022 02:16:05 - INFO - __main__ - Global step 2650 Train loss 0.26 Classification-F1 0.3243664717348928 on epoch=441
05/22/2022 02:16:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.25 on epoch=443
05/22/2022 02:16:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.27 on epoch=444
05/22/2022 02:16:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.27 on epoch=446
05/22/2022 02:16:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.27 on epoch=448
05/22/2022 02:16:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.25 on epoch=449
05/22/2022 02:16:21 - INFO - __main__ - Global step 2700 Train loss 0.26 Classification-F1 0.3697477700114957 on epoch=449
05/22/2022 02:16:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.27 on epoch=451
05/22/2022 02:16:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.30 on epoch=453
05/22/2022 02:16:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.28 on epoch=454
05/22/2022 02:16:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.26 on epoch=456
05/22/2022 02:16:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.32 on epoch=458
05/22/2022 02:16:37 - INFO - __main__ - Global step 2750 Train loss 0.29 Classification-F1 0.3617722879603567 on epoch=458
05/22/2022 02:16:39 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.26 on epoch=459
05/22/2022 02:16:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.26 on epoch=461
05/22/2022 02:16:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.28 on epoch=463
05/22/2022 02:16:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.27 on epoch=464
05/22/2022 02:16:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=466
05/22/2022 02:16:53 - INFO - __main__ - Global step 2800 Train loss 0.26 Classification-F1 0.4180653430601102 on epoch=466
05/22/2022 02:16:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.29 on epoch=468
05/22/2022 02:16:58 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.21 on epoch=469
05/22/2022 02:17:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.23 on epoch=471
05/22/2022 02:17:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.30 on epoch=473
05/22/2022 02:17:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.22 on epoch=474
05/22/2022 02:17:09 - INFO - __main__ - Global step 2850 Train loss 0.25 Classification-F1 0.2840385580534068 on epoch=474
05/22/2022 02:17:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=476
05/22/2022 02:17:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=478
05/22/2022 02:17:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.24 on epoch=479
05/22/2022 02:17:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.27 on epoch=481
05/22/2022 02:17:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=483
05/22/2022 02:17:24 - INFO - __main__ - Global step 2900 Train loss 0.24 Classification-F1 0.30505464339537397 on epoch=483
05/22/2022 02:17:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=484
05/22/2022 02:17:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.22 on epoch=486
05/22/2022 02:17:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=488
05/22/2022 02:17:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.23 on epoch=489
05/22/2022 02:17:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=491
05/22/2022 02:17:40 - INFO - __main__ - Global step 2950 Train loss 0.19 Classification-F1 0.3750944540966879 on epoch=491
05/22/2022 02:17:43 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.21 on epoch=493
05/22/2022 02:17:45 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.23 on epoch=494
05/22/2022 02:17:48 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.21 on epoch=496
05/22/2022 02:17:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.22 on epoch=498
05/22/2022 02:17:53 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.18 on epoch=499
05/22/2022 02:17:55 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:17:55 - INFO - __main__ - Printing 3 examples
05/22/2022 02:17:55 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/22/2022 02:17:55 - INFO - __main__ - ['contradiction']
05/22/2022 02:17:55 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/22/2022 02:17:55 - INFO - __main__ - ['contradiction']
05/22/2022 02:17:55 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/22/2022 02:17:55 - INFO - __main__ - ['contradiction']
05/22/2022 02:17:55 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:17:55 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:17:55 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 02:17:55 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:17:55 - INFO - __main__ - Printing 3 examples
05/22/2022 02:17:55 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/22/2022 02:17:55 - INFO - __main__ - ['contradiction']
05/22/2022 02:17:55 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/22/2022 02:17:55 - INFO - __main__ - ['contradiction']
05/22/2022 02:17:55 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/22/2022 02:17:55 - INFO - __main__ - ['contradiction']
05/22/2022 02:17:55 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:17:55 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:17:55 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 02:17:56 - INFO - __main__ - Global step 3000 Train loss 0.21 Classification-F1 0.19566960852528656 on epoch=499
05/22/2022 02:17:56 - INFO - __main__ - save last model!
05/22/2022 02:17:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 02:17:56 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 02:17:56 - INFO - __main__ - Printing 3 examples
05/22/2022 02:17:56 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 02:17:56 - INFO - __main__ - ['contradiction']
05/22/2022 02:17:56 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 02:17:56 - INFO - __main__ - ['entailment']
05/22/2022 02:17:56 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 02:17:56 - INFO - __main__ - ['contradiction']
05/22/2022 02:17:56 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:17:57 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:17:58 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 02:18:10 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 02:18:10 - INFO - __main__ - task name: anli
05/22/2022 02:18:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 02:18:11 - INFO - __main__ - Starting training!
05/22/2022 02:18:28 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_87_0.5_8_predictions.txt
05/22/2022 02:18:28 - INFO - __main__ - Classification-F1 on test data: 0.0871
05/22/2022 02:18:29 - INFO - __main__ - prefix=anli_32_87, lr=0.5, bsz=8, dev_performance=0.45094785131215004, test_performance=0.08712393724418838
05/22/2022 02:18:29 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.4, bsz=8 ...
05/22/2022 02:18:30 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:18:30 - INFO - __main__ - Printing 3 examples
05/22/2022 02:18:30 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/22/2022 02:18:30 - INFO - __main__ - ['contradiction']
05/22/2022 02:18:30 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/22/2022 02:18:30 - INFO - __main__ - ['contradiction']
05/22/2022 02:18:30 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/22/2022 02:18:30 - INFO - __main__ - ['contradiction']
05/22/2022 02:18:30 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:18:30 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:18:30 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 02:18:30 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:18:30 - INFO - __main__ - Printing 3 examples
05/22/2022 02:18:30 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/22/2022 02:18:30 - INFO - __main__ - ['contradiction']
05/22/2022 02:18:30 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/22/2022 02:18:30 - INFO - __main__ - ['contradiction']
05/22/2022 02:18:30 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/22/2022 02:18:30 - INFO - __main__ - ['contradiction']
05/22/2022 02:18:30 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:18:30 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:18:30 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 02:18:49 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 02:18:49 - INFO - __main__ - task name: anli
05/22/2022 02:18:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 02:18:50 - INFO - __main__ - Starting training!
05/22/2022 02:18:53 - INFO - __main__ - Step 10 Global step 10 Train loss 6.17 on epoch=1
05/22/2022 02:18:55 - INFO - __main__ - Step 20 Global step 20 Train loss 2.67 on epoch=3
05/22/2022 02:18:58 - INFO - __main__ - Step 30 Global step 30 Train loss 1.15 on epoch=4
05/22/2022 02:19:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.89 on epoch=6
05/22/2022 02:19:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.75 on epoch=8
05/22/2022 02:19:06 - INFO - __main__ - Global step 50 Train loss 2.32 Classification-F1 0.15873015873015875 on epoch=8
05/22/2022 02:19:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15873015873015875 on epoch=8, global_step=50
05/22/2022 02:19:08 - INFO - __main__ - Step 60 Global step 60 Train loss 0.60 on epoch=9
05/22/2022 02:19:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.64 on epoch=11
05/22/2022 02:19:13 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=13
05/22/2022 02:19:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=14
05/22/2022 02:19:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.61 on epoch=16
05/22/2022 02:19:21 - INFO - __main__ - Global step 100 Train loss 0.60 Classification-F1 0.1679790026246719 on epoch=16
05/22/2022 02:19:21 - INFO - __main__ - Saving model with best Classification-F1: 0.15873015873015875 -> 0.1679790026246719 on epoch=16, global_step=100
05/22/2022 02:19:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=18
05/22/2022 02:19:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=19
05/22/2022 02:19:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=21
05/22/2022 02:19:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=23
05/22/2022 02:19:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=24
05/22/2022 02:19:36 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.27872688594338085 on epoch=24
05/22/2022 02:19:37 - INFO - __main__ - Saving model with best Classification-F1: 0.1679790026246719 -> 0.27872688594338085 on epoch=24, global_step=150
05/22/2022 02:19:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=26
05/22/2022 02:19:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=28
05/22/2022 02:19:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=29
05/22/2022 02:19:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=31
05/22/2022 02:19:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=33
05/22/2022 02:19:52 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.20611691831204026 on epoch=33
05/22/2022 02:19:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=34
05/22/2022 02:19:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=36
05/22/2022 02:20:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=38
05/22/2022 02:20:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=39
05/22/2022 02:20:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=41
05/22/2022 02:20:07 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
05/22/2022 02:20:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=43
05/22/2022 02:20:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=44
05/22/2022 02:20:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=46
05/22/2022 02:20:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=48
05/22/2022 02:20:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=49
05/22/2022 02:20:23 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.1679790026246719 on epoch=49
05/22/2022 02:20:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=51
05/22/2022 02:20:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=53
05/22/2022 02:20:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=54
05/22/2022 02:20:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=56
05/22/2022 02:20:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=58
05/22/2022 02:20:39 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.23956799214531177 on epoch=58
05/22/2022 02:20:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=59
05/22/2022 02:20:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=61
05/22/2022 02:20:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=63
05/22/2022 02:20:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=64
05/22/2022 02:20:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=66
05/22/2022 02:20:55 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/22/2022 02:20:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
05/22/2022 02:21:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=69
05/22/2022 02:21:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=71
05/22/2022 02:21:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=73
05/22/2022 02:21:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=74
05/22/2022 02:21:11 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.3474770843191896 on epoch=74
05/22/2022 02:21:11 - INFO - __main__ - Saving model with best Classification-F1: 0.27872688594338085 -> 0.3474770843191896 on epoch=74, global_step=450
05/22/2022 02:21:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=76
05/22/2022 02:21:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=78
05/22/2022 02:21:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=79
05/22/2022 02:21:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=81
05/22/2022 02:21:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=83
05/22/2022 02:21:27 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.22708719851576994 on epoch=83
05/22/2022 02:21:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=84
05/22/2022 02:21:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=86
05/22/2022 02:21:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=88
05/22/2022 02:21:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=89
05/22/2022 02:21:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=91
05/22/2022 02:21:42 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.1843809523809524 on epoch=91
05/22/2022 02:21:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=93
05/22/2022 02:21:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=94
05/22/2022 02:21:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=96
05/22/2022 02:21:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=98
05/22/2022 02:21:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=99
05/22/2022 02:21:58 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.28981224663964156 on epoch=99
05/22/2022 02:22:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=101
05/22/2022 02:22:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=103
05/22/2022 02:22:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=104
05/22/2022 02:22:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=106
05/22/2022 02:22:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=108
05/22/2022 02:22:13 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.16272965879265092 on epoch=108
05/22/2022 02:22:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=109
05/22/2022 02:22:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=111
05/22/2022 02:22:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=113
05/22/2022 02:22:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=114
05/22/2022 02:22:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=116
05/22/2022 02:22:28 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.19355766465343685 on epoch=116
05/22/2022 02:22:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=118
05/22/2022 02:22:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=119
05/22/2022 02:22:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=121
05/22/2022 02:22:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=123
05/22/2022 02:22:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=124
05/22/2022 02:22:43 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.19963866305329722 on epoch=124
05/22/2022 02:22:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=126
05/22/2022 02:22:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=128
05/22/2022 02:22:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=129
05/22/2022 02:22:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=131
05/22/2022 02:22:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=133
05/22/2022 02:22:59 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.3515873015873016 on epoch=133
05/22/2022 02:22:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3474770843191896 -> 0.3515873015873016 on epoch=133, global_step=800
05/22/2022 02:23:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=134
05/22/2022 02:23:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=136
05/22/2022 02:23:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=138
05/22/2022 02:23:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=139
05/22/2022 02:23:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=141
05/22/2022 02:23:15 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.23755716976055954 on epoch=141
05/22/2022 02:23:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=143
05/22/2022 02:23:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=144
05/22/2022 02:23:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=146
05/22/2022 02:23:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=148
05/22/2022 02:23:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=149
05/22/2022 02:23:30 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.22426616703571225 on epoch=149
05/22/2022 02:23:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=151
05/22/2022 02:23:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=153
05/22/2022 02:23:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=154
05/22/2022 02:23:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=156
05/22/2022 02:23:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=158
05/22/2022 02:23:46 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.29121102392067977 on epoch=158
05/22/2022 02:23:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=159
05/22/2022 02:23:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=161
05/22/2022 02:23:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=163
05/22/2022 02:23:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=164
05/22/2022 02:23:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=166
05/22/2022 02:24:01 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.2085278555866791 on epoch=166
05/22/2022 02:24:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=168
05/22/2022 02:24:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=169
05/22/2022 02:24:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=171
05/22/2022 02:24:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=173
05/22/2022 02:24:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=174
05/22/2022 02:24:17 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.2798388472169931 on epoch=174
05/22/2022 02:24:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=176
05/22/2022 02:24:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=178
05/22/2022 02:24:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=179
05/22/2022 02:24:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.33 on epoch=181
05/22/2022 02:24:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=183
05/22/2022 02:24:33 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.3695774647887324 on epoch=183
05/22/2022 02:24:33 - INFO - __main__ - Saving model with best Classification-F1: 0.3515873015873016 -> 0.3695774647887324 on epoch=183, global_step=1100
05/22/2022 02:24:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=184
05/22/2022 02:24:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=186
05/22/2022 02:24:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=188
05/22/2022 02:24:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=189
05/22/2022 02:24:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=191
05/22/2022 02:24:49 - INFO - __main__ - Global step 1150 Train loss 0.35 Classification-F1 0.23843892007484627 on epoch=191
05/22/2022 02:24:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=193
05/22/2022 02:24:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=194
05/22/2022 02:24:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=196
05/22/2022 02:24:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=198
05/22/2022 02:25:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.28 on epoch=199
05/22/2022 02:25:05 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.36388888888888893 on epoch=199
05/22/2022 02:25:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=201
05/22/2022 02:25:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=203
05/22/2022 02:25:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=204
05/22/2022 02:25:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.28 on epoch=206
05/22/2022 02:25:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.30 on epoch=208
05/22/2022 02:25:20 - INFO - __main__ - Global step 1250 Train loss 0.30 Classification-F1 0.3607782898105479 on epoch=208
05/22/2022 02:25:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=209
05/22/2022 02:25:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=211
05/22/2022 02:25:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=213
05/22/2022 02:25:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=214
05/22/2022 02:25:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=216
05/22/2022 02:25:36 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.3072344322344322 on epoch=216
05/22/2022 02:25:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=218
05/22/2022 02:25:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.24 on epoch=219
05/22/2022 02:25:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=221
05/22/2022 02:25:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=223
05/22/2022 02:25:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=224
05/22/2022 02:25:52 - INFO - __main__ - Global step 1350 Train loss 0.27 Classification-F1 0.30271147631262413 on epoch=224
05/22/2022 02:25:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=226
05/22/2022 02:25:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=228
05/22/2022 02:26:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=229
05/22/2022 02:26:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=231
05/22/2022 02:26:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=233
05/22/2022 02:26:08 - INFO - __main__ - Global step 1400 Train loss 0.29 Classification-F1 0.4464388917219106 on epoch=233
05/22/2022 02:26:08 - INFO - __main__ - Saving model with best Classification-F1: 0.3695774647887324 -> 0.4464388917219106 on epoch=233, global_step=1400
05/22/2022 02:26:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=234
05/22/2022 02:26:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=236
05/22/2022 02:26:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=238
05/22/2022 02:26:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=239
05/22/2022 02:26:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=241
05/22/2022 02:26:24 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.31295256020930556 on epoch=241
05/22/2022 02:26:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=243
05/22/2022 02:26:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=244
05/22/2022 02:26:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=246
05/22/2022 02:26:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.24 on epoch=248
05/22/2022 02:26:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=249
05/22/2022 02:26:40 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.3434343434343434 on epoch=249
05/22/2022 02:26:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=251
05/22/2022 02:26:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=253
05/22/2022 02:26:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=254
05/22/2022 02:26:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=256
05/22/2022 02:26:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=258
05/22/2022 02:26:56 - INFO - __main__ - Global step 1550 Train loss 0.25 Classification-F1 0.3377516523850919 on epoch=258
05/22/2022 02:26:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=259
05/22/2022 02:27:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=261
05/22/2022 02:27:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=263
05/22/2022 02:27:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=264
05/22/2022 02:27:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=266
05/22/2022 02:27:12 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.35376220837213745 on epoch=266
05/22/2022 02:27:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=268
05/22/2022 02:27:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=269
05/22/2022 02:27:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=271
05/22/2022 02:27:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=273
05/22/2022 02:27:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=274
05/22/2022 02:27:28 - INFO - __main__ - Global step 1650 Train loss 0.22 Classification-F1 0.28580676572908253 on epoch=274
05/22/2022 02:27:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=276
05/22/2022 02:27:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=278
05/22/2022 02:27:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.24 on epoch=279
05/22/2022 02:27:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.24 on epoch=281
05/22/2022 02:27:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=283
05/22/2022 02:27:44 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.36092486023851844 on epoch=283
05/22/2022 02:27:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.25 on epoch=284
05/22/2022 02:27:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=286
05/22/2022 02:27:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=288
05/22/2022 02:27:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=289
05/22/2022 02:27:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=291
05/22/2022 02:27:59 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.29069414173436364 on epoch=291
05/22/2022 02:28:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=293
05/22/2022 02:28:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.16 on epoch=294
05/22/2022 02:28:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=296
05/22/2022 02:28:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.16 on epoch=298
05/22/2022 02:28:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=299
05/22/2022 02:28:15 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.3168198110133594 on epoch=299
05/22/2022 02:28:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=301
05/22/2022 02:28:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=303
05/22/2022 02:28:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=304
05/22/2022 02:28:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=306
05/22/2022 02:28:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=308
05/22/2022 02:28:31 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.26165501165501165 on epoch=308
05/22/2022 02:28:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=309
05/22/2022 02:28:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=311
05/22/2022 02:28:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=313
05/22/2022 02:28:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=314
05/22/2022 02:28:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.15 on epoch=316
05/22/2022 02:28:47 - INFO - __main__ - Global step 1900 Train loss 0.15 Classification-F1 0.22290759554910497 on epoch=316
05/22/2022 02:28:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=318
05/22/2022 02:28:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=319
05/22/2022 02:28:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=321
05/22/2022 02:28:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=323
05/22/2022 02:29:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=324
05/22/2022 02:29:03 - INFO - __main__ - Global step 1950 Train loss 0.14 Classification-F1 0.2672832745121902 on epoch=324
05/22/2022 02:29:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.11 on epoch=326
05/22/2022 02:29:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=328
05/22/2022 02:29:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=329
05/22/2022 02:29:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=331
05/22/2022 02:29:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=333
05/22/2022 02:29:19 - INFO - __main__ - Global step 2000 Train loss 0.13 Classification-F1 0.33918654077541865 on epoch=333
05/22/2022 02:29:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=334
05/22/2022 02:29:24 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=336
05/22/2022 02:29:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=338
05/22/2022 02:29:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=339
05/22/2022 02:29:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=341
05/22/2022 02:29:35 - INFO - __main__ - Global step 2050 Train loss 0.13 Classification-F1 0.30042849058129195 on epoch=341
05/22/2022 02:29:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=343
05/22/2022 02:29:40 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=344
05/22/2022 02:29:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=346
05/22/2022 02:29:45 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=348
05/22/2022 02:29:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=349
05/22/2022 02:29:51 - INFO - __main__ - Global step 2100 Train loss 0.11 Classification-F1 0.3013564645100435 on epoch=349
05/22/2022 02:29:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.13 on epoch=351
05/22/2022 02:29:56 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=353
05/22/2022 02:29:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=354
05/22/2022 02:30:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=356
05/22/2022 02:30:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=358
05/22/2022 02:30:07 - INFO - __main__ - Global step 2150 Train loss 0.12 Classification-F1 0.16133078001941956 on epoch=358
05/22/2022 02:30:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.11 on epoch=359
05/22/2022 02:30:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=361
05/22/2022 02:30:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=363
05/22/2022 02:30:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=364
05/22/2022 02:30:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=366
05/22/2022 02:30:22 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.2292099792099792 on epoch=366
05/22/2022 02:30:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=368
05/22/2022 02:30:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=369
05/22/2022 02:30:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=371
05/22/2022 02:30:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=373
05/22/2022 02:30:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=374
05/22/2022 02:30:38 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.32326087012323623 on epoch=374
05/22/2022 02:30:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=376
05/22/2022 02:30:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=378
05/22/2022 02:30:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=379
05/22/2022 02:30:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=381
05/22/2022 02:30:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=383
05/22/2022 02:30:54 - INFO - __main__ - Global step 2300 Train loss 0.10 Classification-F1 0.3026640044609933 on epoch=383
05/22/2022 02:30:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=384
05/22/2022 02:31:00 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=386
05/22/2022 02:31:02 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=388
05/22/2022 02:31:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=389
05/22/2022 02:31:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=391
05/22/2022 02:31:10 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.22102950321919812 on epoch=391
05/22/2022 02:31:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=393
05/22/2022 02:31:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=394
05/22/2022 02:31:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=396
05/22/2022 02:31:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=398
05/22/2022 02:31:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=399
05/22/2022 02:31:26 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.2443722943722944 on epoch=399
05/22/2022 02:31:29 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=401
05/22/2022 02:31:31 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=403
05/22/2022 02:31:34 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=404
05/22/2022 02:31:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=406
05/22/2022 02:31:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=408
05/22/2022 02:31:42 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.2114695340501792 on epoch=408
05/22/2022 02:31:45 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=409
05/22/2022 02:31:47 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=411
05/22/2022 02:31:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=413
05/22/2022 02:31:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=414
05/22/2022 02:31:55 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=416
05/22/2022 02:31:58 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.22384147118816927 on epoch=416
05/22/2022 02:32:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=418
05/22/2022 02:32:03 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=419
05/22/2022 02:32:06 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=421
05/22/2022 02:32:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=423
05/22/2022 02:32:11 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=424
05/22/2022 02:32:14 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.1755777163682967 on epoch=424
05/22/2022 02:32:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=426
05/22/2022 02:32:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=428
05/22/2022 02:32:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=429
05/22/2022 02:32:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=431
05/22/2022 02:32:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=433
05/22/2022 02:32:30 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.217985347985348 on epoch=433
05/22/2022 02:32:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=434
05/22/2022 02:32:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=436
05/22/2022 02:32:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=438
05/22/2022 02:32:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=439
05/22/2022 02:32:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=441
05/22/2022 02:32:47 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.17922836352929106 on epoch=441
05/22/2022 02:32:49 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=443
05/22/2022 02:32:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=444
05/22/2022 02:32:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=446
05/22/2022 02:32:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=448
05/22/2022 02:33:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=449
05/22/2022 02:33:03 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.17280312755280663 on epoch=449
05/22/2022 02:33:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=451
05/22/2022 02:33:08 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=453
05/22/2022 02:33:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=454
05/22/2022 02:33:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=456
05/22/2022 02:33:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=458
05/22/2022 02:33:19 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.1682330827067669 on epoch=458
05/22/2022 02:33:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=459
05/22/2022 02:33:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=461
05/22/2022 02:33:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=463
05/22/2022 02:33:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=464
05/22/2022 02:33:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=466
05/22/2022 02:33:35 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.2299969713890913 on epoch=466
05/22/2022 02:33:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=468
05/22/2022 02:33:40 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=469
05/22/2022 02:33:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=471
05/22/2022 02:33:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=473
05/22/2022 02:33:48 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=474
05/22/2022 02:33:51 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.11265394565193357 on epoch=474
05/22/2022 02:33:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=476
05/22/2022 02:33:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=478
05/22/2022 02:33:59 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=479
05/22/2022 02:34:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=481
05/22/2022 02:34:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=483
05/22/2022 02:34:07 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.09153541008870358 on epoch=483
05/22/2022 02:34:10 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=484
05/22/2022 02:34:13 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=486
05/22/2022 02:34:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=488
05/22/2022 02:34:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=489
05/22/2022 02:34:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=491
05/22/2022 02:34:23 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.11896040467469038 on epoch=491
05/22/2022 02:34:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=493
05/22/2022 02:34:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=494
05/22/2022 02:34:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=496
05/22/2022 02:34:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=498
05/22/2022 02:34:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=499
05/22/2022 02:34:38 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:34:38 - INFO - __main__ - Printing 3 examples
05/22/2022 02:34:38 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/22/2022 02:34:38 - INFO - __main__ - ['contradiction']
05/22/2022 02:34:38 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/22/2022 02:34:38 - INFO - __main__ - ['contradiction']
05/22/2022 02:34:38 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/22/2022 02:34:38 - INFO - __main__ - ['contradiction']
05/22/2022 02:34:38 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:34:38 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:34:38 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 02:34:38 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:34:38 - INFO - __main__ - Printing 3 examples
05/22/2022 02:34:38 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/22/2022 02:34:38 - INFO - __main__ - ['contradiction']
05/22/2022 02:34:38 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/22/2022 02:34:38 - INFO - __main__ - ['contradiction']
05/22/2022 02:34:38 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/22/2022 02:34:38 - INFO - __main__ - ['contradiction']
05/22/2022 02:34:38 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:34:38 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:34:38 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 02:34:39 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.17045454545454544 on epoch=499
05/22/2022 02:34:39 - INFO - __main__ - save last model!
05/22/2022 02:34:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 02:34:39 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 02:34:39 - INFO - __main__ - Printing 3 examples
05/22/2022 02:34:39 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 02:34:39 - INFO - __main__ - ['contradiction']
05/22/2022 02:34:39 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 02:34:39 - INFO - __main__ - ['entailment']
05/22/2022 02:34:39 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 02:34:39 - INFO - __main__ - ['contradiction']
05/22/2022 02:34:39 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:34:39 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:34:40 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 02:34:57 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 02:34:57 - INFO - __main__ - task name: anli
05/22/2022 02:34:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 02:34:57 - INFO - __main__ - Starting training!
05/22/2022 02:35:03 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_87_0.4_8_predictions.txt
05/22/2022 02:35:03 - INFO - __main__ - Classification-F1 on test data: 0.0828
05/22/2022 02:35:03 - INFO - __main__ - prefix=anli_32_87, lr=0.4, bsz=8, dev_performance=0.4464388917219106, test_performance=0.08279314604667806
05/22/2022 02:35:03 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.3, bsz=8 ...
05/22/2022 02:35:04 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:35:04 - INFO - __main__ - Printing 3 examples
05/22/2022 02:35:04 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/22/2022 02:35:04 - INFO - __main__ - ['contradiction']
05/22/2022 02:35:04 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/22/2022 02:35:04 - INFO - __main__ - ['contradiction']
05/22/2022 02:35:04 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/22/2022 02:35:04 - INFO - __main__ - ['contradiction']
05/22/2022 02:35:04 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:35:04 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:35:04 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 02:35:04 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:35:04 - INFO - __main__ - Printing 3 examples
05/22/2022 02:35:04 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/22/2022 02:35:04 - INFO - __main__ - ['contradiction']
05/22/2022 02:35:04 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/22/2022 02:35:04 - INFO - __main__ - ['contradiction']
05/22/2022 02:35:04 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/22/2022 02:35:04 - INFO - __main__ - ['contradiction']
05/22/2022 02:35:04 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:35:04 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:35:04 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 02:35:23 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 02:35:23 - INFO - __main__ - task name: anli
05/22/2022 02:35:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 02:35:24 - INFO - __main__ - Starting training!
05/22/2022 02:35:27 - INFO - __main__ - Step 10 Global step 10 Train loss 6.48 on epoch=1
05/22/2022 02:35:30 - INFO - __main__ - Step 20 Global step 20 Train loss 3.45 on epoch=3
05/22/2022 02:35:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.66 on epoch=4
05/22/2022 02:35:35 - INFO - __main__ - Step 40 Global step 40 Train loss 1.00 on epoch=6
05/22/2022 02:35:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.76 on epoch=8
05/22/2022 02:35:40 - INFO - __main__ - Global step 50 Train loss 2.67 Classification-F1 0.3013297272932161 on epoch=8
05/22/2022 02:35:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3013297272932161 on epoch=8, global_step=50
05/22/2022 02:35:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.79 on epoch=9
05/22/2022 02:35:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.70 on epoch=11
05/22/2022 02:35:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=13
05/22/2022 02:35:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.73 on epoch=14
05/22/2022 02:35:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=16
05/22/2022 02:35:56 - INFO - __main__ - Global step 100 Train loss 0.69 Classification-F1 0.16666666666666666 on epoch=16
05/22/2022 02:35:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=18
05/22/2022 02:36:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=19
05/22/2022 02:36:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=21
05/22/2022 02:36:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=23
05/22/2022 02:36:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=24
05/22/2022 02:36:12 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=24
05/22/2022 02:36:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=26
05/22/2022 02:36:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=28
05/22/2022 02:36:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=29
05/22/2022 02:36:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=31
05/22/2022 02:36:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=33
05/22/2022 02:36:28 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.25015634771732337 on epoch=33
05/22/2022 02:36:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=34
05/22/2022 02:36:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=36
05/22/2022 02:36:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=38
05/22/2022 02:36:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=39
05/22/2022 02:36:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=41
05/22/2022 02:36:43 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=41
05/22/2022 02:36:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=43
05/22/2022 02:36:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=44
05/22/2022 02:36:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=46
05/22/2022 02:36:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=48
05/22/2022 02:36:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=49
05/22/2022 02:36:58 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.16272965879265092 on epoch=49
05/22/2022 02:37:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=51
05/22/2022 02:37:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=53
05/22/2022 02:37:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=54
05/22/2022 02:37:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=56
05/22/2022 02:37:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=58
05/22/2022 02:37:14 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.2744360902255639 on epoch=58
05/22/2022 02:37:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=59
05/22/2022 02:37:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=61
05/22/2022 02:37:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=63
05/22/2022 02:37:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=64
05/22/2022 02:37:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=66
05/22/2022 02:37:29 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
05/22/2022 02:37:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=68
05/22/2022 02:37:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=69
05/22/2022 02:37:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=71
05/22/2022 02:37:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=73
05/22/2022 02:37:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=74
05/22/2022 02:37:45 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=74
05/22/2022 02:37:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=76
05/22/2022 02:37:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=78
05/22/2022 02:37:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=79
05/22/2022 02:37:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=81
05/22/2022 02:37:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=83
05/22/2022 02:38:00 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.2324514991181658 on epoch=83
05/22/2022 02:38:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=84
05/22/2022 02:38:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=86
05/22/2022 02:38:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=88
05/22/2022 02:38:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=89
05/22/2022 02:38:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=91
05/22/2022 02:38:16 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=91
05/22/2022 02:38:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=93
05/22/2022 02:38:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=94
05/22/2022 02:38:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=96
05/22/2022 02:38:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=98
05/22/2022 02:38:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=99
05/22/2022 02:38:31 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.1986376620522962 on epoch=99
05/22/2022 02:38:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=101
05/22/2022 02:38:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=103
05/22/2022 02:38:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=104
05/22/2022 02:38:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=106
05/22/2022 02:38:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=108
05/22/2022 02:38:47 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.20564323290291728 on epoch=108
05/22/2022 02:38:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=109
05/22/2022 02:38:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=111
05/22/2022 02:38:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=113
05/22/2022 02:38:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=114
05/22/2022 02:39:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=116
05/22/2022 02:39:02 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.18553535353535353 on epoch=116
05/22/2022 02:39:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=118
05/22/2022 02:39:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.50 on epoch=119
05/22/2022 02:39:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=121
05/22/2022 02:39:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=123
05/22/2022 02:39:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=124
05/22/2022 02:39:18 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.1762291908633372 on epoch=124
05/22/2022 02:39:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=126
05/22/2022 02:39:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=128
05/22/2022 02:39:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=129
05/22/2022 02:39:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=131
05/22/2022 02:39:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=133
05/22/2022 02:39:34 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.2721757404022959 on epoch=133
05/22/2022 02:39:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=134
05/22/2022 02:39:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=136
05/22/2022 02:39:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=138
05/22/2022 02:39:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=139
05/22/2022 02:39:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=141
05/22/2022 02:39:50 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.24034885212261123 on epoch=141
05/22/2022 02:39:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=143
05/22/2022 02:39:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=144
05/22/2022 02:39:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.74 on epoch=146
05/22/2022 02:40:01 - INFO - __main__ - Step 890 Global step 890 Train loss 1.70 on epoch=148
05/22/2022 02:40:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=149
05/22/2022 02:40:06 - INFO - __main__ - Global step 900 Train loss 0.76 Classification-F1 0.1986376620522962 on epoch=149
05/22/2022 02:40:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=151
05/22/2022 02:40:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=153
05/22/2022 02:40:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=154
05/22/2022 02:40:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=156
05/22/2022 02:40:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=158
05/22/2022 02:40:22 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.19396764851310308 on epoch=158
05/22/2022 02:40:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=159
05/22/2022 02:40:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=161
05/22/2022 02:40:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=163
05/22/2022 02:40:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=164
05/22/2022 02:40:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=166
05/22/2022 02:40:38 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.2075471698113208 on epoch=166
05/22/2022 02:40:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=168
05/22/2022 02:40:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=169
05/22/2022 02:40:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=171
05/22/2022 02:40:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=173
05/22/2022 02:40:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=174
05/22/2022 02:40:54 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.18414611184334398 on epoch=174
05/22/2022 02:40:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=176
05/22/2022 02:40:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=178
05/22/2022 02:41:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=179
05/22/2022 02:41:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=181
05/22/2022 02:41:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=183
05/22/2022 02:41:09 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.19396764851310308 on epoch=183
05/22/2022 02:41:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=184
05/22/2022 02:41:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=186
05/22/2022 02:41:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=188
05/22/2022 02:41:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=189
05/22/2022 02:41:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=191
05/22/2022 02:41:25 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.23574487266170838 on epoch=191
05/22/2022 02:41:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=193
05/22/2022 02:41:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=194
05/22/2022 02:41:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=196
05/22/2022 02:41:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=198
05/22/2022 02:41:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=199
05/22/2022 02:41:41 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.19396764851310308 on epoch=199
05/22/2022 02:41:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=201
05/22/2022 02:41:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=203
05/22/2022 02:41:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=204
05/22/2022 02:41:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=206
05/22/2022 02:41:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=208
05/22/2022 02:41:57 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.18888888888888888 on epoch=208
05/22/2022 02:41:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=209
05/22/2022 02:42:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=211
05/22/2022 02:42:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=213
05/22/2022 02:42:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=214
05/22/2022 02:42:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=216
05/22/2022 02:42:12 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.30129870129870134 on epoch=216
05/22/2022 02:42:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=218
05/22/2022 02:42:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=219
05/22/2022 02:42:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=221
05/22/2022 02:42:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=223
05/22/2022 02:42:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=224
05/22/2022 02:42:28 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.18888888888888888 on epoch=224
05/22/2022 02:42:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=226
05/22/2022 02:42:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=228
05/22/2022 02:42:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=229
05/22/2022 02:42:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=231
05/22/2022 02:42:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=233
05/22/2022 02:42:44 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.17863868711326336 on epoch=233
05/22/2022 02:42:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=234
05/22/2022 02:42:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=236
05/22/2022 02:42:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=238
05/22/2022 02:42:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=239
05/22/2022 02:42:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=241
05/22/2022 02:43:00 - INFO - __main__ - Global step 1450 Train loss 0.40 Classification-F1 0.24669370961863987 on epoch=241
05/22/2022 02:43:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=243
05/22/2022 02:43:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=244
05/22/2022 02:43:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=246
05/22/2022 02:43:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=248
05/22/2022 02:43:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=249
05/22/2022 02:43:16 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.20606060606060606 on epoch=249
05/22/2022 02:43:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=251
05/22/2022 02:43:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=253
05/22/2022 02:43:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=254
05/22/2022 02:43:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=256
05/22/2022 02:43:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=258
05/22/2022 02:43:31 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.2403278063655422 on epoch=258
05/22/2022 02:43:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=259
05/22/2022 02:43:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=261
05/22/2022 02:43:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=263
05/22/2022 02:43:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=264
05/22/2022 02:43:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=266
05/22/2022 02:43:47 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.3965770575940067 on epoch=266
05/22/2022 02:43:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3013297272932161 -> 0.3965770575940067 on epoch=266, global_step=1600
05/22/2022 02:43:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=268
05/22/2022 02:43:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=269
05/22/2022 02:43:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=271
05/22/2022 02:43:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.38 on epoch=273
05/22/2022 02:44:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=274
05/22/2022 02:44:03 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.17398119122257052 on epoch=274
05/22/2022 02:44:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=276
05/22/2022 02:44:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=278
05/22/2022 02:44:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=279
05/22/2022 02:44:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=281
05/22/2022 02:44:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=283
05/22/2022 02:44:19 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.2167286904129009 on epoch=283
05/22/2022 02:44:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=284
05/22/2022 02:44:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=286
05/22/2022 02:44:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=288
05/22/2022 02:44:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=289
05/22/2022 02:44:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=291
05/22/2022 02:44:35 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.4044302824479757 on epoch=291
05/22/2022 02:44:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3965770575940067 -> 0.4044302824479757 on epoch=291, global_step=1750
05/22/2022 02:44:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=293
05/22/2022 02:44:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=294
05/22/2022 02:44:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=296
05/22/2022 02:44:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.36 on epoch=298
05/22/2022 02:44:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=299
05/22/2022 02:44:51 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.17863868711326336 on epoch=299
05/22/2022 02:44:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=301
05/22/2022 02:44:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=303
05/22/2022 02:44:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=304
05/22/2022 02:45:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=306
05/22/2022 02:45:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=308
05/22/2022 02:45:07 - INFO - __main__ - Global step 1850 Train loss 0.36 Classification-F1 0.19496855345911954 on epoch=308
05/22/2022 02:45:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.37 on epoch=309
05/22/2022 02:45:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=311
05/22/2022 02:45:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.32 on epoch=313
05/22/2022 02:45:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=314
05/22/2022 02:45:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=316
05/22/2022 02:45:23 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.37762128325508604 on epoch=316
05/22/2022 02:45:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=318
05/22/2022 02:45:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=319
05/22/2022 02:45:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.43 on epoch=321
05/22/2022 02:45:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=323
05/22/2022 02:45:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=324
05/22/2022 02:45:39 - INFO - __main__ - Global step 1950 Train loss 0.36 Classification-F1 0.14876033057851237 on epoch=324
05/22/2022 02:45:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=326
05/22/2022 02:45:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=328
05/22/2022 02:45:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=329
05/22/2022 02:45:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=331
05/22/2022 02:45:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.32 on epoch=333
05/22/2022 02:45:55 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.29181450357920946 on epoch=333
05/22/2022 02:45:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=334
05/22/2022 02:46:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.36 on epoch=336
05/22/2022 02:46:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.41 on epoch=338
05/22/2022 02:46:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.31 on epoch=339
05/22/2022 02:46:08 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.34 on epoch=341
05/22/2022 02:46:11 - INFO - __main__ - Global step 2050 Train loss 0.34 Classification-F1 0.45550745550745547 on epoch=341
05/22/2022 02:46:11 - INFO - __main__ - Saving model with best Classification-F1: 0.4044302824479757 -> 0.45550745550745547 on epoch=341, global_step=2050
05/22/2022 02:46:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.32 on epoch=343
05/22/2022 02:46:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.34 on epoch=344
05/22/2022 02:46:19 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.40 on epoch=346
05/22/2022 02:46:22 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=348
05/22/2022 02:46:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.28 on epoch=349
05/22/2022 02:46:27 - INFO - __main__ - Global step 2100 Train loss 0.34 Classification-F1 0.2669683257918552 on epoch=349
05/22/2022 02:46:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.37 on epoch=351
05/22/2022 02:46:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.35 on epoch=353
05/22/2022 02:46:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=354
05/22/2022 02:46:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.35 on epoch=356
05/22/2022 02:46:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.34 on epoch=358
05/22/2022 02:46:43 - INFO - __main__ - Global step 2150 Train loss 0.34 Classification-F1 0.21946386946386945 on epoch=358
05/22/2022 02:46:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.29 on epoch=359
05/22/2022 02:46:48 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.33 on epoch=361
05/22/2022 02:46:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.37 on epoch=363
05/22/2022 02:46:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.33 on epoch=364
05/22/2022 02:46:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.34 on epoch=366
05/22/2022 02:46:59 - INFO - __main__ - Global step 2200 Train loss 0.33 Classification-F1 0.2584293640631669 on epoch=366
05/22/2022 02:47:02 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.31 on epoch=368
05/22/2022 02:47:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=369
05/22/2022 02:47:07 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.35 on epoch=371
05/22/2022 02:47:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.33 on epoch=373
05/22/2022 02:47:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.27 on epoch=374
05/22/2022 02:47:15 - INFO - __main__ - Global step 2250 Train loss 0.32 Classification-F1 0.2063492063492063 on epoch=374
05/22/2022 02:47:18 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.32 on epoch=376
05/22/2022 02:47:20 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.34 on epoch=378
05/22/2022 02:47:23 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.32 on epoch=379
05/22/2022 02:47:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.37 on epoch=381
05/22/2022 02:47:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=383
05/22/2022 02:47:31 - INFO - __main__ - Global step 2300 Train loss 0.34 Classification-F1 0.23701788407670757 on epoch=383
05/22/2022 02:47:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.31 on epoch=384
05/22/2022 02:47:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.34 on epoch=386
05/22/2022 02:47:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.34 on epoch=388
05/22/2022 02:47:42 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.32 on epoch=389
05/22/2022 02:47:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.31 on epoch=391
05/22/2022 02:47:47 - INFO - __main__ - Global step 2350 Train loss 0.32 Classification-F1 0.3213724537896752 on epoch=391
05/22/2022 02:47:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.28 on epoch=393
05/22/2022 02:47:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.32 on epoch=394
05/22/2022 02:47:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.27 on epoch=396
05/22/2022 02:47:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.35 on epoch=398
05/22/2022 02:48:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.32 on epoch=399
05/22/2022 02:48:03 - INFO - __main__ - Global step 2400 Train loss 0.31 Classification-F1 0.22527472527472528 on epoch=399
05/22/2022 02:48:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.29 on epoch=401
05/22/2022 02:48:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.32 on epoch=403
05/22/2022 02:48:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.30 on epoch=404
05/22/2022 02:48:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.34 on epoch=406
05/22/2022 02:48:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.31 on epoch=408
05/22/2022 02:48:19 - INFO - __main__ - Global step 2450 Train loss 0.31 Classification-F1 0.34527550060675116 on epoch=408
05/22/2022 02:48:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.24 on epoch=409
05/22/2022 02:48:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.26 on epoch=411
05/22/2022 02:48:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.27 on epoch=413
05/22/2022 02:48:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.26 on epoch=414
05/22/2022 02:48:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.25 on epoch=416
05/22/2022 02:48:39 - INFO - __main__ - Global step 2500 Train loss 0.26 Classification-F1 0.2638076673164393 on epoch=416
05/22/2022 02:48:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.32 on epoch=418
05/22/2022 02:48:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.28 on epoch=419
05/22/2022 02:48:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.26 on epoch=421
05/22/2022 02:48:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.25 on epoch=423
05/22/2022 02:48:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.28 on epoch=424
05/22/2022 02:48:55 - INFO - __main__ - Global step 2550 Train loss 0.28 Classification-F1 0.2610693400167084 on epoch=424
05/22/2022 02:48:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.22 on epoch=426
05/22/2022 02:49:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.29 on epoch=428
05/22/2022 02:49:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.27 on epoch=429
05/22/2022 02:49:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.26 on epoch=431
05/22/2022 02:49:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.21 on epoch=433
05/22/2022 02:49:11 - INFO - __main__ - Global step 2600 Train loss 0.25 Classification-F1 0.3176931595536247 on epoch=433
05/22/2022 02:49:14 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.23 on epoch=434
05/22/2022 02:49:16 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.26 on epoch=436
05/22/2022 02:49:19 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.22 on epoch=438
05/22/2022 02:49:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.25 on epoch=439
05/22/2022 02:49:24 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.25 on epoch=441
05/22/2022 02:49:27 - INFO - __main__ - Global step 2650 Train loss 0.24 Classification-F1 0.3421455938697318 on epoch=441
05/22/2022 02:49:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.23 on epoch=443
05/22/2022 02:49:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.26 on epoch=444
05/22/2022 02:49:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.22 on epoch=446
05/22/2022 02:49:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.22 on epoch=448
05/22/2022 02:49:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.24 on epoch=449
05/22/2022 02:49:43 - INFO - __main__ - Global step 2700 Train loss 0.23 Classification-F1 0.22265366820885657 on epoch=449
05/22/2022 02:49:45 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.19 on epoch=451
05/22/2022 02:49:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=453
05/22/2022 02:49:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.16 on epoch=454
05/22/2022 02:49:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.21 on epoch=456
05/22/2022 02:49:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.24 on epoch=458
05/22/2022 02:49:58 - INFO - __main__ - Global step 2750 Train loss 0.20 Classification-F1 0.2157622739018088 on epoch=458
05/22/2022 02:50:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.22 on epoch=459
05/22/2022 02:50:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=461
05/22/2022 02:50:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.18 on epoch=463
05/22/2022 02:50:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.17 on epoch=464
05/22/2022 02:50:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=466
05/22/2022 02:50:14 - INFO - __main__ - Global step 2800 Train loss 0.19 Classification-F1 0.26436781609195403 on epoch=466
05/22/2022 02:50:17 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.25 on epoch=468
05/22/2022 02:50:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=469
05/22/2022 02:50:22 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=471
05/22/2022 02:50:25 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.23 on epoch=473
05/22/2022 02:50:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.22 on epoch=474
05/22/2022 02:50:30 - INFO - __main__ - Global step 2850 Train loss 0.21 Classification-F1 0.27219911533637026 on epoch=474
05/22/2022 02:50:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=476
05/22/2022 02:50:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=478
05/22/2022 02:50:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.17 on epoch=479
05/22/2022 02:50:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=481
05/22/2022 02:50:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.19 on epoch=483
05/22/2022 02:50:46 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.2751471584101683 on epoch=483
05/22/2022 02:50:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=484
05/22/2022 02:50:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.13 on epoch=486
05/22/2022 02:50:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=488
05/22/2022 02:50:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=489
05/22/2022 02:50:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.20 on epoch=491
05/22/2022 02:51:01 - INFO - __main__ - Global step 2950 Train loss 0.17 Classification-F1 0.250384024577573 on epoch=491
05/22/2022 02:51:04 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=493
05/22/2022 02:51:07 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=494
05/22/2022 02:51:09 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.21 on epoch=496
05/22/2022 02:51:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.15 on epoch=498
05/22/2022 02:51:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=499
05/22/2022 02:51:16 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:51:16 - INFO - __main__ - Printing 3 examples
05/22/2022 02:51:16 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/22/2022 02:51:16 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:16 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/22/2022 02:51:16 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:16 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/22/2022 02:51:16 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:16 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:51:16 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:51:16 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 02:51:16 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:51:16 - INFO - __main__ - Printing 3 examples
05/22/2022 02:51:16 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/22/2022 02:51:16 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:16 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/22/2022 02:51:16 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:16 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/22/2022 02:51:16 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:16 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:51:16 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:51:16 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 02:51:17 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.23253270748027566 on epoch=499
05/22/2022 02:51:17 - INFO - __main__ - save last model!
05/22/2022 02:51:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 02:51:17 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 02:51:17 - INFO - __main__ - Printing 3 examples
05/22/2022 02:51:17 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 02:51:17 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:17 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 02:51:17 - INFO - __main__ - ['entailment']
05/22/2022 02:51:17 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 02:51:17 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:17 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:51:18 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:51:19 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 02:51:35 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 02:51:35 - INFO - __main__ - task name: anli
05/22/2022 02:51:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 02:51:36 - INFO - __main__ - Starting training!
05/22/2022 02:51:47 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_87_0.3_8_predictions.txt
05/22/2022 02:51:47 - INFO - __main__ - Classification-F1 on test data: 0.2147
05/22/2022 02:51:48 - INFO - __main__ - prefix=anli_32_87, lr=0.3, bsz=8, dev_performance=0.45550745550745547, test_performance=0.21470326162162
05/22/2022 02:51:48 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.2, bsz=8 ...
05/22/2022 02:51:48 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:51:48 - INFO - __main__ - Printing 3 examples
05/22/2022 02:51:48 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/22/2022 02:51:48 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:48 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/22/2022 02:51:48 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:48 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/22/2022 02:51:48 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:48 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:51:49 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:51:49 - INFO - __main__ - Loaded 96 examples from train data
05/22/2022 02:51:49 - INFO - __main__ - Start tokenizing ... 96 instances
05/22/2022 02:51:49 - INFO - __main__ - Printing 3 examples
05/22/2022 02:51:49 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/22/2022 02:51:49 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:49 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/22/2022 02:51:49 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:49 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/22/2022 02:51:49 - INFO - __main__ - ['contradiction']
05/22/2022 02:51:49 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:51:49 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:51:49 - INFO - __main__ - Loaded 96 examples from dev data
05/22/2022 02:52:03 - INFO - __main__ - try to initialize prompt embeddings
05/22/2022 02:52:03 - INFO - __main__ - task name: anli
05/22/2022 02:52:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/22/2022 02:52:04 - INFO - __main__ - Starting training!
05/22/2022 02:52:07 - INFO - __main__ - Step 10 Global step 10 Train loss 6.75 on epoch=1
05/22/2022 02:52:10 - INFO - __main__ - Step 20 Global step 20 Train loss 4.39 on epoch=3
05/22/2022 02:52:13 - INFO - __main__ - Step 30 Global step 30 Train loss 2.62 on epoch=4
05/22/2022 02:52:16 - INFO - __main__ - Step 40 Global step 40 Train loss 1.69 on epoch=6
05/22/2022 02:52:18 - INFO - __main__ - Step 50 Global step 50 Train loss 1.36 on epoch=8
05/22/2022 02:52:20 - INFO - __main__ - Global step 50 Train loss 3.36 Classification-F1 0.19980981148962354 on epoch=8
05/22/2022 02:52:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.19980981148962354 on epoch=8, global_step=50
05/22/2022 02:52:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.99 on epoch=9
05/22/2022 02:52:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.87 on epoch=11
05/22/2022 02:52:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.78 on epoch=13
05/22/2022 02:52:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.75 on epoch=14
05/22/2022 02:52:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=16
05/22/2022 02:52:37 - INFO - __main__ - Global step 100 Train loss 0.79 Classification-F1 0.18061964403427822 on epoch=16
05/22/2022 02:52:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.63 on epoch=18
05/22/2022 02:52:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.65 on epoch=19
05/22/2022 02:52:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.62 on epoch=21
05/22/2022 02:52:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=23
05/22/2022 02:52:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=24
05/22/2022 02:52:53 - INFO - __main__ - Global step 150 Train loss 0.60 Classification-F1 0.22424242424242424 on epoch=24
05/22/2022 02:52:53 - INFO - __main__ - Saving model with best Classification-F1: 0.19980981148962354 -> 0.22424242424242424 on epoch=24, global_step=150
05/22/2022 02:52:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=26
05/22/2022 02:52:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=28
05/22/2022 02:53:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=29
05/22/2022 02:53:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=31
05/22/2022 02:53:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=33
05/22/2022 02:53:09 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.3291502138164399 on epoch=33
05/22/2022 02:53:09 - INFO - __main__ - Saving model with best Classification-F1: 0.22424242424242424 -> 0.3291502138164399 on epoch=33, global_step=200
05/22/2022 02:53:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=34
05/22/2022 02:53:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=36
05/22/2022 02:53:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=38
05/22/2022 02:53:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=39
05/22/2022 02:53:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=41
05/22/2022 02:53:24 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.25793650793650796 on epoch=41
05/22/2022 02:53:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=43
05/22/2022 02:53:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=44
05/22/2022 02:53:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=46
05/22/2022 02:53:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=48
05/22/2022 02:53:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=49
05/22/2022 02:53:40 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16402116402116398 on epoch=49
05/22/2022 02:53:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=51
05/22/2022 02:53:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=53
05/22/2022 02:53:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=54
05/22/2022 02:53:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=56
05/22/2022 02:53:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=58
05/22/2022 02:53:56 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.22207573427085622 on epoch=58
05/22/2022 02:53:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=59
05/22/2022 02:54:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=61
05/22/2022 02:54:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=63
05/22/2022 02:54:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=64
05/22/2022 02:54:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=66
05/22/2022 02:54:11 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=66
05/22/2022 02:54:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=68
05/22/2022 02:54:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.52 on epoch=69
05/22/2022 02:54:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=71
05/22/2022 02:54:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=73
05/22/2022 02:54:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=74
05/22/2022 02:54:27 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=74
05/22/2022 02:54:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=76
05/22/2022 02:54:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=78
05/22/2022 02:54:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=79
05/22/2022 02:54:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=81
05/22/2022 02:54:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=83
05/22/2022 02:54:43 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.2532601557369359 on epoch=83
05/22/2022 02:54:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=84
05/22/2022 02:54:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=86
05/22/2022 02:54:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=88
05/22/2022 02:54:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=89
05/22/2022 02:54:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=91
05/22/2022 02:54:59 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=91
05/22/2022 02:55:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=93
05/22/2022 02:55:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=94
05/22/2022 02:55:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=96
05/22/2022 02:55:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=98
05/22/2022 02:55:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=99
05/22/2022 02:55:15 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=99
05/22/2022 02:55:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=101
05/22/2022 02:55:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=103
05/22/2022 02:55:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=104
05/22/2022 02:55:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=106
05/22/2022 02:55:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=108
05/22/2022 02:55:31 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.2901386896221903 on epoch=108
05/22/2022 02:55:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=109
05/22/2022 02:55:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=111
05/22/2022 02:55:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=113
05/22/2022 02:55:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=114
05/22/2022 02:55:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=116
05/22/2022 02:55:46 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=116
05/22/2022 02:55:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=118
05/22/2022 02:55:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=119
05/22/2022 02:55:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=121
05/22/2022 02:55:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=123
05/22/2022 02:56:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=124
05/22/2022 02:56:02 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.23059581320450884 on epoch=124
05/22/2022 02:56:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=126
05/22/2022 02:56:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=128
05/22/2022 02:56:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=129
05/22/2022 02:56:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=131
05/22/2022 02:56:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=133
05/22/2022 02:56:18 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.2448512585812357 on epoch=133
05/22/2022 02:56:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=134
05/22/2022 02:56:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=136
05/22/2022 02:56:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=138
05/22/2022 02:56:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=139
05/22/2022 02:56:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=141
05/22/2022 02:56:33 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.20342857142857143 on epoch=141
05/22/2022 02:56:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=143
05/22/2022 02:56:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=144
05/22/2022 02:56:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=146
05/22/2022 02:56:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=148
05/22/2022 02:56:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=149
05/22/2022 02:56:50 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.28503010399562123 on epoch=149
05/22/2022 02:56:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=151
05/22/2022 02:56:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=153
05/22/2022 02:56:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=154
05/22/2022 02:57:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=156
05/22/2022 02:57:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=158
05/22/2022 02:57:05 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.2847166263765781 on epoch=158
05/22/2022 02:57:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=159
05/22/2022 02:57:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=161
05/22/2022 02:57:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=163
05/22/2022 02:57:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=164
05/22/2022 02:57:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=166
05/22/2022 02:57:21 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.1881810228266921 on epoch=166
05/22/2022 02:57:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.47 on epoch=168
05/22/2022 02:57:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=169
05/22/2022 02:57:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=171
05/22/2022 02:57:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=173
05/22/2022 02:57:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=174
05/22/2022 02:57:37 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.2717611592440594 on epoch=174
05/22/2022 02:57:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=176
05/22/2022 02:57:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=178
05/22/2022 02:57:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=179
05/22/2022 02:57:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=181
05/22/2022 02:57:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=183
05/22/2022 02:57:53 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.23287823287823287 on epoch=183
05/22/2022 02:57:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=184
05/22/2022 02:57:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=186
05/22/2022 02:58:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=188
05/22/2022 02:58:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=189
05/22/2022 02:58:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=191
05/22/2022 02:58:09 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.1895141895141895 on epoch=191
05/22/2022 02:58:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=193
05/22/2022 02:58:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=194
05/22/2022 02:58:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=196
05/22/2022 02:58:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=198
05/22/2022 02:58:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=199
05/22/2022 02:58:26 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.20702426564495532 on epoch=199
05/22/2022 02:58:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=201
05/22/2022 02:58:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=203
05/22/2022 02:58:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=204
05/22/2022 02:58:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=206
05/22/2022 02:58:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=208
05/22/2022 02:58:42 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.30696457326892107 on epoch=208
05/22/2022 02:58:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=209
05/22/2022 02:58:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=211
05/22/2022 02:58:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=213
05/22/2022 02:58:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=214
05/22/2022 02:58:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=216
05/22/2022 02:58:58 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.1836290071584189 on epoch=216
05/22/2022 02:59:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=218
05/22/2022 02:59:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=219
05/22/2022 02:59:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=221
05/22/2022 02:59:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=223
05/22/2022 02:59:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=224
05/22/2022 02:59:14 - INFO - __main__ - Global step 1350 Train loss 0.39 Classification-F1 0.3299145299145299 on epoch=224
05/22/2022 02:59:14 - INFO - __main__ - Saving model with best Classification-F1: 0.3291502138164399 -> 0.3299145299145299 on epoch=224, global_step=1350
05/22/2022 02:59:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=226
05/22/2022 02:59:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=228
05/22/2022 02:59:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=229
05/22/2022 02:59:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=231
05/22/2022 02:59:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=233
05/22/2022 02:59:31 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.3438856425534172 on epoch=233
05/22/2022 02:59:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3299145299145299 -> 0.3438856425534172 on epoch=233, global_step=1400
05/22/2022 02:59:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=234
05/22/2022 02:59:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=236
05/22/2022 02:59:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=238
05/22/2022 02:59:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=239
05/22/2022 02:59:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=241
05/22/2022 02:59:47 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.2905865376453612 on epoch=241
05/22/2022 02:59:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=243
05/22/2022 02:59:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.34 on epoch=244
05/22/2022 02:59:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=246
05/22/2022 02:59:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=248
05/22/2022 03:00:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=249
05/22/2022 03:00:03 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.32698412698412693 on epoch=249
05/22/2022 03:00:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=251
05/22/2022 03:00:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=253
05/22/2022 03:00:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=254
05/22/2022 03:00:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.39 on epoch=256
05/22/2022 03:00:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=258
05/22/2022 03:00:19 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.2553120788069639 on epoch=258
05/22/2022 03:00:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=259
05/22/2022 03:00:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=261
05/22/2022 03:00:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=263
05/22/2022 03:00:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=264
05/22/2022 03:00:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=266
05/22/2022 03:00:35 - INFO - __main__ - Global step 1600 Train loss 0.38 Classification-F1 0.2698412698412698 on epoch=266
05/22/2022 03:00:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=268
05/22/2022 03:00:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=269
05/22/2022 03:00:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=271
05/22/2022 03:00:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=273
05/22/2022 03:00:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=274
05/22/2022 03:00:51 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.3126194067370538 on epoch=274
05/22/2022 03:00:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=276
05/22/2022 03:00:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=278
05/22/2022 03:00:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.34 on epoch=279
05/22/2022 03:01:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=281
05/22/2022 03:01:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=283
05/22/2022 03:01:08 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.28975864318330075 on epoch=283
05/22/2022 03:01:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=284
05/22/2022 03:01:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=286
05/22/2022 03:01:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=288
05/22/2022 03:01:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=289
05/22/2022 03:01:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=291
05/22/2022 03:01:24 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.32688318570671515 on epoch=291
05/22/2022 03:01:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=293
05/22/2022 03:01:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=294
05/22/2022 03:01:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=296
05/22/2022 03:01:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=298
05/22/2022 03:01:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=299
05/22/2022 03:01:40 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.2615468409586057 on epoch=299
05/22/2022 03:01:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=301
05/22/2022 03:01:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.33 on epoch=303
05/22/2022 03:01:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=304
05/22/2022 03:01:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.29 on epoch=306
05/22/2022 03:01:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=308
05/22/2022 03:01:56 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.3295096716149348 on epoch=308
05/22/2022 03:01:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=309
05/22/2022 03:02:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=311
05/22/2022 03:02:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=313
05/22/2022 03:02:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.29 on epoch=314
05/22/2022 03:02:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=316
05/22/2022 03:02:12 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.2184146111843344 on epoch=316
05/22/2022 03:02:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=318
05/22/2022 03:02:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=319
05/22/2022 03:02:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=321
05/22/2022 03:02:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.33 on epoch=323
05/22/2022 03:02:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=324
05/22/2022 03:02:28 - INFO - __main__ - Global step 1950 Train loss 0.33 Classification-F1 0.3268542990061978 on epoch=324
05/22/2022 03:02:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.30 on epoch=326
05/22/2022 03:02:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=328
05/22/2022 03:02:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=329
05/22/2022 03:02:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=331
05/22/2022 03:02:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=333
05/22/2022 03:02:44 - INFO - __main__ - Global step 2000 Train loss 0.32 Classification-F1 0.2810320114667941 on epoch=333
05/22/2022 03:02:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=334
05/22/2022 03:02:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.31 on epoch=336
05/22/2022 03:02:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.29 on epoch=338
05/22/2022 03:02:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=339
05/22/2022 03:02:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.30 on epoch=341
05/22/2022 03:03:00 - INFO - __main__ - Global step 2050 Train loss 0.31 Classification-F1 0.2848191196550374 on epoch=341
05/22/2022 03:03:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.31 on epoch=343
05/22/2022 03:03:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.27 on epoch=344
05/22/2022 03:03:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.27 on epoch=346
05/22/2022 03:03:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.30 on epoch=348
05/22/2022 03:03:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.31 on epoch=349
05/22/2022 03:03:16 - INFO - __main__ - Global step 2100 Train loss 0.29 Classification-F1 0.24814814814814815 on epoch=349
05/22/2022 03:03:19 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.31 on epoch=351
05/22/2022 03:03:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.31 on epoch=353
05/22/2022 03:03:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.31 on epoch=354
05/22/2022 03:03:27 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.29 on epoch=356
05/22/2022 03:03:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.28 on epoch=358
05/22/2022 03:03:33 - INFO - __main__ - Global step 2150 Train loss 0.30 Classification-F1 0.2641259794257964 on epoch=358
05/22/2022 03:03:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=359
05/22/2022 03:03:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=361
05/22/2022 03:03:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.30 on epoch=363
05/22/2022 03:03:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.30 on epoch=364
05/22/2022 03:03:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.28 on epoch=366
05/22/2022 03:03:49 - INFO - __main__ - Global step 2200 Train loss 0.28 Classification-F1 0.28221214411690604 on epoch=366
05/22/2022 03:03:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=368
05/22/2022 03:03:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.30 on epoch=369
05/22/2022 03:03:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.24 on epoch=371
05/22/2022 03:04:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.27 on epoch=373
05/22/2022 03:04:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.22 on epoch=374
05/22/2022 03:04:05 - INFO - __main__ - Global step 2250 Train loss 0.25 Classification-F1 0.2396135265700483 on epoch=374
05/22/2022 03:04:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.30 on epoch=376
05/22/2022 03:04:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.27 on epoch=378
05/22/2022 03:04:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.25 on epoch=379
05/22/2022 03:04:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.27 on epoch=381
05/22/2022 03:04:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.24 on epoch=383
05/22/2022 03:04:21 - INFO - __main__ - Global step 2300 Train loss 0.27 Classification-F1 0.2811463046757165 on epoch=383
05/22/2022 03:04:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.30 on epoch=384
05/22/2022 03:04:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.29 on epoch=386
05/22/2022 03:04:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.28 on epoch=388
05/22/2022 03:04:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.28 on epoch=389
05/22/2022 03:04:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=391
05/22/2022 03:04:37 - INFO - __main__ - Global step 2350 Train loss 0.27 Classification-F1 0.21118286692057184 on epoch=391
05/22/2022 03:04:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.25 on epoch=393
05/22/2022 03:04:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.22 on epoch=394
05/22/2022 03:04:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.26 on epoch=396
05/22/2022 03:04:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.29 on epoch=398
05/22/2022 03:04:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.21 on epoch=399
05/22/2022 03:04:53 - INFO - __main__ - Global step 2400 Train loss 0.25 Classification-F1 0.24661711454164284 on epoch=399
05/22/2022 03:04:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=401
05/22/2022 03:04:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.24 on epoch=403
05/22/2022 03:05:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.22 on epoch=404
05/22/2022 03:05:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.26 on epoch=406
05/22/2022 03:05:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.24 on epoch=408
05/22/2022 03:05:09 - INFO - __main__ - Global step 2450 Train loss 0.24 Classification-F1 0.2566524956699732 on epoch=408
05/22/2022 03:05:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.27 on epoch=409
05/22/2022 03:05:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.24 on epoch=411
05/22/2022 03:05:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.24 on epoch=413
05/22/2022 03:05:20 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.26 on epoch=414
05/22/2022 03:05:23 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.25 on epoch=416
05/22/2022 03:05:25 - INFO - __main__ - Global step 2500 Train loss 0.25 Classification-F1 0.28182220256523666 on epoch=416
05/22/2022 03:05:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.31 on epoch=418
05/22/2022 03:05:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.25 on epoch=419
05/22/2022 03:05:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.23 on epoch=421
05/22/2022 03:05:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.24 on epoch=423
05/22/2022 03:05:39 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=424
05/22/2022 03:05:41 - INFO - __main__ - Global step 2550 Train loss 0.24 Classification-F1 0.3095768965334183 on epoch=424
05/22/2022 03:05:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.21 on epoch=426
05/22/2022 03:05:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=428
05/22/2022 03:05:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.25 on epoch=429
05/22/2022 03:05:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.22 on epoch=431
05/22/2022 03:05:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.23 on epoch=433
05/22/2022 03:05:58 - INFO - __main__ - Global step 2600 Train loss 0.22 Classification-F1 0.30397706868295105 on epoch=433
05/22/2022 03:06:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=434
05/22/2022 03:06:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=436
05/22/2022 03:06:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=438
05/22/2022 03:06:09 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.23 on epoch=439
05/22/2022 03:06:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.25 on epoch=441
05/22/2022 03:06:14 - INFO - __main__ - Global step 2650 Train loss 0.21 Classification-F1 0.27737804979184294 on epoch=441
05/22/2022 03:06:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.24 on epoch=443
05/22/2022 03:06:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.25 on epoch=444
05/22/2022 03:06:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.21 on epoch=446
05/22/2022 03:06:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=448
05/22/2022 03:06:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=449
05/22/2022 03:06:31 - INFO - __main__ - Global step 2700 Train loss 0.21 Classification-F1 0.2818511129936812 on epoch=449
05/22/2022 03:06:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.21 on epoch=451
05/22/2022 03:06:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=453
05/22/2022 03:06:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.21 on epoch=454
05/22/2022 03:06:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=456
05/22/2022 03:06:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=458
05/22/2022 03:06:47 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.36388888888888893 on epoch=458
05/22/2022 03:06:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3438856425534172 -> 0.36388888888888893 on epoch=458, global_step=2750
05/22/2022 03:06:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=459
05/22/2022 03:06:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=461
05/22/2022 03:06:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.16 on epoch=463
05/22/2022 03:06:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.15 on epoch=464
05/22/2022 03:07:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.25 on epoch=466
05/22/2022 03:07:03 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.317602678453969 on epoch=466
05/22/2022 03:07:06 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.18 on epoch=468
05/22/2022 03:07:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=469
05/22/2022 03:07:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=471
05/22/2022 03:07:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.19 on epoch=473
05/22/2022 03:07:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=474
05/22/2022 03:07:20 - INFO - __main__ - Global step 2850 Train loss 0.18 Classification-F1 0.23869126043039088 on epoch=474
05/22/2022 03:07:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=476
05/22/2022 03:07:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.17 on epoch=478
05/22/2022 03:07:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.17 on epoch=479
05/22/2022 03:07:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=481
05/22/2022 03:07:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.23 on epoch=483
05/22/2022 03:07:36 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.23593483280650007 on epoch=483
05/22/2022 03:07:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.19 on epoch=484
05/22/2022 03:07:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=486
05/22/2022 03:07:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.14 on epoch=488
05/22/2022 03:07:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=489
05/22/2022 03:07:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.23 on epoch=491
05/22/2022 03:07:52 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.2658184412277295 on epoch=491
05/22/2022 03:07:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.14 on epoch=493
05/22/2022 03:07:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=494
05/22/2022 03:08:01 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.18 on epoch=496
05/22/2022 03:08:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.12 on epoch=498
05/22/2022 03:08:06 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=499
05/22/2022 03:08:09 - INFO - __main__ - Global step 3000 Train loss 0.15 Classification-F1 0.23243243243243242 on epoch=499
05/22/2022 03:08:09 - INFO - __main__ - save last model!
05/22/2022 03:08:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/22/2022 03:08:09 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 03:08:09 - INFO - __main__ - Printing 3 examples
05/22/2022 03:08:09 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/22/2022 03:08:09 - INFO - __main__ - ['contradiction']
05/22/2022 03:08:09 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/22/2022 03:08:09 - INFO - __main__ - ['entailment']
05/22/2022 03:08:09 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/22/2022 03:08:09 - INFO - __main__ - ['contradiction']
05/22/2022 03:08:09 - INFO - __main__ - Tokenizing Input ...
05/22/2022 03:08:09 - INFO - __main__ - Tokenizing Output ...
05/22/2022 03:08:10 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 03:08:40 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down32shot/singletask-anli/anli_32_87_0.2_8_predictions.txt
05/22/2022 03:08:40 - INFO - __main__ - Classification-F1 on test data: 0.1737
05/22/2022 03:08:40 - INFO - __main__ - prefix=anli_32_87, lr=0.2, bsz=8, dev_performance=0.36388888888888893, test_performance=0.17368352548078464
